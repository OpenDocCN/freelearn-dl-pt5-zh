<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer075">
<h1 class="chapter-number" id="_idParaDest-81"><a id="_idTextAnchor085"/>6</h1>
<h1 id="_idParaDest-82"><a id="_idTextAnchor086"/>Building Practical Examples with Jina</h1>
<p>In this chapter, we will build simple real-world applications using Jina’s neural search framework. Building on the concepts we have learned in the previous chapters, we will now look at how to use Jina to create valuable applications.</p>
<p>We will learn about the practical aspects of the Jina framework and how you can leverage them to quickly build and deploy sophisticated search solutions. We will walk you through the code base of three different applications built on Jina, and see how the different components that you learned about in the previous chapter work in tandem to create a search application.</p>
<p>We will cover the following three examples in this chapter, which will get you started on the journey of building with Jina:</p>
<ul>
<li>The Q/A chatbot</li>
<li>Fashion image search</li>
<li>Multimodal search</li>
</ul>
<p>With this chapter, we aim to get you started by building practical examples to understand the potential of Jina’s neural search framework. It is a great stepping stone for venturing into the world of neural search for building state-of-the-art search solutions.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor087"/>Technical requirements</h1>
<p>To follow along with the application code discussed in this chapter, clone the GitHub repository available at <a href="">https://github.com/jina-ai/jina/tree/master/jina/helloworld</a>. </p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor088"/>Getting started with the Q/A chatbot</h1>
<p>The <strong class="bold">Q/A chatbot</strong> is a <a id="_idIndexMarker376"/>pre-built example that comes with the Jina installation. To experience the power of Jina firsthand and quickly get started, you can run the Q/A chatbot example directly from the command line without even getting into the code. The Q/A chatbot uses the public Covid Q/A dataset (<a href="">https://www.kaggle.com/datasets/xhlulu/covidqa</a>) from Kaggle, which contains 418 Q/A pairs (<a href="">https://www.kaggle.com/xhlulu/covidqa</a>). </p>
<p>Follow these instructions to set up the development environment and run the Q/A chatbot example:</p>
<ol>
<li>The <a id="_idIndexMarker377"/>first step is to install the Jina library from <a id="_idIndexMarker378"/>the <strong class="bold">Python Package Index</strong> (<strong class="bold">PyPI</strong>) along with the required dependencies:<p class="source-code"><strong class="bold">pip install "jina[demo]"</strong></p></li>
<li>After that, simply type the following command to launch your app:<p class="source-code"><strong class="bold">jina hello chatbot</strong></p></li>
</ol>
<p>After typing this command, you will see<a id="_idIndexMarker379"/> the<a id="_idIndexMarker380"/> following text on your <strong class="bold">command-line interface </strong>(<strong class="bold">CLI</strong>):</p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<img alt="Figure 6.1 – Q/A chatbot command line  " height="1001" src="image/Figure_6.01_B17488.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Q/A chatbot command line </p>
<p>If your screen<a id="_idIndexMarker381"/> displays the same text on the command line, it means you have successfully launched the Q/A chatbot example. Now, it’s time to open the <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) and play with the chatbot.</p>
<p>By default, a simple chat interface<a id="_idIndexMarker382"/> will open up, allowing you to chat with the Q/A chatbot. If the page doesn’t open up itself, you can open <strong class="source-inline">index.xhtml</strong> by going to <strong class="source-inline">jina/helloworld/chatbot/static</strong>.</p>
<p>You will see the following web page either by default or after opening the <strong class="source-inline">index.xhtml</strong> file:</p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="Figure 6.2 – Q/A chatbot interface " height="845" src="image/Figure_6.02_B17488.jpg" width="510"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Q/A chatbot interface</p>
<p>You have successfully <a id="_idIndexMarker383"/>launched the Q/A chatbot application; it’s time to play with it and have some fun. You can ask the chatbot for any Covid-related facts, figures, or queries and see the magic in action!</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor089"/>Navigating through the code</h2>
<p>Let’s now go<a id="_idIndexMarker384"/> through the logic behind the application and see how Jina’s framework ties all the components together to produce a functioning Q/A chatbot application.</p>
<p>In order to see the code and understand the different components that work together to bring up this application after installing Jina, go to the chatbot directory by following the <strong class="source-inline">jina/helloworld/chatbot</strong> path. This is the main directory that contains the code for the chatbot example: </p>
<pre class="source-code">└── chatbot                    
    ├── app.py
    ├── my_executors.py         
    ├── static/         </pre>
<p>The following are the files that you will see within the chatbot directory: </p>
<ul>
<li><strong class="source-inline">app.py</strong>: This is the main entry point/brain of the application. </li>
<li><strong class="source-inline">my_executors.py</strong>: This file is responsible for all the backend processing. It includes the logic behind the application, which we <a id="_idIndexMarker385"/>call <strong class="bold">executors</strong> in Jina terminology. It hosts multiple executors to transform, encode, and index the data.</li>
<li><strong class="source-inline">static</strong>: This folder hosts all the frontend code responsible for rendering the chatbot interface on the web browser that helps you interact with the chatbot application.</li>
</ul>
<p>We will have a detailed look at the functioning of each of these files in the following subsections.</p>
<h3>app.py</h3>
<p>The <strong class="source-inline">app.py</strong> file<a id="_idIndexMarker386"/> is the entry point of the example application. As soon as you<a id="_idIndexMarker387"/> type the <strong class="source-inline">jina hello chatbot</strong> command, the control goes to this file. It’s the main entry point for the application and performs all the major tasks of bringing up the application’s UI and running the backend code.</p>
<p>The <strong class="source-inline">app.py</strong> file performs the following tasks to ensure that multiple components work in tandem with each other to produce the desired result.</p>
<p>The first thing it does is import the required executors from the <strong class="source-inline">my_executors.py</strong> file using the following code:</p>
<pre class="source-code">from my_executors import MyTransformer, MyIndexer</pre>
<p>Both of these executors are derived from the base <strong class="source-inline">Executor</strong> class of Jina:</p>
<ul>
<li>The <strong class="source-inline">MyTransformer</strong> executor is responsible for encoding and transforming the data.</li>
<li>The <strong class="source-inline">MyIndexer</strong> executor is used for indexing the data.</li>
</ul>
<p>We will learn about the functioning of both of these executors in detail when we talk about the <strong class="source-inline">my_executors.py</strong> file.</p>
<p><strong class="source-inline">Flow</strong> allows you to add encoding and indexing in the form of executors, and in the chatbot example, we use the following executors. You can use the following code to create a flow and add these executors to it: </p>
<pre class="source-code">from jina import Flow
flow = (
    Flow(cors=True)
    .add(uses=MyTransformer)
    .add(uses=MyIndexer)
    )</pre>
<p>This is one of the<a id="_idIndexMarker388"/> simple flows with just two executors. For a complex flow<a id="_idIndexMarker389"/> with many executors, Jina provides the functionality to distinguish each of the executors with distinct names (for example, by using the <strong class="source-inline">name</strong> parameter, you can give your executors some really cool names). It then allows you to visualize the flow to understand how your data flows through different components. Let’s visualize this flow by adding a single line to the existing code:</p>
<pre class="source-code">from jina import Flow
flow = (
    Flow(cors=True)
    .add(name='MyTransformer', uses=MyTransformer)
    .add(name='MyIndexer', uses=MyIndexer) 
    .plot('chatbot_flow.svg')
    )</pre>
<p>Running the <a id="_idIndexMarker390"/>preceding code will generate the following <strong class="source-inline">SVG</strong> file that<a id="_idIndexMarker391"/> visualizes the chatbot flow:</p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 6.3 – Chatbot flow " height="231" src="image/Figure_6.3_B17488.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Chatbot flow</p>
<p class="callout-heading">Note </p>
<p class="callout">Since we want to call our flow from the browser, it’s important to enable Cross-Origin Resource Sharing (<a href="">https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS</a>) within Flow (<strong class="source-inline">cors=True</strong>).</p>
<p>Once we have the flow ready, it’s time to dive into the <strong class="source-inline">hello_world</strong> function in the <strong class="source-inline">app.py</strong> file, which brings together everything from different sources and opens a query endpoint (a backend endpoint) for you to query and interact with the chatbot application:</p>
<ol>
<li value="1">The <strong class="source-inline">hello_world</strong> function starts by creating a <strong class="source-inline">workspace</strong> directory to store the indexed data and ensures that the required dependencies are imported. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">To run this example, we require two major dependencies/Python libraries: <strong class="source-inline">torch</strong> and <strong class="source-inline">transformers</strong>. </p>
<ol>
<li value="2">Install the dependencies by using the following commands before we move forward with the code:<ul><li><strong class="source-inline">pip i<a id="_idTextAnchor090"/>nstall torch</strong></li>
<li><strong class="source-inline">pip install transformers</strong></li>
</ul></li>
</ol>
<p>After installing these dependencies, it’s time to continue with the <strong class="source-inline">hello_world</strong> function.</p>
<ol>
<li value="3">The next step<a id="_idIndexMarker392"/> is to download the data from Kaggle. For that, we will <a id="_idIndexMarker393"/>use the <strong class="source-inline">download_data</strong> function, which basically uses the <strong class="source-inline">urllib</strong> library to fetch and save the data from the given URL.</li>
</ol>
<p>The <strong class="source-inline">urllib</strong> module takes <strong class="source-inline">url</strong> and <strong class="source-inline">filename</strong> as the target and downloads the data. You can refer to the following code to see how we set the target: </p>
<p class="source-code">targets = {</p>
<p class="source-code">        'covid-csv': {</p>
<p class="source-code">            'url': url_of_your_data,</p>
<p class="source-code">            'filename': file_name_to_be_fetched,</p>
<p class="source-code">        }</p>
<p class="source-code">    }</p>
<p>Passing the target variable into the <strong class="source-inline">download_data</strong> function will download the data and save it as a <strong class="source-inline">.csv</strong> file in a random folder within the same working directory. </p>
<ol>
<li value="4">Now we have all the basic components required to index the data, we will use the dataset downloaded in the previous step and index it using the flow that we created previously. Indexing will follow this logic:<ul><li>It will use the <strong class="source-inline">MyTransformer</strong> executor to encode and transform the data by computing the corresponding embeddings.</li>
<li>It will use the <strong class="source-inline">MyIndexer</strong> executor to index the data via the <strong class="source-inline">/index</strong> endpoint and open the <strong class="source-inline">/search</strong> endpoint to query and interact with the chatbot.</li>
</ul></li>
</ol>
<p>The following<a id="_idIndexMarker394"/> is the code that indexes the data and creates <a id="_idIndexMarker395"/>a search endpoint to interact with the chatbot:</p>
<p class="source-code">with f:</p>
<p class="source-code">  f.index(</p>
<p class="source-code">    DocumentArray.from_csv(</p>
<p class="source-code">      targets['covid-csv']['filename'], </p>
<p class="source-code">        field_resolver={'question': 'text'}</p>
<p class="source-code">    ),</p>
<p class="source-code">    show_progress=True,)</p>
<p class="source-code">  url_html_path = 'file://' + os.path.abspath(</p>
<p class="source-code">    os.path.join(os.path.dirname(</p>
<p class="source-code">       os.path.realpath(__file__)),'static/index.xhtml'</p>
<p class="source-code">    )</p>
<p class="source-code">  )</p>
<p class="source-code">  try:</p>
<p class="source-code">    webbrowser.open(url_html_path, new=2)</p>
<p class="source-code">  except:</p>
<p class="source-code">    pass</p>
<p class="source-code">  finally:</p>
<p class="source-code">    default_logger.info(</p>
<p class="source-code">      f'You should see a demo page opened in your </p>
<p class="source-code">      browser,'f'if not, you may open {url_html_path} </p>
<p class="source-code">      manually'</p>
<p class="source-code">    )</p>
<p class="source-code">  if not args.unblock_query_flow:</p>
<p class="source-code">    f.block()</p>
<p>In the preceding code, we <a id="_idIndexMarker396"/>open the flow and the dataset with a context manager <a id="_idIndexMarker397"/>and send the data in the form of a <strong class="source-inline">'question': 'text'</strong> pair to the index endpoint. For this example, we will use the web browser to interact with the chatbot, which requires configuring and serving the flow on a specific port with the HTTP protocol using the <strong class="source-inline">port_expose</strong> parameter, so that the web browser can make requests to the flow. Toward the end, we will use <strong class="source-inline">f.block()</strong> to keep the flow open for search queries and to prevent it from exiting. </p>
<h3>my_executors.py</h3>
<p>The other key <a id="_idIndexMarker398"/>component of the chatbot example is the <strong class="source-inline">my_executors.py</strong> file, which contains the logical elements of the application, also <a id="_idIndexMarker399"/>known as <strong class="bold">executors</strong>. It consists of two different executors, which<a id="_idIndexMarker400"/> we will discuss in detail.</p>
<h4>The MyTransformer executor </h4>
<p>The <strong class="source-inline">MyTransformer</strong> executor <a id="_idIndexMarker401"/>performs the following tasks: </p>
<ol>
<li value="1">It loads the pre-trained sentence transformer model from the <strong class="source-inline">sentence-transformers</strong> library.</li>
<li>It takes in the user arguments and sets up the model parameters (such as <strong class="source-inline">model name</strong>/<strong class="source-inline">path</strong>) and <strong class="source-inline">pooling strategy</strong>, fetches the tokenizer corresponding to the model, and sets up<a id="_idIndexMarker402"/> the device to <strong class="source-inline">cpu</strong>/<strong class="source-inline">gpu</strong>, depending on the user’s preference:<p class="source-code">class MyTransformer(Executor):</p><p class="source-code">  """Transformer executor class """</p><p class="source-code">  def __init__(</p><p class="source-code">    self,</p><p class="source-code">    pretrained_model_name_or_path: str = </p><p class="source-code">    'sentence-transformers/paraphrase-mpnet-base-v2',    </p><p class="source-code">    pooling_strategy: str = 'mean',</p><p class="source-code">    layer_index: int = -1,</p><p class="source-code">    *args,</p><p class="source-code">    **kwargs,</p><p class="source-code">  ):</p><p class="source-code">  super().__init__(*args, **kwargs)</p><p class="source-code">  self.pretrained_model_name_or_path = </p><p class="source-code">    pretrained_model_name_or_path</p><p class="source-code">  self.pooling_strategy = pooling_strategy</p><p class="source-code">  self.layer_index = layer_index</p><p class="source-code">  self.tokenizer = AutoTokenizer.from_pretrained(</p><p class="source-code">    self.pretrained_model_name_or_path</p><p class="source-code">  )</p><p class="source-code">  self.model = AutoModel.from_pretrained(</p><p class="source-code">    pretrained_model_name_or_path, </p><p class="source-code">      output_hidden_states=True</p><p class="source-code">  )</p><p class="source-code">  self.model.to(torch.device('cpu'))</p></li>
<li>After setting up these parameters, it computes the embedding for the textual data and encodes textual data/question-answer as a key-value pair in the form of an embedding map.</li>
<li>Encoding is performed through a <strong class="source-inline">sentence-transformers</strong> model (<strong class="source-inline">paraphrase-mpnet-base-v2</strong>, by default). We get the text attributes of documents in batches and then compute embeddings, which we later set as the embedding attribute for each of the documents.</li>
<li>The <strong class="source-inline">MyTransformer</strong> executor exposes only one endpoint, <strong class="source-inline">encode</strong>, which is called whenever we request the flow, either on a query or index. The endpoint creates embeddings for the indexed or query documents so the search endpoint can use similarity scores to determine the closest match for a given query. </li>
</ol>
<p>Let’s look at a<a id="_idIndexMarker403"/> simplified version of the <strong class="source-inline">encode</strong> function for the <strong class="source-inline">MyTransformer</strong> executor that we have in the main chatbot application:</p>
<pre class="source-code">  @requests
  def encode(self, docs: 'DocumentArray', *args, **kwargs):
    with torch.inference_mode():
      if not self.tokenizer.pad_token: 
        self.tokenizer.add_special_tokens({'pad_token':
           '[PAD]'}) 
        self.model.resize_token_embeddings(len(
          self.tokenizer.vocab))
      input_tokens = self.tokenizer(
                  docs[:, 'content'],
                  padding='longest',
                  truncation=True,
                  return_tensors='pt',
      )
      input_tokens = {
        k: v.to(torch.device('cpu')) for k, 
          v in input_tokens.items()
              }
      outputs = self.model(**input_tokens)
      hidden_states = outputs.hidden_states
      docs.embeddings = self._compute_embedding(
        hidden_states, input_tokens)</pre>
<h4>The MyIndexer executor </h4>
<p>The <strong class="source-inline">MyIndexer</strong> executor<a id="_idIndexMarker404"/> performs the following tasks:</p>
<ol>
<li value="1">It uses a document store (SQLite, in our case) that contains all the documents of <strong class="source-inline">DocumentArray</strong>. The look and feel of <strong class="source-inline">DocumentArray</strong> with an external store are almost the same as a regular in-memory <strong class="source-inline">DocumentArray</strong>, but it makes the process more efficient and allows faster retrieval. </li>
<li>The executor exposes two endpoints: <strong class="source-inline">index</strong> and <strong class="source-inline">search</strong>. The <strong class="source-inline">index</strong> endpoint is responsible for taking in the documents and indexing them, while the <strong class="source-inline">search</strong> endpoint is responsible for traversing the indexed <strong class="source-inline">DocumentArray</strong> to find the relevant match for the user queries.</li>
<li>The <strong class="source-inline">search</strong> endpoint uses the <strong class="source-inline">match</strong> method (a built-in method associated with <strong class="source-inline">DocumentArray</strong>), which returns the top closest match for the query documents<a id="_idTextAnchor091"/> using cosine similarity.</li>
</ol>
<p>Let’s look at<a id="_idIndexMarker405"/> a simplified version of code for the <strong class="source-inline">MyIndexer</strong> executor that we have in the main chatbot application:</p>
<pre class="source-code">class MyIndexer(Executor):
  """Simple indexer class """
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    self.table_name = 'qabot_docs'
    self._docs = DocumentArray(
      storage='sqlite',
      config={
        'connection': os.path.join(
         self.workspace, 'indexer.db'),
        'table_name': self.table_name,
      },
    )
  @requests(on='/index')
  def index(self, docs: 'DocumentArray', **kwargs):
    self._docs.extend(docs)
  @requests(on='/search')
  def search(self, docs: 'DocumentArray', **kwargs):
    """Append best matches to each document in docs
    :param docs: documents that are searched
    :param parameters: dictionary of pairs 
      (parameter,value)
    :param kwargs: other keyword arguments
    """
    docs.match(
      self._docs,
      metric='cosine',
      normalization=(1, 0),
      limit=1,
    )</pre>
<p>These two<a id="_idIndexMarker406"/> executors are the building blocks of the chatbot application, and combining them allows us to create an interactive and intelligent chatbot backend. To interact with the chatbot in the web browser via the UI, you can use the HTML template provided in the <strong class="source-inline">static</strong> folder. Running the application by default will open a web page with the chatbot UI; if it doesn’t, then you can open the <strong class="source-inline">index.xhtml</strong> file from the <strong class="source-inline">static</strong> folder. </p>
<p>In this section, we looked at the code behind the Q/A chatbot application for the Covid-19 dataset. The application is a form of text-to-text search engine created using Jina’s framework. The same logic can be used to create a variety of text search applications depending on your use case. </p>
<p>In the next section, we will explore how to extend the search capabilities for unstructured data types such as images, and see how Jina’s neural search makes it easy to build an image-to-image search engine using the fashion image search example.</p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor092"/>Understanding fashion image search</h1>
<p><strong class="bold">Fashion image search</strong> is <a id="_idIndexMarker407"/>another pre-built example that comes with the Jina installation, which you can run just like the Q/A chatbot example directly from the comfort of your command line without even getting into the code.</p>
<p>The fashion image search example uses the famous <em class="italic">Fashion-MNIST</em> dataset of Zalando’s article images (<a href="">https://github.com/zalandoresearch/fashion-mnist</a>) consisting of 60,000 training examples and 10,000 examples in the test set. Each example is a 28x28 grayscale image, associated with a label from 10 classes just like the original MNIST dataset.</p>
<p>Each training <a id="_idIndexMarker408"/>and test set example is assigned one of the following labels: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>T-shirt/Top</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>Trouser</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>Pullover</p>
</td>
</tr>
</tbody>
</table>
<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>Dress</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>Coat</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>Sandal</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>Shirt</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>Sneaker</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>Bag</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p>Ankle boot</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 – Fashion dataset labels and description</p>
<p>In the previous <a id="_idIndexMarker409"/>section, we installed the <strong class="source-inline">jina[demo]</strong> library from PyPI, which took care of all the dependencies required to run this example: </p>
<ol>
<li value="1">Let’s go to the command line and run the fashion image search example:<p class="source-code"><strong class="bold"> jina hello fashion</strong></p></li>
<li>After typing this command, you will see the following text on your CLI:</li>
</ol>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 6.4 – Fashion image search command line  " height="526" src="image/Figure_6.04_B17488.jpg" width="906"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Fashion image search command line </p>
<p>If your screen displays the same text on the command line, it means you have successfully launched the fashion image search example, so now it’s time to open the UI and play with the application.</p>
<p>By default, a simple web page will open up with a random sample of images from the test set as queries, along with the retrieved results from the training data. Behind the scenes, Jina downloads the <em class="italic">Fashion-MNIST</em> dataset and indexes 60,000 training images via the indexing flow. After that, it selects randomly sampled unseen images from the test set as queries and asks Jina to retrieve relevant results.</p>
<p>If the page doesn’t open up itself, you can open the <strong class="source-inline">demo.xhtml</strong> file present at the <strong class="source-inline">*/demo.xhtml</strong> path. You will see the following web page either by default or after opening the<a id="_idIndexMarker410"/> downloaded <strong class="source-inline">demo.xhtml</strong> file manually: </p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 6.5 – Fashion image search web interface  " height="861" src="image/Figure_6.05_B17488.jpg" width="1346"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Fashion image search web interface </p>
<p>You can see in the preceding figure how Jina does an amazing job in finding the relevant search results for the image queries selected randomly from the test set.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor093"/>Navigating through the code</h2>
<p>Let’s now go through <a id="_idIndexMarker411"/>the logic behind the app and see how Jina’s framework ties all the components together to create an image search application.</p>
<p>After installing Jina, go to the chatbot directory by following the <strong class="source-inline">jina/helloworld/fashion</strong> path. This is the main directory that contains the code for the fashion image search example:</p>
<pre class="source-code">└── fashion
    ├── app.py
    ├── my_executors.py
    ├── helper.py
    ├── demo.xhtml</pre>
<p>The following are the files that you will see within the fashion directory:</p>
<ul>
<li><strong class="source-inline">app.py</strong>: Similar to the application discussed in the previous section.</li>
<li><strong class="source-inline">my_executors.py</strong>: Similar to the application discussed in the previous section.</li>
<li><strong class="source-inline">helper.py</strong>: This consists of the supplementary logic functions to modularize the logical code blocks and keep them in a separate file. </li>
<li><strong class="source-inline">demo.xhtml</strong>: This hosts all the frontend code responsible for rendering the chatbot interface on the web browser, which helps you interact with the chatbot application.</li>
</ul>
<h3>app.py</h3>
<p>The <strong class="source-inline">app.py</strong> file is <a id="_idIndexMarker412"/>the entry point of the example application; as <a id="_idIndexMarker413"/>soon as you type the <strong class="source-inline">jina hello fashion</strong> command, the control goes to this file. This is the main entry point for the application and performs all the major tasks to bring up the application’s frontend and the backend.</p>
<p>The <strong class="source-inline">app.py</strong> file performs the following tasks to ensure that multiple components work in tandem with each other to produce the desired application.</p>
<p>The first thing it does is import the required executors from the <strong class="source-inline">my_executors.py</strong> file using the following code:</p>
<pre class="source-code">from my_executors import MyEncoder, MyIndexer</pre>
<p>All of these executors are derived from the base <strong class="source-inline">Executor</strong> class of Jina:</p>
<ul>
<li><strong class="source-inline">MyEncoder</strong> is responsible for transforming and encoding the data.</li>
<li><strong class="source-inline">MyIndexer</strong> is used for indexing the data; after indexing, it hosts a <strong class="source-inline">/search</strong> endpoint for querying the data. </li>
</ul>
<p>We will learn about<a id="_idIndexMarker414"/> the functioning of all these executors in detail when <a id="_idIndexMarker415"/>we talk about the <strong class="source-inline">my_executors.py</strong> file. The flow for this example consists of the aforementioned executors.</p>
<p>You can use the following code to create and visualize the flow:</p>
<pre class="source-code">from jina import Flow
flow = (
    Flow()
    .add(name='MyEncoder', uses=MyEncoder, replicas=2)
    .add(name='MyIndexer', uses=MyIndexer)
    .plot('fashion_image_flow.svg')
    )</pre>
<p>Running the code will generate the following flow diagram, which shows how the data moves through different components of the applications:</p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 6.6 – Fashion image search flow " height="478" src="image/Figure_6.6_B17488.jpg" width="1365"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Fashion image search flow</p>
<p>In the preceding code, the <strong class="source-inline">replicas</strong> parameter is set to <strong class="source-inline">2</strong> for the <strong class="source-inline">MyEncoder</strong> executor to divide the input data stream into two different executors for faster processing and encoding.</p>
<p>Once we have the <a id="_idIndexMarker416"/>flow ready, it’s time to dive into the <strong class="source-inline">hello_world</strong> function in the <strong class="source-inline">app.py</strong> files, which brings together everything from different sources. The <strong class="source-inline">hello_world</strong> function performs the following tasks: </p>
<ol>
<li value="1">It creates a <strong class="source-inline">workspace</strong> directory in<a id="_idIndexMarker417"/> the root folder to store the indexed data.</li>
<li>It creates a <strong class="source-inline">targets</strong> dictionary to associate the URL of the data with the local filenames where the data will be saved. It saves the training data under the <strong class="source-inline">index</strong> and <strong class="source-inline">index-label</strong> files, and the test data under the <strong class="source-inline">query</strong> and <strong class="source-inline">query-label</strong> files:<p class="source-code">targets = {</p><p class="source-code">        'index-labels': {</p><p class="source-code">            'url': args.index_labels_url,</p><p class="source-code">            'filename': os.path.join(args.workdir, </p><p class="source-code">            'index-labels'),</p><p class="source-code">        },</p><p class="source-code">        'query-labels': {</p><p class="source-code">            'url': args.query_labels_url,</p><p class="source-code">            'filename': os.path.join(args.workdir, </p><p class="source-code">            'query-labels'),</p><p class="source-code">        },</p><p class="source-code">        'index': {</p><p class="source-code">            'url': args.index_data_url,</p><p class="source-code">            'filename': os.path.join(args.workdir,   </p><p class="source-code">          'index-original'),</p><p class="source-code">     },</p><p class="source-code">        'query': {</p><p class="source-code">            'url': args.query_data_url,</p><p class="source-code">            'filename': os.path.join(args.workdir, </p><p class="source-code">             'query-original'),},</p><p class="source-code">    }</p></li>
<li>After that, it passes the <strong class="source-inline">targets</strong> variable to the <strong class="source-inline">download_data</strong> function and download<a id="_idTextAnchor094"/>s the <em class="italic">Fashion-MNIST</em> dataset. The <strong class="source-inline">download_data</strong> function uses the <strong class="source-inline">urllib</strong> package to download the data from the given URL and iterate through the dictionary to save the data and the labels for the training and the test set.</li>
<li>It creates the flow and adds the <strong class="source-inline">MyEncoder</strong> and <strong class="source-inline">MyIndexer</strong> executors.</li>
<li>It opens the flow with the context manager and uses the indexing flow to index the data by creating the embeddings for all the images in the training data.</li>
<li>It then includes the ground truth (labels) along with the query images, which allows us to evaluate the performance of the model.</li>
<li>After indexing the <a id="_idIndexMarker418"/>data, it calls the <strong class="source-inline">search</strong> function, which randomly<a id="_idIndexMarker419"/> samples 128 unseen images as queries and returns the top 50 similar images for each of the query images.</li>
<li>Finally, we <a id="_idIndexMarker420"/>use the <strong class="source-inline">write_html</strong> function to render the frontend in the <a id="_idIndexMarker421"/>web browser using the <strong class="source-inline">demo.xhtml</strong> file:<p class="source-code">with f:</p><p class="source-code">  f.index(index_generator(num_docs=targets['index']</p><p class="source-code">    ['data'].shape[0], target=targets), </p><p class="source-code">    show_progress=True,</p><p class="source-code">    )</p><p class="source-code">  groundtruths = get_groundtruths(targets)</p><p class="source-code">  evaluate_print_callback = partial(print_result, </p><p class="source-code">    groundtruths)</p><p class="source-code">  evaluate_print_callback.__name__ = </p><p class="source-code">    'evaluate_print_callback'</p><p class="source-code">  f.post(</p><p class="source-code">    '/search,</p><p class="source-code">    query_generator(num_docs=args.num_query, </p><p class="source-code">      target=targets),</p><p class="source-code">    shuffle=True,</p><p class="source-code">    on_done= evaluate_print_callback,</p><p class="source-code">    parameters={'top_k': args.top_k},</p><p class="source-code">    show_progress=True,</p><p class="source-code">    )</p><p class="source-code">  #write result to html</p><p class="source-code">  write_html(os.path.join(args.workdir, 'demo.xhtml'))</p></li>
</ol>
<h3>my_exe<a id="_idTextAnchor095"/>cutors.py</h3>
<p>The other <a id="_idIndexMarker422"/>key component of the fashion image search <a id="_idIndexMarker423"/>example is the <strong class="source-inline">my_executors.py</strong> file. It consists of three different executors that work together in the flow to create an end-to-end application experience.</p>
<h4>The MyEncoder executor </h4>
<p>The <strong class="source-inline">MyEncoder</strong> executor <a id="_idIndexMarker424"/>performs the following tasks:</p>
<ol>
<li value="1">It is used in both indexing and the querying flow. It is fed with the index and query data yielded from the respective generator functions. It uses <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>) to <a id="_idIndexMarker425"/>encode the incoming data.</li>
<li>In the constructor, it creates a random matrix of shape (<strong class="source-inline">784,64</strong>) and applies SVD to get <strong class="source-inline">oth_mat</strong>.</li>
<li>In the <strong class="source-inline">encode</strong> function, it fetches the content from the docs array (<strong class="source-inline">DocumentArray</strong> in Jina), stacks images together, extracts the single-channel content, and reshapes images to make it ready to fetch the embeddings.</li>
<li>In the next step, we use the <strong class="source-inline">content</strong> matrix along with <strong class="source-inline">oth_mat</strong> (the result of SVD) to get the embeddings. </li>
<li>It then associates each document tensor with the respective embeddings and converts the tensor<a id="_idIndexMarker426"/> into a <strong class="bold">uniform resource identifier</strong> (<strong class="bold">URI</strong>) (a long string that is an equivalent representation of an image) for standardized representation and then it pops the tensor.</li>
<li>It repeats the same process for all the images in the loop to encode the e<a id="_idTextAnchor096"/>ntire dataset:<p class="source-code">class MyEncoder(Executor):</p><p class="source-code">  """</p><p class="source-code">  Encode data using SVD decomposition</p><p class="source-code">  """</p><p class="source-code">  def __init__(self, **kwargs):</p><p class="source-code">    super().__init__(**kwargs)</p><p class="source-code">    np.random.seed(1337)</p><p class="source-code">    # generate a random orthogonal matrix</p><p class="source-code">    H = np.random.rand(784, 64)</p><p class="source-code">    u, s, vh = np.linalg.svd(H, full_matrices=False)</p><p class="source-code">    self.oth_mat = u @ vh</p><p class="source-code">  @requests</p><p class="source-code">  def encode(self, docs: 'DocumentArray', **kwargs):</p><p class="source-code">    """Encode the data using an SVD decomposition</p><p class="source-code">    :param docs: input documents to update with an </p><p class="source-code">      embedding</p><p class="source-code">    :param kwargs: other keyword arguments</p><p class="source-code">    """</p><p class="source-code">    # reduce dimension to 50 by random orthogonal </p><p class="source-code">    # projection</p><p class="source-code">    content = np.stack(docs.get_attributes('content'))</p><p class="source-code">    content = content[:, :, :, 0].reshape(-1, 784)</p><p class="source-code">    embeds = (content / 255) @ self.oth_mat</p><p class="source-code">    for doc, embed, cont in zip(docs, embeds, </p><p class="source-code">      content):</p><p class="source-code">      doc.embedding = embed</p><p class="source-code">      doc.content = cont</p><p class="source-code">      doc.convert_image_tensor_to_uri()</p><p class="source-code">      doc.pop('tensor')</p></li>
</ol>
<h4>The MyIndexer executor </h4>
<p>The <strong class="source-inline">MyIndexer</strong> executor <a id="_idIndexMarker427"/>performs the following tasks:</p>
<ol>
<li value="1">Its constructor creates a <strong class="source-inline">workspace</strong> directory to store the indexed data.</li>
<li>It hosts an <strong class="source-inline">index</strong> endpoint, which takes in the documents as input and structures them into the <strong class="source-inline">workspace</strong> folder.</li>
<li>It also hosts the <strong class="source-inline">search</strong> endpoint, which gives out the best matches for a given query. It takes in the document and <strong class="source-inline">top-k</strong> as a parameter and performs a cosine similarity match to find the <strong class="source-inline">top-k</strong> results: <p class="source-code">class MyIndexer(Executor):</p><p class="source-code">  """</p><p class="source-code">  Executor with basic exact search using cosine </p><p class="source-code">  distance</p><p class="source-code">  """</p><p class="source-code">  def __init__(self, **kwargs):</p><p class="source-code">    super().__init__(**kwargs)</p><p class="source-code">    if os.path.exists(self.workspace + '/indexer'):</p><p class="source-code">      self._docs = DocumentArray.load(self.workspace + </p><p class="source-code">      '/indexer')</p><p class="source-code">    else:</p><p class="source-code">      self._docs = DocumentArray()  </p><p class="source-code">  @requests(on='/index')</p><p class="source-code">  def index(self, docs: 'DocumentArray', **kwargs):</p><p class="source-code">    """Extend self._docs</p><p class="source-code">    :param docs: DocumentArray containing Documents</p><p class="source-code">    :param kwargs: other keyword arguments</p><p class="source-code">    """</p><p class="source-code">    self._docs.extend(docs)</p><p class="source-code">  @requests(on=['/search', '/eval'])</p><p class="source-code">  def search(self, docs: 'DocumentArray',</p><p class="source-code">    parameters: Dict, **kwargs):</p><p class="source-code">    """Append best matches to each document in docs</p><p class="source-code">    :param docs: documents that are searched</p><p class="source-code">    :param parameters: dictionary of pairs </p><p class="source-code">      (parameter,value)</p><p class="source-code">    :param kwargs: other keyword arguments</p><p class="source-code">    """</p><p class="source-code">    docs.match(</p><p class="source-code">      self._docs,</p><p class="source-code">      metric='cosine',</p><p class="source-code">      normalization=(1, 0),</p><p class="source-code">      limit=int(parameters['top_k']),</p><p class="source-code">    )</p><p class="source-code">  def close(self):</p><p class="source-code">    """</p><p class="source-code">    Stores the DocumentArray to disk</p><p class="source-code">    """</p><p class="source-code">    self._docs.save(self.workspace + '/inde<a id="_idTextAnchor097"/>xer')</p></li>
</ol>
<h3>helper.py</h3>
<p>The <strong class="source-inline">helper.py</strong> file<a id="_idIndexMarker428"/> provides the helper functions to<a id="_idIndexMarker429"/> support the logical elements in the <strong class="source-inline">app.py</strong> file. It implements key functions such as <strong class="source-inline">index_generator</strong> and <strong class="source-inline">query_generator</strong>, which we use in the <strong class="source-inline">app.py</strong> file to index and query the data. Let’s go through both of these functions and understand what they do.</p>
<h4>index_generator()</h4>
<p>This function generates<a id="_idIndexMarker430"/> the index tag for the training data using the following steps:</p>
<ol>
<li value="1">This generator will iterate over all 60,000 documents (images) and process each one individually to make them index-ready.</li>
<li>It fetches the 28x28 images from the dictionary and inverts them to make them suitable to be displayed on the web browser.</li>
<li>It converts the black and white image into an RGB image and then converts the image into Jina’s internal data type, <strong class="source-inline">Document</strong>.</li>
<li>It then associates a tag ID with the document and yields it as the index data.</li>
</ol>
<p>The following is the code for the <strong class="source-inline">index_generator()</strong> function:</p>
<p class="source-code">def index_generator(num_docs: int, target: dict):</p>
<p class="source-code">  """</p>
<p class="source-code">  Generate the index data.</p>
<p class="source-code">  :param num_docs: Number of documents to be indexed.</p>
<p class="source-code">  :param target: Dictionary which stores the data </p>
<p class="source-code">    paths</p>
<p class="source-code">  :yields: index data</p>
<p class="source-code">  """</p>
<p class="source-code">  for internal_doc_id in range(num_docs):</p>
<p class="source-code">    # x_blackwhite.shape is (28,28)</p>
<p class="source-code">    x_blackwhite=</p>
<p class="source-code">      255-target['index']['data'][internal_doc_id]</p>
<p class="source-code">    # x_color.shape is (28,28,3)</p>
<p class="source-code">    x_color = np.stack((x_blackwhite,) * 3, axis=-1)</p>
<p class="source-code">    d = Document(content=x_color)</p>
<p class="source-code">    d.tags['id'] = internal_doc_id</p>
<p class="source-code">    yield d</p>
<h4>query_generator()</h4>
<p>This is similar<a id="_idIndexMarker431"/> to the <strong class="source-inline">index_generator</strong> function and follows the same logic to generate the query data with some modifications. It fetches a random number of documents (based on the value of the <strong class="source-inline">num_docs</strong> parameter) from the dataset to generate the query data. The following is the code for the <strong class="source-inline">query_generator()</strong> function: </p>
<pre class="source-code">def query_generator(num_docs: int, target: dict):
  """
  Generate the query data.
  :param num_docs: Number of documents to be queried
  :param target: Dictionary which stores the data paths
  :yields: query data
  """
  for _ in range(num_docs):
    num_data = len(target['query-labels']['data'])
    idx = random.randint(0, num_data - 1)
    # x_blackwhite.shape is (28,28)
    x_blackwhite = 255 - target['query']['data'][idx]
    # x_color.shape is (28,28,3)
    x_color = np.stack((x_blackwhite,) * 3, axis=-1)
    d = Document(
        content=x_color,
         tags={
         'id': -1,
         'query_label': float(target['query-labels'] 
          ['data'][idx][0]),
         },
    )
    yield d</pre>
<h3>demo.xhtml</h3>
<p>To view the <a id="_idIndexMarker432"/>query results in the web browser, the <a id="_idIndexMarker433"/>application uses the <strong class="source-inline">demo.xhtml</strong> file to render the frontend. By default, running the application will open a web page with the query images along with the search results; if it doesn’t, then you can open the <strong class="source-inline">demo.xhtml</strong> file, which will be available in the random folder generated at the start.</p>
<p>In this section, we saw how Jina’s framework makes it really efficient to build search applications for image data types by leveraging state-of-the-art deep learning models. The same functionality will be extended to other data types, such as audio, video, and even 3D mesh, which you will learn about in <a href="B17488_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Advanced Use Cases of Jina</em>.</p>
<p>Next, we will look at how to combine two data types to create a multimodal search that can easily elevate the search experience for your product or platform. We will dive into the multimodal search example, which uses the <em class="italic">people-image</em> dataset consisting of <em class="italic">image-caption</em> pairs to build a search application that lets you query using both the image and the text.</p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor098"/>Working with multimodal search</h1>
<p>Multimodal search<a id="_idIndexMarker434"/> is another pre-built example that comes with the Jina installation, which you can run directly from the comfort of your command line without even getting into the code.</p>
<p>This example uses Kaggle’s public people image dataset (<a href="">https://www.kaggle.com/ahmadahmadzada/images2000</a>), which consists of 2,000 image-caption pairs. The data type here is a multimodal document containing multiple data types such as a PDF document that contains text and images together. Jina lets you build the search for multimodal data types with the same ease and comfort:</p>
<ol>
<li value="1">To run this example from the command line, you need to install the following dependencies: <ul><li><strong class="bold">pip install transformers</strong></li>
<li><strong class="bold">pip install torch</strong></li>
<li><strong class="bold">pip install torchvision</strong></li>
<li><strong class="bold">pip install “jina[demo]”</strong></li>
</ul></li>
<li>Once all the dependencies are installed, simply type the following command to launch the application:<p class="source-code"><strong class="bold">jina hello multimodal</strong></p></li>
<li>After typing this <a id="_idIndexMarker435"/>command, you will see the following text on your CLI:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 6.7 – Multimodal search command line  " height="698" src="image/Figure_6.07_B17488.jpg" width="972"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Multimodal search command line </p>
<p>If your screen <a id="_idIndexMarker436"/>displays the same text on the command line, it means you have successfully launched the Jina multimodal example; now, it’s time to open the UI and play with the application.</p>
<p>By default, a UI with a query and results section will open up, allowing you to query with text and image and get the results in the same form. If the page doesn’t open up itself, you can open the <strong class="source-inline">index.xhtml</strong> file by going to <strong class="source-inline">jina/helloworld/multimodal/static</strong>.</p>
<p>You will see the following <a id="_idIndexMarker437"/>web page either by default or after opening the <strong class="source-inline">index.xhtml</strong> file:</p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 6.8 – Multimodal search interface " height="788" src="image/Figure_6.08_B17488.jpg" width="1639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Multimodal search interface</p>
<p>You have successfully launched the multimodal example application; it’s now time to play with it and have some fun.</p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor099"/>Navigating through the code</h2>
<p>Let’s now go<a id="_idIndexMarker438"/> through the logic behind the app and see how Jina’s framework ties all the components together to produce a functioning multimodal search application.</p>
<p>Once you install Jina, go to the chatbot directory by following the <strong class="source-inline">jina/helloworld/multimodal</strong> path. This is the main directory and contains the code for the multimodal search example:</p>
<pre class="source-code">└── multimodal                    
    ├── app.py
    ├── my_executors.py
    ├── flow_index.yml
    ├── flow_search.yml
    ├── static/        </pre>
<p>The following are the files that you will see within the multimodal directory. We will go through the functioning of each of them in detail:</p>
<ul>
<li><strong class="source-inline">app.py</strong>: Similar to the previous applications. </li>
<li><strong class="source-inline">my_executors.py</strong>: Similar to the previous applications. </li>
<li>The <strong class="source-inline">static</strong> folder: This hosts all the frontend code responsible for rendering the UI on the web browser, which helps you interact with the application.</li>
<li><strong class="source-inline">flow_index.yml</strong>: This contains the YAML code for the indexing flow, which is run when we index the data for the first time.</li>
<li><strong class="source-inline">flow_search.yml</strong>: This contains the YAML code for the search flow, which runs every time we send any query to the application.</li>
</ul>
<p>This <a id="_idIndexMarker439"/>application uses the MobileNet and MPNet models to index the image-caption pairs. The indexing process takes about 3 minutes on the CPU. Then, it opens a web page where you can query the multimodal documents. We have also prepared a YouTube video (<a href="">https://youtu.be/B_nH8GCmBfc</a>) to walk you through this demo.</p>
<h3>app.py</h3>
<p>When <a id="_idIndexMarker440"/>you type the <strong class="source-inline">jina hello multimodal</strong> command, the control of the application goes to the <strong class="source-inline">app.py</strong> file. The <strong class="source-inline">app.py</strong> file performs<a id="_idIndexMarker441"/> the following tasks to ensure that all the components of the multimodal search application work in tandem with each other to produce the desired result.</p>
<p>The first thing it does is import the required libraries. After that, the control goes to the <strong class="source-inline">hello_world()</strong> function, which hosts the main logic of the script. The <strong class="source-inline">hello_world()</strong> function creates a random directory using the <strong class="source-inline">mkdir</strong> command to store the artifacts, such as the downloaded data. Then, it checks to ensure that all the required Python libraries are installed and imported.</p>
<p class="callout-heading">Note</p>
<p class="callout">To run this example, we require three major dependencies/Python libraries: <strong class="source-inline">torch</strong>, <strong class="source-inline">transformers</strong>, and <strong class="source-inline">torchvision</strong>.</p>
<p>Following are the steps to understand the functioning of <strong class="source-inline">app.py</strong> file:</p>
<ol>
<li value="1">Please check that all the aforementioned dependencies are installed correctly in your Python environment. </li>
<li>After checking that these dependencies are correctly installed, the <strong class="source-inline">hello_world()</strong> function calls the <strong class="source-inline">download_data()</strong> function to fetch and download the data from Kaggle. The <strong class="source-inline">download_data()</strong> function uses the <strong class="source-inline">urllib</strong> package to fetch and save the data from the given URL. <strong class="source-inline">urllib</strong> takes the URL and filename as the targets<a id="_idIndexMarker442"/> and downloads the data. You can refer to the following code to see <a id="_idIndexMarker443"/>how we set the targets:<p class="source-code">targets = {</p><p class="source-code">        'people-img: {</p><p class="source-code">            'url': url_of_the_data,</p><p class="source-code">            'filename': file_name_to_be_fetched,</p><p class="source-code">        }</p><p class="source-code">    }</p></li>
</ol>
<p>Passing the <strong class="source-inline">targets</strong> variable into the <strong class="source-inline">download_data()</strong> function will download the data and save it in the random folder created at the beginning of the <strong class="source-inline">hello_world</strong> function. It then loads the indexing flow from the YAML file and passes the image metadata to the flow:</p>
<p class="source-code"># Indexing Flow</p>
<p class="source-code">f = Flow.load_config('flow-index.yml')</p>
<p class="source-code">with f, open(f'{args.workdir}/people-img/meta.csv', newline='') as fp:</p>
<p class="source-code">  f.index(inputs=DocumentArray.from_csv(fp), </p>
<p class="source-code">    request_size=10, show_progress=True)</p>
<p class="source-code">  f.post(on='/dump', target_executor='textIndexer')</p>
<p class="source-code">  f.post(on='/dump', target_executor='imageIndexer')</p>
<p class="source-code">  f.post(on='/dump', </p>
<p class="source-code">    target_executor='keyValueIndexer')</p>
<ol>
<li value="3">Similarly, it<a id="_idIndexMarker444"/> then loads the search flow from the YAML file<a id="_idIndexMarker445"/> and sets it to fetch the input queries from the HTML frontend:<p class="source-code"># Search Flow</p><p class="source-code">f = Flow.load_config('flow-search.yml')</p><p class="source-code"># switch to HTTP gateway</p><p class="source-code">f.protocol = 'http'</p><p class="source-code">f.port_expose = args.port_expose</p><p class="source-code">url_html_path = 'file://' + os.path.abspath(</p><p class="source-code">            os.path.join(cur_dir, </p><p class="source-code">            'static/index.xhtml'))</p><p class="source-code">with f:</p><p class="source-code">  try:</p><p class="source-code">         webbrowser.open(url_html_path, new=2)</p><p class="source-code">  except:</p><p class="source-code">    pass  # intentional pass</p><p class="source-code">  finally:</p><p class="source-code">         default_logger.info(</p><p class="source-code">    f'You should see a demo page opened in your </p><p class="source-code">      browser,'f'if not, you may open {url_html_path} </p><p class="source-code">      manually'</p><p class="source-code">            )</p><p class="source-code">  if not args.unblock_query_flow:</p><p class="source-code">    f.block()</p></li>
</ol>
<p>In both of the <a id="_idIndexMarker446"/>preceding code snippets, we open the flow with a <a id="_idIndexMarker447"/>context manager. For this example, we will use the web browser to interact with the application. It requires configuring and serving the flow on a specific port with the HTTP protocol using the <strong class="source-inline">port_expose</strong> parameter. Toward the end, we use the <strong class="source-inline">f.block()</strong> method to keep the flow open for search queries and to prevent it from exiting. </p>
<h3>my_executors.py</h3>
<p>If <strong class="source-inline">app.py</strong> is the <a id="_idIndexMarker448"/>brain of this example, then the <strong class="source-inline">my_executors.py</strong> file contains the neurons in the form of executors that power<a id="_idIndexMarker449"/> the core logic. </p>
<p>The multimodal example contains two modalities of data: image and text, which are stored in the document <strong class="source-inline">tags</strong> and <strong class="source-inline">uri</strong> attributes, respectively. To process these two modalities of data, at index time, we need to preprocess, encode, and index them separately using the following executors.</p>
<h4>The Segmenter executor </h4>
<p>The <strong class="source-inline">Segmenter</strong> executor <a id="_idIndexMarker450"/>takes the documents as the input and splits them into two chunks: image chunk and text chunk. The text chunk will contain the plain text data and the image chunk (which we call <strong class="source-inline">chunk_uri</strong> in the code) contains the URI of the image. Then, we add them both to the<a id="_idIndexMarker451"/> document chunks and send them further to the pre-processing stage, as shown here:</p>
<pre class="source-code">class Segmenter(Executor):
    @requests
    def segment(self, docs: DocumentArray, **kwargs):
        for doc in docs:
            text = doc.tags['caption']
            uri={os.environ["HW_WORKDIR"]}/
              people-img/{doc.tags["image"]}'
            chunk_text = Document(text=text, 
              mime_type='text/plain')
            chunk_uri = Document(uri=uri, 
              mime_type='image/jpeg')
            doc.chunks = [chunk_text, chunk_uri]
            doc.uri = uri
            doc.convert_uri_to_datauri()</pre>
<h4>The TextCrafter executor </h4>
<p>For the preprocessing <a id="_idIndexMarker452"/>of the text chunk, we use the <strong class="source-inline">TextCrafter</strong> executor, which takes the text chunk as the input and returns a flattened traversable sequence of all the documents, as shown here:</p>
<pre class="source-code">class TextCrafter(Executor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    @requests()
    def filter(self, docs: DocumentArray, **kwargs):
        filtered_docs = DocumentArray(
            d for d in docs.traverse_flat(['c']) if 
              d.mime_type == 'text/plain'
        )
        return filtered_docs</pre>
<h4>The ImageCrafter executor </h4>
<p>Similarly, for the<a id="_idIndexMarker453"/> preprocessing of the image chunk, we use the <strong class="source-inline">ImageCrafter</strong> executor, which takes the image chunk as the input and returns a flattened traversable sequence of all the documents: </p>
<pre class="source-code">class ImageCrafter(Executor):
    @requests(on=['/index', '/search'])
    def craft(self, docs: DocumentArray, **kwargs):
        filtered_docs = DocumentArray(
            d for d in docs.traverse_flat(['c']) if 
              d.mime_type == 'image/jpeg'
        )
        target_size = 224
        for doc in filtered_docs:
            doc.convert_uri_to_image_blob()
           doc.set_image_blob_shape(shape=(target_size, 
             target_size))
            doc.set_image_blob_channel_axis(-1, 0)
        return filtered_docs</pre>
<h4>The TextEncoder executor </h4>
<p>After the<a id="_idIndexMarker454"/> preprocessing step, the preprocessed data of the text chunk goes to the <strong class="source-inline">TextEncoder</strong> executor as the input and produces the text embedding as the output. We persist the result in the form of embeddings using the <strong class="source-inline">DocVectorIndexer</strong> executor. Let’s look at the functioning of <strong class="source-inline">TextEncoder</strong> by starting with the code of its constructor:</p>
<pre class="source-code">class TextEncoder(Executor):
  """Transformer executor class"""
  def __init__(
        self,
        pretrained_model_name_or_path: str=
        'sentence-transformers/paraphrase-mpnet-base-v2',
        pooling_strategy: str = 'mean',
        layer_index: int = -1,
        *args,
        **kwargs,
  ):
        super().__init__(*args, **kwargs)
        self.pretrained_model_name_or_path = 
          pretrained_model_name_or_path
        self.pooling_strategy = pooling_strategy
        self.layer_index = layer_index
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.pretrained_model_name_or_path
        )
        self.model = AutoModel.from_pretrained(
            self.pretrained_model_name_or_path, 
            output_hidden_states=True
        )
        self.model.to(torch.device('cpu'))</pre>
<p>To compute <a id="_idIndexMarker455"/>the embeddings, it uses the pre-trained <strong class="source-inline">sentence-transformers/paraphrase-mpnet-base-v2</strong> model with the <strong class="source-inline">'mean'</strong> pooling strategy. Let’s look at the code for the <strong class="source-inline">compute_embedding()</strong> function:</p>
<pre class="source-code">def _compute_embedding(self, hidden_states: 'torch.Tensor', input_tokens:   Dict):
  fill_vals = {'cls': 0.0,'mean': 0.0,'max': -np.inf,'min': 
    np.inf}
      fill_val = torch.tensor(
        fill_vals[self.pooling_strategy], 
          device=torch.device('cpu')
      )
  layer = hidden_states[self.layer_index]
      attn_mask = 
        input_tokens['attention_mask']
        .unsqueeze(-1).expand_as(layer)
      layer = torch.where(attn_mask.bool(), layer,
        fill_val)
      embeddings = layer.sum(dim=1) / attn_mask.sum(dim=1)
      return embeddings.cpu().numpy()</pre>
<p>It then uses the <strong class="source-inline">encode()</strong> function to store the embeddings in the <strong class="source-inline">doc.embeddings</strong> attribute of the document:</p>
<pre class="source-code">@requests
def encode(self, docs: 'DocumentArray', **kwargs):
  with torch.inference_mode():
        if not self.tokenizer.pad_token:
              self.tokenizer.add_special_tokens({
                'pad_token': '[PAD]'})
      self.model.resize_token_embeddings(len(
        self. tokenizer.vocab))
    input_tokens = self.tokenizer(
      docs.get_attributes('content'),
      padding='longest',
      truncation=True,
      return_tensors='pt',
            )
            input_tokens = {
      k: v.to(torch.device('cpu')) for k, v in 
        input_tokens.items()
            }
            outputs = self.model(**input_tokens)
            hidden_states = outputs.hidden_states
            docs.embeddings = self._compute_embedding(
              hidden_states, input_tokens)</pre>
<h4>The ImageEncoder executor </h4>
<p>Similarly, the<a id="_idIndexMarker456"/> preprocessed data of the image chunk goes to the <strong class="source-inline">ImageEncoder</strong> executor as the input and produces the embedding as the output. We persist the result in the form of embeddings using the <strong class="source-inline">DocVectorIndexer</strong> executor. Let’s look at the functioning of <strong class="source-inline">ImageEncoder</strong> by going through the code:</p>
<pre class="source-code">class ImageEncoder(Executor):
  def __init__(
        self,
    model_name: str = 'mobilenet_v2',
    pool_strategy: str = 'mean',
    channel_axis: int = -1, *args, **kwargs,
  ):
    super().__init__(*args, **kwargs)
    self.channel_axis = channel_axis
    self.model_name = model_name
    self.pool_strategy = pool_strategy
    self.pool_fn = getattr(np, self.pool_strategy)
        model = getattr(models, 
          self.model_name)(pretrained=True)
    self.model = model.features.eval()
    self.model.to(torch.device('cpu'))    </pre>
<p>It uses the pre-trained <strong class="source-inline">mobilenet -v2</strong> model to generate the embeddings. To preprocess the images, it uses the <strong class="source-inline">'mean'</strong> pooling strategy to compute the average value of all the pixels in the image to compute the embeddings:</p>
<pre class="source-code">def _get_features(self, content):
  return self.model(content)
def _get_pooling(self, feature_map: 'np.ndarray') -&gt; 'np.ndarray':
  if feature_map.ndim == 2 or self.pool_strategy is None:
    return feature_map
  return self.pool_fn(feature_map, axis=(2, 3))
@requests
def encode(self, docs: DocumentArray, **kwargs):
  with torch.inference_mode():
    _input = torch.from_numpy(docs.blobs.astype('float32'))
            _features = self._get_features(_input).detach()
            _features = _features.numpy()
            _features = self._get_pooling(_features)
            docs.embeddings = _features</pre>
<p>Toward the<a id="_idIndexMarker457"/> end, the <strong class="source-inline">encode</strong> function stores the embeddings in the <strong class="source-inline">doc.embeddings</strong> attribute of the document. </p>
<h4>The DocVectorIndexer executor </h4>
<p>Now, let’s <a id="_idIndexMarker458"/>look at the <strong class="source-inline">DocVectorIndexer</strong> executor, which persists the encoding from both <strong class="source-inline">TextEncoder</strong> and <strong class="source-inline">ImageEncoder</strong> to index them. Here, we have two different modalities of data (text and image), so we need to store the indexed results separately in two different files. The <strong class="source-inline">DocVectorIndexer</strong> executor takes care of that. It stores the indexed text embeddings into the <strong class="source-inline">text.json</strong> file and the image embeddings into the <strong class="source-inline">image.json</strong> file, which we will use in the <strong class="source-inline">flow_index.yml</strong> file as <strong class="source-inline">index_file_name</strong>. Let’s look at the code for <strong class="source-inline">DocVectorIndexer</strong> to understand its functioning in detail:</p>
<pre class="source-code">class DocVectorIndexer(Executor):
  def __init__(self, index_file_name: str, **kwargs):
        super().__init__(**kwargs)
    self._index_file_name = index_file_name
    if os.path.exists(self.workspace + 
      f'/{index_file_name}'):
      self._docs = DocumentArray.load(
        self.workspace + f'/{index_file_name}')
    else:
      self._docs = DocumentArray()
  @requests(on='/index')
  def index(self, docs: 'DocumentArray', **kwargs):
    self._docs.extend(docs)
  @requests(on='/search')
  def search(self, docs: 'DocumentArray', parameters: Dict, 
    **kwargs):
    docs.match(
      self._docs,
      metric='cosine',
              normalization=(1, 0),
              limit=int(parameters['top_k']),
    ) 
  @requests(on='/dump')
  def dump(self, **kwargs):
    self._docs.save(self.workspace + 
      f'/{self._index_file_name}')
  def close(self):
    """
    Stores the DocumentArray to disk
    """
    self.dump()
    super().close()</pre>
<p>It uses <strong class="source-inline">DocumentArray</strong> to store <a id="_idIndexMarker459"/>all the documents directly on the disk because we have a large number of documents. It hosts two different endpoints to index the data and open the <strong class="source-inline">'search'</strong> flow. It uses the cosine similarity score to find the relevant documents.</p>
<h4>The KeyValueIndexer executor </h4>
<p>Apart from <strong class="source-inline">DocVectorIndexer</strong> to persist embeddings, we also create a <strong class="source-inline">KeyValueIndexer</strong> executor to help <a id="_idIndexMarker460"/>the chunks (text chunk and image chunk) to find their parent/root document. Let’s look at the code to understand its functionality in detail:</p>
<pre class="source-code">class KeyValueIndexer(Executor):
  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    if os.path.exists(self.workspace + '/kv-idx'):
      self._docs = DocumentArray.load(self.workspace + 
            '/kv-idx')
    else:
      self._docs = DocumentArray()
  @requests(on='/index')
  def index(self, docs: DocumentArray, **kwargs):
    self._docs.extend(docs)
  @requests(on='/search')
  def query(self, docs: DocumentArray, **kwargs):
    for doc in docs:
              for match in doc.matches:
        extracted_doc = self._docs[match.parent_id]
        extracted_doc.scores = match.scores
        new_matches.append(extracted_doc)
      doc.matches = new_matches
  @requests(on='/dump')
  def dump(self, **kwargs):
    self._docs.save(self.workspace + 
      f'/{self._index_file_name}')
  
  def close(self):
    """
    Stores the DocumentArray to disk
    """
    self.dump()
    super().close()</pre>
<p>It uses <strong class="source-inline">DocumentArray</strong> just like <strong class="source-inline">DocVectorIndexer</strong> to store all the documents directly on<a id="_idIndexMarker461"/> the disk.</p>
<p>It hosts two different <a id="_idIndexMarker462"/>endpoints to index the data and open the search flow. In the search logic, given a document, it loops through the tree to find its root/parent document.</p>
<h4>The WeightedRanker executor </h4>
<p>Toward the end, when both the <a id="_idIndexMarker463"/>chunks find their parents, we aggregate the score using the <strong class="source-inline">WeightedRanker</strong> executor to produce the final output. </p>
<p>Let’s look at the code to understand its functionality in detail:</p>
<ol>
<li value="1">It opens a search endpoint to combine the results from both the text and image chunks to calculate the final similarity score, which we will use to determine the results: <p class="source-code">class WeightedRanker(Executor):</p><p class="source-code">  @requests(on='/search')</p><p class="source-code">  def rank(</p><p class="source-code">    self, docs_matrix: List['DocumentArray'], </p><p class="source-code">    parameters: Dict, **kwargs</p><p class="source-code">  ) -&gt; 'DocumentArray':</p><p class="source-code">    """</p><p class="source-code">    :param docs_matrix: list of :class:`DocumentArray` </p><p class="source-code">      on multiple     requests to get bubbled up </p><p class="source-code">      matches.</p><p class="source-code">    :param parameters: the parameters passed into the </p><p class="source-code">      ranker, in     this case stores </p><p class="source-code">        :param kwargs: not used (kept to maintain </p><p class="source-code">          interface)</p><p class="source-code">    """</p><p class="source-code">    result_da = DocumentArray()  </p><p class="source-code">    for d_mod1, d_mod2 in zip(*docs_matrix):</p><p class="source-code">              final_matches = {}  # type: Dict[str, </p><p class="source-code">                Document]</p></li>
<li>You can <a id="_idIndexMarker464"/>assign the <strong class="source-inline">weight</strong> parameter beforehand to determine which modality (between text and image) will contribute more toward calculating the final relevance score. If you set the weight of the text chunk as <strong class="source-inline">2</strong> and the image chunk as <strong class="source-inline">1</strong>, then the text chunk will contribute a higher score to the final relevance.</li>
<li>The final similarity score is calculated by summing up cosine similarity * weight for both the modalities and then sorting them in descending order:<p class="source-code">  for m in d_mod1.matches:</p><p class="source-code">    relevance_score = m.scores['cosine'].value * </p><p class="source-code">      d_mod1.weight</p><p class="source-code">    m.scores['relevance'].value = relevance_score</p><p class="source-code">    final_matches[m.parent_id] = Document(</p><p class="source-code">      m, copy=True)</p><p class="source-code">  for m in d_mod2.matches:</p><p class="source-code">    if m.parent_id in final_matches:</p><p class="source-code">      final_matches[m.parent_id].scores[</p><p class="source-code">        'relevance'</p><p class="source-code">      ].value = final_matches[m.parent_id].scores['relevance']</p><p class="source-code">      .value + (</p><p class="source-code">        m.scores['cosine'].value * d_mod2.weight</p><p class="source-code">      )</p><p class="source-code">    else:</p><p class="source-code">      m.scores['relevance'].value = (</p><p class="source-code">        m.scores['cosine'].value * d_mod2.weight</p><p class="source-code">      )</p><p class="source-code">          final_matches[m.parent_id] = Document(m, </p><p class="source-code">            copy=True)</p><p class="source-code">  da = DocumentArray(list(final_matches.values()))</p><p class="source-code">  da.sorted(da, key=lambda ma: </p><p class="source-code">    ma.scores['relevance'].value, reverse=True)</p><p class="source-code">  d = Document(matches=da[: int(parameters['top_k'])])</p><p class="source-code">  result_da.append(d)</p><p class="source-code">return result_da</p></li>
</ol>
<p>We have looked at how the executors work together to produce the results. Let’s now look at how these executors are arranged and utilized in the index and search flow.</p>
<h3>flow_index.yml</h3>
<p>As you <a id="_idIndexMarker465"/>already know, Jina provides two ways to create <a id="_idIndexMarker466"/>and work with the flows. The first is by using native Python, and the second is by using a YAML file to create a flow and call it in the main <strong class="source-inline">app.py</strong> file. Now, we will look at how the <strong class="source-inline">flow_index.yml</strong> file is created by leveraging the individual executor components that we discussed in the previous section.</p>
<p>The <strong class="source-inline">flow_index.yml</strong> file uses different executors that we have defined in the <strong class="source-inline">my_executors.py</strong> file and arranges them to produce the indexing flow. Let’s go through the YAML code to understand it in detail:</p>
<ol>
<li value="1">It starts with the <strong class="source-inline">Segmenter</strong> executor, which segments the document into text and image chunks:<p class="source-code">jtype: Flow</p><p class="source-code">version: '1'</p><p class="source-code">executors:</p><p class="source-code">  - name: segment</p><p class="source-code">    uses:</p><p class="source-code">      jtype: Segmenter</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p></li>
<li>After that, we<a id="_idIndexMarker467"/> have two different pipelines, one for <a id="_idIndexMarker468"/>text and the other for the image. The text pipeline preprocesses the data using the <strong class="source-inline">TextCrafter</strong> executor, encodes it using the <strong class="source-inline">TextEncoder</strong> executor, and then indexes it using <strong class="source-inline">DocVectorIndexer</strong>: <p class="source-code">  - name: craftText</p><p class="source-code">    uses:</p><p class="source-code">      jtype: TextCrafter</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: encodeText</p><p class="source-code">    uses:</p><p class="source-code">      jtype: TextEncoder</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: textIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: DocVectorIndexer</p><p class="source-code">      with:</p><p class="source-code">        index_file_name: "text.json"</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p></li>
<li>The image <a id="_idIndexMarker469"/>pipeline preprocesses the data using<a id="_idIndexMarker470"/> the <strong class="source-inline">ImageCrafter</strong> executor, encodes it using the <strong class="source-inline">ImageEncoder</strong> executor, and then indexes it using <strong class="source-inline">DocVectorIndexer</strong>:<p class="source-code">  - name: craftImage</p><p class="source-code">    uses:</p><p class="source-code">      jtype: ImageCrafter</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">    needs: segment</p><p class="source-code">  - name: encodeImage</p><p class="source-code">    uses:</p><p class="source-code">      jtype: ImageEncoder</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: imageIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: DocVectorIndexer</p><p class="source-code">      with:</p><p class="source-code">        index_file_name: "image.json"</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p></li>
<li>After <a id="_idIndexMarker471"/>indexing the text and image to the <a id="_idIndexMarker472"/>respective <strong class="source-inline">text.json</strong> and <strong class="source-inline">image.json</strong> files, we join both the indexers with <strong class="source-inline">KeyValueIndexer</strong> to link them together:<p class="source-code">  - name: keyValueIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: KeyValueIndexer</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">    needs: segment</p><p class="source-code">  - name: joinAll</p><p class="source-code">    needs: [textIndexer, imageIndexer, </p><p class="source-code">      keyValueIndexer]</p></li>
</ol>
<h3>flow_search.yml</h3>
<p>Similar to the <strong class="source-inline">flow_index.yml</strong> file, we also have a <strong class="source-inline">flow_search.yml</strong> file, which defines the<a id="_idIndexMarker473"/> search/query flow for the multimodal example<a id="_idIndexMarker474"/> application. Let’s look at the YAML code to understand its functionality in detail:</p>
<ol>
<li value="1">It gets the input in the form of text and images and treats them both differently using a pipeline of executors. For the text input, it uses the <strong class="source-inline">TextCrafter</strong> executor to preprocess the data, followed by the <strong class="source-inline">TextEncoder</strong> executor to encode the textual data, and finally, indexes it using <strong class="source-inline">DocVectorIndexer</strong>:<p class="source-code">jtype: Flow</p><p class="source-code">version: '1'</p><p class="source-code">with:</p><p class="source-code">  cors: True</p><p class="source-code">executors:</p><p class="source-code">  - name: craftText</p><p class="source-code">    uses:</p><p class="source-code">      jtype: TextCrafter</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: encodeText</p><p class="source-code">    uses:</p><p class="source-code">      jtype: TextEncoder</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: textIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: DocVectorIndexer</p><p class="source-code">      with:</p><p class="source-code">        index_file_name: "text.json"</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p></li>
<li>For the <a id="_idIndexMarker475"/>image input, it uses the <strong class="source-inline">ImageCrafter</strong> executor <a id="_idIndexMarker476"/>to preprocess the data, followed by the <strong class="source-inline">ImageEncoder</strong> executor to encode the image data, and finally, indexes it using <strong class="source-inline">DocVectorIndexer</strong>:<p class="source-code">  - name: craftImage</p><p class="source-code">    uses:</p><p class="source-code">      jtype: ImageCrafter</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">    needs: gateway</p><p class="source-code">  - name: encodeImage</p><p class="source-code">    uses:</p><p class="source-code">      jtype: ImageEncoder</p><p class="source-code">      metas:</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">  - name: imageIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: DocVectorIndexer</p><p class="source-code">      with:</p><p class="source-code">        index_file_name: "image.json"</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p></li>
<li>It then<a id="_idIndexMarker477"/> passes the result of both <strong class="source-inline">TextIndexer</strong> and <strong class="source-inline">ImageIndexer</strong> into the <strong class="source-inline">WeightedRanker</strong> executor, which calculates the<a id="_idIndexMarker478"/> final relevance score and produces the output:<p class="source-code">  - name: weightedRanker</p><p class="source-code">    uses:</p><p class="source-code">      jtype: WeightedRanker</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">    needs: [ textIndexer, imageIndexer ]</p><p class="source-code">  - name: keyvalueIndexer</p><p class="source-code">    uses:</p><p class="source-code">      jtype: KeyValueIndexer</p><p class="source-code">      metas:</p><p class="source-code">        workspace: ${{ ENV.HW_WORKDIR }}</p><p class="source-code">        py_modules:</p><p class="source-code">          - ${{ ENV.PY_MODULE }}</p><p class="source-code">    needs: weightedRanker</p></li>
</ol>
<p>To interact with the <a id="_idIndexMarker479"/>multimodal application in the web<a id="_idIndexMarker480"/> browser via the UI, you can use the <strong class="source-inline">index.xhtml</strong> file provided in the <strong class="source-inline">static</strong> folder. Running the application should open the HTML file by default, but if it doesn’t, then you can open the <strong class="source-inline">index.xhtml</strong> file from the <strong class="source-inline">static</strong> folder. </p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor100"/>Summary</h1>
<p>In this chapter, we have covered how to put together all the components and concepts that we have learned in the previous chapters. We have walked you through the process of building basic search examples with Jina for different data types, including a text-to-text search, image-to-image search, and multimodal search, which combines both the text and the images. The things that we learned in this chapter will act as a building block for <a href="B17488_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Advanced Use Cases of Jina</em>, where you will learn about building advanced examples using Jina.</p>
<p>In the next chapter, we will continue on the same journey and see how to build advanced search applications with Jina using what we have learned so far.</p>
</div>
<div>
<div id="_idContainer076">
</div>
</div>
</div>
</body></html>