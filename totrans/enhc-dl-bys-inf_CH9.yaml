- en: Chapter 9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next Steps in Bayesian Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we’ve covered the fundamental concepts behind Bayesian
    deep learning (BDL), from understanding what uncertainty is and its role in developing
    robust machine learning systems, right through to learning how to implement and
    analyze the performance of several fundamental BDL. While what you’ve learned
    will equip you to start developing your own BDL solutions, the field is moving
    quickly, and there are many new techniques on the horizon.
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up the book, in this chapter we’ll take a look at the current trends
    in BDL, before we dive into some of the latest developments in the field. We’ll
    conclude by introducing some alternatives to BDL, and provide some advice on additional
    resources you can use to continue your journey into Bayesian machine learning
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Current trends in BDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are BDL methods being applied to solve real-world problems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latest methods in BDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatives to BDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your next steps in BDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.1 Current trends in BDL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll explore the current trends in BDL. We’ll look at which
    models are particularly popular in the literature and discuss why certain models
    have been selected for certain applications. This should give you a good imdivssion
    of how the fundamentals covered throughout the book apply more broadly across
    a variety of application domains.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Popularity of key BDL search terms over time'
  prefs: []
  type: TYPE_NORMAL
- en: As we see in *Figure* [9.1](#x1-179002r1), there’s been a marked increase in
    popularity of search terms related to BDL over the past decade. Unsurprisingly,
    this follows the trend in the popularity of deep learning search terms, as we
    see in *Figure* [*9.2*](#x1-179004r2); as deep learning has become more popular,
    there has been increased interest in quantifying the uncertainty associated with
    the predictions produced by DNNs. Interestingly, these plots both show a similar
    dip in popularity in mid-late 2021, indicating that as long as deep learning is
    popular, there will also be interest in BDL.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Popularity of key deep learning search terms over time'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure* [9.1](#x1-179002r1) demonstrates another interesting point in that,
    generally speaking, the term *variational inference* is more popular than the
    other two BDL-related search terms we’ve used here. As mentioned in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) when
    we covered variational autoencoders, variational inference is one component of
    BDL that has made significant waves in the machine learning community, now being
    a feature of many different deep learning architectures. As such, it’s no surprise
    that it is generally more popular than the terms that explicitly include the word
    ”Bayesian.”'
  prefs: []
  type: TYPE_NORMAL
- en: But where do the methods that we’ve explored in the book fit in terms of their
    popularity and integration into a wide variety of deep learning solutions? We
    can learn more about this simply by looking at the citations for each of the original
    papers.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Popularity of key deep learning search terms over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure* [*9.3*](#x1-179007r3), we see that the MC dropout paper is by far
    the most popular paper when it comes to citations – coming in at nearly double
    that of the next most popular method. At this point in the book, the reasons for
    this should be fairly clear: not only is it one of the easiest methods to implement
    (as we saw in [*Chapter 6*](CH6.xhtml#x1-820006), [*Using the Standard Toolbox
    for Bayesian Deep Learning*](CH6.xhtml#x1-820006)), but it’s also one of the most
    attractive from a computation standpoint. It requires no more memory than a standard
    neural network and, as we saw in [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007), it’s also
    one of the fastest models for running inference. These practical factors often
    weigh in more heavily than considerations such as uncertainty quality when it
    comes to selecting models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practical considerations are again likely the reason behind the second most
    popular method being deep ensembles. While this may not be the most efficient
    method in terms of training time, it’s often the speed of inference that counts
    the most: and again, looking back to [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007)’s results,
    we see that the ensemble excels here, despite needing to run inference on multiple
    different networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep ensembles often strike a good balance between ease of implementation and
    theoretical considerations: as discussed in [*Chapter 6*](CH6.xhtml#x1-820006),
    [*Using the Standard Toolbox* *for Bayesian Deep Learning*](CH6.xhtml#x1-820006),
    ensembling is a powerful tool within ML, and so it’s no surprise that NN ensembles
    perform well and often produce well-calibrated uncertainty estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two methods, taking third and fourth place respectively, are BBB and
    PBP. While BBB is far easier to implement than PBP, the fact that it requires
    some probabilistic components often means that – while in many cases it may be
    the best tool for the job – machine learning engineers may not be aware of it
    or aren’t comfortable implementing it. PBP takes this to a further extreme: as
    we saw in [*Chapter 5*](CH5.xhtml#x1-600005), [*Principled Approaches for* *Bayesian
    Deep Learning*](CH5.xhtml#x1-600005), implementing PBP is not a straightforward
    task. At the time of writing, there are no deep learning frameworks that incorporate
    an easy-to-use and well-optimized implementation of PBP, and – apart from the
    BDL community – many machine learning researchers and practitioners just aren’t
    aware of its existence, as is made clear by the fact that it has fewer citations
    (although it still has a fairly impressive citation count!).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This analysis of the popularity of Bayesian deep learning methods seems to
    tell a pretty clear story: BNN approaches are chosen based primarily on the ease
    of implementation. Indeed, there is a significant amount of literature that discusses
    the use of BDL methods for their uncertainty estimates without considering the
    *quality* of the model uncertainty estimates. Fortunately, with the increasing
    popularity of uncertainty-aware methods, this trend is beginning to decline, and
    we hope that this book has equipped you with the necessary tools to allow you
    to be more principled in your selection of BNN methods. Irrespective of the methods
    used or how they’re selected, it’s clear that machine learning researchers and
    practitioners are increasingly interested in BDL approaches – so what are these
    methods being used for? Let’s take a look.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 How are BDL methods being applied to solve real-world problems?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as deep learning is having an impact on a diverse variety of application
    domains, BDL is becoming an increasingly important tool, particularly where large
    amounts of data are being used within safety-critical or mission-critical systems.
    In these cases – as is the case for most real-world applications – being able
    to quantify when models ”know they don’t know” is crucial to developing reliable
    and robust systems.
  prefs: []
  type: TYPE_NORMAL
- en: One significant application area for BDL is in safety-critical systems. In their
    2019 paper titled *Safe Reinforcement Learning with Model Uncertainty* *Estimates*,
    Björn Lütjens *et al.* demonstrate that the use of BDL methods can produce safer
    behavior in collision-avoidance scenarios (the inspiration for our reinforcement
    learning example in [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian Deep*
    *Learning*](CH8.xhtml#x1-1320008)).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the paper *Uncertainty-Aware Deep Learning for Safe Landing* *Site
    Selection*, authors Katharine Skinner *et al.* explore how Bayesian neural networks
    can be used for autonomous hazard detection for landing sites on planetary surfaces.
    This technology is crucial for facilitating autonomous landing, and recently DNNs
    have demonstrated significant aptitude for this application. In their paper, Skinner
    *et al.* demonstrate that the use of uncertainty-aware models can improve the
    selection of safe landing sites, and even make it possible to select safe landing
    sites from sensor data with large amounts of noise. This is testament to BDL’s
    capacity to improve both the *safety* and *robustness* of deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given their rising popularity in safety-critical scenarios, it should be no
    surprise that Bayesian neural networks have also been adopted within medical applications.
    As we touched on in [*Chapter 1*](CH1.xhtml#x1-150001), [*Bayesian Inference in
    the Age of* *Deep Learning*](CH1.xhtml#x1-150001), deep learning has exhibited
    particularly strong performance in the field of medical imaging. However, in these
    sorts of critical applications, uncertainty quantification is crucial: technicians
    and diagnosticians need to be able to understand the margin of error associated
    with model predictions. In the paper *Towards Safe Deep Learning: Accurately Quantifying
    Biomarker* *Uncertainty in Neural Network predictions*, Zach Eaton-Rosen *et al.*
    applied BDL methods for quantifying biomarker uncertainty when using deep networks
    for tumor volume estimation. Their work demonstrates that Bayesian neural networks
    can be used to design deep learning systems with well-calibrated error bars. These
    high-quality uncertainty estimates are necessary for the safe clinical use of
    models based on deep networks, making BDL methods crucial when it comes to incorporating
    these models in diagnostic applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As technology advances, so does our ability to collect and organize data. This
    trend is turning a lot of ”small data” problems into ”big data” problems – which
    is no bad thing, as more data means we’re able to learn much more about the underlying
    process generating the data. One such example is that of seismic monitoring: over
    recent years, there has been a significant increase in dense seismic monitoring
    networks. This is excellent from a monitoring standpoint: scientists now have
    more data than ever before, and are thus able to better understand and monitor
    geophysical processes. However, in order to do so, they also need to be able to
    learn from large amounts of high dimensional data.'
  prefs: []
  type: TYPE_NORMAL
- en: In their paper, *Bayesian Deep Learning and Uncertainty Quantification Applied*
    *to Induced Seismicity Locations in the Groningen Gas Field in the Netherlands:*
    *What Do We Need for Safe AI?*, authors Chen Gu *et al.* tackle the problem of
    seismic monitoring of the Groningen gas reservoir. As they mention in the paper,
    while deep learning has been applied to many geophysical problems, the use of
    uncertainty-aware deep networks is rare. Their work demonstrates that Bayesian
    neural networks can successfully be applied to geophysical problems and, in the
    case of the Groningen gas reservoir, could be crucial from both a safety-critical
    *and* mission-critical standpoint. From the safety perspective, these methods
    can be used to leverage the vast amounts of data to develop models that can infer
    ground motion activity and be used for seismic early warning systems. From the
    mission-critical perspective, the same data can be ingested by these methods to
    produce models capable of reservoir production estimates.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, uncertainty quantification is key if these methods are going
    to be incorporated into any real-world systems, as the consequences for trusting
    incorrect predictions could be costly or even catastrophic.
  prefs: []
  type: TYPE_NORMAL
- en: These examples have given us some insight into how BDL is being applied in the
    real world. As with other machine learning solutions before them, we learn more
    about potential shortcomings as the methods are used in more and more diverse
    sets of applications. In the next section, we’ll learn about some of the latest
    developments in the field, building on the core approaches covered in the book
    to develop increasingly robust BNN approximations.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Latest methods in BDL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this book, we’ve introduced some of the core techniques used within BDL:
    Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Monte-Carlo dropout
    (MC dropout), and deep ensembles. Many BNN approaches you’ll encounter in the
    literature will be based on one of these methods, and having these under your
    belt provides you with a versatile toolbox of approaches for developing your own
    BDL solutions. However, as with all aspects of machine learning, the field of
    BDL is progressing rapidly, and new techniques are being developed on a regular
    basis. In this section, we’ll explore a selection of recent developments from
    the field.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Combining MC dropout and deep ensembles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Why use just one Bayesian neural network technique when you could use two?
    This is exactly the approach taken by University of Edinburgh researchers Remus
    Pop and Patric Fulop in their paper, *Deep Ensemble Bayesian* *Active Learning:
    Addressing the Mode Collapse Issue in Monte Carlo* *Dropout via Ensembles*. In
    this work, Pop and Fulop describe the problem of using **active learning** to
    make deep learning methods feasible in applications for which labeling data is
    time consuming or costly. The issue here is that, as we’ve discussed previously,
    deep learning methods have proven to be incredibly successful across a range of
    medical imaging tasks. The issue is that this data needs to be carefully labeled,
    and for deep networks to achieve high levels of performance, they need *a lot*
    of this data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, active learning has been proposed by machine learning researchers
    to automatically evaluate new data points and add them to the dataset, using **acquisition
    functions** to determine when new data should be added to the training set. Model
    uncertainty estimates are a key piece of puzzle: they provide a key measure of
    how new data points relate to the model’s existing understanding of the domain.
    In their paper, Pop and Fulop demonstrate that a popular method for **Deep Bayesian
    Active** **Learning (DBAL)** has a key shortcoming: that of over-confidence stemming
    from the MC dropout models used in DBAL. In their paper, the authors address this
    through combining both deep ensembles and MC dropout in a single model. They demonstrate
    that the resulting model has better calibrated uncertainty estimates, thus correcting
    for the over-confident predictions exhibited by MC dropout. The resulting method,
    dubbed **deep** **ensemble Bayesian active learning**, provides a framework for
    robustly employing deep learning methods in applications for which data acquisition
    is difficult or expensive – demonstrating again how BDL is proving to be an important
    building block in deploying deep networks in the real world.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Illustration of a combined MC dropout and deep ensemble network'
  prefs: []
  type: TYPE_NORMAL
- en: This approach of combining deep ensembles and MC dropout has also been applied
    in other applications. For example, the collision avoidance paper mentioned previously
    by Lütjens *et al.* also uses a combined MC dropout and deep ensemble network.
    This goes to show that it’s not always simply a case of choosing one network over
    another – sometimes combining approaches is key to developing robust, better calibrated
    BDL solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Improving deep ensembles by promoting diversity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw earlier in the chapter, judging by the number of citations, deep ensembles
    are the second most popular of the key BDL techniques covered in this book. As
    such, it’s no surprise that researchers have been investigating methods to improve
    on the standard implementation of deep ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Tim Pearce *et al.*’s paper, *Uncertainty in Neural Networks: Approximately*
    *Bayesian Ensembling*, the authors highlight that the standard deep ensemble approach
    has been criticized for not being Bayesian and argue that the standard approach
    likely lacks diversity in many cases, thus producing a poorly descriptive posterior.
    In other words, deep ensembles often result in over-confident predictions due
    to a lack of diversity in the ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this, the authors propose a method they term **anchored** **ensembling**.
    Anchored ensembling, like deep ensembles, uses an ensemble of NNs. However, it
    uses a specially adapted loss function that penalizes the ensemble members’ parameters
    from drifting too far from their initial values. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lossj = 1-||y − ˆy||2+ -1||Γ 12 × (𝜃j − 𝜃anc,j)||2 N 2 N 2 ](img/file186.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *Loss*[*j*] is the loss computed for the *j*th network in the ensemble.
    We see a familiar loss in the equation in the form of ||**y** −**y**||[2]². Γ
    is a diagonal regularization matrix, and *𝜃*[*j*] are the parameters for the network.
    The key point here is the relationship between *𝜃*[*j*] and the *𝜃*[*anc,j*] variable.
    Here, the *anc* indicates the anchoring from which the method gets its name. These
    parameters, *𝜃*[*anc,j*], are the set of initial parameters for the *j*th network.
    As such (as we see by the multiplication), if this value is large – in other words,
    if *𝜃*[*j*] and *𝜃*[*anc,j*] are significantly different – the loss will increase.
    Thus, this penalizes the networks in the ensemble if they deviate too far from
    their initial values, forcing them to find parameter values that minimize the
    first term in the equation while staying as close to their initial values as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important because, if we use an initialization strategy that is more
    likely to produce a diverse set of initial parameter values, then maintaining
    that diversity will ensure that our ensemble comprises diverse networks after
    training. As the authors demonstrate in the paper, this diversity is key to producing
    principled uncertainty estimates: ensuring that the network predictions converge
    for regions of high data, and diverge in regions of low data, just as we saw in
    our GP examples from [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian*
    *Inference*](CH2.xhtml#x1-250002):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Illustration of principled uncertainty estimates obtained using
    a Gaussian process'
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, here the solid line is the true function, the dots are the samples
    from the function, the dotted line are the mean GP predictions, the faint dotted
    lines are a sample of possible functions, and the shaded area is the uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: In their paper, Pearce *et al.* demonstrate that their anchored ensemble approach
    is able to approximate a descriptive posterior distribution such as this far more
    closely than the standard deep ensemble approach.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Uncertainty in very large networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the core aim topic of the book has been to introduce methods for approximating
    Bayesian inference in DNNs, we haven’t addressed how this is applied to one of
    the most successful NN architecture varieties of recent years: the transformer.
    Transformers – just as more typical deep networks before them – have achieved
    landmark performance in a variety of tasks. While deep networks were already crunching
    large amounts of data, transformers take this to the next level: crunching enormous
    volumes of data, and comprising hundreds of billions of parameters. One of the
    most well-known transformer networks is GPT-3, a transformer developed by OpenAI
    that comprises over 175 billion parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers were first used in **Natural Language Processing** (**NLP**) tasks,
    and demonstrated that, through the use of self-attention and sufficient volumes
    of data, competitive performance can be achieved without the use of recurrent
    neural networks. This was an important step in NN architecture development: demonstrating
    that sequential context can be learned through self-attention and providing architectures
    capable of learning from hitherto unprecedented volumes of data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Illustration of the transformer architecture'
  prefs: []
  type: TYPE_NORMAL
- en: However, just as with the trend of more typical deep networks before them, the
    parameters of transformers are point estimates, rather than distributions, thus
    preventing them from being used for uncertainty quantification. Authors Boyang
    Xue *et al.* sought to remedy this in their paper, *Bayesian Transformer* *Language
    Models for Speech Recognition*. In their work, they demonstrate the variational
    inference can be successfully applied to transformer models, facilitating approximate
    Bayesian inference. However, due to the large size of transformers, Bayesian parameter
    estimation for all parameters is incredibly expensive. As such, Xue *et al.* apply
    Bayesian estimation to a subset of model parameters, specifically the parameters
    in the feed-forward and multi-head self-attention modules. As we see from *Figure*
    [*9.6*](#x1-184003r6), this excludes quite a few layers from the variational sampling
    process, thus saving compute cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Another method proposed in the paper *Transformers Can Do Bayesian* *Inference*,
    by Samuel Müller *et al.*, approximates Bayesian inference by exploiting the large
    amount of data used to train transformers. In their approach, dubbed **Prior-Data
    Fitted Networks (PFNs)**, the authors restate the problem of posterior approximation
    as a supervised learning task. That is to say, rather than obtaining a distribution
    of predictions via sampling, their method learns to approximate the posterior
    predictive distribution directly from dataset samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1: PFN model training procedure   **Input:** A prior distribution
    over datasets *p*(*D*), from which samples can be drawn and the number of samples
    *K* to draw'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** A model *q𝜃* that will approximate the PPD Initialize the neural
    network *q𝜃*'
  prefs: []
  type: TYPE_NORMAL
- en: '**for** i:=1 to 10 **do**- 1:    `Sample` *D* ∪ (*x*[*i*]*,y*[*i*])[*i*=1]^(*m*)
    ≈ *p*(*D*)'
  prefs: []
  type: TYPE_NORMAL
- en: 2:  `Compute stochastic loss approximation` *l*[*𝜃*] = ∑ [*i*=1]^(*m*)(−log
    *q*[*𝜃*](*y*[*i*]|*x*[*i*]*,D*))
  prefs: []
  type: TYPE_NORMAL
- en: 3:  *Update parameters* *𝜃* *with stochastic gradient descent on* ▿[*𝜃*]*l*[*𝜃*]
    =0
  prefs: []
  type: TYPE_NORMAL
- en: As represented in the pseudo code here, during training, the model samples multiple
    subsets of data comprising inputs *x* and labels *y*. It then masks one of the
    labels and learns to make a probabilistic prediction for this label based on the
    other data points. This allows the PFN to do probabilistic inference in a single
    forward pass – similarly to what we saw in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) with
    PBP. While approximating Bayesian inference in a single forward pass is desirable
    for any application, this is even more valuable with transformers given their
    huge numbers of parameters – thus making the PFN approach described here particularly
    attractive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, transformers are popularly used within transfer learning contexts:
    using the rich feature embeddings from transformers as inputs to smaller, less
    computationally demanding networks. As such, perhaps the most obvious way to use
    transformers in a Bayesian context would be to use its embeddings as an input
    to a BDL network – in fact, this is probably the most sensible first step in many
    cases.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve explored some of the recent developments in BDL. All
    of these build on, and apply directly to, the methods we’ve introduced in the
    book, and you may want to consider implementing these when developing your own
    solutions for approximate Bayesian inference with deep networks. However, given
    the pace of research in machine learning, the list of improvements to Bayesian
    approximations is ever-growing, and we encourage you to explore the literature
    for yourself to discover the variety of ways in which researchers are learning
    to implement Bayesian inference at scale and with a variety of computational and
    theoretical advantages. That said, BDL isn’t always the correct solution, and
    in the next section, we’ll explore why.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Alternatives to Bayesian deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the focus of the book is on Bayesian inference with DNNs, these aren’t
    always the best choice for the job. Generally speaking, they’re a great choice
    when you have large amounts of high dimensional data. As we discussed in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003) (and as you probably know),
    deep networks excel in these scenarios, and thus adapting them for Bayesian inference
    is a sensible choice. On the other hand, if you have small amounts of low-dimensional
    data (with tens of features, fewer than 10,000 data points), then you may be better
    off with more traditional, well-principled Bayesian inference, such as via sampling
    or GPs.
  prefs: []
  type: TYPE_NORMAL
- en: That said, there has been interest in scaling GPs, and the research community
    has developed GP-based methods that both scale to large amounts of data and are
    capable of complex non-linear transformations. In this section, we’ll introduce
    these alternatives in case you wish to pursue them further.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 Scalable Gaussian processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the beginning of the book, we introduced GPs and discussed why they are
    the gold standard when it comes to principled and reasonably computationally tractable
    uncertainty quantification in machine learning. Crucially, we talked about the
    limits of GPs: they become computationally infeasible with high-dimensional data
    or large amounts of data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, GPs are extremely powerful tools, and the machine learning community
    wasn’t ready to give up on them. In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian* *Inference*](CH2.xhtml#x1-250002), we discussed the key prohibitive
    factor in GP training and inference: inverting the covariance matrix. While methods
    exist for making this more computationally tractable (such as Cholesky decomposition),
    these methods only get us so far. As such, the key methods for making GPs scalable
    are termed *sparse GPs*, and they looks to solve the problem of intractable GP
    training by modifying the covariance matrix via sparse GP approximation. Simply
    put, if we can shrink or simplify the covariance matrix (for example, by reducing
    the number of data points), we can make the inversion of the covariance matrix
    tractable, and thus make GP training and inference tractable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular approaches for this was introduced in the paper, *Sparse*
    *Gaussian Processes using Pseudo-Inputs* by Edward Snelson and Zoubin Ghahramani.
    As with other sparse GP approaches, the authors developed a method for tractable
    GPs that leverages large datasets. In the paper, the authors show that they can
    closely approximate training with the full dataset by using a subset of data:
    they effectively circumvent the problem of large data by turning a *large data*
    problem into a *small data* problem. However, doing so requires selecting an appropriate
    subset of data points, which the authors term **pseudo** **inputs**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors achieve this using a joint optimization process that selects the
    subset of data *M* from the full set *N* while also optimizing the hyperparameters
    of the kernel. This optimization process essentially finds the subset of data
    points that can be used to best describe the overall data: we illustrate this
    in *Figure* [*9.7*](#x1-186005r7).'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Simple illustration of pseudo inputs'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this diagram, all data points are illustrated, but we see that certain data
    points have been selected as they describe the key relationship between our variables.
    However, these points not only need to describe the relationship, in terms of
    mean, as a polynomial regression may do – they also need to replicate the *variance*
    in the underlying data, such that the GP still produces well-calibrated uncertainty
    estimates. That is to say, while the pseudo inputs effectively reduce the number
    of data points, the distribution of the pseudo inputs still needs to approximate
    that of the true inputs: if an area in the true data distribution is rich in data,
    thereby producing confident predictions in this region, this needs to be true
    for the pseudo inputs too.'
  prefs: []
  type: TYPE_NORMAL
- en: Another method for scalable GPs was introduced more recently by Ke Wang *et*
    *al.* in their paper, *Exact Gaussian Processes on a Million Data Points*. In
    this work, the authors leverage recent developments in multi-GPU parallelization
    methods to implement scalable GPs. The method that enabled this is called **Blackbox
    Matrix-Matrix Multiplication** (**BBMM**), reduces the problem of GP inference
    to iterations of matrix multiplication. In so doing, it makes the process easier
    to parallelize, as the matrix multiplications can be partitioned and distributed
    over multiple GPUs. The authors show that doing so reduces the memory requirement
    for GP training to *O*(*n*) per GPU. This allows GPs to benefit from the kind
    of computational gains that have been benefiting deep learning methods for over
    a decade!
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of the methods presented here do a great job of addressing the scalability
    issues faced by GPs. The second method is particularly impressive as it achieves
    exact GP inference, but it does require significant compute infrastructure. The
    pseudo-inputs method, on the other hand, is practical for a larger proportion
    of use cases. However, neither of these approaches tackle one of the key advantages
    of BDL: the ability of deep networks to learn rich embeddings through complex
    non-linear transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 Deep Gaussian processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Introduced by Andreas Damianou and Neil Lawrence in their straightforwardly
    titled paper, *Deep Gaussian Processes*, deep GPs tackle the problem of rich embeddings
    through having layers of GPs, much in the same way that deep networks have multiple
    layers of neurons. Unlike the scalable GP work mentioned previously, deep GPs
    were motivated by the inverse of the scalability problem: how can we get the performance
    of deep networks with very little data?'
  prefs: []
  type: TYPE_NORMAL
- en: Faced with this problem, and with the knowledge that GPs perform very well on
    small amounts of data, Damianou and Lawrence set out to see whether GPs could
    be layered to produce similarly rich embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Illustration of a deep GP'
  prefs: []
  type: TYPE_NORMAL
- en: 'Their approach, while complex in terms of implementation, is simple in principle:
    just as a DNN comprises many layers, each receiving an input from the layer before
    it and feeding its output into the layer after it, deep GPs also assume this form
    of graphical structure – as we see in *Figure* [*9.8*](#x1-187003r8). Mathematically,
    just as with deep networks, a deep GP can be viewed as a composition of functions.
    The GP illustrated previously could thus be described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![y = g(x ) = f2(f1(x)) ](img/file190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'While this introduces the kind of rich non-linear transformations we’re accustomed
    to in deep learning to GPs, it comes at a price. As we already know, standard
    GPs have limitations when it comes to scalability. Unfortunately for deep GPs,
    composing them in this way is analytically intractable. As such, Damianou and
    Lawrence had to find a tractable way of implementing deep GPs, and they did so
    using a tool that should now be familiar to you: variational approximation. Just
    as this forms an import building block for some of the BDL methods introduced
    in this book, it’s also a key component in making deep GPs possible. In their
    paper, they show how deep GPs can be implemented with the help of variational
    approximations – making it possible not only to produce rich, non-linear embeddings
    with GPs but for rich, non-linear embeddings to be achieved with *small* *amounts
    of data*. This makes deep GPs an important tool in the arsenal of Bayesian methods
    and is thus a method worth bearing in mind going forward.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Your next steps in BDL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve concluded our introduction to BDL by taking a
    look at a variety of techniques that could help you to improve on the fundamental
    methods explored in the book. We’ve also taken a look at how the powerful gold-standard
    of Bayesian inference – the GP – can be adapted to tasks generally reserved for
    deep learning. While it is indeed possible to adapt GPs to these tasks, we also
    advise that it’s generally easier and more practical to use the methods presented
    in this book, or methods derived from them. As always, it’s up to you as the machine
    learning engineer to determine what is best for the task at hand, and we are confident
    that the material from the book will equip you well for the challenges ahead.
  prefs: []
  type: TYPE_NORMAL
- en: While this book provides you with the necessary fundamentals to get started,
    there’s always more learn – particularly in such a rapidly moving field! In the
    next section, we’ll provide a few key final recommendations, helping you to plan
    your next steps in learning about and applying BDL.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that you’ve found this introduction to Bayesian deep learning to be
    comprehensive, practical, and enjoyable. Thank you for reading – we wish you success
    in exploring these methods further and applying them within your own machine learning
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following reading recommendations are provided for those who wish to learn
    more about the recent methods presented in this chapter. These give a great insight
    into current challenges in the field, looking beyond Bayesian neural networks
    and into scalable Bayesian inference more generally:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep Ensemble Bayesian Active Learning*, Pop and Fulop: This paper demonstrates
    the advantages of combining deep ensembles with MC dropout to produce better-calibrated
    uncertainty estimates, as shown when applying their method to active learning
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Uncertainty in Neural Networks: Approximately Bayesian Ensembling*, Pearce
    *et al.*: This paper introduces a simple and effective method for improving the
    performance of deep ensembles. The authors show that by promoting diversity through
    a simple adaptation to the loss function, the ensemble is able to produces better-calibrated
    uncertainty estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sparse Gaussian Processes Using Pseudo-Inputs*, Snelson and Gharamani: This
    paper introduces the concept of pseudo-input-based GPs, introducing a key method
    in scalable GP inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exact Gaussian Processes on a Million Data Points*, Wang *et al.*: An important
    paper demonstrating that Gaussian Processes can benefit from developments in compute
    hardware through the use of BBMM, making exact GP inference possible for big data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Gaussian Processes*, Damianou and Lawrence: Introducing the concept of
    deep GPs, this paper demonstrates how GPs can be used to achieve complex non-linear
    transformations with datasets far smaller than those required for deep learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve selected a few key resources to help you in your next steps into BDL,
    allowing you to dive deeper into the theory and helping you to get the most out
    of the content presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning: A Probabilistic Perspective*, Murphy: Released in 2012,
    this has since become one of the key texts on machine learning, presenting a well-principled
    approach to understanding all of the key methods within machine learning. The
    probabilistic angle of the book makes it a great addition to your collection of
    Bayesian literature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Probabilistic Machine Learning: An Introduction*, Murphy: Another more recent
    Murphy text. Released in 2022, this is another important text providing a detailed
    treatment of probabilistic machine learning (including a section on Bayesian neural
    networks). While there is some overlap between this and Murphy’s previous text,
    both are worth having at your disposal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: Perhaps
    the most important text on Gaussian Processes, this is a hugely valuable text
    when it comes to Bayesian inference. The authors’ detailed explanation of GPs
    will give you a thorough understanding of this important piece of the Bayesian
    puzzle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian Analysis with Python*, Martin: Covering all the fundamentals of Bayesian
    analysis, this title is an excellent piece of foundational literature and will
    help you to dive deeper into the fundamentals of Bayesian inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
