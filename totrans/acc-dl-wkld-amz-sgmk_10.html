<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer075">
<h1 class="chapter-number" id="_idParaDest-157"><a id="_idTextAnchor154"/>10</h1>
<h1 id="_idParaDest-158"><a id="_idTextAnchor155"/>Operationalizing Inference Workloads</h1>
<p>In <a href="B17519_08.xhtml#_idTextAnchor121"><em class="italic">Chapter 8</em></a>, <em class="italic">Considering Hardware for Inference</em>, and <a href="B17519_09.xhtml#_idTextAnchor137"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing Model Servers</em>, we discussed how to engineer your <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) inference workloads on Amazon SageMaker. We also reviewed how to select appropriate hardware for inference workloads, optimize model performance, and tune model servers based on specific use case requirements. In this chapter, we will focus on how to operationalize your DL inference workloads once they have been deployed to test and production environments.</p>
<p>In this chapter, we will start by reviewing advanced model hosting options such as <strong class="bold">multi-model</strong>, <strong class="bold">multi-container</strong>, and <strong class="bold">Serverless Inference</strong> endpoints to optimize your resource utilization and workload costs. Then, we will cover the <strong class="bold">Application Auto Scaling</strong> service for SageMaker, which provides another mechanism to improve resource utilization. Auto Scaling allows you to dynamically match your inference traffic requirements with provisioned inference resources. </p>
<p>After that, we will discuss how to continuously promote models and model versions without this impacting your end users. We will also cover some advanced deployment patterns required for A/B testing and quality assurance of model candidates. For this, we will review SageMaker’s <strong class="bold">Model Variant</strong> and <strong class="bold">Deployment Guardrails</strong> capabilities. </p>
<p>Then, we review how to monitor model and inference data quality using SageMaker <strong class="bold">Model Monitor</strong>. We will close this chapter by discussing how to select an optimal inference workload configuration based on your use case type, its business, and technical requirements.</p>
<p>In this chapter, we will cover the following topics: </p>
<ul>
<li>Managing inference deployments</li>
<li>Monitoring inference workloads</li>
<li>Selecting your workload configuration</li>
</ul>
<p>By the end of this chapter, you will have an understanding and practical skills on how to operationalize SageMaker inference workloads.</p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor156"/>Technical requirements</h1>
<p>In this chapter, we will provide code samples so that you can develop practical skills. The full code examples are available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/</a>.</p>
<p>To follow along with this code, you will need the following:</p>
<ul>
<li>An AWS account and IAM user with permission to manage Amazon SageMaker resources.</li>
<li>A SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible environment established.</li>
<li>Access to GPU training instances in your AWS account. Each example in this chapter will provide the recommended instance types to use. You may need to increase your compute quota for <em class="italic">SageMaker Training Job</em> to have GPU instances enabled. In this case, please follow the instructions at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml</a>.</li>
<li>You will need to install the required Python libraries by running <strong class="source-inline">pip install -r requirements.txt</strong>. The file that contains the required libraries can be found in the <strong class="source-inline">chapter10</strong> directory.</li>
<li>In this chapter, we will provide examples of compiling models for inference, which requires access to specific accelerator types. Please review the instance recommendations as part of the model server examples.</li>
</ul>
<h1 id="_idParaDest-160"><a id="_idTextAnchor157"/>Managing inference deployments</h1>
<p>In <a href="B17519_01.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing Deep Learning with Amazon SageMaker</em>, we discussed that SageMaker provides<a id="_idIndexMarker806"/> several options when it comes to running your inference workloads, depending on your use case’s requirements, as follows:</p>
<ul>
<li><strong class="bold">Real-time endpoints</strong> are designed for inference use cases with low latency requirements. It comes with <a id="_idIndexMarker807"/>certain limitations on payload size (up to 5 MB) and response latency (up to 60 seconds).</li>
<li><strong class="bold">Batch transform jobs</strong> are an option <a id="_idIndexMarker808"/>for processing large-scale batched inference requests in an offline fashion.</li>
<li><strong class="bold">Asynchronous endpoints</strong> allow you to queue and process inference requests in near-real time. It also<a id="_idIndexMarker809"/> has a much higher limit on<a id="_idIndexMarker810"/> inference payload size (up to 1 GB) compared to real-time endpoints.</li>
</ul>
<p>So far in this book, we have covered how to deploy a <strong class="bold">single model</strong> for your inference workload. This is supported by all three inference options listed previously. </p>
<p>However, for real-time endpoints, it’s possible to package and deploy several models and model versions (known as <strong class="bold">production </strong><strong class="bold">variants</strong>) behind a single endpoint. In this section, we will dive <a id="_idIndexMarker811"/>deeper into these model deployment strategies and highlight implementation details, their advantages, and certain limitations.</p>
<p>Additionally, we will review the<a id="_idIndexMarker812"/> recently introduced <strong class="bold">Serverless Inference</strong> endpoints. Like real-time endpoints, serverless endpoints are designed to serve users in real time. However, in the case of serverless endpoints, you will have access to compute resources without the need to choose provision and scale inference instances.</p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor158"/>Considering model deployment options</h2>
<p>In many situations, hosting a single<a id="_idIndexMarker813"/> model behind a dedicated SageMaker real-time endpoint can lead to sub-optimal resource utilization and additional costs that can be avoided. For example, when you need to simultaneously host a fleet of models, each with low resource requirements, hosting each model behind an individual endpoint would be a major avoidable cost. </p>
<p>SageMaker provides a range of model deployment options that can address more complex use cases. In the following subsections, we will discuss their target use cases, advantages, and limitations.</p>
<h3>Multi-model endpoints</h3>
<p>A <strong class="bold">multi-model endpoint</strong> (<strong class="bold">MME</strong>) is a special type of SageMaker model endpoint that allows you to host thousands of models <a id="_idIndexMarker814"/>behind a single<a id="_idIndexMarker815"/> endpoint simultaneously. This type of endpoint is suitable for scenarios for similarly sized models with relatively low resource requirements that can be served from the same inference container. </p>
<p>MMEs and their underlying model servers manage resource allocation, such as unloading infrequently used models and loading requested ones when an instance runs out of memory. This leads to additional inference latency when the user requests a model that is currently not loaded into memory. Hence, MMEs may not be a good fit for scenarios where consistently low latency is required. This additional latency can increase when hosting large models with evenly distributed traffic patterns as this will lead to frequent unloading and loading of models.</p>
<p>To provision an MME, you need to package each model (model artifacts and inference code) into a separate archive and upload it to Amazon S3. Once the MME instance has been provisioned, it is downloaded from the S3 location to the instance disk, which loads the models into instance memory. By default, if the MME runs out of instance disk space and/or instance memory, SageMaker deletes the least recently used models from the local disk and/or unloads models from memory to accommodate the requested models. </p>
<p>The following diagram shows the MME architecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="Figure 10.1 – MME architecture " height="1134" src="image/B17519_10_001.jpg" width="1643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – MME architecture</p>
<p>MMEs are supported by PyTorch and TensorFlow inference containers. You can also automatically scale MMEs in and out to match your inference traffic. MMEs allow you to directly invoke models as <a id="_idIndexMarker816"/>well as inference pipelines comprising several models.</p>
<p>When selecting an instance type and family, consider the following aspects:</p>
<ul>
<li>Instance memory defines how many models can be loaded simultaneously</li>
<li>Instance disk size defines <a id="_idIndexMarker817"/>how many models can be cached locally to avoid expensive download procedures from S3</li>
<li>The number of vCPUs defines how many inference requests can be handled simultaneously</li>
</ul>
<p>Note that GPU-based instances are not supported for MMEs, which limits what model architectures can be served using MMEs within reasonable SLAs.</p>
<p>Now, let’s learn how to implement an MME.</p>
<h4>Implementing an MME</h4>
<p>In this code sample, we <a id="_idIndexMarker818"/>will learn how to deploy two NLP models simultaneously using an MME. One model analyzes the sentiment of German text, while the other analyzes the sentiment of English text. We will use the HuggingFace PyTorch container for this. The full code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb</a>.</p>
<p>For this task, we will use two models, trained to predict the sentiment of English and German texts: <strong class="source-inline">distilbert-base-uncased-finetuned-sst-2-english</strong> and <strong class="source-inline">oliverguhr/german-sentiment-bert</strong>, respectively. Follow these steps:</p>
<ol>
<li>We will start by fetching the models from the HuggingFace Model hub and saving them locally. The following code shows the English model:<p class="source-code">import torch</p><p class="source-code">from transformers <strong class="bold">import</strong> DistilBertTokenizer, DistilBertForSequenceClassification</p><p class="source-code">en_tokenizer = DistilBertTokenizer.from_pretrained(EN_MODEL)</p><p class="source-code">en_model = DistilBertForSequenceClassification.from_pretrained(EN_MODEL)</p><p class="source-code">en_model_path = "models/english_sentiment"</p><p class="source-code">os.makedirs(en_model_path, exist_ok=True)</p><p class="source-code">en_model.save_pretrained(save_directory=en_model_path)</p><p class="source-code">en_tokenizer.save_pretrained(save_directory=en_model_path)</p></li>
</ol>
<p>As a result, the following artifacts are downloaded:</p>
<p class="source-code">('models/english_sentiment/tokenizer_config.json',</p>
<p class="source-code"> 'models/english_sentiment/special_tokens_map.json',</p>
<p class="source-code"> 'models/english_sentiment/vocab.txt',</p>
<p class="source-code"> 'models/english_sentiment/added_tokens.json')</p>
<p>These model artifacts will be added to the model data package later. But first, we need to develop the inference script.</p>
<ol>
<li value="2">An MME has the same requirements as those for the inference scripts of single-model endpoints. The <a id="_idIndexMarker819"/>following code shows the inference script for the English model, which implements the required methods for model loading, inference, and data pre-/post-processing:<p class="source-code">def model_fn(model_dir):</p><p class="source-code">    tokenizer = DistilBertTokenizer.from_pretrained(model_dir)</p><p class="source-code">    model = DistilBertForSequenceClassification.from_pretrained(model_dir)</p><p class="source-code">    return model, tokenizer</p><p class="source-code">def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):</p><p class="source-code">    if content_type == JSON_CONTENT_TYPE:</p><p class="source-code">        input_data = json.loads(serialized_input_data)</p><p class="source-code">        return input_data</p><p class="source-code">    else:</p><p class="source-code">        Exception("Requested unsupported ContentType in Accept: " + content_type)</p><p class="source-code">def predict_fn(input_data, model_tokenizer_tuple):</p><p class="source-code">    model, tokenizer = model_tokenizer_tuple</p><p class="source-code">    inputs = tokenizer(input_data, return_tensors="pt")</p><p class="source-code">    with torch.no_grad():</p><p class="source-code">        logits = model(**inputs).logits</p><p class="source-code">    predicted_class_id = logits.argmax().item()</p><p class="source-code">    predictions = model.config.id2label[predicted_class_id]</p><p class="source-code">    return predictions</p><p class="source-code">def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):</p><p class="source-code">    if accept == JSON_CONTENT_TYPE:</p><p class="source-code">        return json.dumps(prediction_output), accept</p><p class="source-code">    raise Exception("Requested unsupported ContentType in Accept: " + accept)</p></li>
<li>Next, we need to <a id="_idIndexMarker820"/>package the model and inference code for the MME. SageMaker requests a specific directory structure that varies for PyTorch and TensorFlow containers. For PyTorch containers, the model and code should be packaged into a single <strong class="source-inline">tar.gz</strong> archive and have the following structure:<p class="source-code">model.tar.gz/</p><p class="source-code">             |- model.pth # and any other model artifacts</p><p class="source-code">             |- code/</p><p class="source-code">                     |- inference.py</p><p class="source-code">                     |- requirements.txt # optional</p></li>
</ol>
<p>Each model should have a model package. Once the packages have been prepared locally, we need to upload them to Amazon S3 and save the respective URI:</p>
<p class="source-code">en_model_data = sagemaker_session.upload_data('models/english_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)</p>
<p class="source-code">ger_model_data = sagemaker_session.upload_data('models/german_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)</p>
<ol>
<li value="4">Once the data has been uploaded, we need to define the respective serving container and <a id="_idIndexMarker821"/>configure it to be used for the MME. The following code locates the PyTorch container based on the desired runtime configuration and task (inference):<p class="source-code">from sagemaker import image_uris</p><p class="source-code">HF_VERSION = '4.17.0'</p><p class="source-code">PT_VERSION = 'pytorch1.10.2'</p><p class="source-code">pt_container_uri = image_uris.retrieve(framework='huggingface',</p><p class="source-code">                                region=region,</p><p class="source-code">                                version=HF_VERSION,</p><p class="source-code">                                image_scope='inference',</p><p class="source-code">                                base_framework_version=PT_VERSION,</p><p class="source-code">                                instance_type='ml.c5.xlarge')</p></li>
<li>Then, we need to configure the MME parameters. Specifically, we must define the <strong class="source-inline">MultiModel</strong> mode. Note that we provide two specific environment variables – <strong class="source-inline">SAGEMAKER_PROGRAM</strong> and <strong class="source-inline">SAGEMAKER_SUBMIT_DIRECTORY</strong> – so that the SageMaker inference framework knows how to register the model handler:<p class="source-code">container  = {</p><p class="source-code">    'Image': pt_container_uri,</p><p class="source-code">    'ContainerHostname': 'MultiModel',</p><p class="source-code">    'Mode': 'MultiModel',</p><p class="source-code">    'ModelDataUrl': mm_data_path,</p><p class="source-code">    'Environment': {</p><p class="source-code">    'SAGEMAKER_PROGRAM':'inference.py',</p><p class="source-code">    'SAGEMAKER_SUBMIT_DIRECTORY':mm_data_path</p><p class="source-code">    }</p><p class="source-code">}</p></li>
<li>The last step of configuring the MME is to create a SageMaker model instance, endpoint configuration, and the endpoint itself. When creating the model, we must provide the<a id="_idIndexMarker822"/> <strong class="source-inline">MultiModel</strong>-enabled container from the preceding step. We have omitted the creation of the endpoint configuration and endpoint for brevity:<p class="source-code">unique_id = datetime.datetime.now().strftime("%Y-%m-%d%H-%M-%S")</p><p class="source-code">model_name = f"mme-sentiment-model-{unique_id}"</p><p class="source-code">create_model_response = sm_client.create_model(</p><p class="source-code">    ModelName=model_name,</p><p class="source-code">    PrimaryContainer=container,</p><p class="source-code">    ExecutionRoleArn=role,</p><p class="source-code">)</p></li>
<li>Once the endpoint has been created, we can run and invoke our models. For this, in the invocation request, we need to supply a special parameter called <strong class="source-inline">TargetModel</strong>, as follows: <p class="source-code">ger_response = runtime_sm_client.invoke_endpoint(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    ContentType="application/json",</p><p class="source-code">    Accept="application/json",</p><p class="source-code">    TargetModel="german_sentiment.tar.gz",</p><p class="source-code">    Body=json.dumps(ger_input),</p><p class="source-code">)</p></li>
</ol>
<p>While the MME capability provides a convenient way to optimize your inference costs when running multiple similar <a id="_idIndexMarker823"/>models, it requires models to have the same runtime environment (in other words, they must use the same inference container). To address scenarios where you need to host multiple models within different inference containers, SageMaker supports <strong class="bold">multi-container endpoints</strong> (<strong class="bold">MCEs</strong>), as shown in the next section.</p>
<h3>Multi-Container Endpoints</h3>
<p>An MCE allows you to<a id="_idIndexMarker824"/> host up to 15 inference containers simultaneously. In this case, each container would serve its own model. MCEs<a id="_idIndexMarker825"/> are a good fit for use cases where models require different runtime environments/containers but not every single model can fully utilize the available instance resources. Another scenario is when models are called at different times. </p>
<p>Unlike an MME, an MCE doesn’t cache or unload containers based on their invocation patterns. Hence, you need to ensure that the inference containers will collectively have enough resources to run on the endpoint instance. If the instance resources (for example, instance memory) are not enough to run all containers, you may see an error during MCE creation time. Hence, you need to consider the total resource requirements of all the inference containers when choosing an instance configuration. Each inference container will have a proportional amount of resources available for it. The following diagram shows the MCE architecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 10.2 – MCE architecture " height="955" src="image/B17519_10_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – MCE architecture</p>
<p>You can automatically scale an MCE. It supports <strong class="bold">Direct</strong> mode (invoking inference containers directly) or <strong class="bold">Serial</strong> mode (invoking several containers sequentially).</p>
<p>At the time of writing this book, MCEs don’t support GPU-based instances. </p>
<p>Now, let’s learn how to<a id="_idIndexMarker826"/> create an MCE by using a simple example of running TensorFlow and PyTorch models simultaneously. This will give you some practical skills in terms of how to create and use an MCE.</p>
<h4>Implementing an MCE</h4>
<p>In this example, we will run an<a id="_idIndexMarker827"/> inference workload with two NLP models using different runtime environments: TensorFlow and PyTorch. We will host the Q&amp;A model in a TensorFlow container and the text summarization model in a PyTorch container.</p>
<p>Creating an MCE is very similar to creating an MME with a few notable exceptions, which we will highlight in the following steps:</p>
<ol>
<li value="1">Fetching the model data, inference scripts, and model packaging is identical to what we did for the MME. Note that since one of our endpoints will run the TensorFlow<a id="_idIndexMarker828"/> container, the Q&amp;A model should comply with the following directory structure:<p class="source-code">model.tar.gz/</p><p class="source-code">             |--[model_version_number]/</p><p class="source-code">                                       |--variables</p><p class="source-code">                                       |--saved_model.pb</p><p class="source-code">            code/</p><p class="source-code">                |--inference.py</p><p class="source-code">                |--requirements.txt # optional</p></li>
<li>Next, we will configure the container and create the model package. Note that we provide two containers and endpoint mode, <strong class="source-inline">Direct</strong>, while creating the model package:<p class="source-code">model_name = f"mce-nlp-model-{unique_id}"</p><p class="source-code">create_model_response = sm_client.create_model(</p><p class="source-code">    ModelName=model_name,</p><p class="source-code">    Containers=[tensorflow_container, pytorch_container],</p><p class="source-code">    InferenceExecutionConfig={"Mode": "Direct"},</p><p class="source-code">    ExecutionRoleArn=role,</p><p class="source-code">)</p></li>
<li>Then, we will create the endpoint configuration and endpoint. This step is similar to that for the MME, so we have omitted the code snippet for brevity.</li>
<li>Once the endpoint has been deployed, we are ready to send inference traffic. Note that we supply the <strong class="source-inline">TargetContainerHostname</strong> header so that SageMaker knows where to route our inference request:<p class="source-code">tf_response = runtime_sm_client.invoke_endpoint(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    ContentType="application/json",</p><p class="source-code">    Accept="application/json",</p><p class="source-code">    TargetContainerHostname="tensorflow-distilbert-qa",</p><p class="source-code">    Body=json.dumps(qa_inputs),</p><p class="source-code">)</p></li>
</ol>
<p>So far, we have discussed how to host multiple models on SageMaker. Next, we will discuss how to safely<a id="_idIndexMarker829"/> promote a new version of the model (or a different model altogether) while keeping the endpoint operational for end users. For this, we will review SageMaker multi-variant endpoints.</p>
<h3>Multi-variant endpoints</h3>
<p>A production variant is a SageMaker-specific<a id="_idIndexMarker830"/> concept that defines a combination of the model, its container, and the resources required to run this <a id="_idIndexMarker831"/>model. As such, this is an extremely flexible concept that can be used for different use cases, such as the following:</p>
<ul>
<li>Different model versions with the same runtime and resource requirements</li>
<li>Different models with different runtimes and/or resource requirements</li>
<li>The same model with different runtimes and/or resource requirements</li>
</ul>
<p>Additionally, as part of the variant configuration, you also define its traffic weights, which can be then updated without them having any impact on endpoint availability. Once deployed, the production variant can be invoked directly (so you can bypass SageMaker traffic shaping) or as part of the SageMaker endpoint call (then, SageMaker traffic shaping is not bypassed). The following diagram provides more details:</p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 10.3 – Using production variants with traffic shaping (left) and with a direct invocation (right) " height="654" src="image/B17519_10_003.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Using production variants with traffic shaping (left) and with a direct invocation (right)</p>
<p>When updating production variants, the real-time endpoint stays available and no interruption occurs<a id="_idIndexMarker832"/> for the end users. This also means that you will incur additional costs, as each production variant will have an associated cost.</p>
<p>Now, let’s see how we can use production variants to test a new production variant.</p>
<h4>Using production variants for A/B testing</h4>
<p>In this example, we will register two <a id="_idIndexMarker833"/>different models for the same Q&amp;A NLP task. Then, we will shape the inference traffic using the<a id="_idIndexMarker834"/> production variant weights and invoke the models directly. The full code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb</a>. Follow these steps:</p>
<ol>
<li value="1">We will start by creating two HuggingFace models using the <strong class="source-inline">HuggingFaceModel</strong> class. We have omitted this for brevity.</li>
<li>Then, we will create two different endpoint variants. We start with the equal weights parameter, which tells SageMaker that inference traffic should split evenly between model variants:<p class="source-code">from sagemaker.session import production_variant</p><p class="source-code">variant1 = production_variant(</p><p class="source-code">    model_name=model1_name,</p><p class="source-code">    instance_type="ml.c5.4xlarge",</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    variant_name="Variant1",</p><p class="source-code">    initial_weight=1,</p><p class="source-code">)</p><p class="source-code">variant2 = production_variant(</p><p class="source-code">    model_name=model2_name,</p><p class="source-code">    instance_type="ml.c5.4xlarge",</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    variant_name="Variant2",</p><p class="source-code">    initial_weight=1,</p><p class="source-code">)</p></li>
<li>After that, we<a id="_idIndexMarker835"/> create the endpoint based<a id="_idIndexMarker836"/> on our configured production variants: <p class="source-code">from datetime import datetime</p><p class="source-code">endpoint_name = f"ab-testing-{datetime.now():%Y-%m-%d-%H-%M-%S}"</p><p class="source-code">sagemaker_session.endpoint_from_production_variants(</p><p class="source-code">    name=endpoint_name, production_variants=[variant1, variant2]))</p></li>
<li>Once the endpoint has been deployed, we can run inference against the newly created endpoint. Once you run the following code, the resulting statistics should show that <a id="_idIndexMarker837"/>each production variant served ~50% of inference traffic:<p class="source-code">results = {"Variant1": 0, "Variant2": 0, "total_count": 0}</p><p class="source-code">for i in range(20):</p><p class="source-code">    response = sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType="application/json", Body=json.dumps(data))</p><p class="source-code">    results[response['InvokedProductionVariant']] += 1</p><p class="source-code">    results["total_count"] += 1</p></li>
<li>Next, we can update the weights of our endpoint variants. Re-running the previous inference test loop<a id="_idIndexMarker838"/> should now show that only ~10% of traffic is served by <strong class="source-inline">"Variant1"</strong>, which is expected based on the provided variant traffic weights:<p class="source-code">sm_client.update_endpoint_weights_and_capacities(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    DesiredWeightsAndCapacities=[</p><p class="source-code">        {"DesiredWeight": 10, "VariantName": "Variant1"},</p><p class="source-code">        {"DesiredWeight": 90, "VariantName": "Variant2"},])</p></li>
<li>We can also bypass SageMaker traffic shaping and directly invoke a specific variant by using the <strong class="source-inline">TargetVariant</strong> parameter, as follows:<p class="source-code">sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, TargetVariant="Variant2", ContentType="application/json", Body=json.dumps(data))</p></li>
</ol>
<p>SageMaker’s production variants provide you with a flexible mechanism to operate your inference workloads in production or production-like environments. </p>
<h3>Serverless inference endpoints</h3>
<p>Using <strong class="bold">serverless inference endpoints</strong> (<strong class="bold">SIEs</strong>) is another deployment option available on SageMaker. It allows you to provision real-time inference endpoints without the need to provision and configure the underlying endpoint instances. SageMaker <a id="_idIndexMarker839"/>automatically provisions and scales the underlying available compute resources based on your inference traffic. Your <a id="_idIndexMarker840"/>SIE can scale them down to 0 in cases where there is no inference traffic.</p>
<p>SIEs are a good fit for scenarios where there’s an uneven traffic pattern and you can tolerate short periods of elevated latency <a id="_idIndexMarker841"/>during a <strong class="bold">cold start</strong>. A cold start period specifies the time needed to provision new serverless resources and deploy your model runtime environment. Since larger models generally have longer deployment times than smaller ones, they will have longer cold start periods too. One potential use case for Serverless Inference is using it in test and sandbox environments. With an SIE, you pay only for the time that the SIE takes to process the inference request.</p>
<p>Serverless Inference is functionally similar to SageMaker real-time inference. It supports many types of inference containers, including PyTorch and TensorFlow inference containers. However, Serverless Inference also has several limitations, including the following:</p>
<ul>
<li>No GPU resources are available</li>
<li>The instance disk size is 5 GB</li>
<li>The maximum concurrency for the endpoint is 200; requests beyond this limit will be throttled by SageMaker</li>
<li>The cold start period depends on your model size and inference container start time</li>
</ul>
<p>When creating SIE resources, you can choose from the list of available memory options, and SageMaker will automatically assign the proportional number of vCPUs. During the memory configuration, you will need the size of the memory to be at least slightly higher than your model size, and the minimum memory size must be 1,024 MB; the maximum is 6,144 MB. If your model performance is CPU-bound, you may choose a bigger memory configuration to have more vCPU resources.</p>
<p>Now, let’s see how we can deploy a serverless endpoint using the SageMaker Python SDK.</p>
<h4>Deploying a serverless endpoint</h4>
<p>In this example, we will deploy the Q&amp;A NLP model from the HuggingFace Model hub. The full code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb</a>. Follow these steps:</p>
<ol>
<li value="1">We will start by <a id="_idIndexMarker842"/>defining the SageMaker model to deploy. For this, we will fetch a CPU version of the HuggingFace inference container using the built-in <strong class="source-inline">image_uris</strong> method, as follows:<p class="source-code">From sagemaker import image_uris</p><p class="source-code">HF_VERSION = '4.17.0'</p><p class="source-code">PT_VERSION = 'pytorch1.10.2'</p><p class="source-code">hf_container_uri = image_uris.retrieve(framework='huggingface',</p><p class="source-code">                          region=region,</p><p class="source-code">                                version=HF_VERSION,</p><p class="source-code">                                image_scope='inference',</p><p class="source-code">                                base_framework_version=PT_VERSION,</p><p class="source-code">                                instance_type='ml.c5.xlarge')</p></li>
<li>Then, we will use the <strong class="source-inline">HuggingFaceModel</strong> instance to configure the model architecture and target NLP task:<p class="source-code">Hub = {</p><p class="source-code">    'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad',</p><p class="source-code">    'HF_TASK':'question-answering'</p><p class="source-code">}</p><p class="source-code">huggingface_model = HuggingFaceModel(</p><p class="source-code">   env=hub,  </p><p class="source-code">   role= role, </p><p class="source-code">   transformers_version=HF_VERSION,</p><p class="source-code">   pytorch_version=PT_VERSION,     </p><p class="source-code">   image_uri=hf_container_uri,     </p><p class="source-code">)</p></li>
<li>Next, we will define the serverless configuration and deploy our first endpoint. Here, the <strong class="source-inline">memory_size_in_mb</strong> parameter defines the initial memory behind your endpoint and the <strong class="source-inline">max_concurrency</strong> parameter defines the maximum number of <a id="_idIndexMarker843"/>concurrent invocations your endpoint can handle before inference traffic gets throttled by SageMaker:<p class="source-code">Serverless_config = ServerlessInferenceConfig(</p><p class="source-code">    memory_size_in_mb=4096, max_concurrency=10,</p><p class="source-code">)</p><p class="source-code">predictor = huggingface_model.deploy(</p><p class="source-code">    serverless_inference_config=serverless_config</p><p class="source-code">)</p></li>
</ol>
<p>That’s it! In several minutes, your endpoint will be deployed. After that, you can use it as any other real-time endpoint.</p>
<p>With Serverless Inference, SageMaker automatically scales in and out of your endpoints without much input from your side other than the memory sizing and concurrency. In the next section, we will review the endpoint autoscaling capability, which provides you more with fine-grained control over scaling behavior.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor159"/>Advanced model deployment techniques</h2>
<p>In this section, we<a id="_idIndexMarker844"/> will discuss some advanced<a id="_idIndexMarker845"/> techniques for managing your SageMaker inference resources, namely autoscaling and blue/green deployments.</p>
<h3>Autoscaling endpoints</h3>
<p>SageMaker allows you to <a id="_idIndexMarker846"/>automatically scale out (increase the number of instances) and scale in (decrease the number of instances) for real-time endpoints and asynchronous endpoints. When inference traffic increases, scaling out maintains steady endpoint performance while keeping costs to a minimum. When inference traffic decreases, scaling in allows you to minimize the inference costs. For real-time endpoints, the minimum instance size is 1; asynchronous endpoints can scale to 0 instances. The following diagram shows this:</p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 10.4 – Autoscaling concepts " height="511" src="image/B17519_10_004.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Autoscaling concepts</p>
<p>During scaling events, SageMaker endpoints remain fully available for end users. In the case of downsizing an endpoint, SageMaker automatically drains traffic from instances so that they can be removed. To ensure additional resiliency, SageMaker places instances in<a id="_idIndexMarker847"/> different <strong class="bold">availability zones</strong>. </p>
<p>To autoscale your endpoint, you need to create a production variant for your model. After that, you<a id="_idIndexMarker848"/> must define the desired<a id="_idIndexMarker849"/> scaling behavior in the <strong class="bold">autoscaling policy</strong>. SageMaker supports four types of scaling policies, as follows: </p>
<ul>
<li><strong class="bold">Simple scaling</strong> (or <strong class="source-inline">TargetTrackingScaling</strong>) allows you to scale endpoints based on the value of specific Amazon CloudWatch metrics. SageMaker supports<a id="_idIndexMarker850"/> several endpoint metrics out of the box, but<a id="_idIndexMarker851"/> you can also use your own custom metrics. The <strong class="bold">CPUUtilization</strong>, <strong class="bold">GPUUtilization</strong>, and <strong class="bold">SageMakerVariantInvocationsPerInstance</strong> metrics are <a id="_idIndexMarker852"/>usually good<a id="_idIndexMarker853"/> starting choices. </li>
<li><strong class="bold">Step scaling</strong> is a more <a id="_idIndexMarker854"/>advanced scaling policy that allows you to have finer control over how many instances are provisioned based on the size of the metric value change. This policy requires careful configuration and testing with various load profile values.</li>
<li><strong class="bold">Scheduled scaling</strong> allows you to <a id="_idIndexMarker855"/>scale endpoints based on a predefined schedule. For instance, you can scale in after hours, and scale out during peak work hours.</li>
<li><strong class="bold">On-Demand scaling</strong> changes the<a id="_idIndexMarker856"/> endpoint instance count based on explicit user requests.</li>
</ul>
<p>When selecting and configuring autoscaling policies, you may start by analyzing your traffic patterns and how they correlate with your endpoint metrics. Load profiles define which type of scaling policy to choose, while correlating to endpoint metrics allows you to select good tracking metrics. It’s recommended that you start with a simple baseline (for example, simple scaling with the CPUUtilization tracking metric). Then, you can fine-tune it over time as you observe other traffic patterns and how autoscaling reacts to them.</p>
<p>In the following example, we will learn how to apply autoscaling policies to a SageMaker real-time endpoint.</p>
<h4>Implementing autoscaling for inference endpoints</h4>
<p>In this example, we will learn <a id="_idIndexMarker857"/>how to apply the target tracking autoscaling policy to a real-time endpoint. The full code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb</a>. Follow these steps:</p>
<ol>
<li value="1">We will start by creating a regular SageMaker real-time endpoint. We have omitted this code for brevity.</li>
<li>Next, we will create two <a id="_idIndexMarker858"/>autoscaling resources: a <strong class="bold">scalable target</strong> and a <strong class="bold">scaling policy</strong>. The<a id="_idIndexMarker859"/> scalable target defines a specific AWS resource that we want to scale using the Application Auto Scaling service.</li>
</ol>
<p>In the following code snippet, we are instantiating the client for the Application Auto Scaling service and registering our SageMaker endpoint as a scalable target. Note that the <strong class="source-inline">ResourceId</strong> parameter defines a reference to a specific endpoint and production variant. The <strong class="source-inline">ScalableDimension</strong> parameter for SageMaker resources always references the number of instances behind the production variant. <strong class="source-inline">MinCapacity</strong> and <strong class="source-inline">MaxCapacity</strong> define the instance scaling range:</p>
<p class="source-code">import boto3 </p>
<p class="source-code">as_client = boto3.client('application-autoscaling') </p>
<p class="source-code">resource_id=f"endpoint/{predictor.endpoint_name}/variant/AllTraffic"</p>
<p class="source-code">policy_name = f'Request-ScalingPolicy-{predictor.endpoint_name}'</p>
<p class="source-code">scalable_dimension = 'sagemaker:variant:DesiredInstanceCount'</p>
<p class="source-code"># scaling configuration</p>
<p class="source-code">response = as_client.register_scalable_target(</p>
<p class="source-code">    ServiceNamespace='sagemaker', #</p>
<p class="source-code">    ResourceId=resource_id,</p>
<p class="source-code">    ScalableDimension='sagemaker:variant:DesiredInstance Count', </p>
<p class="source-code">    MinCapacity=1,</p>
<p class="source-code">    MaxCapacity=4</p>
<p class="source-code">)</p>
<ol>
<li value="3">Next, we will create a policy <a id="_idIndexMarker860"/>for our scalable target. Here, we chose to use the target tracking policy type with the following parameters:<p class="source-code">response = as_client.put_scaling_policy(</p><p class="source-code">    PolicyName=policy_name,</p><p class="source-code">    ServiceNamespace='sagemaker',</p><p class="source-code">    ResourceId=resource_id,</p><p class="source-code">    ScalableDimension=scalable_dimension,</p><p class="source-code">    PolicyType='TargetTrackingScaling',</p><p class="source-code">    TargetTrackingScalingPolicyConfiguration={</p><p class="source-code">        'TargetValue': 10.0, # Threshold</p><p class="source-code">        'PredefinedMetricSpecification': {</p><p class="source-code">            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',</p><p class="source-code">        },</p><p class="source-code">        'ScaleInCooldown': 300, # duration until scale in</p><p class="source-code">        'ScaleOutCooldown': 60 # duration between scale out</p><p class="source-code">    }</p><p class="source-code">)</p></li>
<li>Once the policy is in place, we can test it. For this, we need to generate sufficient inference traffic to breach the target metric value for a duration longer than the scale-out cooldown period. For this purpose, we can use the Locust.io load<a id="_idIndexMarker861"/> testing framework (<a href="https://locust.io/">https://locust.io/</a>), which provides a simple mechanism to mimic various load patterns. Follow the instructions in the notebook to<a id="_idIndexMarker862"/> create a Locust configuration for your endpoint and provide your AWS credentials for authorization purposes. </li>
<li>Once the configuration is complete, you can start your Locust client to generate load using the following terminal command. It generates an inference load of up to 20 concurrent users for 5 minutes. This load profile should trigger a scaling-out event for our endpoint:<p class="source-code">locust -f ../utils/load_testing/locustfile.py --headless -u 20 -r 1 --run-time 5m</p></li>
<li>During the load test, you can observe your endpoint status as well as the associated scaling alerts in the Amazon CloudWatch console. First, you can see that scale-out and scale-in alerts have been configured based on the provided cooldown periods and target metric value:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 10.5 – Autoscaling alerts for SageMaker endpoints " height="229" src="image/B17519_10_005.jpg" width="1050"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Autoscaling alerts for SageMaker endpoints</p>
<ol>
<li value="7">After the initial scale-out cooldown period has passed, the scale-out alert switches to the <strong class="bold">In alarm</strong> state, which causes the endpoint to scale out. Note that in the following screenshot, the red line is the desired value of the tracking metric, while the blue line is the number of invocations per endpoint instance:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 10.6 – Triggered a scaling-out alert " height="388" src="image/B17519_10_006.jpg" width="653"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Triggered a scaling-out alert</p>
<ol>
<li value="8">After triggering scaling out, your endpoint status will change from <strong class="source-inline">in Service</strong> to <strong class="source-inline">Updating</strong>. Now, we can run the <strong class="source-inline">describe_endpoint()</strong> method to confirm that the number of instances has been increased. Since we are generating<a id="_idIndexMarker863"/> a sufficiently large concurrent load in a short period, SageMaker immediately scaled our endpoint to the maximum number of instances. The following code is for the <strong class="source-inline">describe_endpoint()</strong> method:<p class="source-code">  ...</p><p class="source-code">  "ProductionVariants": [</p><p class="source-code">    {</p><p class="source-code">      "VariantName": "AllTraffic",</p><p class="source-code">      ...</p><p class="source-code">      ],</p><p class="source-code">      "CurrentWeight": 1,</p><p class="source-code">      "DesiredWeight": 1,</p><p class="source-code">      "CurrentInstanceCount": 1,</p><p class="source-code">      "DesiredInstanceCount": 4</p><p class="source-code">    }</p><p class="source-code">  ]</p><p class="source-code">  'EndpointStatus': 'Updating'</p><p class="source-code">  ...</p></li>
</ol>
<p>Since we are no longer running an inference traffic generator, we should expect our endpoint to scale in once the scale-in cooldown period has passed.</p>
<p>In the next section, we <a id="_idIndexMarker864"/>will review how to securely and reliably deploy model candidates using SageMaker Deployment Guardrails.</p>
<h3>Using blue/green deployment patterns</h3>
<p>So far, we have discussed how to <a id="_idIndexMarker865"/>deploy and update SageMaker endpoints via APIs or SDK calls. However, this approach may not fit when you’re updating mission-critical workloads in <a id="_idIndexMarker866"/>production, where you need to have additional checks to ensure smooth production rollout.</p>
<p>SageMaker <strong class="bold">Deployment Guardrails</strong> is a fully managed endpoint promotion mechanism. Guardrails follows the blue/green deployment<a id="_idIndexMarker867"/> concept, which is common for DevOps practices. Here, the blue fleet is the old deployment (the production variant in the case of SageMaker endpoints), while the green fleet is the new version to be deployed. SageMaker provisions a green fleet next to the blue fleet. Once the green fleet is ready and healthy, SageMaker starts shifting traffic according to the predefined rules from the blue fleet to the green fleet. </p>
<p>Deployment Guardrails supports several modes of traffic shifting:</p>
<ul>
<li><strong class="bold">All at once</strong> mode shifts all traffic from blue to green in one step once the green fleet is up and healthy. At this point, SageMaker decommissions the blue fleet. </li>
<li><strong class="bold">Canary</strong> mode shifts a small portion of traffic to the green fleet. Then, if the canaries are healthy, SageMaker shifts the remainder of the traffic to the green fleet. After that, SageMaker decommissions the blue fleet.</li>
<li><strong class="bold">Linear</strong> mode gradually shifts the traffic from the blue fleet to the green fleet.</li>
</ul>
<p>Note that during a blue-green <a id="_idIndexMarker868"/>deployment, you will incur costs for both the blue and green fleets while they are running. If, during the rollout, the green fleet becomes unhealthy, SageMaker will execute an automatic rollback to the initial deployment to avoid any impact on the end user experience.</p>
<p>Deployment Guardrails doesn’t support the following features:</p>
<ul>
<li>Marketplace containers</li>
<li>Multi-container endpoints</li>
<li>Multi-model endpoints</li>
<li>Multi-variant endpoints</li>
<li>Endpoints that use Inferentia-based instances</li>
<li>Endpoints that use Amazon SageMaker Model Monitor (with data capture enabled)</li>
</ul>
<p>Practicing setting up <a id="_idIndexMarker869"/>deployment guardrails is outside the scope of this book, as these types of tasks are typically performed by dedicated DevOps/MLOps teams. However, it’s important to understand that SageMaker supports such capabilities out of the box.</p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor160"/>Monitoring inference workloads</h1>
<p>In this section, we will cover the available <a id="_idIndexMarker870"/>mechanisms for monitoring inference workloads.</p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor161"/>Using Amazon CloudWatch </h2>
<p>Throughout this<a id="_idIndexMarker871"/> book, we have frequently referenced Amazon CloudWatch. SageMaker relies on it for all monitoring needs, specifically the following:</p>
<ul>
<li>Uses CloudWatch Logs to collect, organize, and manage SageMaker logs (for example, your model server logs)</li>
<li>Uses CloudWatch Metrics to measure endpoint characteristics such as latency, resource utilization, and others</li>
<li>Uses CloudWatch alarms to trigger autoscaling events</li>
</ul>
<p>SageMaker inference workloads support several metrics out of the box. Depending on the chosen inference workload option and deployment pattern, your default SageMaker metrics may vary. For instance, for an MME, you will have additional default metrics to measure some specific characteristics, such as the model’s performance and loading time. We recommend that you refer to the SageMaker documentation for the most up-to-date information <a id="_idIndexMarker872"/>on default SageMaker metrics: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml</a>.</p>
<p>If, for some reason, the out-of-the-box metrics are not sufficient for your use case, you can always create custom metrics. Some scenarios where custom metrics can be useful are as follows:</p>
<ul>
<li>Your model and model server require custom metrics for appropriate scaling.</li>
<li>You need a higher resolution of metrics. Note that SageMaker default metrics to a 1-second resolution.</li>
<li>You need to do custom metrics pre-processing. For instance, you may need to apply a sliding window average that’s not supported by CloudWatch.</li>
</ul>
<p>You can also create custom CloudWatch alarms. Note that you can create alarms for both metrics and logs. CloudWatch alarms can be used to notify you about specific events via email or text notifications (this will require integrating your alarms with the Amazon SNS service). </p>
<p>Another popular use case for CloudWatch alarms is to perform actions once an alarm is triggered. We have already seen how CloudWatch alarms are used to scale your SageMaker endpoint in<a id="_idIndexMarker873"/> and out. However, you can use alarms for any other custom logic. For example, you may integrate your custom alarm with an Amazon Lambda serverless function. Your function and its custom logic (for example, an endpoint update action) will be executed once the alarms is triggered.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor162"/>Monitoring inference workload quality</h2>
<p>SageMaker Model Monitor is<a id="_idIndexMarker874"/> a purpose-built capability for measuring and continuously monitoring the quality of your inference. It allows you to calculate baseline statistics for your inference inputs and model outputs and then monitor how your models perform against baseline statistics in near-real time. In the case of significant deviations from the predefined statistical constraints, SageMaker Model Monitor will generate an alert to notify you that your model may be not performing according to the desired quality metrics.</p>
<p>Model Monitor comprises several components for monitoring different aspects of your inference quality:</p>
<ul>
<li><strong class="bold">Data quality monitoring</strong> allows you to detect <strong class="bold">data drift</strong> between data used to train your model <a id="_idIndexMarker875"/>and real inference traffic against the deployed model. Data drift usually results in a lower-than-expected quality of your model predictions. To detect data drift, Model Monitor calculates statistics for the training data (baseline), captures the inference traffic, and continuously compares these statistics to the baseline.</li>
<li><strong class="bold">Model quality monitoring</strong> allows you to compare your model predictions to the predefined ground truth<a id="_idIndexMarker876"/> labels. If your model predictions violate the ground truth predictions by predefined constraints, Model Monitor will generate an alert.</li>
<li><strong class="bold">Bias drift monitoring</strong> allows you to detect bias in your model predictions and how it changes over time. Model bias can be introduced when inference traffic is different from the <a id="_idIndexMarker877"/>data used for model training. To detect bias, Model Monitor <a id="_idIndexMarker878"/>calculates a specific bias metric called <strong class="bold">Difference in Positive Proportions in Predicted Label</strong> (<strong class="bold">DPPL</strong>). When the<a id="_idIndexMarker879"/> DPPL metric violates a predefined range of values, an alert is generated.</li>
<li><strong class="bold">Model feature attribution monitoring</strong> is another way to ensure that new bias is not introduced <a id="_idIndexMarker880"/>during model deployment. Feature attribution drift means that the influence that a specific feature has over an inference result changes over time.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Model Monitor only supports tabular data as inference inputs. This limits its applicability to DL inference since in most cases, DL models are used to perform inference on unstructured data, such as images or text.</p>
<p> There are several scenarios where Model Monitor can apply to DL inference:</p>
<ul>
<li>If you are using DL models to run classification or regression inference tasks. In practice, this rarely happens since classical <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) algorithms (for example, XGBoost) often <a id="_idIndexMarker881"/>outperform DL models on such tasks and require a fraction of the resources for training and inference compared to more expensive DL models.</li>
<li>If your inference input can be converted from an unstructured format into a structured format before it’s sent to the SageMaker inference resource – for example, if you convert your unstructured text into a tokenized input and send it for inference. In this case, the tokenized input can be represented as a tabular dataset so that it can be used with Model Monitor.</li>
</ul>
<p>Note that you can still use Model Monitor to ensure your model accuracy with DL workloads for scenarios where your model has either classification or regression outputs.</p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor163"/>Selecting your workload configuration</h1>
<p>In the previous three chapters, we<a id="_idIndexMarker882"/> reviewed the different capabilities Amazon SageMaker provides to engineer and operate inference workloads: from selecting optimal compute instances and runtime environments to configuring model servers and managing and monitoring deployed models.</p>
<p>In this section, we will summarize various selection criteria that you can use when selecting inference workload configurations. Then, we will suggest a simple algorithm that will guide the decision-making process when you’re choosing your inference configuration.</p>
<p>When engineering your inference workload, you may consider the following selection criteria:</p>
<ul>
<li><strong class="bold">Business use case</strong>: This allows you to understand your business opportunity and end user experience by using <a id="_idIndexMarker883"/>your inference service. Analyzing your use case drives important decisions such as selecting the right SageMaker inference option and end user SLAs.</li>
<li><strong class="bold">Inference SLAs</strong>: We have discussed two key inference SLAs in this book: latency and throughput. Understanding<a id="_idIndexMarker884"/> the desired SLAs drives decisions such as which instance type to use, model server configuration, and others.</li>
<li><strong class="bold">Budget and cost</strong>: It’s important to forecast both the inference budget and the setup mechanisms to monitor<a id="_idIndexMarker885"/> for budget usage (the running cost of inference). In the case of a budget overrun, you may want to have a mechanism to react to such an event (for example, sending a notification, scaling down an endpoint, and so on).</li>
<li><strong class="bold">Compute instances</strong>: When you choose compute instances, you need to consider multiple factors, such as which model architecture you intend to use, your SLAs, and others. The process <a id="_idIndexMarker886"/>of selecting an instance type is called rightsizing and requires load tests and benchmarking to be performed.</li>
<li><strong class="bold">Input data and inference traffic</strong>: You need to understand your data size (for offline inference) and inference<a id="_idIndexMarker887"/> traffic patterns (for online inference). For instance, if your traffic is seasonality patterns, you may be able to use endpoint autoscaling to minimize your inference costs. </li>
<li><strong class="bold">Model runtime and deployment</strong>: Depending on your model characteristics, inference traffic <a id="_idIndexMarker888"/>patterns, and chosen compute instances, you<a id="_idIndexMarker889"/> need to choose specific SageMaker containers and model packaging configurations (single model versus several models behind an endpoint). Another aspect to explore is the model promotion strategy and quality assurance in productions. For instance, earlier in this chapter, we discussed how to organize A/B testing on live SageMaker endpoints using production variants.</li>
</ul>
<p>The following table highlights the key characteristics of the available SageMaker inference options:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">Real-Time Inference</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Batch Transform</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Asynchronous Inference</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Serverless Inference</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Inference Type</p>
</td>
<td class="No-Table-Style">
<p>Online </p>
<p>(real-time response).</p>
</td>
<td class="No-Table-Style">
<p>Offline.</p>
</td>
<td class="No-Table-Style">
<p>Online </p>
<p>(near-real-time inference, cold start during scale out from 0).</p>
</td>
<td class="No-Table-Style">
<p>Online</p>
<p>(cold start during scale out from 0).</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Resource Scaling </p>
</td>
<td class="No-Table-Style">
<p>1 to hundreds of instances behind a single endpoint.</p>
</td>
<td class="No-Table-Style">
<p>1 to hundreds of instances in one inference job.</p>
</td>
<td class="No-Table-Style">
<p>0 to hundreds of instances.</p>
</td>
<td class="No-Table-Style">
<p>0 to 200 concurrent inference requests.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Payload Size</p>
</td>
<td class="No-Table-Style">
<p>Up to 6 MB.</p>
</td>
<td class="No-Table-Style">
<p>Up to 100 MB.</p>
</td>
<td class="No-Table-Style">
<p>Up to 1 GB.</p>
</td>
<td class="No-Table-Style">
<p>Up to 4 MB.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Inference Timeout</p>
</td>
<td class="No-Table-Style">
<p>60 seconds.</p>
</td>
<td class="No-Table-Style">
<p>No.</p>
</td>
<td class="No-Table-Style">
<p>Up to 15 minutes.</p>
</td>
<td class="No-Table-Style">
<p>60 seconds.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Multi-Model/Multi-Container Support</p>
</td>
<td class="No-Table-Style">
<p>Yes.</p>
</td>
<td class="No-Table-Style">
<p>No.</p>
</td>
<td class="No-Table-Style">
<p>No.</p>
</td>
<td class="No-Table-Style">
<p>No.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Target Use Case</p>
</td>
<td class="No-Table-Style">
<p>When you need to have consistent real-time inference latency. Supports a wide range of compute instances and model servers.</p>
</td>
<td class="No-Table-Style">
<p>Offline inference or processing when the input dataset is available upfront.</p>
</td>
<td class="No-Table-Style">
<p>When you need to handle larger payload sizes and/or processing times and additional inference latency is acceptable. There is a cost-saving opportunity to scale to 0 when there is no inference traffic.</p>
</td>
<td class="No-Table-Style">
<p>When you need to have real-time inference with the lowest management overhead and associated costs. Pay only for served inference requests. You can scale to 0.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Comparing SageMaker inference options</p>
<p>In the following diagram, we <a id="_idIndexMarker890"/>have organized several decision points you need to be aware of when selecting your inference workload implementation:</p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 10.8 – Selection algorithm for inference options " height="838" src="image/B17519_10_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Selection algorithm for inference options</p>
<p>Note that your workload configuration is not static. Some non-extensive examples that may require you to reconsider your workload configuration choices include the following:</p>
<ul>
<li>Traffic pattern changes may result in changes in your scaling policies</li>
<li>Changes in user SLAs may result in changes in the selected compute instances and/or updates in scaling policies</li>
<li>New versions of the model architecture and/or available compute instances may require benchmarking against the baseline to measure potential accuracy or performance gains</li>
</ul>
<p>Hence, you should <a id="_idIndexMarker891"/>plan and budget for continuous monitoring and workload optimizations as part of your initial workload design.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor164"/>Using SageMaker Inference Recommender</h2>
<p>Choosing an optimal inference<a id="_idIndexMarker892"/> configuration requires considerable engineering and testing efforts. To simplify this process, AWS recently introduced <strong class="bold">SageMaker Inference Recommender</strong>, which provides you with a simple way to assess your inference performance and costs for real-time endpoints in different configurations. </p>
<p>Inference Recommender deploys your model to real-time endpoints with different configurations, runs load testing against those endpoints, and then provides latency and throughput measures, as well as associated costs. Based on the generated measures, you can select the most appropriate configuration based on your SLAs and cost budget. SageMaker Inference <a id="_idIndexMarker893"/>Recommender provides the following benchmarks:</p>
<ul>
<li>End-to-end <strong class="bold">model latency</strong> in milliseconds</li>
<li><strong class="bold">Maximum invocations</strong> per minute</li>
<li><strong class="bold">Cost per hour</strong> and <strong class="bold">cost per inference</strong></li>
</ul>
<p>SageMaker Inference Recommender is well suited for the following use cases:</p>
<ul>
<li>Finding the optimal instance type. Note that you can either provide your own list of instance types <a id="_idIndexMarker894"/>you are interested in benchmarking or let SageMaker benchmark this list across all supported instances.</li>
<li>Benchmarking compiled by SageMaker Neo models. Here, you can compare the performance of your original model to the performance of the compiled model variant.</li>
<li>Running a custom load test. Inference Recommender supports modeling different traffic patterns to benchmark your endpoint performance under different conditions. Hence, you can use SageMaker Inference Recommender to benchmark and fine-tune your model server configurations, different model versions, and more.</li>
</ul>
<p>Note that at the time of writing, Inference Recommender only supports real-time endpoints. So, if you need to<a id="_idIndexMarker895"/> benchmark different inference options (for instance, Serverless Inference), you may need to use the custom benchmarking and load testing facilities. Also, benchmark statistics by Inference Recommender  as well as the supported traffic patterns are limited.</p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor165"/>Summary</h1>
<p>In this chapter, we discussed how to operationalize and optimize your inference workloads. We covered various inference options offered by Amazon SageMaker and model hosting options, such as multi-model, multi-container, and Serverless Inference. Then, we reviewed how to promote and test model candidates using the Production Variant capability. </p>
<p>After that, we provided a high-level overview of advanced model deployment strategies using SageMaker Deployment Guardrails, as well as workload monitoring using the Amazon CloudWatch service and SageMaker’s Model Monitor capability. Finally, we summarized the key selection criteria and algorithms you should use when defining your inference workload configuration.</p>
</div>
</div></body></html>