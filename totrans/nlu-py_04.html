<html><head></head><body>
		<div id="_idContainer033">
			<h1 id="_idParaDest-69" class="chapter-number"><a id="_idTextAnchor085"/>4</h1>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor086"/>Selecting Libraries and Tools for Natural Language Understanding</h1>
			<p><a id="_idTextAnchor087"/><a id="_idTextAnchor088"/>This chapter will get you set up to process natural language. We will begin by discussing how to install Python, and then we will discuss general software development tools such as JupyterLab and GitHub. We will also review major Python <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>)  libraries, including the <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>), <strong class="bold">spaCy</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">TensorFlow/Keras</strong></span><span class="No-Break">.</span></p>
			<p><strong class="bold">Natural language understanding</strong> (<strong class="bold">NLU</strong>) technology has benefited from a wide assortment of very capable, freely available tools. While these tools are very powerful, there is no one library that can do all of the NLP tasks needed for all applications, so it is important to understand what the strengths of the different libraries are and how to <span class="No-Break">combine them.</span></p>
			<p>Making the best use of these tools will greatly accelerate any NLU development project. These tools include the Python language itself, development tools such as JupyterLab, and a number of specific natural language libraries that can perform many NLU tasks. It is equally important to know that because these tools are widely used by many developers, active online communities such as Stack Overflow (<a href="https://stackoverflow.com/">https://stackoverflow.com/</a>) have developed. These are great resources for getting answers to specific <span class="No-Break">technical questions.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Installing Python</span></li>
				<li>Developing software—JupyterLab <span class="No-Break">and GitHub</span></li>
				<li>Exploring <span class="No-Break">the libraries</span></li>
				<li>Looking at <span class="No-Break">an example</span></li>
			</ul>
			<p>Since there are many online resources for using tools such as Python, JupyterLab, and GitHub, we will only briefly outline their usage here in order to be able to spend more time <span class="No-Break">on NLP.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">For simplicity, we will illustrate the installation of the libraries in the base system. However, you may wish to install the libraries in a virtual environment, especially if you are working on several different Python projects. The following link may be helpful for installing a virtual <span class="No-Break">environment: </span><a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/"><span class="No-Break">https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor089"/>Technical requirements</h1>
			<p>To run the examples in this chapter, you will need the <span class="No-Break">following software:</span></p>
			<ul>
				<li><span class="No-Break">Python 3</span></li>
				<li><strong class="source-inline">pip</strong> or <strong class="source-inline">conda</strong> (<span class="No-Break">preferably </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">JupyterLab</span></li>
				<li><span class="No-Break">NLTK</span></li>
				<li><span class="No-Break">spaCy</span></li>
				<li><span class="No-Break">Keras</span></li>
			</ul>
			<p>The next sections will go over the process of installing these packages, which should be installed in the order in which they are <span class="No-Break">listed here.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor090"/>Installing Python</h1>
			<p>The first step in setting up your <a id="_idIndexMarker196"/>development environment is to install Python. If you have already installed Python on your system, you can skip to the next section, but do make sure that your Python installation includes Python 3, which is required by most NLP libraries. You can check your Python version by entering the following command in a command-line window, and the version will <span class="No-Break">be displayed:</span></p>
			<pre class="source-code">
 $ python --version</pre>
			<p>Note that if you have both Python 2 and Python 3 installed, you may have to run the <strong class="source-inline">python3 –version</strong> command to check the Python 3 version. If you don’t have Python 3, you’ll need to install it. Some NLP libraries require not just Python 3 but Python 3.7 or greater, so if your version of Python is older than 3.7, you’ll need to <span class="No-Break">update it.</span></p>
			<p>Python runs on almost any operating system that you choose to use, including Windows, macOS, and Linux. Python<a id="_idIndexMarker197"/> can be downloaded for your operating system from <a href="http://www.python.org">http://www.python.org</a>. Download<a id="_idIndexMarker198"/> the executable installer for your operating system and run the installer. When Python is installed, you can check the installation by running the preceding command on your command line or terminal. You will see the version you’ve just installed, as shown in the following <span class="No-Break">command-line output:</span></p>
			<pre class="source-code">
$ python --version
Python 3.8.5</pre>
			<p>This installs Python, but you will also need to install add-on libraries for NLP. Installing libraries is done with the auxiliary programs <strong class="source-inline">pip</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">conda</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">pip</strong> and <strong class="source-inline">conda</strong> are two cross-platform tools that can be used for installing Python libraries. We will be using them to install several important natural language and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) libraries. We will primarily use <strong class="source-inline">pip</strong> in this book, but you can also use <strong class="source-inline">conda</strong> if it’s your preferred <a id="_idIndexMarker199"/>Python management tool. <strong class="source-inline">pip</strong> is included by default with Python versions 3.4 and newer, and since you’ll need 3.7 for the NLP libraries, <strong class="source-inline">pip</strong> should be available in your Python environment. You can check the version with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ pip --version</pre>
			<p>You should see the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
$ pip 21.3.1 from c:\&lt;installation dir&gt;\pip (python 3.9)</pre>
			<p>In the next section, we will discuss the development environment we will be <span class="No-Break">using: JupyterLab.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor091"/>Developing software – JupyterLab and GitHub</h1>
			<p>The development environment can make all the difference in the efficiency of the development process. In this section, we will discuss two popular development resources: JupyterLab and GitHub. If you are familiar with other Python <strong class="bold">interactive development environments</strong> (<strong class="bold">IDEs</strong>), then<a id="_idIndexMarker200"/> you can go ahead and use the tools that you’re familiar with. However, the examples discussed in this book will be shown in a <span class="No-Break">JupyterLab environment.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor092"/>JupyterLab</h2>
			<p>JupyterLab is a cross-platform <a id="_idIndexMarker201"/>coding environment that makes it easy to experiment with different tools and techniques without requiring a lot of setup time. It operates in a browser environment but doesn’t require a cloud server—a local server <span class="No-Break">is sufficient.</span></p>
			<p>Installing JupyterLab is done with the following <span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ pip install jupyterlab</pre>
			<p>Once JupyterLab is installed, you can run it using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ jupyter lab</pre>
			<p>This command should be run in a command line in the directory where you would like to keep your code. The command will launch a local server, and the Jupyter environment will appear in a browser window, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B19005_04_01.jpg" alt="Figure 4.1 – JupyterLab user interface on startup"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – JupyterLab user interface on startup</p>
			<p>The environment shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> includes three types of content, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Notebook</strong>—Contains your <span class="No-Break">coding projects</span></li>
				<li><strong class="bold">Console</strong>—Gives <a id="_idIndexMarker202"/>you access to command-line or terminal functions from directly within the <span class="No-Break">Jupyter notebook</span></li>
				<li><strong class="bold">Other</strong>—Refers to other types of files that may be included in the local directory where the <strong class="source-inline">start</strong> command <span class="No-Break">was run</span></li>
			</ul>
			<p>When you click on the <strong class="bold">Python 3</strong> icon under <strong class="bold">Notebook</strong>, you’ll get a new notebook showing a coding cell, and you’ll be ready to start coding in Python. We’ll return to the JupyterLab environment and start coding in Python in the <em class="italic">Looking at an example</em> section later in this chapter and again in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor093"/>GitHub</h2>
			<p>Many of you are probably already familiar with GitHub, a popular open source code repository system (<a href="https://github.com">https://github.com</a>). GitHub <a id="_idIndexMarker203"/>provides very extensive capabilities for storing and sharing code, developing <a id="_idIndexMarker204"/>code branches, and documenting code. The core features of GitHub are <span class="No-Break">currently free.</span></p>
			<p>The code examples used in this book can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python"><span class="No-Break">https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python</span></a><span class="No-Break">.</span></p>
			<p>The next step is to learn about several important libraries, including NLTK, spaCy, and Keras, which we will be using extensively in the <span class="No-Break">following chapters.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor094"/>Exploring the libraries</h1>
			<p>In this section, we will review several of the major Python libraries that are used in NLP; specifically, NLTK, spaCy, and Keras. These are very useful libraries, and they can perform most basic NLP tasks. However, as <a id="_idIndexMarker205"/>you gain experience with NLP, you will also find additional NLP libraries that may be appropriate for specific tasks as well, and you are encouraged to <span class="No-Break">explore those.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor095"/>Using NLTK</h2>
			<p>NLTK (<a href="https://www.nltk.org/">https://www.nltk.org/</a>) is a <a id="_idIndexMarker206"/>very popular open source Python library that greatly reduces the effort involved in developing<a id="_idIndexMarker207"/> natural language applications by providing<a id="_idIndexMarker208"/> support for many frequently performed tasks. NLTK also includes many corpora (sets of ready-to-use natural language texts) that can be used for exploring NLP problems and <span class="No-Break">testing algorithms.</span></p>
			<p>In this section, we will go over what NLTK can do, and then discuss the NLTK <span class="No-Break">installation process.</span></p>
			<p>As we discussed in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, many distinct tasks can be performed in an NLU pipeline as the processing moves from raw words to a final determination of the meaning of a document. NLTK can perform many of these tasks. Most of these functions don’t provide results that are directly useful in themselves, but they can be very helpful as part of <span class="No-Break">a pipeline.</span></p>
			<p>Some of the basic tasks that are needed in nearly all natural language projects can easily be done with NLTK. For example, texts to be processed need to be broken down into words before<a id="_idIndexMarker209"/> processing. We can do this with NLTK’s <strong class="source-inline">word_tokenize</strong> function, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
import nltk
import string
from nltk import word_tokenize
text = "we'd like to book a flight from boston to London"
tokenized_text = word_tokenize(text)
print(tokenized_text)</pre>
			<p>The result will be an array <span class="No-Break">of words:</span></p>
			<pre class="source-code">
['we',
 "'d",
 'like',
 'to',
 'book',
 'a',
 'flight',
 'from',
 'boston',
 'to',
'London']</pre>
			<p>Note that the word <strong class="source-inline">we'd</strong> is separated into two components, <strong class="source-inline">we</strong> and <strong class="source-inline">'d</strong>, because it is a contraction that actually represents two words: <em class="italic">we</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">would</em></span><span class="No-Break">.</span></p>
			<p>NLTK also provides some functions for basic statistics such as counting word frequencies in a text. For example, continuing from the text we just looked at, <em class="italic">we’d like to book a flight from Boston to London</em>, we can use the NLTK <strong class="source-inline">FreqDist()</strong> function to count how often each <span class="No-Break">word occurs:</span></p>
			<pre class="source-code">
from nltk.probability import FreqDist
FreqDist(tokenized_text)
FreqDist({'to': 2, 'we': 1, "'d": 1, 'like': 1, 'book': 1, 'a': 1, 'flight': 1, 'from': 1, 'boston': 1, 'london': 1})</pre>
			<p>In this example, we imported the <strong class="source-inline">FreqDist()</strong> function from NLTK’s <strong class="source-inline">probability</strong> package and used it to count the frequencies of each word in the text. The result is a Python dict where the<a id="_idIndexMarker210"/> keys are the words and the values are how often the words occur. The word <strong class="source-inline">to</strong> occurs twice, and each of the other words occurs once. For such a short text, the frequency distribution is not particularly insightful, but it can be very helpful when you’re looking at larger amounts of data. We will see the frequency distribution for a large corpus in the <em class="italic">Looking at an example</em> section later in <span class="No-Break">this chapter.</span></p>
			<p>NLTK can also do <strong class="bold">part-of-speech</strong> (<strong class="bold">POS</strong>) tagging, which <a id="_idIndexMarker211"/>was discussed in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Continuing with our example, the <strong class="source-inline">nltk.pos_tag(tokenized_text)</strong> function is used for <span class="No-Break">POS tagging:</span></p>
			<pre class="source-code">
nltk.pos_tag(tokenized_text)
[('we', 'PRP'),
 ("'d", 'MD'),
 ('like', 'VB'),
 ('to', 'TO'),
 ('book', 'NN'),
 ('a', 'DT'),
 ('flight', 'NN'),
 ('from', 'IN'),
 ('boston', 'NN'),
 ('to', 'TO'),
 ('london', 'VB')]</pre>
			<p>Similarly, NLTK provides functions for parsing texts. Recall that parsing was discussed in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. As discussed in <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, NLTK also provides functions for creating and <a id="_idIndexMarker212"/>applying <strong class="bold">regular </strong><span class="No-Break"><strong class="bold">expressions</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">regexes</strong></span><span class="No-Break">).</span></p>
			<p>These are some of the <a id="_idIndexMarker213"/>most useful capabilities of NLTK. The full set of NLTK capabilities is too large to list here, but we will be reviewing some of these other capabilities in <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> and <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<h3>Installing NLTK</h3>
			<p>NLTK requires<a id="_idIndexMarker214"/> Python 3.7 or greater. The installation process for Windows is to run the following command in a <span class="No-Break">command window:</span></p>
			<pre class="source-code">
$ pip install nltk</pre>
			<p>For a Mac or Unix environment, run the following command in a <span class="No-Break">terminal window:</span></p>
			<pre class="source-code">
$ pip install --user -U nltk</pre>
			<p>In the next section, we will go over another popular NLU library, spaCy, and explain what it can do. As with NLTK, we will be using spaCy extensively in <span class="No-Break">later chapters.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor096"/>Using spaCy</h2>
			<p>spaCy is another very popular package that can do many of the same NLP tasks as NLTK. Both toolkits are very<a id="_idIndexMarker215"/> capable. spaCy is generally faster, and so is more suitable for deployed applications. Both<a id="_idIndexMarker216"/> toolkits support many languages, but not all NLU tasks are supported for all languages, so in making a choice between NLTK and spaCy, it is important to consider the specific language requirements for <span class="No-Break">that application.</span></p>
			<p>As with NLTK, spaCy can perform many basic text-processing functions. Let's check <span class="No-Break">it out!</span></p>
			<p>The code to set up tokenization in spaCy is very similar to the code for NLTK, with a slightly different function name. The result is an array of words, where each element is one token. Note that the <strong class="source-inline">nlp</strong> object is initialized with an <strong class="source-inline">en_core_web_sm</strong> model that tells it to use the statistics from a particular set of web-based <span class="No-Break">data, </span><span class="No-Break"><strong class="source-inline">en_core_web_sm</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import spacy
from spacy.lang.en import English
nlp = spacy.load('en_core_web_sm')
text = "we'd like to book a flight from boston to london"
doc = nlp(text)
print ([token.text for token in doc])
['we', "'d", 'like', 'to', 'book', 'a', 'flight', 'from', 'boston', 'to', 'london']</pre>
			<p>We can also calculate <a id="_idIndexMarker217"/>statistics such as the frequency of the words that occur in <span class="No-Break">the text:</span></p>
			<pre class="source-code">
from collections import Counter
word_freq = Counter(words)
print(word_freq)
Counter({'to': 2, 'we': 1, "'d": 1, 'like': 1, 'book': 1, 'a': 1, 'flight': 1, 'from': 1, 'boston': 1, 'london': 1})</pre>
			<p>The only difference between spaCy and NLTK is that NLTK uses the <strong class="source-inline">FreqDist</strong> function and spaCy uses the <strong class="source-inline">Counter</strong> function. The result, a Python dict with the words as keys and the frequencies as values, is the same for <span class="No-Break">both libraries.</span></p>
			<p>Just as with NLTK, we can perform POS tagging <span class="No-Break">with spaCy:</span></p>
			<pre class="source-code">
for token in doc:
    print(token.text, token.pos_)</pre>
			<p>This results in the following <span class="No-Break">POS assignments:</span></p>
			<pre class="source-code">
we PRON
'd AUX
like VERB
to PART
book VERB
a DET
flight NOUN
from ADP
boston PROPN
to ADP
london PROPN</pre>
			<p>Unfortunately, NLTK and spaCy use different labels for the different parts of speech. This is not necessarily a problem, because there is no <em class="italic">correct</em> or <em class="italic">standard</em> set of parts of speech, even for one language. However, it’s important for the parts of speech to be consistent within an application, so developers should be aware of this difference and be sure not to confuse the NLTK and spaCy parts <span class="No-Break">of speech.</span></p>
			<p>Another very useful <a id="_idIndexMarker218"/>capability that spaCy has is <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>). NER is the task of identifying references to specific persons, organizations, locations, or other entities that <a id="_idIndexMarker219"/>occur in a text. NER can be either an end in itself or it can be part of another task. For example, a company might be interested in finding when their products are mentioned on Facebook, so NER for their products would be all that they need. On the other hand, a company might be interested in finding out if their products are mentioned in a positive or negative way, so in that case, they <a id="_idIndexMarker220"/>would want to perform both NER and <strong class="bold">sentiment </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SA</strong></span><span class="No-Break">).</span></p>
			<p>NER can be performed in most NLP libraries; however, it is particularly easy to do in spaCy. Given a document, we just have to request rendering of the document using the <strong class="source-inline">ent</strong> style, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
import spacy
nlp = spacy.load("en_core_web_sm")
text = "we'd like to book a flight from boston to new york"
doc = nlp(text)
displacy.render(doc,style='ent',jupyter=True,options={'distance':200})</pre>
			<p>The rendered result<a id="_idIndexMarker221"/> shows that the <strong class="source-inline">boston</strong> and <strong class="source-inline">new york</strong> named entities are<a id="_idIndexMarker222"/> assigned a <strong class="bold">geopolitical entity</strong> (<strong class="bold">GPE</strong>) label, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B19005_04_02.jpg" alt="Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”</p>
			<p>Parsing or the analysis of syntactic relationships among the words in a sentence can be done very easily with almost the same code, just by changing the value of the <strong class="source-inline">style</strong> parameter to <strong class="source-inline">dep</strong> from <strong class="source-inline">ent</strong>. We’ll see an example of a syntactic parse later on in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
nlp = spacy.load('en_core_web_sm')
doc = nlp('they get in an accident')
displacy.render(doc,style='dep',jupyter=True,options={'distance':200})</pre>
			<p>Installing spaCy is done with the following <span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ pip install -U spacy</pre>
			<p>The next library we will look at is the Keras <span class="No-Break">ML library.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor097"/>Using Keras</h2>
			<p>Keras (<a href="https://keras.io/">https://keras.io/</a>) is another<a id="_idIndexMarker223"/> popular Python NLP library. Keras is much more focused on ML than NLTK or spaCy and will be the go-to library for NLP <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) applications<a id="_idIndexMarker224"/> in this book. It’s built <a id="_idIndexMarker225"/>on top of another<a id="_idIndexMarker226"/> package called TensorFlow (<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>), which was developed by Google. Because Keras is built on TensorFlow, TensorFlow functions can be used <span class="No-Break">in Keras.</span></p>
			<p>Since Keras focuses on ML, it has limited capabilities for preprocessing text. For example, unlike NLTK or spaCy, it does not support POS tagging or parsing directly. If these capabilities are needed, then it’s best to preprocess the text with NLTK or spaCy. Keras does support tokenization and removal of extraneous tokens such as punctuation and <span class="No-Break">HTML markup.</span></p>
			<p>Keras is especially strong for text-processing<a id="_idIndexMarker227"/> applications using <strong class="bold">neural networks</strong> (<strong class="bold">NN</strong>). This will be discussed in much more detail in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. Although Keras includes few high-level functions for performing NLP functions such as <a id="_idIndexMarker228"/>POS tagging or parsing in one step, it does include capabilities for training POS taggers from a dataset and then deploying the tagger in <span class="No-Break">an </span><span class="No-Break"><a id="_idIndexMarker229"/></span><span class="No-Break">application.</span></p>
			<p>Since Keras is included in TensorFlow, Keras is automatically installed when TensorFlow is installed. It is not necessary to install Keras as an additional step. Thus, the following command <span class="No-Break">is suf<a id="_idTextAnchor098"/>ficient:</span></p>
			<pre class="source-code">
$ pip install tensorflow</pre>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor099"/>Learning about other NLP libraries</h2>
			<p>There are quite a few<a id="_idIndexMarker230"/> other Python libraries that include NLP capabilities and that can be useful in some cases. These<a id="_idIndexMarker231"/> include PyTorch (<a href="https://pytorch.org/">https://pytorch.org/</a>) for processing <a id="_idIndexMarker232"/>based on <strong class="bold">deep neural networks</strong> (<strong class="bold">DNN</strong>), scikit-learn (<a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a>), which<a id="_idIndexMarker233"/> includes general ML functions, and <a id="_idIndexMarker234"/>Gensim (<a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a>), for topic modeling, among others. However, I would recommend working with the basic packages that we’ve covered here for a few projects at first until you get more familiar with NLP. If you later have a requirement for additional functionality, a different language, or faster processing speed than what the basic packages provide, you can explore some of these other packages at <span class="No-Break">that time.</span></p>
			<p>In the next topic, we will discuss how to choose among NLP libraries. It’s good to keep in mind that choosing libraries isn’t an all-or-none process—libraries can easily be mixed and matched if one library has strengths that another <span class="No-Break">one doesn’t.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor100"/>Choosing among NLP libraries</h2>
			<p>The libraries <a id="_idIndexMarker235"/>discussed in the preceding sections are all very useful and powerful. In some cases, they have overlapping capabilities. This raises the question of selecting <a id="_idIndexMarker236"/>which libraries to use in a particular application. Although all of the libraries can be combined in the same application, it reduces the complexity of applications if fewer libraries <span class="No-Break">are used.</span></p>
			<p>NLTK is very strong in corpus statistics and rule-based linguistic preprocessing. For example, some useful corpus statistics include counting words, counting parts of speech, counting pairs of words (bigrams), and tabulating words in context (concordances). spaCy is fast, and its displaCy visualization library is very helpful in gaining insight into processing results. Keras is very strong <span class="No-Break">in DL.</span></p>
			<p>During the lifetime of a project, it is often useful to start with tools that help you quickly get a good overall picture of the data, such as NLTK and spaCy. This initial analysis will be very helpful for <a id="_idIndexMarker237"/>selecting the tools that are needed for full-scale processing and deployment. Since training DL models using tools such as Keras can be very time-consuming, doing some preliminary investigation with more traditional approaches will help narrow down the possibilities that need to be investigated in order to select a <span class="No-Break">DL approach.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor101"/>Learning about other packages useful for NLP</h2>
			<p>In addition to packages<a id="_idIndexMarker238"/> that directly support NLP, there are also a number of other useful general-purpose open source Python packages that provide tools for generally managing data, including natural language data. These include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">NumPy</strong>: NumPy (<a href="https://numpy.org/">https://numpy.org/</a>) is a powerful package that includes many functions for the numerical calculations<a id="_idIndexMarker239"/> that we’ll be working with in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>,<em class="italic"> </em><a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, and <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a></li>
				<li><strong class="bold">pandas</strong>: pandas (<a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a>) provides general tools for data analysis and <a id="_idIndexMarker240"/>manipulation, including natural language data, especially data in the form <span class="No-Break">of tables</span></li>
				<li><strong class="bold">scikit-learn</strong>: scikit-learn is a powerful <a id="_idIndexMarker241"/>package for ML, including text <span class="No-Break">processing (</span><a href="https://scikit-learn.org/stable/"><span class="No-Break">https://scikit-learn.org/stable/</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>There are also several visualization packages that will be very helpful for graphical representations of data and for processing results. Visualization is important in NLP development because it can often give you a much more comprehensible representation of results than a numerical table. For example, visualization can help you see trends, pinpoint errors, and compare experimental conditions. We’ll be using visualization tools throughout the book, but especially in <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Visualization tools include generic tools for representing different kinds of numerical results, whether they have to do with NLP or not, as well as tools specifically designed to<a id="_idIndexMarker242"/> represent natural language information such as parses and NER results. Visualization tools include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Matplotlib</strong>: Matplotlib (<a href="https://matplotlib.org/">https://matplotlib.org/</a>) is a popular Python visualization library that’s<a id="_idIndexMarker243"/> especially good at creating plots of data, including NLP data. If you’re trying to compare the results of processing with several different techniques, plotting the results can often provide insights very quickly about how well the different techniques are working, which can be helpful for evaluation. We will be returning to the topic of evaluation in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break"><em class="italic">.</em></span></li>
				<li><strong class="bold">Seaborn</strong>: Seaborn (<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>) is based on Matplotlib. It enables developers to produce <a id="_idIndexMarker244"/>attractive graphs representing <span class="No-Break">statistical information.</span></li>
				<li><strong class="bold">displaCy</strong>: displaCy is part <a id="_idIndexMarker245"/>of the spaCy tools, and is especially good at representing natural language results such as POS tags, parses, and named entities, which we discussed in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">WordCloud</strong>: WordCloud (<a href="https://amueller.github.io/word_cloud/">https://amueller.github.io/word_cloud/</a>) is a specialized library for<a id="_idIndexMarker246"/> visualizing word frequencies in a corpus, which can be useful when word frequencies are of interest. We’ll see an example of a word cloud in the <span class="No-Break">next section.</span></li>
			</ul>
			<p>Up to this point, we’ve reviewed the technical requirements for our software development environment as well as the NLP libraries that we’ll be working with. In the next section, we’ll put everything together with <span class="No-Break">an example.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor102"/>Looking at an example</h1>
			<p>To illustrate some of these concepts, we’ll work through an example using JupyterLab where we explore an SA task for <a id="_idIndexMarker247"/>movie reviews. We’ll look at how we can apply the NLTK and spaCy packages to get some ideas about what the data is like, which will help us plan <span class="No-Break">further processing.</span></p>
			<p>The corpus (or dataset) that we’ll be looking at is a popular set of 2,000 movie reviews, classified as to whether the writer expressed a positive or negative sentiment about the <span class="No-Break">movie (</span><a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/"><span class="No-Break">http://www.cs.cornell.edu/people/pabo/movie-review-data/</span></a><span class="No-Break">).</span></p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout"><em class="italic">Bo Pang</em> and <em class="italic">Lillian Lee</em>, <em class="italic">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</em>, <em class="italic">Proceedings of the </em><span class="No-Break"><em class="italic">ACL</em></span><span class="No-Break">, </span><span class="No-Break"><em class="italic">2005</em></span><span class="No-Break">.</span></p>
			<p>This is a good example of the task of SA, which was introduced in <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor103"/>Setting up JupyterLab</h2>
			<p>We’ll be working with JupyterLab, so let’s start it up. As we saw earlier, you can start JupyterLab by simply typing the<a id="_idIndexMarker248"/> following command into a command (Windows) or terminal (<span class="No-Break">Mac) window:</span></p>
			<pre class="source-code">
$ jupyter lab</pre>
			<p>This will start a local web server and open a JupyterLab window in a web browser. In the JupyterLab window, open a new notebook by selecting <strong class="bold">File</strong> | <strong class="bold">New</strong> | <strong class="bold">Notebook</strong>, and an untitled notebook will appear (you can rename it at any time by selecting <strong class="bold">File</strong> | <span class="No-Break"><strong class="bold">Rename Notebook</strong></span><span class="No-Break">).</span></p>
			<p>We’ll start by importing the libraries that we’ll be using in this example, as shown next. We’ll be using the NLTK and spaCy NLP libraries, as well as some general-purpose libraries for numerical operations and visualization. We’ll see how these are used as we go through <span class="No-Break">the example:</span></p>
			<pre class="source-code">
# NLP imports
import nltk
import spacy
from spacy import displacy
# general numerical and visualization imports
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import numpy as np</pre>
			<p>Enter the preceding code into a JupyterLab cell and run the cell. Running this cell (<strong class="bold">Run</strong> | <strong class="bold">Run Selected Cells</strong>) will import<a id="_idIndexMarker249"/> the libraries and give you a new <span class="No-Break">code cell.</span></p>
			<p>Download the movie review data by typing <strong class="source-inline">nltk.download()</strong> into the new code cell. This will open a new <strong class="bold">NLTK Downloader</strong> window, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B19005_04_03.jpg" alt="Figure 4.3 – NLTK ﻿Downloader window"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – NLTK Downloader window</p>
			<p>On the <strong class="bold">Corpora</strong> tab of the <strong class="bold">Download</strong> window, you can select <strong class="source-inline">movie_reviews</strong>. Click the <strong class="bold">Download</strong> button to download the corpus. You can select <strong class="bold">File</strong> | <strong class="bold">Change Download Directory</strong> if you<a id="_idIndexMarker250"/> want to change where the data is downloaded. Click <strong class="bold">File</strong> | <strong class="bold">Exit</strong> from the downloader window to exit from the <strong class="bold">Download</strong> window and return to the <span class="No-Break">JupyterLab interface.</span></p>
			<p>If you take a look at the directory where you downloaded the data, you will see two directories: <strong class="source-inline">neg</strong> and <strong class="source-inline">pos</strong>. The directories contain negative and positive reviews, respectively. This represents the annotation of the reviews; that is, a human annotator’s opinion of whether the review was positive or negative. This directory structure is a common approach for representing text classification annotations, and you’ll see it in many datasets. The <strong class="source-inline">README</strong> file in the <strong class="source-inline">movie_reviews</strong> folder explains some details on how the annotation <span class="No-Break">was done.</span></p>
			<p>If you look at some of the movie reviews in the corpus, you’ll see that the correct annotation for a text is not <span class="No-Break">always obvious.</span></p>
			<p>The next code block shows importing the reviews and printing one sentence from <span class="No-Break">the corpus:</span></p>
			<pre class="source-code">
#import the training data
from nltk.corpus import movie_reviews
sents = movie_reviews.sents()
print(sents)
[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ...]
In [5]:
sample = sents[9]
print(sample)
['they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.']</pre>
			<p>Since <strong class="source-inline">movie_reviews</strong> is an NLTK corpus, a number of corpora methods are available, including listing the sentences<a id="_idIndexMarker251"/> as an array of individual sentences. We can also select individual sentences from the corpus by number, as shown in the previous code block, where we selected and printed sentence number nine in <span class="No-Break">the corpus.</span></p>
			<p>You can see that the sentences have been tokenized, or separated into individual words (including punctuation marks). This is an important preparatory step in nearly all <span class="No-Break">NLP applications.</span></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor104"/>Processing one sentence</h2>
			<p>Now, let’s do some actual<a id="_idIndexMarker252"/> NLP processing for this sample sentence. We’ll use the spaCy library to perform POS tagging and rule-based parsing and then visualize the results with the <span class="No-Break">displaCy library.</span></p>
			<p>We first need to create an <strong class="source-inline">nlp</strong> object based on web data, <strong class="source-inline">en_core_web_sm</strong>, which is a basic small English model. There are larger models available, but they take longer to load, so we will stick with the small model here for brevity. Then, we use the <strong class="source-inline">nlp</strong> object to identify the parts of speech and parse this sentence, as shown in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
nlp = spacy.load('en_core_web_sm')
doc = nlp('they get in an accident')
displacy.render(doc,style='dep',jupyter=True,options={'distance':200})</pre>
			<p>In the <strong class="source-inline">displacy.render</strong> command, we have requested a dependency parse (<strong class="source-inline">styles='dep'</strong>). This is a type of analysis we’ll go into in more detail in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. For<a id="_idIndexMarker253"/> now, it’s enough to say that it’s one common approach to showing how the words in a sentence are related to each other. The resulting dependency parse is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B19005_04_04.jpg" alt="Figure 4.4 – Dependency parse for “they get in an accident”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Dependency parse for “they get in an accident”</p>
			<p>Now that we’ve loaded the corpus and looked at a few examples of the kinds of sentences it contains, we will look at some of the overall properties of <span class="No-Break">the corpus.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor105"/>Looking at corpus properties</h2>
			<p>While corpora have many <a id="_idIndexMarker254"/>properties, some of the most interesting and insightful properties are word frequencies and POS frequencies, which we will be reviewing in the next two sections. Unfortunately, there isn’t space to explore additional corpus properties in detail, but looking at word and POS frequencies should get you started. </p>
			<h3>Word frequencies </h3>
			<p>In this section, we will look at some properties of the full corpus. For example, we can look at the most frequent words using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
words = movie_reviews.words()
word_counts = nltk.FreqDist(word.lower() for word in words if word.isalpha())
top_words = word_counts.most_common(25)
all_fdist = pd.Series(dict(top_words))
# Setting fig and ax into variables
fig, ax = plt.subplots(figsize=(10,10))
# Plot with Seaborn plotting tools
plt.xticks(rotation = 70)
plt.title("Frequency -- Top 25 Words in the Movie Review Corpus", fontsize = 30)
plt.xlabel("Words", fontsize = 30)
plt.ylabel("Frequency", fontsize = 30)
all_plot = sns.barplot(x = all_fdist.index, y = all_fdist.values, ax=ax)
plt.xticks(rotation=60)
plt.show()</pre>
			<p>As seen in the preceding code, we start by collecting the words in the <strong class="source-inline">movie_review</strong> corpus by using the <strong class="source-inline">words()</strong> method <a id="_idIndexMarker255"/>of the corpus object. We then count the words with NLTK’s <strong class="source-inline">FreqDist()</strong> function. At the same time, we are lowercasing the words and ignoring non-alpha words such as numbers and punctuation. Then, for clarity in the visualization, we’ll restrict the words we’ll look at to the most frequent 25 words. You may be interested in trying different values of <strong class="source-inline">top_words</strong> in the code block to see how the graph looks with more and <span class="No-Break">fewer words.</span></p>
			<p>When we call <strong class="source-inline">plt.show()</strong>, the<a id="_idIndexMarker256"/> distribution of word frequencies is displayed. This can be seen in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B19005_04_05.jpg" alt="Figure 4.5 – Visualizing the most frequent words in the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Visualizing the most frequent words in the movie review corpus</p>
			<p>As <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em> shows, the most frequent word, not surprisingly, is <strong class="bold">the</strong>, which is about twice as frequent as the second most common <span class="No-Break">word, </span><span class="No-Break"><strong class="bold">a</strong></span><span class="No-Break">.</span></p>
			<p>An alternative visualization of word<a id="_idIndexMarker257"/> frequencies that can also be helpful is a <strong class="bold">word cloud</strong>, where more frequent words are shown in a larger font. The following snippet shows the code for computing a word cloud from our word frequency distribution, <strong class="source-inline">all_fdist</strong>, and displaying it <span class="No-Break">with Matplotlib:</span></p>
			<pre class="source-code">
from wordcloud import WordCloud
wordcloud = WordCloud(background_color = 'white',
                      max_words = 25,
                      relative_scaling = 0,
                      width = 600,height = 300,
                      max_font_size = 150,
                      colormap = 'Dark2',
                      min_font_size = 10).generate_from_frequencies(all_fdist)
# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()</pre>
			<p>The resulting word cloud is<a id="_idIndexMarker258"/> shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.6</em>. We can see that very frequent words such as <strong class="bold">the</strong> and <strong class="bold">a</strong> appear in very large fonts in comparison to <span class="No-Break">other words:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B19005_04_06.jpg" alt="Figure 4.6 – A word cloud for the top 25 words in the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – A word cloud for the top 25 words in the movie review corpus</p>
			<p>Notice that nearly all of the frequent words are words generally used in most English texts. The exception is <strong class="bold">film</strong>, which is to be expected in a corpus of movie reviews. Since most of these frequent words occur in the majority of texts, their occurrence won’t enable us to distinguish different categories of texts. If we’re dealing with a classification problem such as SA, we <a id="_idIndexMarker259"/>should consider removing these common words from texts before we try to train an SA classifier on this corpus. These kinds of words are called <strong class="bold">stopwords</strong>, and their<a id="_idIndexMarker260"/> removal is a common preprocessing step. We will discuss stopword removal in detail in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break"><em class="italic">.</em></span></p>
			<h3>POS frequencies</h3>
			<p>We can also look at the most frequent<a id="_idIndexMarker261"/> parts of speech with the following code. To reduce the complexity of the graph, we’ll restrict the display to the 18 most common parts of speech. After we tag the words, we loop through the sentences, counting up the occurrences of each tag. Then, the list of tags is sorted with the most frequent <span class="No-Break">tags first.</span></p>
			<p>The parts of speech used in the NLTK POS tagging code are the widely used Penn Treebank parts of speech, documented at <a href="https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html">https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html</a>. This tagset includes 36 tags overall. Previous work on NLP has found that the traditional English parts of speech (noun, verb, adjective, adverb, conjunction, interjection, pronoun, and preposition) are not fine-grained enough for computational purposes, so additional parts of speech are normally added. For example, different forms of verbs, such as <em class="italic">walk</em>, <em class="italic">walks</em>, <em class="italic">walked</em>, and <em class="italic">walking</em>, are usually assigned different parts of speech. For example, <em class="italic">walk</em> is assigned the <em class="italic">VB</em>—or <em class="italic">Verb base </em>form—POS, but <em class="italic">walks</em> is assigned the <em class="italic">VBZ</em>—or <em class="italic">Verb, third - person singular present</em>—POS. Traditionally, these would all be <span class="No-Break">called </span><span class="No-Break"><em class="italic">verbs</em></span><span class="No-Break">.</span></p>
			<p>First, we’ll extract the sentences from the corpus, and then tag each word with its part of speech. It’s important to perform POS tagging on an entire sentence rather than just individual words, because <a id="_idIndexMarker262"/>many words have multiple parts of speech, and the POS assigned to a word depends on the other words in its sentence. For example, <em class="italic">book</em> at the beginning of <em class="italic">book a flight</em> can be recognized and tagged as a verb, while in <em class="italic">I read the book</em>, <em class="italic">book</em> can be tagged as a noun. In the following code, we display the frequencies of the parts of speech in <span class="No-Break">this corpus:</span></p>
			<pre class="source-code">
movie_reviews_sentences = movie_reviews.sents()
tagged_sentences = nltk.pos_tag_sents(movie_reviews_sentences)
total_counts = {}
for sentence in tagged_sentences:
    counts = Counter(tag for word,tag in sentence)
    total_counts = Counter(total_counts) + Counter(counts)
sorted_tag_list = sorted(total_counts.items(), key = lambda x: x[1],reverse = True)
all_tags = pd.DataFrame(sorted_tag_list)
most_common_tags = all_tags.head(18)
# Setting figure and ax into variables
fig, ax = plt.subplots(figsize=(15,15))
all_plot = sns.barplot(x = most_common_tags[0], y = most_common_tags[1], ax = ax)
plt.xticks(rotation = 70)
plt.title("Part of Speech Frequency  in Movie Review Corpus", fontsize = 30)
plt.xlabel("Part of Speech", fontsize = 30)
plt.ylabel("Frequency", fontsize = 30)
plt.show()</pre>
			<p>In the movie review corpus, we can see that by far the most common tag is <em class="italic">NN</em> or <em class="italic">common noun</em>, followed by <em class="italic">IN</em> or <em class="italic">preposition or coordinating conjunction</em>, and <em class="italic">DT</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">determiner</em></span><span class="No-Break">.</span></p>
			<p>The result, again using the<a id="_idIndexMarker263"/> Matplotlib and Seaborn libraries, is shown graphically in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B19005_04_07.jpg" alt="Figure 4.7 – Visualizing the most frequent parts of speech in the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Visualizing the most frequent parts of speech in the movie review corpus</p>
			<p>We can look at other text properties such as the distribution of the lengths of the texts, and we can compare the properties of the positive and negative reviews to see if we can find some properties that distinguish the two categories. Do positive and negative reviews have different <a id="_idIndexMarker264"/>average lengths or different distributions of parts of speech? If we notice some differences, then we can make use of them in classifying <span class="No-Break">new reviews.</span></p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor106"/>Summary</h1>
			<p>In this chapter, we covered the major development tools and Python libraries that are used in NLP application development. We discussed the JupyterLab development environment and the GitHub software repository system. The major libraries that we covered were NLTK, spaCy, and Keras. Although this is by no means an exhaustive list of NLP libraries, it’s sufficient to get a start on almost any <span class="No-Break">NLP project.</span></p>
			<p>We covered installation and basic usage for the major libraries, and we provided some suggested tips on selecting libraries. We summarized some useful auxiliary packages, and we concluded with a simple example of how the libraries can be used to do some <span class="No-Break">NLP tasks.</span></p>
			<p>The topics discussed in this chapter have given you a basic understanding of the most useful Python packages for NLP, which you will be using for the rest of the book. In addition, the discussion in this chapter has given you a start on understanding the principles for selecting tools for future projects. We have achieved our goal of getting you set up with tools for processing natural language, along with an illustration of some simple text processing using NLTK and spaCy and visualization with Matplotlib <span class="No-Break">and Seaborn.</span></p>
			<p>In the next chapter, we will look at how to identify and prepare data for processing with NLP techniques. We will discuss data from databases, the web, and other documents, as well as privacy and ethics considerations. For readers who don’t have access to their own data or who wish to compare their results to those of other researchers, this chapter will also discuss generally available corpora. It will then go on to discuss preprocessing steps such as tokenization, stemming, stopword removal, <span class="No-Break">and lemmatization.</span></p>
		</div>
	</body></html>