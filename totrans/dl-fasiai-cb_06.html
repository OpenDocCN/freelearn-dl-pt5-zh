<html><head></head><body>
		<div id="_idContainer231">
			<h1 id="_idParaDest-145"><em class="italic"><a id="_idTextAnchor152"/>Chapter 6</em>: Training Models with Visual Data</h1>
			<p>Deep learning has been successfully applied to many different types of data, including tabular data, text data, and recommender system data. You saw fastai's approach to these types of data in <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Models with Tabular Data</em>, <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, and <a href="B16216_05_Final_VK_ePub.xhtml#_idTextAnchor134"><em class="italic">Chapter 5</em></a>, <em class="italic">Training Recommender Systems</em>. These types of data are all part of the story of deep learning, but <strong class="bold">visual data</strong> or <strong class="bold">image data</strong> is the type of data that is traditionally associated most closely with deep learning. </p>
			<p>Visual data is also the type of data that is most thoroughly supported by the fastai framework. The fastai high-level API is mostly developed for visual data, and 70% of the curated fastai datasets are visual datasets. In this chapter, we will explore some of the features that fastai provides for exploring visual datasets and building high-performance deep learning models with image datasets.</p>
			<p>In this chapter, you will learn how to use the rich set of facilities available in fastai for preparing image datasets and using them to train deep learning models. In particular, you will learn how to create fastai deep learning models that classify images, that is, determine what objects are in images, and also how to use fastai to identify multiple objects in the same image.</p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Training a classification model with a simple curated vision dataset</li>
				<li>Exploring a curated image location dataset</li>
				<li>Training a classification model with a standalone vision dataset</li>
				<li>Training a multi-image classification model with a curated vision dataset</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor153"/>Technical requirements</h1>
			<p>Ensure that you have completed the setup sections from <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, and have a working Gradient instance or Colab set up. The recipes described in this chapter assume that you are using Gradient. Ensure that you have cloned the repo for this book from <a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook</a> and have access to the <strong class="source-inline">ch6</strong> folder. This folder contains the code samples described in this chapter.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor154"/>Training a classification model with a simple curated vision dataset</h1>
			<p>You may recall <a id="_idIndexMarker420"/>the first<a id="_idIndexMarker421"/> fastai model that you trained back in <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>. That model was trained on the MNIST dataset of hand-written digits. Given an image of a hand-written digit, that model was able to classify the image, that is, determine which of the digits from 0 to 9 were shown in the image. </p>
			<p>In this recipe, you are going to apply the same approach you saw in the MNIST model to another fastai curated dataset: the CIFAR dataset. This dataset, which is a subset of a larger curated CIFAR_100 dataset, is made up of 6,000 images organized into 10 categories. The model that you train in this section will be able to determine the category that an image from this dataset belongs to.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor155"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_curated_image_datasets.ipynb</strong> notebook in the <strong class="source-inline">ch6</strong> directory of your repo.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The images in the <strong class="source-inline">CIFAR</strong> dataset are quite small. In this section, we have rendered them in a larger size to make them easier to recognize in the context of the book, but the outcome is that they can look a bit blurry.</p>
			<p>The <strong class="source-inline">CIFAR</strong> dataset featured in this section is introduced in the paper <em class="italic">Learning Multiple Layers of Features from Tiny Image</em>, Krizhevsky, 2009. I am grateful for the opportunity to include this dataset in this book.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Alex Krizhevsky. (2009). <em class="italic">Learning Multiple Layers of Features from Tiny Image</em>. University of Toronto: <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor156"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_with_curated_image_datasets.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li>Run <a id="_idIndexMarker422"/>the cells in<a id="_idIndexMarker423"/> the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the following cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = untar_data(URLs.CIFAR)</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The output shows the directory structure of the dataset, as shown in the following screenshot:</p><div id="_idContainer160" class="IMG---Figure"><img src="image/B16216_6_1.jpg" alt="Figure 6.1 – Output of path.ls()&#13;&#10;"/></div><p class="figure-caption">Figure 6.1 – Output of path.ls()</p></li>
				<li>Run the following cell to define an <strong class="source-inline">ImageDataLoaders</strong> object for this dataset:<p class="source-code">dls = ImageDataLoaders.from_folder(path, train='train', valid='test')</p><p>Here are the arguments for the definition of the <strong class="source-inline">ImageDataLoaders</strong> object:</p><p>a) <strong class="source-inline">path</strong>: Specifies that the <strong class="source-inline">ImageDataLoaders</strong> object is defined using the <strong class="source-inline">path</strong> object you created in the previous cell</p><p>b) <strong class="source-inline">train='train'</strong>: Specifies that the training data is in the <strong class="source-inline">/storage/data/cifar10/train</strong> directory</p><p>c) <strong class="source-inline">valid='test'</strong>: Specifies that the validation data is in the <strong class="source-inline">/storage/data/cifar10/test</strong> directory</p></li>
				<li>Run the following cell to display a batch from the dataset:<p class="source-code">dls.train.show_batch(max_n=4, nrows=1)</p><p>The <a id="_idIndexMarker424"/>output <a id="_idIndexMarker425"/>of this cell is a set of <strong class="source-inline">4</strong> items from a batch, showing images along with their corresponding categories, as shown in the following figures:</p><div id="_idContainer161" class="IMG---Figure"><img src="image/B16216_6_2.jpg" alt="Figure 6.2 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 6.2 – Output of show_batch()</p></li>
				<li>Run the following cell to examine the contents of the <strong class="source-inline">train</strong> subdirectory:<p class="source-code">(path/'train').ls()</p><p>The output shows the structure of the <strong class="source-inline">train</strong> subdirectory, as shown in the following screenshot:</p><div id="_idContainer162" class="IMG---Figure"><img src="image/B16216_6_3.jpg" alt="Figure 6.3 – Contents of the train subdirectory&#13;&#10;"/></div><p class="figure-caption">Figure 6.3 – Contents of the train subdirectory</p></li>
				<li>Run the following cell to examine the contents of the <strong class="source-inline">train/dog</strong> subdirectory:<p class="source-code">(path/'train/dog').ls()</p><p>The<a id="_idIndexMarker426"/> output<a id="_idIndexMarker427"/> shows the structure of the <strong class="source-inline">train/dog</strong> subdirectory:</p><div id="_idContainer163" class="IMG---Figure"><img src="image/B16216_6_4.jpg" alt="Figure 6.4 – Contents of the train/dog subdirectory&#13;&#10;"/></div><p class="figure-caption">Figure 6.4 – Contents of the train/dog subdirectory</p></li>
				<li>If you want to take a different perspective as regards the directory structure of this dataset, you can use the <strong class="source-inline">tree</strong> command. To do this, from the Gradient terminal, enter the following commands:<p class="source-code"><strong class="bold">cd /storage/data/cifar10</strong></p><p class="source-code"><strong class="bold">tree -d</strong></p><p>The output of the command shows the structure of the dataset. You can see that each of the <strong class="source-inline">test</strong> and <strong class="source-inline">train</strong> directories have subdirectories for each of the 10 categories of the dataset:</p><p class="source-code">├── test</p><p class="source-code">│   ├── airplane</p><p class="source-code">│   ├── automobile</p><p class="source-code">│   ├── bird</p><p class="source-code">│   ├── cat</p><p class="source-code">│   ├── deer</p><p class="source-code">│   ├── dog</p><p class="source-code">│   ├── frog</p><p class="source-code">│   ├── horse</p><p class="source-code">│   ├── ship</p><p class="source-code">│   └── truck</p><p class="source-code">└── train</p><p class="source-code">    ├── airplane</p><p class="source-code">    ├── automobile</p><p class="source-code">    ├── bird</p><p class="source-code">    ├── cat</p><p class="source-code">    ├── deer</p><p class="source-code">    ├── dog</p><p class="source-code">    ├── frog</p><p class="source-code">    ├── horse</p><p class="source-code">    ├── ship</p><p class="source-code">    └── truck</p></li>
				<li>Run <a id="_idIndexMarker428"/>the<a id="_idIndexMarker429"/> following cell to view a single item in the dataset:<p class="source-code">img_files = get_image_files(path)</p><p class="source-code">img = PILImage.create(img_files[100])</p><p class="source-code">img</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">img_files = get_image_files(path)</strong>: Specifies that <strong class="source-inline">path</strong> is recursively examined and returns all the image files in the path. If you want more details about <strong class="source-inline">get_image_files</strong>, you can check out the documentation at the following link: <a href="https://docs.fast.ai/data.transforms.html#get_image_files">https://docs.fast.ai/data.transforms.html#get_image_files</a>.</p><p>b) <strong class="source-inline">img = PILImage.create(img_files[100])</strong>: Creates the image object, <strong class="source-inline">img</strong>, from <a id="_idIndexMarker430"/>a <a id="_idIndexMarker431"/>specific file returned by the previous statement.</p><p>The output of this cell is one of the dataset files rendered as an image in the notebook, as shown here:</p><div id="_idContainer164" class="IMG---Figure"><img src="image/B16216_6_5.jpg" alt="Figure 6.5 – An image from the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.5 – An image from the dataset</p></li>
				<li>Run the following cell to display another image from the dataset:<p class="source-code">img = PILImage.create(img_files[3000])</p><p class="source-code">img</p><p>The output is another of the dataset files rendered as an image in the notebook as follows:</p><div id="_idContainer165" class="IMG---Figure"><img src="image/B16216_6_6.jpg" alt="Figure 6.6 – Another image from the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.6 – Another image from the dataset</p></li>
				<li>Run<a id="_idIndexMarker432"/> the<a id="_idIndexMarker433"/> following cell to define the model as a <strong class="source-inline">cnn_learner</strong> object:<p class="source-code">learn = cnn_learner(dls, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy))</p><p>Here are the arguments for the definition of the <strong class="source-inline">cnn_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: Specifies that the model uses the <strong class="source-inline">ImageDataLoaders</strong> object, <strong class="source-inline">dls</strong>, that you defined in <em class="italic">Step 4</em></p><p>b) <strong class="source-inline">resnet18</strong>: Specifies the pre-trained model to use as a starting point for this model</p><p>c) <strong class="source-inline">loss_func=LabelSmoothingCrossEntropy()</strong>: Specifies the loss function to use in the training process</p><p>d) <strong class="source-inline">metrics=accuracy</strong>: Specifies that <strong class="source-inline">accuracy</strong> is the performance metric that will be optimized in the training process</p></li>
				<li>Run the following cell to train the model:<p class="source-code">learn.fine_tune(5)</p><p>The argument indicates that the training run will be for <strong class="source-inline">5</strong> epochs.</p><p>The output displays the training loss, validation loss, and accuracy for each epoch, as<a id="_idIndexMarker434"/> shown in the<a id="_idIndexMarker435"/> following screenshot: </p><div id="_idContainer166" class="IMG---Figure"><img src="image/B16216_6_7.jpg" alt="Figure 6.7 – Output of training the model&#13;&#10;"/></div><p class="figure-caption">Figure 6.7 – Output of training the model</p></li>
				<li>Let's try out the trained model on some examples from the test dataset. First, run the following cell to define an object for one of the images in the test dataset:<p class="source-code">img_test_files = get_image_files(path/"test")</p><p class="source-code">img2 = PILImage.create(img_test_files[700])</p><p class="source-code">img2</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">img_files = get_image_files(path/"test"))</strong>: Returns all the image files under the <strong class="source-inline">test</strong> directory</p><p>b) <strong class="source-inline">img = PILImage.create(img_files[700])</strong>: Creates the image object, <strong class="source-inline">img2</strong>, from a specific file returned by the previous statement</p><p>The output of this cell is an image of a dog:</p><div id="_idContainer167" class="IMG---Figure"><img src="image/B16216_6_8.jpg" alt="Figure 6.8 – Dog image from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.8 – Dog image from the test dataset</p></li>
				<li>Run<a id="_idIndexMarker436"/> the<a id="_idIndexMarker437"/> following cell to define an object for another one of the images in the test dataset:<p class="source-code">img3 = PILImage.create(img_test_files[8000])</p><p class="source-code">img3</p><p>The output of this cell is an image of a bird:</p><div id="_idContainer168" class="IMG---Figure"><img src="image/B16216_6_9.jpg" alt="Figure 6.9 – Bird image from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.9 – Bird image from the test dataset</p></li>
				<li>Now that we have defined objects for a couple of images from the test dataset, let's exercise the trained image classification model on them. First, run the following cell to apply the model to the dog image:<p class="source-code">learn.predict(img2)</p><p>The output of this cell is the model's prediction, as shown in the following screenshot. Note that the model correctly predicts the category of the image:</p><div id="_idContainer169" class="IMG---Figure"><img src="image/B16216_6_10.jpg" alt="Figure 6.10 – Image classification model's prediction on the dog image&#13;&#10;"/></div><p class="figure-caption">Figure 6.10 – Image classification model's prediction on the dog image</p></li>
				<li>Let's now <a id="_idIndexMarker438"/>see how <a id="_idIndexMarker439"/>the model does with the image of a bird. Run the following cell to apply the model to the bird image:<p class="source-code">learn.predict(img3)</p><p>The output of this cell is the model's prediction, as shown in the following screenshot. Note that the model correctly predicts that the image is a bird:</p><div id="_idContainer170" class="IMG---Figure"><img src="image/B16216_6_11.jpg" alt="Figure 6.11 – Image classification model's prediction on the bird image&#13;&#10;"/></div><p class="figure-caption">Figure 6.11 – Image classification model's prediction on the bird image</p></li>
				<li>Now that we have successfully exercised the model, let's save it. Run the following cell to save the model:<p class="source-code">learn.path = Path('/notebooks/temp')</p><p class="source-code">learn.export('cifar_apr20_2021.pkl') </p><p>Here is what the statements in this cell do:</p><p>a) <strong class="source-inline">learn.path = Path('/notebooks/temp')</strong>: Sets the path of the <strong class="source-inline">learn</strong> object to a directory that can be written to. Remember that in Gradient, by default, the path for the <strong class="source-inline">learn</strong> object is read-only, so you need to adjust the path to a writeable directory before you can save the model.</p><p>b) <strong class="source-inline">learn.export('cifar_apr20_2021.pkl')</strong>: Specifies that the name of the model to be saved is <strong class="source-inline">cifar_apr20_2021.pkl</strong>.</p><p>After you have run this cell in Gradient, your model is saved in <strong class="source-inline">/notebooks/temp/model/cifar_apr20_2021.pkl</strong>.</p></li>
			</ol>
			<p>Congratulations! You <a id="_idIndexMarker440"/>have<a id="_idIndexMarker441"/> trained and exercised a fastai image classification model using a curated image dataset.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor157"/>How it works…</h2>
			<p>Some important things are going on in the recipe in this section that are worth reviewing. In this section, we'll go over some of the details that might not have been evident in the main part of the recipe.</p>
			<h3>Labels are encoded in directory names and filenames</h3>
			<p>First, consider<a id="_idIndexMarker442"/> the<a id="_idIndexMarker443"/> names of the files that make up the dataset. The files in the <strong class="source-inline">train/dog</strong> subdirectory are shown here:</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B16216_6_12.jpg" alt="Figure 6.12 – Files in the train/dog subdirectory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – Files in the train/dog subdirectory</p>
			<p>The files in the <strong class="source-inline">train/cat</strong> subdirectory are shown here:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/B16216_6_13.jpg" alt="Figure 6.13 – Files in the train/cat subdirectory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – Files in the train/cat subdirectory</p>
			<p>Let's take a look at a sample file from each of these subdirectories. To display an image file from the <strong class="source-inline">train/dog</strong> subdirectory, run the following cell:</p>
			<p class="source-code">dog_files = get_image_files(path/"train/dog")</p>
			<p class="source-code">dog_img = PILImage.create(dog_files[30])</p>
			<p class="source-code">dog_img</p>
			<p>The<a id="_idIndexMarker444"/> output <a id="_idIndexMarker445"/>of this cell is indeed an image of a dog, as shown here:</p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/B16216_6_14.jpg" alt="Figure 6.14 – A dog image from the training dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – A dog image from the training dataset</p>
			<p>To display an image file from the <strong class="source-inline">train/cat</strong> subdirectory, run the following cell:</p>
			<p class="source-code">cat_files = get_image_files(path/"train/cat")</p>
			<p class="source-code">cat_img = PILImage.create(cat_files[30])</p>
			<p class="source-code">cat_img</p>
			<p>The output of this cell is, as expected, an image of a cat, as shown here:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B16216_6_15.jpg" alt="Figure 6.15 – A cat image from the training dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – A cat image from the training dataset</p>
			<p>For this image classification problem, the model is trying to predict the label for an image, that is, the category, such as dog, deer, or bird, of the object that is shown in the image. For<a id="_idIndexMarker446"/> this dataset, the label is encoded in the following<a id="_idIndexMarker447"/> two ways:</p>
			<ul>
				<li><strong class="bold">The name of the directory containing the image file</strong>. As you can see in <em class="italic">Figure 6.14</em> and <em class="italic">Figure 6.15</em>, the label for an image is encoded in the name of the subdirectory where the image is located.</li>
				<li><strong class="bold">The name of the file</strong>. As you can see in <em class="italic">Figure 6.12</em> and <em class="italic">Figure 6.13</em>, the dog images all have filenames of the form <strong class="source-inline">xxxx_dog.png</strong>, and the cat images have filenames of the form <strong class="source-inline">xxxx_cat.png</strong>.</li>
			</ul>
			<h3>You used transfer learning to train the image classification model</h3>
			<p>In <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, we used transfer learning to adapt an existing <a id="_idIndexMarker448"/>trained model to work<a id="_idIndexMarker449"/> with a dataset for a particular use case. You may not have noticed it, but we did the same thing in this recipe. There's a clue in the cell where you defined the <strong class="source-inline">cnn_learner</strong> object as follows:</p>
			<p class="source-code">learn = cnn_learner(dls, resnet18, </p>
			<p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p>
			<p class="source-code">                    metrics=accuracy))</p>
			<p>In the definition of <strong class="source-inline">cnn_learner</strong>, the <strong class="source-inline">resnet18</strong> argument is the pre-trained model that is used as a basis for the image classification model. The first time you run this cell, you will see a message like the one shown in <em class="italic">Figure 6.16</em>, indicating that the model is being set up in your environment:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B16216_6_16.jpg" alt="Figure 6.16 – Message you get the first time you run the cnn_learner definition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Message you get the first time you run the cnn_learner definition</p>
			<p>The second clue is in the cell where you train the model, as follows:</p>
			<p class="source-code">learn.fine_tune(5)</p>
			<p>For most <a id="_idIndexMarker450"/>of <a id="_idIndexMarker451"/>the models you have seen so far in this book, you trained the model using <strong class="source-inline">learn.fit_one_cycle()</strong>. Here, because we want to update an existing model for our particular use case, we use <strong class="source-inline">learn.fine_tune()</strong> instead. This is exactly what we did in <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, for the language models we trained using transfer learning on top of the pre-trained <strong class="source-inline">AWD_LSTM</strong> model. </p>
			<p>Why did we use transfer learning for this use case instead of training the model from scratch? The simple answer is that we get decent performance from the model faster by using transfer learning. You can try it out yourself by making two changes to the <strong class="source-inline">training_with_curated_image_datasets.ipynb</strong> notebook and rerunning it. Here are the steps to follow to do this:</p>
			<ol>
				<li value="1">Update the definition of the <strong class="source-inline">cnn_learner</strong> object to include the <strong class="source-inline">pretrained=False</strong> argument, as shown here:<p class="source-code">learn = cnn_learner(dls, resnet18, pretrained=False,</p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)</p><p>This change means that the model will be trained from scratch with the <strong class="source-inline">CIFAR</strong> dataset rather than by taking advantage of the pre-trained model. </p></li>
				<li>Change the training statement to the following:<p class="source-code">learn.fit_one_cycle(5)</p></li>
			</ol>
			<p>When you run the notebook after making these changes, this new model will not do a very good job of classifying images. Transfer learning works, and as Howard and Gugger explain in their book, it doesn't get nearly enough attention in standard introductions to deep learning. It's lucky for us that fastai is designed to make it easy to exploit the power of <a id="_idIndexMarker452"/>transfer learning, as <a id="_idIndexMarker453"/>shown by the image classification model that you trained in the <em class="italic">How to do it…</em> section of this recipe.</p>
			<h3>A closer look at the model</h3>
			<p>Before moving on to the next recipe, it's worth taking a closer look at the model from this recipe. A deeply detailed description of the model is beyond the scope of this book, so we will just focus on some highlights here. </p>
			<p>As shown in the recipe, the model is defined as a <strong class="source-inline">cnn_learner</strong> object (documentation here: <a href="https://docs.fast.ai/vision.learner.html#cnn_learner">https://docs.fast.ai/vision.learner.html#cnn_learner</a>) that uses the pre-trained <strong class="source-inline">resnet18</strong> model (documentation here: <a href="https://pytorch.org/vision/stable/models.html">https://pytorch.org/vision/stable/models.html</a>). The output of <strong class="source-inline">learn.summary()</strong> for this model shows that the model includes a series of convolutional layers and associated layers. For a description of convolutional neural networks (CNNs), see: <a href="https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/">https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/</a>.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor158"/>Exploring a curated image location dataset</h1>
			<p>Back in <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a>, <em class="italic">Exploring and Cleaning Up Data with fastai</em>, we went through the process<a id="_idIndexMarker454"/> of ingesting and exploring a variety of datasets using fastai. </p>
			<p>In this section, we are going to explore a special curated image dataset called <strong class="source-inline">COCO_TINY</strong>. This<a id="_idIndexMarker455"/> is an <strong class="bold">image location</strong> dataset. Unlike the <strong class="source-inline">CIFAR</strong> dataset that we used in the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe, which had a single labeled object in each image, the images in image location datasets are labeled with bounding boxes (which indicate where in the image a particular object occurs) as well as the name of the object. Furthermore, images in the <strong class="source-inline">COCO_TINY</strong> dataset can contain multiple labeled objects, as shown here:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B16216_6_17.jpg" alt="Figure 6.17 – Labeled image from an image location dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Labeled image from an image location dataset</p>
			<p>In the recipe in this section, we'll ingest the dataset and apply its annotation information to create a <strong class="source-inline">dataloaders</strong> object for the dataset. The annotation information for each image can be quite complex. Each image can depict multiple labeled objects. For each labeled object in an image, the annotation information includes the filename of the image, the <em class="italic">x</em> and <em class="italic">y</em> coordinates of the bounding box that surrounds the <a id="_idIndexMarker456"/>object, and the label for the category of the object. For example, the image shown in <em class="italic">Figure 6.17</em> contains several <strong class="bold">couch</strong> objects and a <strong class="bold">chair</strong> object.</p>
			<p>Note that the images in the dataset don't actually show the annotation. A typical image from the dataset looks as follows:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B16216_6_18.jpg" alt="Figure 6.18 – Raw image from the COCO_TINY dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – Raw image from the COCO_TINY dataset</p>
			<p>There are no bounding boxes or labels in the image itself. That annotation information is contained in a separate file. The recipe in this section will show you how to combine the image files with the annotation information.</p>
			<p>At the end of this recipe, when we use <strong class="source-inline">show_batch()</strong> to display samples from the batch that incorporates annotation, the bounding boxes and labels are shown in the images. For example, <em class="italic">Figure 6.19</em> shows how the image from <em class="italic">Figure 6.18</em> looks when it is displayed by <strong class="source-inline">show_batch()</strong> – now you can see the bounding box and label for the <strong class="bold">vase</strong> object in the image:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B16216_6_19.jpg" alt="Figure 6.19 – Annotated version of the image in Figure 6.18&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19 – Annotated version of the image in Figure 6.18</p>
			<p>In the recipe in this section, you will combine the image files from the <strong class="source-inline">COCO_TINY</strong> dataset, such as the image shown in <em class="italic">Figure 6.18</em>, with the information from the annotation file to get a <strong class="source-inline">dataloaders</strong> object that includes the images along with bounding boxes and labels for all the labeled objects in each image, as shown in <em class="italic">Figure 6.19</em>.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor159"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">exploring_image_location_datasets.ipynb</strong> notebook in the <strong class="source-inline">ch6</strong> directory of your repository.</p>
			<p>The approach<a id="_idIndexMarker457"/> to examining the dataset shown in this recipe includes approaches inspired by this Kaggle kernel, https://www.kaggle.com/jachen36/coco-tiny-test-prediction, which demonstrates how to use fastai to explore an image location dataset.</p>
			<p>The <strong class="source-inline">COCO_TINY</strong> dataset featured in this section is introduced in the paper <em class="italic">Microsoft COCO: Common Objects in Context</em>, Lin et al., 2014. I am grateful for the opportunity to include an example using this dataset in this book.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. (2014). <em class="italic">Microsoft COCO: Common Objects in Context</em>: <a href="https://arxiv.org/abs/1405.0312">https://arxiv.org/abs/1405.0312</a>.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor160"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">exploring_image_location_datasets.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the<a id="_idIndexMarker458"/> required libraries and set up your notebook.</li>
				<li>Run the following cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = untar_data(URLs.COCO_TINY)</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The output shows the directory structure of the dataset, as shown in <em class="italic">Figure 6.20</em>:</p><div id="_idContainer179" class="IMG---Figure"><img src="image/B16216_6_20.jpg" alt="Figure 6.20 – Output of path.ls()&#13;&#10;"/></div><p class="figure-caption">Figure 6.20 – Output of path.ls()</p><p>The dataset consists of a set of image files in the <strong class="source-inline">train</strong> subdirectory and the <strong class="source-inline">train.json</strong> annotation file. In the next few steps of this recipe, we will take a closer look at what's in the <strong class="source-inline">train.json</strong> file.</p></li>
				<li>Run the following cell to bring the <strong class="source-inline">train.json</strong> file into a series of Python dictionaries:<p class="source-code">with open(path/'train.json') as json_file:</p><p class="source-code">    data = json.load(json_file)</p><p class="source-code">    # each nested structure is a list of dictionaries</p><p class="source-code">    categories = data['categories']</p><p class="source-code">    images = data['images']</p><p class="source-code">    annotations = data['annotations']  </p><p>Here are the key parts of the code used in this cell:</p><p>a) <strong class="source-inline">data = json.load(json_file)</strong>: Loads the contents of the whole <strong class="source-inline">train.json</strong> file into the <strong class="source-inline">data</strong> dictionary.</p><p>b) <strong class="source-inline">categories = data['categories']</strong>: Creates a separate list of dictionaries <a id="_idIndexMarker459"/>just for the category definitions. This dictionary defines the objects in the images.</p><p>c) <strong class="source-inline">images = data['images']</strong>: Creates a separate list of dictionaries just for image files.</p><p>d) <strong class="source-inline">annotations = data['annotations']</strong>: Creates a separate list of dictionaries just for the bounding boxes.</p></li>
				<li>Run the following cell to see the structure of each of the dictionaries you created in the previous cell:<p class="source-code">print("categories ", categories)</p><p class="source-code">print()</p><p class="source-code">print("subset of images",list(images)[:5])</p><p class="source-code">print()</p><p class="source-code">print("subset of annotations",list(annotations)[:5])</p><p>The output of this cell shows samples of the contents of each of the three dictionaries, as shown in the following screenshot:</p><div id="_idContainer180" class="IMG---Figure"><img src="image/B16216_6_21.jpg" alt="Figure 6.21 – Contents of annotation dictionaries&#13;&#10;"/></div><p class="figure-caption">Figure 6.21 – Contents of annotation dictionaries</p></li>
				<li>The dictionaries <a id="_idIndexMarker460"/>that we created in the previous step aren't quite what we need to get complete annotations for the images. What we want is a consolidated annotation for each image that lists the bounding boxes for each object in the image along with the object's category. We could do this manually by manipulating the dictionaries we created in the previous step, but fastai provides a very handy function called <strong class="source-inline">get_annotations</strong> that does the work for us. Run the following cell to define the annotation structures:<p class="source-code">image_files, bbox_lbl = get_annotations(path/'train.json<a id="_idTextAnchor161"/>')</p><p class="source-code">img_bbox_combo = dict(zip(image_files, bbox_lbl))</p><p>Here are the key parts of the code in this cell:</p><p>a) <strong class="source-inline">aget_annotations(path/'train.json')</strong>: Applies the <strong class="source-inline">get_annotations</strong> function to the <strong class="source-inline">train.json</strong> file to get an annotation structure. The output of this function is a list of filenames and a list of labeled bounding boxes. </p><p>b) <strong class="source-inline">dict(zip(image_files, bbox_lbl))</strong>: Creates a dictionary that combines the file list and the labeled bounding box list output from the previous command.</p></li>
				<li>Run the following cell to examine one of the elements of the annotation dictionary, <strong class="source-inline">img_bbox_combo</strong>, that you created in the previous cell:<p class="source-code">img_bbox_combo[image_files[5]]</p><p>The output of this cell, as shown in the following screenshot, shows that elements of the dictionary are tuples made up of a list of bounding boxes (sets of 2 <em class="italic">x</em> and <em class="italic">y</em> coordinates that define the top-left and bottom-right points of the box around <a id="_idIndexMarker461"/>the object) and a list of corresponding object categories:</p><div id="_idContainer181" class="IMG---Figure"><img src="image/B16216_6_22.jpg" alt="Figure 6.22 – An element of img_bbox_combo&#13;&#10;"/></div><p class="figure-caption">Figure 6.22 – An element of img_bbox_combo</p></li>
				<li>Run the following cell to see the image associated with the annotation you examined in the previous cell:<p class="source-code">image_subpath = 'train/'+image_files[5]</p><p class="source-code">img = PILImage.create(path/image_subpath)</p><p class="source-code">img</p><p>The output of this cell is the following image:</p><div id="_idContainer182" class="IMG---Figure"><img src="image/B16216_6_23.jpg" alt="Figure 6.23 – Example image&#13;&#10;"/></div><p class="figure-caption">Figure 6.23 – Example image</p></li>
				<li>Run the following cell to define a function to return the bounding box associated<a id="_idIndexMarker462"/> with the input image file:<p class="source-code">def get_bbox(filename):</p><p class="source-code">    return np.array(img_bbox_combo[os.path.basename(filename)][0])</p></li>
				<li>Run the following cell to define a function to return the label (that is, the category) associated with the input image file:<p class="source-code">def get_lbl(filename):</p><p class="source-code">    return np.array(img_bbox_combo[os.path.basename(filename)][1],dtype=object)</p></li>
				<li>Run the following cell to define a function to return the image files in the dataset:<p class="source-code">def get_items(noop):</p><p class="source-code">    return get_image_files(path/'train')</p></li>
				<li>Run the following cell to define a <strong class="source-inline">DataBlock</strong> object for the dataset using the functions that you defined in the previous three cells:<p class="source-code">db = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),</p><p class="source-code">                 get_items=get_image_files,</p><p class="source-code">                 splitter=RandomSplitter(),</p><p class="source-code">                 get_y=[get_bbox, get_lbl],</p><p class="source-code">                 n_inp=1)</p><p>For almost all <a id="_idIndexMarker463"/>of the recipes so far in this book, we have used the top-level fastai API, which means we would be defining some kind of <strong class="source-inline">dataloaders</strong> object at this step. For this dataset, however, we need more flexibility than we can get from a <strong class="source-inline">dataloaders</strong> object, which is why we have defined a <strong class="source-inline">DataBlock</strong> object here. For details on <strong class="source-inline">DataBlock</strong> objects, refer to the following documentation: https://docs.fast.ai/data.block.html#DataBlock.</p><p>Here are the arguments for the definition of the <strong class="source-inline">DataBlock</strong> object:</p><p>a) <strong class="source-inline">blocks=(ImageBlock, BBoxBlock, BBoxLblBlock)</strong>: Specifies that the input to the model is images (<strong class="source-inline">ImageBlock</strong>) and the target has two parts: bounding boxes for object in the images (<strong class="source-inline">BBoxBlock</strong>), and labels (categories) associated with each of the bounding boxes (<strong class="source-inline">BBoxLblBlock</strong>).</p><p>b) <strong class="source-inline">get_items=get_image_files</strong>: Specifies that the <strong class="source-inline">get_image_files</strong> function is called to get the input to the <strong class="source-inline">DataBlock</strong> object.</p><p>c) <strong class="source-inline">get_y=[get_bbox, get_lbl]</strong>: Specifies the functions that are applied to the results of <strong class="source-inline">get_items</strong>. The image filenames are sent as arguments to each of these functions. The first function, <strong class="source-inline">get_bbox</strong>, returns the list of bounding boxes associated with the image file according to the annotation information we ingested from the <strong class="source-inline">train.json</strong> file. The second function, <strong class="source-inline">get_lbl</strong>, returns the list of labels (categories) for the bounding boxes associated with the image file, according to the annotation information we ingested from the <strong class="source-inline">train.json</strong> file.</p><p>d) <strong class="source-inline">n_inp=1</strong>: Specifies the number of elements in the tuples specified in the <strong class="source-inline">blocks</strong> argument that should be considered part of the input, in our case, just the image files.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">dataloaders</strong> object using the <strong class="source-inline">DataBlock</strong> object, <strong class="source-inline">db</strong>, you created in the previous cell:<p class="source-code">dls = db.dataloaders(path,bs=32)</p><p>Here are the <a id="_idIndexMarker464"/>arguments for the <strong class="source-inline">dataloaders</strong> definition:</p><p>a) <strong class="source-inline">path</strong>: Specifies that the path to use for the <strong class="source-inline">dataloaders</strong> object is the <strong class="source-inline">path</strong> object you defined for the dataset at the beginning of the notebook</p><p>b) <strong class="source-inline">bs=32</strong>: Specifies that the batch size is 32</p></li>
				<li>Now it is finally time to see what batches for the <strong class="source-inline">dataloaders</strong> object look like. Run the following cell to see a sample of entries from a batch:<p class="source-code">dls.show_batch(max_n=4, figsize=(10,10))</p><p>Here are the arguments for <strong class="source-inline">show_batch()</strong>:</p><p>a) <strong class="source-inline">max_n=4</strong>: Specifies the number of samples to show</p><p>b) <strong class="source-inline">figsize=(10,10)</strong>: Specifies the dimensions of the samples</p><p>The output of <strong class="source-inline">show_batch()</strong> is shown in the following image. Note that you see the images with the labeled objects highlighted with bounding boxes and annotation text. Also note that you may see different sample images when you run <strong class="source-inline">show_batch()</strong> in your notebook:</p></li>
			</ol>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B16216_6_24.jpg" alt="Figure 6.24 – Sample image output by show_batch()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.24 – Sample image output by show_batch()</p>
			<p>Congratulations! You<a id="_idIndexMarker465"/> have successfully ingested and prepared an image location dataset, one of the most complex data manipulation tasks that you will tackle in this book.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor162"/>How it works…</h2>
			<p>After working through the recipe in this section, you may have some questions about the <strong class="source-inline">COCO_TINY</strong> dataset and the image location dataset in general. In this section, we will go through some of the questions that may have come up as you went through the recipe.</p>
			<h3>What kinds of objects are annotated in the images in the dataset?</h3>
			<p>If you rerun<a id="_idIndexMarker466"/> the following cell several times, you will see a variety of annotated images:</p>
			<p class="source-code">dls.show_batch(max_n=4, figsize=(10,10))</p>
			<p>You can see an example of the output of this cell in <em class="italic">Figure 6.25</em>. Note that the annotations don't cover every possible object in the images. For example, the animal in the first image isn't annotated. Which objects are annotated for images in <strong class="source-inline">COCO_TINY</strong>? We'll answer that question in this section:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B16216_6_25.jpg" alt="Figure 6.25 – Output of show_batch()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.25 – Output of show_batch()</p>
			<p>When you ran the following cell to bring the annotation file, <strong class="source-inline">train.json</strong>, into a Python dictionary, you created a sub-dictionary called <strong class="source-inline">categories</strong>, which contains all the categories of objects annotated in the <strong class="source-inline">COCO_TINY</strong> dataset as follows:</p>
			<p class="source-code">with open(path/'train.json') as json_file:</p>
			<p class="source-code">    data = json.load(json_file)</p>
			<p class="source-code">    # each nested structure is a list of dictionaries</p>
			<p class="source-code">    categories = data['categories']</p>
			<p class="source-code">    images = data['images']</p>
			<p class="source-code">    annotations = data['annotations']  </p>
			<p>To see what's<a id="_idIndexMarker467"/> in the <strong class="source-inline">categories</strong> dictionary, run the following cell:</p>
			<p class="source-code">print("categories ", categories)</p>
			<p>The output of this cell is as follows:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B16216_6_26.jpg" alt="Figure 6.26 – The categories dictionary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.26 – The categories dictionary</p>
			<p>From this dictionary, you can see that there are only six objects annotated for the images in the <strong class="source-inline">COCO_TINY</strong> dataset: <strong class="bold">chair</strong>, <strong class="bold">couch</strong>, <strong class="bold">tv</strong>, <strong class="bold">remote</strong>, <strong class="bold">book</strong>, and <strong class="bold">vase</strong>. </p>
			<p>If you run <strong class="source-inline">show_batch()</strong> repeatedly, you will see that even objects that are only partially shown in an image are annotated. For example, the first image in <em class="italic">Figure 6.25</em> has an annotation for a chair, as shown in the following image, even though only part of the legs of the chair is shown in the image:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B16216_6_27.jpg" alt="Figure 6.27 – Close-up of the chair annotation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.27 – Close-up of the chair annotation</p>
			<p>You will also see that some images contain many annotated objects, and the bounding boxes for these objects can overlap, as shown in the following image:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B16216_6_28.jpg" alt="Figure 6.28 – An image with multiple, overlapping bounding boxes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.28 – An image with multiple, overlapping bounding boxes</p>
			<p>This subsection <a id="_idIndexMarker468"/>describes which objects are annotated in the images in <strong class="source-inline">COCO_TINY</strong>. As you rerun the <strong class="source-inline">show_batch()</strong> command repeatedly, you will see for yourself how complex the annotations can be for some objects.</p>
			<h3>How are the bounding boxes for annotated objects defined?</h3>
			<p>While<a id="_idIndexMarker469"/> you were working through the recipe in<a id="_idIndexMarker470"/> this section, you may have asked yourself where the bounding boxes were defined. When you ran the following cell to bring the annotation file, <strong class="source-inline">train.json</strong>, into a Python dictionary, you created a sub-dictionary called <strong class="source-inline">annotations</strong> that contains all the annotations of objects annotated in the <strong class="source-inline">COCO_TINY</strong> dataset:</p>
			<p class="source-code">with open(path/'train.json') as json_file:</p>
			<p class="source-code">    data = json.load(json_file)</p>
			<p class="source-code">    # each nested structure is a list of dictionaries</p>
			<p class="source-code">    categories = data['categories']</p>
			<p class="source-code">    images = data['images']</p>
			<p class="source-code">    annotations = data['annotations']  </p>
			<p>You can <a id="_idIndexMarker471"/>run <a id="_idIndexMarker472"/>the following cell to see the contents of a subset of the <strong class="source-inline">annotations</strong> dictionary:</p>
			<p class="source-code">print("subset of annotations",list(annotations)[:5])</p>
			<p>The output of this cell shows examples of annotations for objects in a particular image, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B16216_6_29.jpg" alt="Figure 6.29 – Example of annotations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.29 – Example of annotations</p>
			<p>The values for the <strong class="source-inline">bbox</strong> keys in this dictionary define the bounding boxes for objects by specifying the <em class="italic">x</em> and <em class="italic">y</em> values at the extremes of the objects.</p>
			<p>Consider the following image. This image has a single annotated object – let's see how to view the coordinates of its bounding box:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B16216_6_30.jpg" alt="Figure 6.30 – Example annotated image with a bounding box&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.30 – Example annotated image with a bounding box</p>
			<p>The <a id="_idIndexMarker473"/>filename<a id="_idIndexMarker474"/> for this image is <strong class="source-inline">000000071159.jpg</strong>. To see the bounding box for this image, run the following cell:</p>
			<p class="source-code">get_bbox(path/'train/000000071159.jpg')</p>
			<p>The output for this cell shows the bounding box coordinates from this image. If the image had multiple objects annotated in it, there would be a bounding box defined for each object:</p>
			<p class="figure-caption"><img src="image/B16216_6_31.png" alt="Figure 6.31 – Bounding box for the object in the example image in Figure 6.30&#13;&#10;"/></p>
			<p class="figure-caption">Figure 6.31 – Bounding box for the object in the example image in Figure 6.30</p>
			<p>In this section, we have reviewed some of the details of the bounding box annotations in the <strong class="source-inline">COCO_TINY</strong> dataset.</p>
			<h3>Why didn't we train a model using COCO_TINY?</h3>
			<p>Unlike most <a id="_idIndexMarker475"/>of the recipes in this book, the recipe in<a id="_idIndexMarker476"/> this section did not include model training. After the effort we put into creating a <strong class="source-inline">dataloaders</strong> object for the <strong class="source-inline">COCO_TINY</strong> dataset, why didn't we go all the way and train a model with it? </p>
			<p>The simple answer is that the fastai framework does not currently incorporate a simple way to train a model on an image location dataset such as <strong class="source-inline">COCO_TINY</strong>. If you want to attempt to train such a model, you can check out this repo for an approach, but be prepared to <a id="_idIndexMarker477"/>get into details well beyond the high-level<a id="_idIndexMarker478"/> fastai API that we have been exploring in this book so far: <a href="https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Object_Detection.ipynb">https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Object_Detection.ipynb</a>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor163"/>Training a classification model with a standalone vision dataset</h1>
			<p>In the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe, you<a id="_idIndexMarker479"/> went through the steps<a id="_idIndexMarker480"/> to ingest a fastai curated dataset and use it to train an image classification model. </p>
			<p>In this section, you will go through the same process for a standalone dataset called <strong class="source-inline">fruits-360</strong>. This dataset (described in more detail here: https://www.kaggle.com/moltean/fruits) contains over 90,000 images of fruits and vegetables organized into over 130 categories. </p>
			<p>In this recipe, we'll begin by bringing this dataset into Gradient. Then we will work through the <strong class="source-inline">training_with_standalone_image_datasets.ipynb</strong> notebook to ingest the dataset and use it to train a fastai image classification model. Finally, we will see how well the trained model classifies images from the test set and save the trained model.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor164"/>Getting ready</h2>
			<p>Ensure that you have uploaded the <strong class="source-inline">fruits-360</strong> dataset to your Gradient environment by following these steps:</p>
			<ol>
				<li value="1">Download <strong class="source-inline">archive.zip</strong> from <a href="https://www.kaggle.com/moltean/fruits">https://www.kaggle.com/moltean/fruits</a>.</li>
				<li>Upload <strong class="source-inline">archive.zip</strong> to your Gradient environment. You can use the upload button in JupyterLab in Gradient to do the upload, but you need to do it in several steps as follows:<p>a) From the terminal in your Gradient environment, make <strong class="source-inline">/notebooks</strong> your current directory:</p><p class="source-code"><strong class="bold">cd /notebooks</strong></p><p>b) If you <a id="_idIndexMarker481"/>have not <a id="_idIndexMarker482"/>already created a <strong class="source-inline">notebooks/temp</strong> directory, make a new <strong class="source-inline">/notebooks/temp</strong> directory:</p><p class="source-code"><strong class="bold">mkdir temp</strong></p><p>c) In the JupyterLab file browser, make <strong class="source-inline">temp</strong> your current folder, select the upload button (see <em class="italic">Figure 6.32</em>), and select <strong class="source-inline">archive.zip</strong> from your local system folder where you downloaded it in <em class="italic">Step 1</em>:</p><div id="_idContainer191" class="IMG---Figure"><img src="image/B16216_6_32.jpg" alt="Figure 6.32 – Upload button in JupyterLab&#13;&#10;"/></div><p class="figure-caption">Figure 6.32 – Upload button in JupyterLab</p></li>
				<li>Now that you have uploaded <strong class="source-inline">archive.zip</strong> to the <strong class="source-inline">/notebooks/temp</strong> directory in your Gradient environment, make that directory your current directory by running the following command in the Gradient terminal:<p class="source-code"><strong class="bold">cd /notebooks/temp</strong></p></li>
				<li>Unzip <strong class="source-inline">archive.zip</strong> into the <strong class="source-inline">/storage/archive</strong> directory by running the following command in the Gradient terminal:<p class="source-code"><strong class="bold">unzip archive.zip -d /storage/archive</strong></p></li>
				<li>Confirm that you now have the dataset set up in your Gradient environment by running the following command from within the Gradient terminal: <p class="source-code"><strong class="bold">cd /storage/archive/fruits-360</strong></p></li>
				<li>Then, run the following command to list the contents of the directory:<p class="source-code"><strong class="bold">ls</strong></p><p>The output <a id="_idIndexMarker483"/>of this <a id="_idIndexMarker484"/>command should look like what is shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B16216_6_33.jpg" alt="Figure 6.33 – Contents of the fruits-360 directory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.33 – Contents of the fruits-360 directory</p>
			<p>With these preparation steps, you have moved the files that make up the <strong class="source-inline">fruits-360</strong> dataset to the correct location in your Gradient environment to be used by a fastai model.</p>
			<p>The <strong class="source-inline">fruits-360</strong> dataset featured in this section is introduced in the Kaggle competition <strong class="bold">Fruits 360</strong> from 2020 (<a href="https://www.kaggle.com/moltean/fruits">https://www.kaggle.com/moltean/fruits</a>). I am grateful for the opportunity to include an example using this dataset in this book.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Mihai Oltean (2017-2020). <em class="italic">Fruits 360 – A dataset with 90,380 images of 131 fruits and vegetables</em> (<a href="https://mihaioltean.github.io">https://mihaioltean.github.io</a>).</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor165"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_with_standalone_image_datasets.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the following cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = URLs.path('fruits-360')</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The <a id="_idIndexMarker485"/>output shows the <a id="_idIndexMarker486"/>directory structure of the dataset, as shown in the following screenshot:</p><div id="_idContainer193" class="IMG---Figure"><img src="image/B16216_6_34.jpg" alt="Figure 6.34 – Output of path.ls()&#13;&#10;"/></div><p class="figure-caption">Figure 6.34 – Output of path.ls()</p></li>
				<li>Run the following cell to define an <strong class="source-inline">ImageDataLoaders</strong> object for this dataset:<p class="source-code">dls = ImageDataLoaders.from_folder(path, train='Training', valid='Test')</p><p>Here are the arguments for the definition of the <strong class="source-inline">ImageDataLoaders</strong> object:</p><p>a) <strong class="source-inline">path</strong>: Specifies that the <strong class="source-inline">ImageDataLoaders</strong> object is defined using the <strong class="source-inline">path</strong> object you created in the previous cell</p><p>b) <strong class="source-inline">train='Training'</strong>: Specifies that the training data is in the <strong class="source-inline">/storage/archive/fruits-360/Training</strong> directory</p><p>c) <strong class="source-inline">valid='Test'</strong>: Specifies that the validation data is in the <strong class="source-inline">/storage/archive/fruits-360/Test</strong> directory</p></li>
				<li>Run the following cell to display a batch from the dataset:<p class="source-code">dls.train.show_batch(max_n=4, nrows=1)</p><p>The output of this cell is a set of <strong class="source-inline">4</strong> items from a batch, showing images along with their <a id="_idIndexMarker487"/>corresponding<a id="_idIndexMarker488"/> categories, as shown in the following screenshot:</p><div id="_idContainer194" class="IMG---Figure"><img src="image/B16216_6_35.jpg" alt="Figure 6.35 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 6.35 – Output of show_batch()</p></li>
				<li>Run the following cell to examine the contents of the <strong class="source-inline">Training</strong> subdirectory:<p class="source-code">(path/'Training').ls() </p><p>The output shows the structure of the <strong class="source-inline">Training</strong> subdirectory, as shown in the following screenshot:</p><div id="_idContainer195" class="IMG---Figure"><img src="image/B16216_6_36.jpg" alt=""/></div><p class="figure-caption">Figure 6.36 – Contents of the Training subdirectory</p></li>
				<li>Run the following cell to see a single item in the dataset:<p class="source-code">img_files = get_image_files(path)</p><p class="source-code">img = PILImage.create(img_files[100])</p><p class="source-code">img</p><p>Here are the <a id="_idIndexMarker489"/>key elements<a id="_idIndexMarker490"/><a id="_idIndexMarker491"/> of this cell:</p><p>a) <strong class="source-inline">img_files = get_image_files(path)</strong>: Specifies that <strong class="source-inline">path</strong> is recursively examined and returns all the image files in the path</p><p>b) <strong class="source-inline">img = PILImage.create(img_files[100])</strong>: Creates the image object, <strong class="source-inline">img</strong>, from a specific file returned by the previous statement</p><p>The output of this cell is one of the dataset files rendered as an image in the notebook:</p><div id="_idContainer196" class="IMG---Figure"><img src="image/B16216_6_37.jpg" alt="Figure 6.37 – An image from the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.37 – An image from the dataset</p></li>
				<li>Run the following cell to define the model as a <strong class="source-inline">cnn_learner</strong> object:<p class="source-code">learn = cnn_learner(dls, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy))</p><p>Here are the arguments for the definition of the <strong class="source-inline">cnn_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: Specifies that the <strong class="source-inline">cnn_learner</strong> object is defined using the <strong class="source-inline">ImageDataLoaders</strong> object<a id="_idIndexMarker492"/> you <a id="_idIndexMarker493"/>defined earlier in this notebook</p><p>b) <strong class="source-inline">resnet18</strong>: Specifies the pre-trained model to use as a starting point for this model</p><p>c) <strong class="source-inline">loss_func=LabelSmoothingCrossEntropy()</strong>: Specifies the loss function to use in the training process</p><p>d) <strong class="source-inline">metrics=accuracy</strong>: Specifies that <strong class="source-inline">accuracy</strong> is the performance metric that will be optimized in the training process</p></li>
				<li>Run the following cell to train the model:<p class="source-code">learn.fine_tune(5)</p><p>The argument indicates that the training run will be for <strong class="source-inline">5</strong> epochs.</p><p>The output displays the training loss, validation loss, and accuracy for each epoch, as shown in the following screenshot: </p><div id="_idContainer197" class="IMG---Figure"><img src="image/B16216_6_38.jpg" alt="Figure 6.38 – Output of training the model&#13;&#10;"/></div><p class="figure-caption">Figure 6.38 – Output of training the model</p></li>
				<li>Let's try <a id="_idIndexMarker494"/>out the trained<a id="_idIndexMarker495"/> model on some examples from the test dataset. First, run the following cell to define an object for one of the images in the test dataset:<p class="source-code">img_test_files = get_image_files(path/"Test")</p><p class="source-code">img2 = PILImage.create(img_test_files[700])</p><p class="source-code">img2</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">img_files = get_image_files(path/"Test"))</strong>: Returns all the image files under the <strong class="source-inline">Test</strong> directory</p><p>b) <strong class="source-inline">img = PILImage.create(img_files[700])</strong>: Creates the image object, <strong class="source-inline">img2</strong>, from a specific file returned by the previous statement</p><p>The output of this cell is an image of a strawberry, as shown in the following screenshot:</p><div id="_idContainer198" class="IMG---Figure"><img src="image/B16216_6_39.jpg" alt="Figure 6.39 – An image of a strawberry from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.39 – An image of a strawberry from the test dataset</p></li>
				<li>Run the following cell to define an object for another one of the images in the test dataset:<p class="source-code">img3 = PILImage.create(img_test_files[8000])</p><p class="source-code">img3</p><p>The output <a id="_idIndexMarker496"/>of <a id="_idIndexMarker497"/>this cell is an image of a tomato, as shown in the following screenshot:</p><div id="_idContainer199" class="IMG---Figure"><img src="image/B16216_6_40.jpg" alt="Figure 6.40 – An image of a tomato from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.40 – An image of a tomato from the test dataset</p></li>
				<li>Now that we have defined objects for a couple of images from the test dataset, let's exercise the trained image classification model on them. First, run the following cell to apply the model to the strawberry image:<p class="source-code">learn.predict(img2)</p><p>The output of this cell is the model's prediction, as shown in the following screenshot. Note that the model correctly predicts the category of the image. The numbers shown in the <strong class="source-inline">TensorImage</strong> array correspond to the likelihood that the<a id="_idIndexMarker498"/> trained<a id="_idIndexMarker499"/> model ascribes to the image being in each of the categories: </p><div id="_idContainer200" class="IMG---Figure"><img src="image/B16216_6_41.jpg" alt="Figure 6.41 – Image classification model's prediction on the image of the strawberry&#13;&#10;"/></div><p class="figure-caption">Figure 6.41 – Image classification model's prediction on the image of the strawberry</p></li>
				<li>Let's see how the model does on the tomato image. Run the following cell to apply the model to the image of the tomato:<p class="source-code">learn.predict(img3)</p><p>The output of this cell is the model's prediction, as shown in the following screenshot. Note that the model correctly predicts that the image is a tomato:</p><div id="_idContainer201" class="IMG---Figure"><img src="image/B16216_6_42.jpg" alt="Figure 6.42 – Image classification model's prediction on the image of a bird&#13;&#10;"/></div><p class="figure-caption">Figure 6.42 – Image classification model's prediction on the image of a bird</p></li>
				<li>The trained <a id="_idIndexMarker500"/>model <a id="_idIndexMarker501"/>seems to be doing a good job of predicting the category of images from the test set, but there is still some ambiguity. The fruit and vegetable images can be ambiguous to a human, so let's see how the trained model predicts the category of images where we know exactly what the category is. First, run the following cell to define an image of an avocado from the test dataset:<p class="source-code">avocado_files = get_image_files(path/"Test/Avocado")</p><p class="source-code">avocado_img = PILImage.create(avocado_files[30])</p><p class="source-code">avocado_img</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">avocado_files = get_image_files(path/"Test/Avocado"))</strong>: Returns all the image files under the <strong class="source-inline">Test/Avocado</strong> directory</p><p>b) <strong class="source-inline">avocado_img = PILImage.create(avocado_files[30])</strong>: Creates the image object, <strong class="source-inline">avocado_img</strong>, from a specific avocado image file from the file set returned by the previous statement</p><p>The output of this cell is an image of an avocado, as shown in the following screenshot:</p><div id="_idContainer202" class="IMG---Figure"><img src="image/B16216_6_43.jpg" alt="Figure 6.43 – Image of an avocado from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.43 – Image of an avocado from the test dataset</p></li>
				<li>Let's get <a id="_idIndexMarker502"/>another <a id="_idIndexMarker503"/>image from a different directory in the test dataset. Run the following cell to define an image of a walnut from the test dataset:<p class="source-code">walnut_files = get_image_files(path/"Test/Walnut")</p><p class="source-code">walnut_img = PILImage.create(walnut_files[30])</p><p class="source-code">walnut_img</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">walnut_files = get_image_files(path/"Test/Walnut")</strong>: Returns all the image files under the <strong class="source-inline">Test/Walnut</strong> directory</p><p>b) <strong class="source-inline">walnut_img = PILImage.create(walnut_files[30])</strong>: Creates the image object, <strong class="source-inline">walnut_img</strong>, from a specific walnut image file from the file set returned by the previous statement</p><p>The output of this cell is an image of a walnut, as shown in the following screenshot:</p><div id="_idContainer203" class="IMG---Figure"><img src="image/B16216_6_44.jpg" alt="Figure 6.44 – Image of a walnut from the test dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.44 – Image of a walnut from the test dataset</p></li>
				<li>Now that<a id="_idIndexMarker504"/> we<a id="_idIndexMarker505"/> have defined objects for a couple of images from specific directories in the test dataset, let's exercise the trained image classification model on them. First, run the following cell to apply the model to the avocado image:<p class="source-code">learn.predict(avocado_img)</p><p>The output of this cell is the model's prediction for the <strong class="source-inline">avocado_img</strong> image, as shown in the following screenshot. Note that the model correctly predicts the category of the image:</p><div id="_idContainer204" class="IMG---Figure"><img src="image/B16216_6_45.jpg" alt="Figure 6.45 – Image classification model's prediction on the image of the avocado&#13;&#10;"/></div><p class="figure-caption">Figure 6.45 – Image classification model's prediction on the image of the avocado</p></li>
				<li>Let's<a id="_idIndexMarker506"/> see<a id="_idIndexMarker507"/> how the model does on the walnut image. Run the following cell to apply the model to the image of the walnut:<p class="source-code">learn.predict(walnut_img) </p><p>The output of this cell is the model's prediction, as shown in the following screenshot. Note that the model correctly predicts that the image is a walnut:</p><div id="_idContainer205" class="IMG---Figure"><img src="image/B16216_6_46.jpg" alt="Figure 6.46 – Image classification model's prediction on the image of the walnut&#13;&#10;"/></div><p class="figure-caption">Figure 6.46 – Image classification model's prediction on the image of the walnut</p></li>
				<li>Now <a id="_idIndexMarker508"/>that<a id="_idIndexMarker509"/> we have exercised the model to confirm that it makes good predictions for a small set of images, run the following cell to save the model:<p class="source-code">learn.save("fruits_model"+modifier)</p><p>The output of this cell confirms that the model was saved in the <strong class="source-inline">models</strong> subdirectory of the dataset path, as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B16216_6_47.jpg" alt="Figure 6.47 – Output of saving the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.47 – Output of saving the model</p>
			<p>Congratulations! You have trained a fastai image classification model on a large standalone image dataset and exercised the trained model with a set of examples from the dataset.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor166"/>How it works…</h2>
			<p>If you compare the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe with the recipe in this section, you will notice that the code is very similar. In fact, once you have brought the <strong class="source-inline">fruits-360</strong> dataset into your Gradient environment, the code in the <strong class="source-inline">training_with_standalone_image_datasets.ipynb</strong> notebook is very close to the code in the <strong class="source-inline">training_with_curated_image_datasets.ipynb</strong> notebook. There are, however, some differences, as shown in the following list:</p>
			<ul>
				<li>The <a id="_idIndexMarker510"/>path<a id="_idIndexMarker511"/> definition statement is different. For the curated dataset, the definition of the path statement has a <strong class="source-inline">URLs</strong> object as its argument:<p class="source-code">path = untar_data(URLs.CIFAR)</p><p>Whereas the path statement for the standalone dataset has the <strong class="source-inline">fruits-360</strong> directory as its argument:</p><p class="source-code">path = URLs.path('fruits-360')</p></li>
				<li>The test and train directories have different names between the two datasets.</li>
				<li>The standalone dataset is much bigger than the curated dataset, both in terms of the volume of images and the number of categories that the images are organized into.</li>
				<li>Out of the box, the trained model for the standalone dataset shows higher accuracy (over 95%) than the curated dataset (~80%). </li>
			</ul>
			<p>Despite these differences, it is remarkable that fastai can train a model to categorize images in the <strong class="source-inline">fruits-360</strong> dataset so easily. As you saw in the recipe in this section, you were able to train the model on the <strong class="source-inline">fruits-360</strong> dataset and get great performance, with just a few lines of code. I think the model you created in this recipe is a great example of the power and flexibility of fastai.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor167"/>Training a multi-image classification model with a curated vision dataset</h1>
			<p>In the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe, you <a id="_idIndexMarker512"/>went through <a id="_idIndexMarker513"/>the steps to ingest a fastai curated dataset and used it to train an image classification model. </p>
			<p>In this section, you will go through the same process for another curated dataset called <strong class="source-inline">PASCAL_2007</strong>. This dataset (described in more detail here: http://host.robots.ox.ac.uk/pascal/VOC/) contains about 5,000 training images and the same number of test images. The dataset includes annotations that identify common objects that appear in each image. The identified objects are from 20 categories, including animals (cow, dog, cat, sheep, and horse), vehicles (boat, bus, train, airplane, bicycle, and car), and other items (person, sofa, bottle, and TV monitor).</p>
			<p>The images in the <strong class="source-inline">CIFAR</strong> dataset introduced in the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe had a single labeled object. By contrast, the images in <strong class="source-inline">PASCAL_2007</strong> can have zero, one, or more labeled objects. As you will see in this section, there are some challenges with training a model to predict multiple objects in the same image. </p>
			<p>In this section, we will ingest the <strong class="source-inline">PASCAL_2007</strong> dataset, including image annotations, explore the dataset, train a fastai model to categorize the images according to what objects are depicted in the images, and then exercise the trained model on some examples from the test dataset.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor168"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_curated_multi_image_classification_datasets.ipynb</strong> notebook in the <strong class="source-inline">ch6</strong> directory of your repo.</p>
			<p>The <strong class="source-inline">PASCAL_2007</strong> dataset featured in this section is introduced in this paper – <em class="italic">The PASCAL Visual Object Classes (VOC) Challenge</em>: http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf. I am grateful for the opportunity to include an example using this dataset in this book.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman (2008). <em class="italic">The PASCAL Visual Object Classes (VOC) Challenge</em> (http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf).</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor169"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_with_curated_multi_image_classification_datasets.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete<a id="_idIndexMarker514"/> the<a id="_idIndexMarker515"/> following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the following cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = untar_data(URLs.PASCAL_2007)</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The output shows the directory structure of the dataset, as shown in the following screenshot:</p><div id="_idContainer207" class="IMG---Figure"><img src="image/B16216_6_48.jpg" alt="Figure 6.48 – Output of path.ls()&#13;&#10;"/></div><p class="figure-caption">Figure 6.48 – Output of path.ls()</p><p>Let's now review the key items in the directory structure of the <strong class="source-inline">PASCAL_2007</strong> dataset that we will be using in this recipe:</p><p>a) <strong class="source-inline">/storage/data/pascal_2007/train</strong>: A directory containing images for training the model</p><p>b) <strong class="source-inline">/storage/data/pascal_2007/test</strong>: A directory containing images for testing the trained model</p><p>c) <strong class="source-inline">/storage/data/pascal_2007/train.json</strong>: A file containing annotations <a id="_idIndexMarker516"/>for<a id="_idIndexMarker517"/> the training images</p><p>d) <strong class="source-inline">/storage/data/pascal_2007/test.json</strong>: A file containing annotations for test images</p></li>
				<li>Run the following cell to bring the annotations in the <strong class="source-inline">train.json</strong> file into Python objects:<p class="source-code">with open(path/'train.json') as json_file:</p><p class="source-code">    data = json.load(json_file)</p><p class="source-code">    # each nested structure is a list of dictionaries</p><p class="source-code">    categories = data['categories']</p><p class="source-code">    images = data['images']</p><p class="source-code">    annotations = data['annotations']</p><p>Here are the key parts of the code used in this cell:</p><p>a) <strong class="source-inline">data = json.load(json_file)</strong>: Loads the contents of the whole <strong class="source-inline">train.json</strong> file into the <strong class="source-inline">data</strong> dictionary.</p><p>b) <strong class="source-inline">categories = data['categories']</strong>: Creates a separate list of dictionaries just for the category definitions. This dictionary defines the objects in the images in the dataset.</p><p>c) <strong class="source-inline">images = data['images']</strong>: Creates a separate list of dictionaries just for image files.</p><p>d) <strong class="source-inline">annotations = data['annotations']</strong>: Creates a separate list of dictionaries just for the annotations that specify the objects in the images and their bounding boxes.</p><p>As you will see in later steps in this recipe, we will not be using the <strong class="source-inline">categories</strong>, <strong class="source-inline">images</strong>, and <strong class="source-inline">annotations</strong> lists of dictionaries to extract the annotations to feed into the fastai model training process. Instead, we will be using the fastai built-in API, <strong class="source-inline">get_annotations</strong>, to work directly with the annotations. The dictionaries we define in this cell will, however, be useful to help us<a id="_idIndexMarker518"/> to <a id="_idIndexMarker519"/>understand the details regarding the annotations.</p></li>
				<li>Run the following cell to see some examples of annotations:<p class="source-code">print("categories ", categories)</p><p class="source-code">print()</p><p class="source-code">print("subset of images",list(images)[:5])</p><p class="source-code">print()</p><p class="source-code">print("subset of annotations",list(annotations)[:5])</p><p>The output of this cell lists entries from each of the lists of dictionaries that you created in the previous cell. As shown in the following screenshot, the <strong class="source-inline">categories</strong> list of dictionaries contains the categories of objects that can appear in the images along with their IDs:</p><div id="_idContainer208" class="IMG---Figure"><img src="image/B16216_6_49.jpg" alt="Figure 6.49 – Entries from the categories list of dictionaries&#13;&#10;"/></div><p class="figure-caption">Figure 6.49 – Entries from the categories list of dictionaries</p><p>The following screenshot shows that the <strong class="source-inline">images</strong> list of dictionaries lists the filenames of the image files in the training set along with their IDs and dimensions:</p><div id="_idContainer209" class="IMG---Figure"><img src="image/B16216_6_50.jpg" alt="Figure 6.50 – Entries from the images list of dictionaries&#13;&#10;"/></div><p class="figure-caption">Figure 6.50 – Entries from the images list of dictionaries</p><p>The following <a id="_idIndexMarker520"/>screenshot<a id="_idIndexMarker521"/> shows that <strong class="source-inline">annotations</strong> lists the bounding boxes and category IDs of the objects that appear in the image with the given <strong class="source-inline">image_id</strong>:</p><div id="_idContainer210" class="IMG---Figure"><img src="image/B16216_6_51.jpg" alt="Figure 6.51 – Entries from the annotations list of dictionaries&#13;&#10;"/></div><p class="figure-caption">Figure 6.51 – Entries from the annotations list of dictionaries</p></li>
				<li>We will use the fastai function, <strong class="source-inline">get_annotations</strong>, to get all the annotation information that we need for a given image, that is, the categories for the objects that are depicted in the image. Run the following cell to define the required annotation structures:<p class="source-code">image_files, bbox_lbl = get_annotations(path/'train.json')</p><p class="source-code">img_bbox_combo = dict(zip(image_files, bbox_lbl))</p><p>Here are the key parts of the code used in this cell:</p><p>a) <strong class="source-inline">get_annotations(path/'train.json')</strong>: Applies the <strong class="source-inline">get_annotations</strong> function to the <strong class="source-inline">train.json</strong> file to get an annotation structure. The output of this function is a list of filenames and a list containing bounding boxes and object categories for each object in the image. The following screenshot shows an example of the contents of <strong class="source-inline">bbox_lbl</strong>, specifying <a id="_idIndexMarker522"/>the <a id="_idIndexMarker523"/>bounding boxes and categories for three objects: </p><div id="_idContainer211" class="IMG---Figure"><img src="image/B16216_6_52.jpg" alt="Figure 6.52 – Example bounding box and categories &#13;&#10;"/></div><p class="figure-caption">Figure 6.52 – Example bounding box and categories </p><p>b) <strong class="source-inline">dict(zip(image_files, bbox_lbl))</strong>: Creates a dictionary that combines the file list and the labeled bounding box list output from the previous command.</p></li>
				<li>Let's now take a look at one of the image files from the training set along with its annotations. First, run the following cell to see the annotations for a specific image file:<p class="source-code">img_bbox_combo[image_files[5]]</p><p>The output shows the annotations associated with this image file:</p><div id="_idContainer212" class="IMG---Figure"><img src="image/B16216_6_53.jpg" alt="Figure 6.53 – Annotations for an image file&#13;&#10;"/></div><p class="figure-caption">Figure 6.53 – Annotations for an image file</p></li>
				<li>Run the following cell to take a look at the image whose annotations we examined in the previous cell:<p class="source-code">image_subpath = 'train/'+image_files[5]</p><p class="source-code">img = PILImage.create(path/image_subpath)</p><p class="source-code">img</p><p>The output<a id="_idIndexMarker524"/> shows<a id="_idIndexMarker525"/> the image associated with <strong class="source-inline">image_files[5]</strong>. As you can see in the following image, this image does indeed contain three planes, as indicated in the annotations you saw in the previous cell:</p><div id="_idContainer213" class="IMG---Figure"><img src="image/B16216_6_54.jpg" alt="Figure 6.54 – Image whose annotation indicates it contains three planes&#13;&#10;"/></div><p class="figure-caption">Figure 6.54 – Image whose annotation indicates it contains three planes</p></li>
				<li>Run the following cell to define the <strong class="source-inline">get_category</strong> function to get values out of the list of dictionaries you created earlier. We will use this function later to examine the annotations of images in the test set:<p class="source-code">def get_category(in_key_value,in_key,out_key,dict_list):</p><p class="source-code">    return([cat[out_key] for cat in dict_list if cat[in_key]==in_key_value] )</p></li>
				<li>Run <a id="_idIndexMarker526"/>the <a id="_idIndexMarker527"/>following cell to define the <strong class="source-inline">get_lbl</strong> function that takes a filename as input and returns the list of category names from the annotation structure for that file:<p class="source-code">def get_lbl(filename):</p><p class="source-code">    return np.array(img_bbox_combo[os.path.basename(filename)][1],dtype=object)</p><p>Here are the key parts of this function:</p><p>a) <strong class="source-inline">os.path.basename(filename)</strong>: Returns the final part of the fully qualified <strong class="source-inline">filename</strong>. For example, if <strong class="source-inline">filename</strong> is the fully qualified name, <strong class="source-inline">/storage/data/pascal_2007/train/006635.jpg</strong>, <strong class="source-inline">os.path.basename</strong> returns <strong class="source-inline">006635.jpg</strong>. This conversion is required because the input image files will include fully qualified paths, but the <strong class="source-inline">img_bbox_combo</strong> structure is indexed with just the final part of the filename.</p><p>b) <strong class="source-inline">img_bbox_combo[os.path.basename(filename)][1]</strong>: Returns the categories associated with the <strong class="source-inline">filename</strong> image file. </p></li>
				<li>Run the following cell to see an example of how <strong class="source-inline">get_lbl</strong> works:<p class="source-code">get_lbl('/storage/data/pascal_2007/train/007911.jpg') </p><p>The output, as shown in the following screenshot, is a NumPy array containing the categories associated with the <strong class="source-inline">/storage/data/pascal_2007/train/006635.jpg</strong> image file:</p><div id="_idContainer214" class="IMG---Figure"><img src="image/B16216_6_55.jpg" alt="Figure 6.55 – Sample output of get_lbl&#13;&#10;"/></div><p class="figure-caption">Figure 6.55 – Sample output of get_lbl</p></li>
				<li>It looks like <a id="_idIndexMarker528"/>a<a id="_idIndexMarker529"/> lot is going on in the image we used as an argument to <strong class="source-inline">get_lbl</strong> in the previous cell. Run the following cell to take a look at the image:<p class="source-code">image_subpath = 'train/007911.jpg'</p><p class="source-code">img = PILImage.create(path/image_subpath)</p><p class="source-code">img</p><p>As shown in the following image, this image matches the annotation we saw in the previous cell. It does indeed contain a motorbike along with a bunch of people:</p><div id="_idContainer215" class="IMG---Figure"><img src="image/B16216_6_56.jpg" alt="Figure 6.56 – Image with a motorbike and several people&#13;&#10;"/></div><p class="figure-caption">Figure 6.56 – Image with a motorbike and several people</p></li>
				<li>Run<a id="_idIndexMarker530"/> the following<a id="_idIndexMarker531"/> cell to examine the number of files in the training and test sets for this dataset:<p class="source-code">print("number of training images: ",len(get_image_files(path/'train')))</p><p class="source-code">print("number of testing images: ",len(get_image_files(path/'test')))</p><p>The output, shown in in the following screenshot, shows the number of files in each set:</p><div id="_idContainer216" class="IMG---Figure"><img src="image/B16216_6_57.jpg" alt="Figure 6.57 – Count of files in the training and test sets&#13;&#10;"/></div><p class="figure-caption">Figure 6.57 – Count of files in the training and test sets</p></li>
				<li>Run the following cells to see the number of categories in the dataset:<p class="source-code">print("number of categories is: ",len(categories))</p><p>The<a id="_idIndexMarker532"/> output, shown<a id="_idIndexMarker533"/> in the following screenshot, shows the number of categories in the dataset:</p><div id="_idContainer217" class="IMG---Figure"><img src="image/B16216_6_58.jpg" alt="Figure 6.58 – Number of categories in the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.58 – Number of categories in the dataset</p></li>
				<li>Run the following cell to define the <strong class="source-inline">get_items</strong> function that will be used in the <strong class="source-inline">DataBlock</strong> definition to get the input data files:<p class="source-code">def get_items(noop):</p><p class="source-code">    return_list = []</p><p class="source-code">    empty_list = []</p><p class="source-code">    # filter the training files and keep only the ones with valid info in the JSON file</p><p class="source-code">    for file_path in get_image_files(path/'train'):</p><p class="source-code">        file_id_list = get_category(os.path.basename(file_path),'file_name','id',images)</p><p class="source-code">        if len(file_id_list) &gt; 0:</p><p class="source-code">            return_list.append(file_path)</p><p class="source-code">        else:</p><p class="source-code">            empty_list.append(file_path)</p><p class="source-code">    print("len(return_list): ",len(return_li<a id="_idTextAnchor170"/>st))</p><p class="source-code">    print("len(empty_list): ",len(empty_list))</p><p class="source-code">    return(return_list)</p><p>Here are the key items in this function's definition:</p><p>a) <strong class="source-inline">return_list = []</strong>: Initializes<a id="_idIndexMarker534"/> the list of image files in the training <a id="_idIndexMarker535"/>set that have annotations.</p><p>b) <strong class="source-inline">empty_list = []</strong>: Initializes the list of image files in the training set that do not have annotations.</p><p>c) <strong class="source-inline">for file_path in get_image_files(path/'train')</strong>: Iterates through the files in the training set.</p><p>d) <strong class="source-inline">file_id_list</strong>: Is the list of file IDs in the annotation corresponding with the current <strong class="source-inline">file_path</strong>.</p><p>e) <strong class="source-inline">if len(file_id_list) &gt; 0</strong>: Checks to see whether <strong class="source-inline">file_id_list</strong> has any entries. If it does, the function appends the current <strong class="source-inline">file_path</strong> to <strong class="source-inline">return_list</strong>. Otherwise, the function appends the current <strong class="source-inline">file_path</strong> to <strong class="source-inline">empty_list</strong>.</p><p>f) <strong class="source-inline">return(return_list)</strong>: This only returns the subset of image files that have annotations associated with them. If you include any of the image files that have no annotations associated with them, you will get an error in the step where you define a <strong class="source-inline">dataloaders</strong> object.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">DataBlock</strong> object for the dataset:<p class="source-code">db = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</p><p class="source-code">                 get_items = get_items,               </p><p class="source-code">                 splitter=RandomSplit<a id="_idTextAnchor171"/>ter(),</p><p class="source-code">                 get_y=[get_lbl],</p><p class="source-code">                 item_tfms = RandomResizedCrop(128,\</p><p class="source-code">min_scale=0.35),</p><p class="source-code">                 n_inp=1)</p><p>Here <a id="_idIndexMarker536"/>are<a id="_idIndexMarker537"/> the key items in the definition of the <strong class="source-inline">DataBlock</strong> object:</p><p>a) <strong class="source-inline">blocks=(ImageBlock, MultiCategoryBlock)</strong>: Specifies the type of the input data (image files) and that the dataset has multi-category labels, that is, each image can have multiple annotated objects in it.</p><p>b) <strong class="source-inline">get_items = get_items</strong>: Specifies the function to apply to get the input items; in this case, the <strong class="source-inline">get_items</strong> function we defined in the previous cell that returns all the files in the training set that have annotations associated with them.</p><p>c) <strong class="source-inline">splitter=RandomSplitter()</strong>: Tells fastai to create a validation set from randomly selected items from the training set, by default using 20% of the items in the training set.</p><p>d) <strong class="source-inline">get_y=[get_lbl]</strong>: Specifies the function to get labels for the input, in this case, the <strong class="source-inline">get_lbl</strong> function. This function takes a filename as input and returns the list of categories from the annotations for that file.</p><p>e) <strong class="source-inline">item_tfms = RandomResizedCrop(168, min_scale=0.3)</strong>: Specifies a transformation to apply during training. Because the image files in the training set are different sizes, we need to transform them all to a common size or we will get errors in <strong class="source-inline">show_batch</strong>. This transformation resizes the images by cropping them so they are all a common size.</p><p>f) <strong class="source-inline">n_inp=1</strong>: Specifies which of the elements defined in the <strong class="source-inline">blocks</strong> clause of the definition should be considered inputs, in this case, <strong class="source-inline">1</strong> or just <strong class="source-inline">ImageBlock</strong>. </p></li>
				<li>Run the following cell to define a <strong class="source-inline">dataloaders</strong> object using the <strong class="source-inline">DataBlock</strong> object, <strong class="source-inline">db</strong>, that you created in the previous cell:<p class="source-code">dls = db.dataloaders(path,bs=32)</p><p>Here are the arguments to the <strong class="source-inline">dataloaders</strong> definition:</p><p>a) <strong class="source-inline">path</strong>: Specifies<a id="_idIndexMarker538"/> that <a id="_idIndexMarker539"/>the source for the <strong class="source-inline">dataloaders</strong> object is the <strong class="source-inline">path</strong> object you created earlier in the notebook</p><p>b) <strong class="source-inline">bs=32</strong>: Specifies that the batch size is <strong class="source-inline">32</strong></p><p>The output of this cell, as shown in the following screenshot, shows the count of elements in <strong class="source-inline">return_list</strong> (the list of image files with valid annotations) and <strong class="source-inline">empty_list</strong> (the list of image files without valid annotations):</p><div id="_idContainer218" class="IMG---Figure"><img src="image/B16216_6_59.jpg" alt="Figure 6.59 – Output of the dataloader definition&#13;&#10;"/></div><p class="figure-caption">Figure 6.59 – Output of the dataloader definition</p></li>
				<li>Run the following cell to display a set of samples from a batch:<p class="source-code">dls.show_batch(max_n=4, figsize=(10,10))</p><p>The output shows a selection of images from the training set along with the categories for the objects in the images that are described in the annotation corresponding to the image files, as shown in the following screenshot:</p><div id="_idContainer219" class="IMG---Figure"><img src="image/B16216_6_60.jpg" alt="Figure 6.60 – Results of show_batch&#13;&#10;"/></div><p class="figure-caption">Figure 6.60 – Results of show_batch</p></li>
				<li>Run <a id="_idIndexMarker540"/>the<a id="_idIndexMarker541"/> following cell to define the model by specifying a <strong class="source-inline">cnn_learner</strong> object:<p class="source-code">learn = cnn_learner(dls, resnet18)</p><p>Here are the arguments to the <strong class="source-inline">cnn_learner</strong> definition:</p><p>a) <strong class="source-inline">dls</strong>: Specifies that the model is trained using the <strong class="source-inline">dataloaders</strong> object you defined in the previous cell</p><p>b) <strong class="source-inline">resnet18</strong>: Specifies that the model is based on the pre-trained <strong class="source-inline">resnet18</strong> mode.</p></li>
				<li>Run the following cell to train the model for <strong class="source-inline">10</strong> epochs:<p class="source-code">learn.fine_tune(10)</p><p>The output lists the training and validation loss for each epoch, as shown in the following <a id="_idIndexMarker542"/>screenshot. The <a id="_idIndexMarker543"/>performance of the model improves as the validation loss decreases:</p><div id="_idContainer220" class="IMG---Figure"><img src="image/B16216_6_61.jpg" alt="Figure 6.61 – Output of training the multi-category image classification model&#13;&#10;"/></div><p class="figure-caption">Figure 6.61 – Output of training the multi-category image classification model</p></li>
				<li>Now that we have trained the model, let's exercise it on some images from the test set. To start with, run the following cell to prepare and display one of the images from the test set:<p class="source-code">img_test_files = get_image_files(path/"test")</p><p class="source-code">img2 = PILImage.create(img_test_files[100])</p><p class="source-code">img2</p><p>This cell displays the following image:</p><div id="_idContainer221" class="IMG---Figure"><img src="image/B16216_6_62.jpg" alt="Figure 6.62 – Image from the test set&#13;&#10;"/></div><p class="figure-caption">Figure 6.62 – Image from the test set</p></li>
				<li>Run <a id="_idIndexMarker544"/>the<a id="_idIndexMarker545"/> following cell to apply the trained model to the image displayed in the previous cell:<p class="source-code">learn.predict(img2)</p><p>From the output, as shown in the following screenshot, you can see that the model gets the right category for one of the objects in the image, but is wrong about the second object in the image:</p><div id="_idContainer222" class="IMG---Figure"><img src="image/B16216_6_63.jpg" alt="Figure 6.63 – Model prediction on the image of the horse&#13;&#10;"/></div><p class="figure-caption">Figure 6.63 – Model prediction on the image of the horse</p></li>
				<li>Run<a id="_idIndexMarker546"/> the<a id="_idIndexMarker547"/> following cell to prepare and display another image from the test set:<p class="source-code">img3 = PILImage.create(img_test_files[200])</p><p class="source-code">img3</p><p>This cell displays the following image:</p><div id="_idContainer223" class="IMG---Figure"><img src="image/B16216_6_64.jpg" alt="Figure 6.64 – Image from the test set&#13;&#10;"/></div><p class="figure-caption">Figure 6.64 – Image from the test set</p></li>
				<li>Run the following cell to apply the trained model to the image displayed in the previous cell:<p class="source-code">learn.predict(img3)</p><p>From the output, as shown in the following screenshot, you can see that the model gets<a id="_idIndexMarker548"/> the <a id="_idIndexMarker549"/>correct category for the object in the image:</p></li>
			</ol>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B16216_6_65.jpg" alt="Figure 6.65 – Model prediction on the image of the cat&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.65 – Model prediction on the image of the cat</p>
			<p>Congratulations! You have trained a fastai model that categorizes multiple objects in images.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor172"/>How it works…</h2>
			<p>The recipe in this section is one of the longest, most complex recipes in this book. The following are some aspects of the recipe that may not have been evident to you as you worked through the steps.</p>
			<h3>This recipe uses transfer learning</h3>
			<p>If you look at<a id="_idIndexMarker550"/> the code from <em class="italic">Step 19</em> and <em class="italic">Step 20</em> of the recipe, you will see a pattern similar to the model definition and training steps from the recipes in the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe and the <em class="italic">Training a classification model with a standalone vision dataset</em> recipe. </p>
			<p>In all of these recipes, you specified a pre-trained model in the model definition and then trained the model using a <strong class="source-inline">fine_tune</strong> statement. Taking advantage of the pre-trained vision classification model for this recipe makes a lot of sense because we had a fairly small <a id="_idIndexMarker551"/>training set, as you will see in the following section.</p>
			<h3>This recipe has a small training set for a deep learning model</h3>
			<p>How big was<a id="_idIndexMarker552"/> the training set for this recipe? The initial training set is a little over 5,000 images, as you can see in the count of the training set from <em class="italic">Step 13</em> of the recipe: </p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B16216_6_66.jpg" alt="Figure 6.66 – Count of items in the training and test sets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.66 – Count of items in the training and test sets</p>
			<p>However, in <em class="italic">Step 15</em>, where we defined the <strong class="source-inline">get_items</strong> function, we had to filter the list of training images to only include those that had valid category annotations. As you can see from the output of <em class="italic">Step 17</em>, where we define the <strong class="source-inline">dataloaders</strong> object and invoke the <strong class="source-inline">get_items</strong> function, less than half of the input training images have valid annotations:</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/B16216_6_67.jpg" alt="Figure 6.67 – Count of items in return_list and empty_list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.67 – Count of items in return_list and empty_list</p>
			<p>What does this mean? It means that we have 2,500 images to train our model. Having only 2,500 images to train a complex model has consequences, as we will see in the next section.</p>
			<h3>The trained model from this recipe does not have great performance</h3>
			<p>The model<a id="_idIndexMarker553"/> we trained in the recipe in this section does not have outstanding performance. It usually identifies one object in an image correctly, but it often doesn't identify more than one object, and when it does identify a second or third object, it often assigns these objects to an incorrect category. Why is the performance of this model not so good? </p>
			<p>In this recipe, we used only 2,500 images to train a complex model to categorize multiple objects in images into 20 categories. Compare this to the <strong class="source-inline">fruits-360</strong> dataset used in the <em class="italic">Training a classification model with a standalone vision dataset</em> recipe. The <strong class="source-inline">fruits-360</strong> dataset has over 90,000 images. To see how many files there are in the training set, run the following command in your Gradient environment:</p>
			<p class="source-code">find /storage/archive/fruits-360/Training -type f | wc -l</p>
			<p>The following screenshot shows the output of this command:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B16216_6_68.jpg" alt="Figure 6.68 – Count of images in the fruits-360 training set&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.68 – Count of images in the fruits-360 training set</p>
			<p>There are over 67,500 files in the training set for the <strong class="source-inline">fruits-360</strong> dataset. This means that for the first curated image dataset problem we saw in this chapter, we had 25 times as many training samples as we had for the recipe in this section, and in the first instance, we applied the model to the simpler problem of identifying a single object in each image. The relative lack of training images in the <strong class="source-inline">PASCAL_2007</strong> dataset could partially explain the mediocre performance of the model we trained with this dataset. </p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor173"/>Test your knowledge</h1>
			<p>Of the four<a id="_idIndexMarker554"/> application areas that fastai explicitly supports (tabular, text, recommender systems, and image/vision), fastai provides the most thorough support for creating models that work with image datasets. In this chapter, we have just scratched the surface of what you can do with fastai and image datasets. In this section, you will get a chance to dig a bit deeper into one of the fastai image dataset recipes from this chapter.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor174"/>Getting ready</h2>
			<p>Ensure that you have followed the <em class="italic">Training a multi-image classification model with a curated vision dataset</em> recipe. In this section, you will be adapting the notebook you worked through in that recipe to try some new variations on deep learning with image datasets.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor175"/>How to do it…</h2>
			<p>You can follow the steps in this section to try some variations on the image cla<a id="_idTextAnchor176"/>ssification model that you <a id="_idIndexMarker555"/>trained with the <strong class="source-inline">PASCAL_2007</strong> dataset in the <em class="italic">Training a multi-image classification model with a curated vision dataset</em> recipe:</p>
			<ol>
				<li value="1">Make a copy of the <strong class="source-inline">training_with_curated_multi_image_classification_datasets.ipynb</strong> notebook that you worked through in the <em class="italic">Training a multi-image classification model with a curated vision dataset</em> recipe. Give your new copy of the notebook the following name: <strong class="source-inline">training_with_curated_multi_image_classification_datasets_variations.ipynb</strong>.</li>
				<li>Run the notebook up to and including the model definition cell:<p class="source-code">learn = cnn_learner(dls, resnet18)</p></li>
				<li>Add the following cell immediately after this cell and run it:<p class="source-code">learn.summary()</p><p>The output of this cell lists the structure of the model. Note the summary of trainable and non-trainable parameters at the end of the output, as shown in the following screenshot:</p><div id="_idContainer228" class="IMG---Figure"><img src="image/B16216_6_69.jpg" alt="Figure 6.69 – Trainable parameter description at the end of the summary output before fine-tuning&#13;&#10;"/></div><p class="figure-caption">Figure 6.69 – Trainable parameter description at the end of the summary output before fine-tuning</p></li>
				<li>Update the model training cell to train for 20 epochs and then run it:<p class="source-code">learn.fine_tune(20)</p><p>The output <a id="_idIndexMarker556"/>of this cell, as shown in the following screenshot, shows how the training loss and validation loss develop through the epochs. Note how the training loss decreases steadily right up to the 20th epoch, while the validation loss decreases up to the 10th epoch, after which it goes up and down. What problem is evident in a situation like this, where the training loss keeps dropping but the validation loss stops dropping?</p><div id="_idContainer229" class="IMG---Figure"><img src="image/B16216_6_70.jpg" alt="Figure 6.70 – Results of a 20-epoch training run&#13;&#10;"/></div><p class="figure-caption">Figure 6.70 – Results of a 20-epoch training run</p></li>
				<li>The previous<a id="_idIndexMarker557"/> step shows us that training for more epochs won't improve the model's performance. You can validate this for yourself by running the rest of the notebook and comparing how well the model trained with 20 epochs predicts the objects in the selected test images. </li>
				<li>Add the following immediately after the model training cell and run it:<p class="source-code">learn.summary()</p><p>The output of this cell lists the structure of the model. Note the summary of trainable and non-trainable parameters at the end of the output, as shown in the following screenshot. Compare this output to the output from <strong class="source-inline">summary()</strong> prior to fine-tuning the model, as shown in <em class="italic">Step 3</em>. What has changed to explain the<a id="_idIndexMarker558"/> difference in the output of <strong class="source-inline">summary()</strong> between <em class="italic">Step 3</em> and this step? </p><div id="_idContainer230" class="IMG---Figure"><img src="image/B16216_6_71.jpg" alt="Figure 6.71 – Trainable parameter description at the end of the summary output after fine-tuning&#13;&#10;"/></div><p class="figure-caption">Figure 6.71 – Trainable parameter description at the end of the summary output after fine-tuning</p></li>
				<li>In the <em class="italic">How it works…</em> section of the previous recipe, we looked at the size of the training set for <strong class="source-inline">PASCAL_2007</strong> and suggested that its small size could be a reason for the unimpressive performance of the model. One other potential culprit is the way that we are resizing the images in the definition of the <strong class="source-inline">DataBlock</strong> object, shown in the following cell:<p class="source-code">db = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</p><p class="source-code">                 get_items = get_items, </p><p class="source-code">                 splitter=RandomSplitte<a id="_idTextAnchor177"/>r(),</p><p class="source-code">                 get_y=[get_lbl],</p><p class="source-code">                 item_tfms = RandomResizedCrop(168,\</p><p class="source-code">min_scale=0.3),</p><p class="source-code">                 n_inp=1)</p><p>The transformation that resizes the images is <strong class="source-inline">RandomResizedCrop(168, min_scale=0.3)</strong>. Update the <strong class="source-inline">DataBlock</strong> definition cell to try different image transformations. First, update the <strong class="source-inline">RandomResizedCrop</strong> function call to try a different image size and a different <strong class="source-inline">min_scale</strong> value. Train and exercise the model again on samples from the test set to see whether the performance <a id="_idIndexMarker559"/>changes for different image sizes and <strong class="source-inline">min_scale</strong> values.</p></li>
				<li>Try using <strong class="source-inline">Resize</strong> rather than <strong class="source-inline">RandomResizedCrop</strong> as the transformation function (as shown in the following cell) and train the model again. See whether you get better results when you exercise the trained model on sample images from the training set:<p class="source-code">db = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</p><p class="source-code">                 get_items = get_items, </p><p class="source-code">                 splitter=RandomSplitter(),</p><p class="source-code">                 get_y=[get_lbl],</p><p class="source-code">                 item_tfms = Resize(168),</p><p class="source-code">                 n_inp=1)</p></li>
			</ol>
			<p>Congratulations! You have completed a review of training the fastai model on image datasets. </p>
		</div>
	</body></html>