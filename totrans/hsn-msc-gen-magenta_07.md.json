["```py\nimport os\nimport tarfile\nimport tensorflow as tf\nfrom six.moves import urllib\n\ndef download_checkpoint(checkpoint_name: str,\n                        target_dir: str = \"checkpoints\"):\n  tf.gfile.MakeDirs(target_dir)\n  checkpoint_target = os.path.join(target_dir, f\"{checkpoint_name}.tar\")\n  if not os.path.exists(checkpoint_target):\n    response = urllib.request.urlopen(\n      f\"http://download.magenta.tensorflow.org/\"\n      f\"models/nsynth/{checkpoint_name}.tar\")\n    data = response.read()\n    local_file = open(checkpoint_target, 'wb')\n    local_file.write(data)\n    local_file.close()\n    tar = tarfile.open(checkpoint_target)\n    tar.extractall(target_dir)\n    tar.close()\n```", "```py\nfrom typing import List\nimport numpy as np\nfrom magenta.models.nsynth import utils\nfrom magenta.models.nsynth.wavenet import fastgen\n\ndef encode(wav_filenames: List[str],\n           checkpoint: str = \"checkpoints/wavenet-ckpt/model.ckpt-200000\",\n           sample_length: int = 16000,\n           sample_rate: int = 16000) -> List[np.ndarray]:\n  # Loads the audio for each filenames\n  audios = []\n  for wav_filename in wav_filenames:\n    audio = utils.load_audio(os.path.join(\"sounds\", wav_filename),\n                             sample_length=sample_length,\n                             sr=sample_rate)\n    audios.append(audio)\n\n  # Encodes the audio for each new wav\n  audios = np.array(audios)\n  encodings = fastgen.encode(audios, checkpoint, sample_length)\n\n  return encodings\n```", "```py\nimport os\nimport numpy as np\n\ndef save_encoding(encodings: List[np.ndarray],\n                  filenames: List[str],\n                  output_dir: str = \"encodings\") -> None:\n  os.makedirs(output_dir, exist_ok=True)\n  for encoding, filename in zip(encodings, filenames):\n    filename = filename if filename.endswith(\".npy\") else filename + \".npy\"\n    np.save(os.path.join(output_dir, filename), encoding)\n```", "```py\ndef load_encodings(filenames: List[str],\n                   input_dir: str = \"encodings\") -> List[np.ndarray]:\n  encodings = []\n  for filename in filenames:\n    encoding = np.load(os.path.join(input_dir, filename))\n    encodings.append(encoding)\n  return encodings\n```", "```py\ndef mix_encoding_pairs(encodings: List[np.ndarray],\n                       encodings_name: List[str]) \\\n    -> Tuple[np.ndarray, List[str]]:\n  encodings_mix = []\n  encodings_mix_name = []\n  # Takes the pair of encodings two by two\n  for encoding1, encoding1_name in zip(encodings, encodings_name):\n    for encoding2, encoding2_name in zip(encodings, encodings_name):\n      if encoding1_name == encoding2_name:\n        continue\n      # Adds the encodings together\n      encoding_mix = encoding1 + encoding2 / 2.0\n      encodings_mix.append(encoding_mix)\n      # Merges the beginning of the track names\n      if \"_\" in encoding1_name and \"_\" in encoding2_name:\n        encoding_name = (f\"{encoding1_name.split('_', 1)[0]}_\"\n                         f\"{encoding2_name.split('_', 1)[0]}\")\n      else:\n        encoding_name = f\"{encoding1_name}_{encoding2_name}\"\n      encodings_mix_name.append(encoding_name)\n  return np.array(encodings_mix), encodings_mix_name\n```", "```py\ndef synthesize(encodings_mix: np.ndarray,\n               encodings_mix_name: List[str],\n               checkpoint: str = \"checkpoints/wavenet-ckpt/model.ckpt-200000\") \\\n    -> None:\n  os.makedirs(os.path.join(\"output\", \"nsynth\"), exist_ok=True)\n  encodings_mix_name = [os.path.join(\"output\", \"nsynth\",\n                                     encoding_mix_name + \".wav\")\n                        for encoding_mix_name in encodings_mix_name]\n  fastgen.synthesize(encodings_mix,\n                     checkpoint_path=checkpoint,\n                     save_paths=encodings_mix_name)\n```", "```py\nfor sample_i in range(total_length):\n  encoding_i = sample_i // hop_length\n  audio = generate_audio_sample(sess, net,\n                                audio, encodings[:, encoding_i, :])\n  audio_batch[:, sample_i] = audio[:, 0]\n  if sample_i % 100 == 0:\n    tf.logging.info(\"Sample: %d\" % sample_i)\n  if sample_i % samples_per_save == 0 and save_paths:\n    save_batch(audio_batch, save_paths)\n```", "```py\nWAV_FILENAMES = [\"83249__zgump__bass-0205__crop.wav\",\n                 \"160045__jorickhoofd__metal-hit-with-metal-bar-resonance\"\n                 \"__crop.wav\",\n                 \"412017__skymary__cat-meow-short__crop.wav\",\n                 \"427567__maria-mannone__flute__crop.wav\"]\n\n# Downloads and extracts the checkpoint to \"checkpoints/wavenet-ckpt\"\ndownload_checkpoint(\"wavenet-ckpt\")\n\n# Encodes the wav files into 4 encodings (and saves them for later use)\nencodings = encode(WAV_FILENAMES)\n\n# Mix the 4 encodings pairs into 12 encodings\nencodings_mix, encodings_mix_name = mix_encoding_pairs(encodings,\n                                                       WAV_FILENAMES)\n\n# Synthesize the 12 encodings into wavs\nsynthesize(encodings_mix, encodings_mix_name)\n```", "```py\nimport os\nimport librosa\nimport glob\nfrom audio_utils import save_rainbowgram_plot\n\nfor path in glob.glob(\"output/nsynth/*.wav\"):\n  audio, _ = librosa.load(path, 16000)\n  filename = os.path.basename(path)\n  output_dir = os.path.join(\"output\", \"nsynth\", \"plots\")\n  print(f\"Writing rainbowgram for {path} in {output_dir}\")\n  save_rainbowgram_plot(audio,\n                        filename=filename.replace(\".wav\", \"_rainbowgram.png\"),\n                        output_dir=output_dir)\n```", "```py\nnsynth_generate --checkpoint_path=\"checkpoints/wavenet-ckpt/model.ckpt-200000\" --source_path=\"sounds\" --save_path=\"output/nsynth\" --batch_size=4 --sample_length=16000\n```", "```py\ndef download_checkpoint(checkpoint_name: str,\n                        target_dir: str = \"checkpoints\"):\n  tf.gfile.MakeDirs(target_dir)\n  checkpoint_target = os.path.join(target_dir, f\"{checkpoint_name}.zip\")\n  if not os.path.exists(checkpoint_target):\n    response = urllib.request.urlopen(\n      f\"https://storage.googleapis.com/magentadata/\"\n      f\"models/gansynth/{checkpoint_name}.zip\")\n    data = response.read()\n    local_file = open(checkpoint_target, 'wb')\n    local_file.write(data)\n    local_file.close()\n    with zipfile.ZipFile(checkpoint_target, 'r') as zip:\n      zip.extractall(target_dir)\n```", "```py\nimport os\nfrom magenta.models.gansynth.lib.generate_util import load_midi\nfrom note_sequence_utils import save_plot\n\ndef get_midi(midi_filename: str = \"cs1-1pre-short.mid\") -> dict:\n  midi_path = os.path.join(\"midi\", midi_filename)\n  _, notes = load_midi(midi_path)\n  return notes\n```", "```py\nfrom magenta.models.gansynth.lib import flags as lib_flags\nfrom magenta.models.gansynth.lib import model as lib_model\nfrom magenta.models.gansynth.lib.generate_util import combine_notes\nfrom magenta.models.gansynth.lib.generate_util import get_random_instruments\nfrom magenta.models.gansynth.lib.generate_util import get_z_notes\n\ndef generate_audio(notes: dict,\n                   seconds_per_instrument: int = 5,\n                   batch_size: int = 16,\n                   checkpoint_dir: str = \"checkpoints/acoustic_only\") \\\n    -> np.ndarray:\n  flags = lib_flags.Flags({\"batch_size_schedule\": [batch_size]})\n  model = lib_model.Model.load_from_path(checkpoint_dir, flags)\n\n  # Distribute latent vectors linearly in time\n  z_instruments, t_instruments = get_random_instruments(\n    model,\n    notes[\"end_times\"][-1],\n    secs_per_instrument=seconds_per_instrument)\n\n  # Get latent vectors for each note\n  z_notes = get_z_notes(notes[\"start_times\"], z_instruments, t_instruments)\n\n  # Generate audio for each note\n  audio_notes = model.generate_samples_from_z(z_notes, notes[\"pitches\"])\n\n  # Make a single audio clip\n  audio_clip = combine_notes(audio_notes,\n                             notes[\"start_times\"],\n                             notes[\"end_times\"],\n                             notes[\"velocities\"])\n\n  return audio_clip\n```", "```py\ndef get_random_instruments(model, total_time, secs_per_instrument=2.0):\n  \"\"\"Get random latent vectors evenly spaced in time.\"\"\"\n  n_instruments = int(total_time / secs_per_instrument)\n  z_instruments = model.generate_z(n_instruments)\n  t_instruments = np.linspace(-.0001, total_time, n_instruments)\n  return z_instruments, t_instruments\n```", "```py\nnp.random.normal(size=[n, self.config['latent_vector_size']])\n```", "```py\ndef get_z_notes(start_times, z_instruments, t_instruments):\n  \"\"\"Get interpolated latent vectors for each note.\"\"\"\n  z_notes = []\n  for t in start_times:\n    idx = np.searchsorted(t_instruments, t, side='left') - 1\n    t_left = t_instruments[idx]\n    t_right = t_instruments[idx + 1]\n    interp = (t - t_left) / (t_right - t_left)\n    z_notes.append(slerp(z_instruments[idx], z_instruments[idx + 1], interp))\n  z_notes = np.vstack(z_notes)\n  return z_notes\n```", "```py\n# Generate waves\nstart_time = time.time()\nwaves_list = []\nfor i in range(num_batches):\n  start = i * self.batch_size\n  end = (i + 1) * self.batch_size\n\n  waves = self.sess.run(self.fake_waves_ph,\n                        feed_dict={self.labels_ph: labels[start:end],\n                                   self.noises_ph: z[start:end]})\n  # Trim waves\n for wave in waves:\n    waves_list.append(wave[:max_audio_length, 0])\n```", "```py\n# Downloads and extracts the checkpoint to \"checkpoint/acoustic_only\"\ndownload_checkpoint(\"acoustic_only\")\n\n# Loads the midi file and get the notes dictionary\nnotes = get_midi_notes()\n\n# Generates the audio clip from the notes dictionary\naudio_clip = generate_audio(notes)\n\n# Saves the audio plot and the audio file\nsave_audio(audio_clip)\n```", "```py\ngansynth_generate --ckpt_dir=\"checkpoints/acoustic_only\" --output_dir=\"output/gansynth\" --midi_file=\"midi/cs1-1pre-short.mid\"\n```"]