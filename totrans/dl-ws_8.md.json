["```py\n    import tensorflow as tf\n    ```", "```py\n    x=tf.Variable(0.0)\n    ```", "```py\n    loss=lambda:abs(x**2-10*x+25)\n    ```", "```py\n    optimizer=tf.optimizers.Adam(.01)\n    ```", "```py\n    for i in range(10000):\n        optimizer.minimize(loss,x)\n    ```", "```py\n    tf.print(x)\n    ```", "```py\n    4.99919891\n    ```", "```py\n    import tensorflow as tf\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n    # Import Keras libraries\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense\n    ```", "```py\n    df = pd.read_csv('sonar.csv')\n    df.head()\n    ```", "```py\n    X_input = df.iloc[:, :-1]\n    Y_label = df['Class'].values\n    ```", "```py\n    labelencoder_Y = LabelEncoder() \n    Y_label = labelencoder_Y.fit_transform(Y_label)\n    Y_label = Y_label.reshape([208, 1])\n    ```", "```py\n    model = Sequential()\n    model.add(Dense(300,input_dim=60, activation = 'relu'))\n    model.add(Dense(200, activation = 'relu'))\n    model.add(Dense(100, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n    ```", "```py\n    model.compile(optimizer='adam',loss='binary_crossentropy', \\\n                  metrics=['accuracy'])\n    ```", "```py\n    model.fit(X_input, Y_label, epochs=30)\n    ```", "```py\n    Train on 208 samples\n    Epoch 1/30\n    208/208 [==============================] - 0s 205us/sample - \n    loss: \n      0.1849 - accuracy: 0.9038\n    Epoch 2/30\n    208/208 [==============================] - 0s 220us/sample – \n    loss: \n      0.1299 - accuracy: 0.9615\n    Epoch 3/30\n    208/208 [==============================] - 0s 131us/sample – \n    loss: \n      0.0947 - accuracy: 0.9856\n    Epoch 4/30\n    208/208 [==============================] - 0s 151us/sample – \n    loss: \n      0.1046 - accuracy: 0.9712\n    Epoch 5/30\n    208/208 [==============================] - 0s 171us/sample – \n    loss: \n      0.0952 - accuracy: 0.9663\n    Epoch 6/30\n    208/208 [==============================] - 0s 134us/sample – \n    loss: \n      0.0777 - accuracy: 0.9856\n    Epoch 7/30\n    208/208 [==============================] - 0s 129us/sample – \n    loss: \n      0.1043 - accuracy: 0.9663\n    Epoch 8/30\n    208/208 [==============================] - 0s 142us/sample – \n    loss: \n      0.0842 - accuracy: 0.9712\n    Epoch 9/30\n    208/208 [==============================] - 0s 155us/sample – \n    loss: \n      0.1209 - accuracy: 0.9423\n    Epoch 10/30\n    208/208 [==============================] - ETA: 0s - loss: \n      0.0540 - accuracy: 0.98 - 0s 334us/sample - los\n    ```", "```py\n    model.evaluate(X_input, Y_label)\n    ```", "```py\n    208/208 [==============================] - 0s 128us/sample – \n    loss: \n      0.0038 - accuracy: 1.0000\n     [0.003758653004367191, 1.0]\n    ```", "```py\n    from tensorflow.keras.datasets import fashion_mnist\n    ```", "```py\n    (features_train, label_train), (features_test, label_test) = \\\n    fashion_mnist.load_data()\n    ```", "```py\n    features_train.shape\n    ```", "```py\n    (60000, 28, 28)\n    ```", "```py\n    features_test.shape\n    ```", "```py\n    (10000, 28, 28)\n    ```", "```py\n    features_train = features_train.reshape(60000, 28, 28, 1)\n    features_test = features_test.reshape(10000, 28, 28, 1)\n    ```", "```py\n    batch_size = 16\n    img_height = 28\n    img_width = 28\n    ```", "```py\n    from tensorflow.keras.preprocessing.image \\\n    import ImageDataGenerator\n    ```", "```py\n    train_img_gen = ImageDataGenerator(rescale=1./255, \\\n                                       rotation_range=40, \\\n                                       width_shift_range=0.1, \\\n                                       height_shift_range=0.1, \\\n                                       shear_range=0.2, \\\n                                       zoom_range=0.2, \\\n                                       horizontal_flip=True, \\\n                                       fill_mode='nearest')\n    ```", "```py\n    val_img_gen = ImageDataGenerator(rescale=1./255)\n    ```", "```py\n    train_data_gen = train_img_gen.flow(features_train, \\\n                                        label_train, \\\n                                        batch_size=batch_size)\n    ```", "```py\n    val_data_gen = train_img_gen.flow(features_test, \\\n                                      label_test, \\\n                                      batch_size=batch_size)\n    ```", "```py\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import layers\n    ```", "```py\n    np.random.seed(8)\n    tf.random.set_seed(8)\n    ```", "```py\n    model = tf.keras.Sequential\\\n            ([layers.Conv2D(64, 3, activation='relu', \\\n                            input_shape=(img_height, \\\n                                         img_width ,1)), \\\n                            layers.MaxPooling2D(), \\\n                            layers.Conv2D(128, 3, \\\n                                          activation='relu'), \\\n                            layers.MaxPooling2D(),\\\n                            layers.Flatten(), \\\n                            layers.Dense(128, \\\n                                         activation='relu'), \\\n                            layers.Dense(10, \\\n                                         activation='softmax')])\n    ```", "```py\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    ```", "```py\n    model.compile(loss='sparse_categorical_crossentropy', \\\n                  optimizer=optimizer, metrics=['accuracy'])\n    ```", "```py\n    model.fit_generator(train_data_gen, \\\n                        steps_per_epoch=len(features_train) \\\n                                        // batch_size, \\\n                        epochs=5, \\\n                        validation_data=val_data_gen, \\\n                        validation_steps=len(features_test) \\\n                                         // batch_size)\n    ```", "```py\n    import tensorflow as tf\n    ```", "```py\n    file_url = 'https://github.com/PacktWorkshops'\\\n               '/The-Deep-Learning-Workshop'\\\n               '/raw/master/Chapter03/Datasets/Activity3.02'\\\n               '/fruits360.zip'\n    ```", "```py\n    zip_dir = tf.keras.utils.get_file('fruits360.zip', \\\n                                      origin=file_url, \\\n                                      extract=True)\n    ```", "```py\n    import pathlib\n    ```", "```py\n    path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'\n    ```", "```py\n    train_dir = path / 'Training'\n    validation_dir = path / 'Test'\n    ```", "```py\n    total_train = 11398\n    total_val = 4752\n    ```", "```py\n    from tensorflow.keras.preprocessing.image \\\n    import ImageDataGenerator\n    ```", "```py\n    train_img_gen = ImageDataGenerator(rescale=1./255, \\\n                                       rotation_range=40, \\\n                                       width_shift_range=0.1, \\\n                                       height_shift_range=0.1, \\\n                                       shear_range=0.2, \\\n                                       zoom_range=0.2, \\\n                                       horizontal_flip=True, \\\n                                       fill_mode='nearest')\n    ```", "```py\n    val_img_gen = ImageDataGenerator(rescale=1./255)\n    ```", "```py\n    batch_size=16\n    img_height = 100\n    img_width = 100\n    channel = 3\n    ```", "```py\n    train_data_gen = train_image_generator.flow_from_directory\\\n                     (batch_size=batch_size, \\\n                     directory=train_dir, \\\n                     target_size=(img_height, img_width))\n    ```", "```py\n    val_data_gen = validation_image_generator.flow_from_directory\\\n                   (batch_size=batch_size, \\\n                   directory=validation_dir, \\\n                   target_size=(img_height, img_width))\n    ```", "```py\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import layers\n    ```", "```py\n    np.random.seed(8)\n    tf.random.set_seed(8)\n    ```", "```py\n    from tensorflow.keras.applications import VGG16\n    ```", "```py\n    base_model = VGG16(input_shape=(img_height, \\\n                                    img_width, channel), \\\n                                    weights='imagenet', \\\n                                    include_top=False)\n    ```", "```py\n    base_model.trainable = False\n    ```", "```py\n    base_model.summary()\n    ```", "```py\n    model = tf.keras.Sequential([base_model, \\\n                                 layers.Flatten(), \\\n                                 layers.Dense(1000, \\\n                                              activation='relu'), \\\n                                 layers.Dense(120, \\\n                                              activation='softmax')])\n    ```", "```py\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    ```", "```py\n    model.compile(loss='categorical_crossentropy', \\\n                  optimizer=optimizer, metrics=['accuracy'])\n    ```", "```py\n    model.fit_generator(train_data_gen, \\\n                        steps_per_epoch=len(features_train) \\\n                                        // batch_size, \\\n                        epochs=5, \\\n                        validation_data=val_data_gen, \\\n                        validation_steps=len(features_test) \\\n                                         // batch_size)\n    ```", "```py\n    txt_sents = tokenize.sent_tokenize(alice_raw.lower())\n    ```", "```py\n    txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\n    ```", "```py\n    from string import punctuation\n    stop_punct = list(punctuation)\n    from nltk.corpus import stopwords\n    stop_nltk = stopwords.words(\"english\")\n    ```", "```py\n    stop_context = [\"--\", \"said\"]\n    ```", "```py\n    stop_final = stop_punct + stop_nltk + stop_context\n    ```", "```py\n    def drop_stop(input_tokens):\n        return [token for token in input_tokens \\\n                if token not in stop_final]\n    ```", "```py\n    alice_words_nostop = [drop_stop(sent) for sent in txt_words]\n    print(alice_words_nostop[:2])\n    ```", "```py\n    [['alice', \"'s\", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', \"'and\", 'use', 'book', 'thought', 'alice', \"'without\", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close']]\n    ```", "```py\n    from nltk.stem import PorterStemmer\n    stemmer_p = PorterStemmer()\n    alice_words_stem = [[stemmer_p.stem(token) for token in sent] \\\n                         for sent in alice_words_nostop]\n    print(alice_words_stem[:5])\n    ```", "```py\n    [['alic', \"'s\", 'adventur', 'wonderland', 'lewi', 'carrol', '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', 'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', 'peep', 'book', 'sister', 'read', 'pictur', 'convers', \"'and\", 'use', 'book', 'thought', 'alic', \"'without\", 'pictur', 'convers'], ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', 'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', 'daisi', 'suddenli', 'white', 'rabbit', 'pink', 'eye', 'ran', 'close'], ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', 'rabbit', 'say', \"'oh\", 'dear'], ['oh', 'dear'], ['shall', 'late']]\n    ```", "```py\n    print(alice_words_nostop[:3])\n    ```", "```py\n    [['alice', \"'s\", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', \"'and\", 'use', 'book', 'thought', 'alice', \"'without\", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close'], ['nothing', 'remarkable', 'alice', 'think', 'much', 'way', 'hear', 'rabbit', 'say', \"'oh\", 'dear']]\n    ```", "```py\n    from gensim.models import word2vec\n    model = word2vec.Word2Vec(alice_words_nostop)\n    ```", "```py\n    model.wv.most_similar(\"rabbit\", topn=5)\n    ```", "```py\n    [('alice', 0.9963310360908508),\n     ('little', 0.9956872463226318),\n     ('went', 0.9955698251724243),\n     (\"'s\", 0.9955658912658691),\n     ('would', 0.9954401254653931)]\n    ```", "```py\n    model = word2vec.Word2Vec(alice_words_nostop, window=2)\n    ```", "```py\n    model.wv.most_similar(\"rabbit\", topn=5)\n    ```", "```py\n    [('alice', 0.9491485357284546),\n     (\"'s\", 0.9364748001098633),\n     ('little', 0.9345826506614685),\n     ('large', 0.9341927170753479),\n     ('duchess', 0.9341296553611755)]\n    ```", "```py\n    model = word2vec.Word2Vec(alice_words_nostop, window=5, sg=1)\n    ```", "```py\n    model.wv.most_similar(\"rabbit\", topn=5)\n    ```", "```py\n    [('gardeners', 0.9995723366737366),\n     ('end', 0.9995588064193726),\n     ('came', 0.9995309114456177),\n     ('sort', 0.9995298385620117),\n     ('upon', 0.9995272159576416)]\n    ```", "```py\n    v1 = model.wv['white']\n    v2 = model.wv['rabbit']\n    res1 = (v1+v2)/2\n    ```", "```py\n    v1 = model.wv['mad']\n    v2 = model.wv['hatter']\n    res2 = (v1+v2)/2\n    ```", "```py\n    model.wv.cosine_similarities(res1, [res2])\n    ```", "```py\n    array([0.9996213], dtype=float32)\n    ```", "```py\n    from gensim.models.keyedvectors import KeyedVectors\n    glove_model = KeyedVectors.load_word2vec_format\\\n    (\"glove.6B.100d.w2vformat.txt\", binary=False)\n    ```", "```py\n    v1 = glove_model['white']\n    v2 = glove_model['rabbit']\n    res1 = (v1+v2)/2\n    v1 = glove_model['mad']\n    v2 = glove_model['hatter']\n    res2 = (v1+v2)/2\n    ```", "```py\n    glove_model.cosine_similarities(res1, [res2])\n    ```", "```py\n    array([0.4514577], dtype=float32)\n    ```", "```py\n    import pandas as pd, numpy as np\n    import matplotlib.pyplot as plt\n    inp0 = pd.read_csv(\"IBM.csv\")\n    inp0 = inp0.sort_index(ascending=False)\n    inp0.plot(\"Date\", \"Close\")\n    plt.show()\n    ```", "```py\n    ts_data = inp0.Close.values.reshape(-1,1)\n    plt.figure(figsize=[14,5])\n    plt.plot(ts_data)\n    plt.show()\n    ```", "```py\n    train_recs = int(len(ts_data) * 0.75)\n    train_data = ts_data[:train_recs]\n    test_data = ts_data[train_recs:]\n    len(train_data), len(test_data)\n    ```", "```py\n    (1888, 630)\n    ```", "```py\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler()\n    train_scaled = scaler.fit_transform(train_data)\n    test_scaled = scaler.transform(test_data)\n    ```", "```py\n    look_back = 10\n    trainX, trainY = get_lookback(train_scaled, look_back=look_back)\n    testX, testY = get_lookback(test_scaled, look_back= look_back)\n    trainX.shape, testX.shape\n    ```", "```py\n    ((1888, 10), (630, 10))\n    ```", "```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import SimpleRNN, Activation, Dropout, Dense, Reshape, Conv1D\n    from sklearn.metrics import mean_squared_error\n    ```", "```py\n    model_comb = Sequential()\n    model_comb.add(Reshape((look_back,1), \\\n                            input_shape = (look_back,)))\n    model_comb.add(Conv1D(5, 3, activation='relu'))\n    model_comb.add(SimpleRNN(32))\n    model_comb.add(Dropout(0.25))\n    model_comb.add(Dense(1))\n    model_comb.add(Activation('linear'))\n    model.summary()\n    ```", "```py\n    model_comb.compile(loss='mean_squared_error', \\\n                       optimizer='adam')\n    model_comb.fit(trainX, trainY, epochs=5, \\\n                   batch_size=1, verbose=2, \\\n                   validation_split=0.1)\n    ```", "```py\n    get_model_perf(model_comb)\n    ```", "```py\n    Train RMSE: 0.03 RMSE\n    Test RMSE: 0.03 RMSE\n    ```", "```py\n    %matplotlib notebook\n    plt.figure(figsize=[10,5])\n    plot_pred(model_comb)\n    ```", "```py\n    import pandas as pd, numpy as np\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    train_df = pd.read_csv(\"Amazon_reviews_train.csv\")\n    test_df = pd.read_csv(\"Amazon_reviews_test.csv\")\n    print(train_df.shape, train_df.shape)\n    train_df.head(5)\n    ```", "```py\n    train_raw = train_df.review_text.values\n    train_labels = train_df.label.values\n    test_raw = test_df.review_text.values\n    test_labels = test_df.label.values\n    train_raw[:2]\n    ```", "```py\n    import nltk\n    nltk.download('punkt')\n    from nltk.tokenize import word_tokenize\n    train_tokens = [word_tokenize(review.lower()) \\\n                    for review in train_raw]\n    test_tokens = [word_tokenize(review.lower()) \\\n                   for review in test_raw]\n    print(train_tokens[0])\n    ```", "```py\n    from string import punctuation\n    stop_punct = list(punctuation)\n    nltk.download(\"stopwords\")\n    from nltk.corpus import stopwords\n    stop_nltk = stopwords.words(\"english\")\n    stop_final = stop_punct + stop_nltk\n    def drop_stop(input_tokens):\n        return [token for token in input_tokens \\\n                if token not in stop_final]\n    ```", "```py\n    train_tokens_no_stop = [drop_stop(sent) \\\n                            for sent in train_tokens]\n    test_tokens_no_stop = [drop_stop(sent) \\\n                           for sent in test_tokens]\n    print(train_tokens_no_stop[0])\n    ```", "```py\n    ['stuning', 'even', 'non-gamer', 'sound', 'track', 'beautiful', \n     'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', \n     'people', 'hate', 'vid', 'game', 'music', 'played', 'game', \n     'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', \n     'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', \n     'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', \n     'impress', 'anyone', 'cares', 'listen', '^_^']\n    ```", "```py\n    from nltk.stem import PorterStemmer\n    stemmer_p = PorterStemmer()\n    train_tokens_stem = [[stemmer_p.stem(token) for token in sent] \\\n                         for sent in train_tokens_no_stop]\n    test_tokens_stem = [[stemmer_p.stem(token) for token in sent] \\\n                         for sent in test_tokens_no_stop]\n    print(train_tokens_stem[0])\n    ```", "```py\n    ['stune', 'even', 'non-gam', 'sound', 'track', 'beauti', 'paint', \n     'seneri', 'mind', 'well', 'would', 'recomend', 'even', 'peopl', \n     'hate', 'vid', 'game', 'music', 'play', 'game', 'chrono', 'cross', \n     'game', 'ever', 'play', 'best', 'music', 'back', 'away', 'crude', \n     'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', \n     'orchestra', 'would', 'impress', 'anyon', 'care', 'listen', '^_^']\n    ```", "```py\n    train_texts = [\" \".join(txt) for txt in train_tokens_stem]\n    test_texts = [\" \".join(txt) for txt in test_tokens_stem]\n    print(train_texts[0])\n    ```", "```py\n    stune even non-gam sound track beauti paint seneri mind well would recommend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^\n    ```", "```py\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    vocab_size = 10000\n    tok = Tokenizer(num_words=vocab_size)\n    ```", "```py\n    tok.fit_on_texts(train_texts)\n    train_sequences = tok.texts_to_sequences(train_texts)\n    test_sequences = tok.texts_to_sequences(test_texts)\n    print(train_sequences[0])\n    ```", "```py\n     [22, 514, 7161, 85, 190, 184, 1098, 283, 20, 11, 1267, 22, \n      56, 370, 9682, 114, 41, 71, 114, 8166, 1455, 114, 51, 71, \n      29, 41, 58, 182, 2931, 2153, 75, 8167, 816, 2666, 829, 719, \n      3871, 11, 483, 120, 268, 110]\n    ```", "```py\n    seq_lens = [len(seq) for seq in train_sequences]\n    plt.hist(seq_lens)\n    plt.show()\n    ```", "```py\n    maxlen = 100\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    X_train = pad_sequences(train_sequences, maxlen=maxlen)\n    X_test = pad_sequences(test_sequences, maxlen=maxlen)\n    X_train.shape\n    ```", "```py\n    (25000, 100)\n    ```", "```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, GRU, LSTM\n    model_lstm = Sequential()\n    ```", "```py\n    model_lstm.add(Embedding(vocab_size, output_dim=32))\n    model_lstm.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_lstm.add(LSTM(64, return_sequences=True))\n    model_lstm.add(LSTM(64, return_sequences=False))\n    model_lstm.add(Dropout(0.4))\n    ```", "```py\n    model_lstm.add(Dense(32, activation='relu'))\n    model_lstm.add(Dropout(0.5))\n    model_lstm.add(Dense(32, activation='relu'))\n    model_lstm.add(Dropout(0.5))\n    ```", "```py\n    model_lstm.add(Dense(1, activation='sigmoid'))\n    model_lstm.compile(loss='binary_crossentropy', \\\n                       optimizer='rmsprop', \\\n                       metrics=['accuracy'])\n    model_lstm.summary()\n    ```", "```py\n    history_lstm = model_lstm.fit(X_train, train_labels, \\\n                                  batch_size=128, \\\n                                  validation_split=0.2, \\\n                                  epochs = 5)\n    ```", "```py\n    from sklearn.metrics import accuracy_score, confusion_matrix\n    test_pred = model_lstm.predict_classes(X_test)\n    print(confusion_matrix(test_labels, test_pred))\n    ```", "```py\n    [[10226,  1931],\n     [ 1603, 11240]]\n    ```", "```py\n    print(accuracy_score(test_labels, test_pred))\n    ```", "```py\n    0.85864\n    ```", "```py\n    # Import the required library functions\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import pyplot\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input\n    from tensorflow.keras.initializers import RandomNormal\n    from tensorflow.keras.models import Model, Sequential\n    from tensorflow.keras.layers \\\n    import Reshape, Dense, Dropout, Flatten,Activation\n    from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n    from tensorflow.keras.layers import Conv2D, UpSampling2D,Conv2DTranspose\n    from tensorflow.keras.datasets import fashion_mnist\n    from tensorflow.keras.optimizers import Adam\n    ```", "```py\n    # Function to generate real data samples\n    def realData(batch):\n        # Get the MNIST data \n        (X_train, _), (_, _) = fashion_mnist.load_data()\n        # Reshaping the input data to include channel\n        X = X_train[:,:,:,np.newaxis]\n        # normalising the data to be between 0 and 1\n        X = (X.astype('float32') - 127.5)/127.5\n        # Generating a batch of data\n        imageBatch = X[np.random.randint(0, X.shape[0], \\\n                                         size=batch)]\n        return imageBatch\n    ```", "```py\n    # Generating a set of  sample images \n    fashionData = realData(25)\n    ```", "```py\n     # for j in range(5*5):\n        pyplot.subplot(5,5,j+1)\n        # turn off axis \n        pyplot.axis('off') \n        pyplot.imshow(fashionData[j,:,:,0],cmap='gray_r')\n    ```", "```py\n    # Function to generate inputs for generator function\n    def fakeInputs(batch,infeats):\n        # Generate random noise data with shape (batch,input features)\n        x_fake = np.random.uniform(-1,1,size=[batch,infeats])\n        return x_fake\n    ```", "```py\n    Activity7.01.ipynb\n    # Function for the generator model\n    def genModel(infeats):\n        # Defining the Generator model\n        Genmodel = Sequential()\n        Genmodel.add(Dense(512,input_dim=infeats))\n        Genmodel.add(Activation('relu'))\n        Genmodel.add(BatchNormalization())\n        # second layer of FC => RElu => BN layers\n        Genmodel.add(Dense(7*7*64))\n        Genmodel.add(Activation('relu'))\n        Genmodel.add(BatchNormalization())\n    The complete code for this step can be found at https://packt.live/3fpobDm\n    ```", "```py\n    # Function to create fake samples using the generator model\n    def fakedataGenerator(Genmodel,batch,infeats):\n        # first generate the inputs to the model\n        genInputs = fakeInputs(batch,infeats)\n        \"\"\"\n        use these inputs inside the generator model \\\n        to generate fake distribution\n        \"\"\"\n        X_fake = Genmodel.predict(genInputs)\n        return X_fake\n    ```", "```py\n    # Define the arguments like batch size and input feature\n    batch = 128\n    infeats = 100\n    Genmodel = genModel(infeats,)\n    Genmodel.summary()\n    ```", "```py\n    # Generating a fake sample and printing the shape\n    fake = fakedataGenerator(Genmodel,batch,infeats)\n    fake.shape\n    ```", "```py\n    (128, 28, 28, 1)\n    ```", "```py\n    # Plotting the fake sample\n    plt.imshow(fake[1, :, :, 0], cmap='gray_r')\n    ```", "```py\n    Activity7.01.ipynb\n    # Descriminator model as a function\n    def discModel():\n        Discmodel = Sequential()\n        Discmodel.add(Conv2D(32,kernel_size=(5,5),strides=(2,2),\\\n                      padding='same',input_shape=(28,28,1)))\n        Discmodel.add(LeakyReLU(0.2))\n        # second layer of convolutions\n        Discmodel.add(Conv2D(64, kernel_size=(5,5), strides=(2, 2), \\\n                      padding='same'))\n        Discmodel.add(LeakyReLU(0.2))\n    The full code for this step can be found at https://packt.live/3fpobDm\n    ```", "```py\n    # Print the summary of the discriminator model\n    Discmodel = discModel()\n    Discmodel.summary()\n    ```", "```py\n    # Define the combined generator and discriminator model, for updating the generator\n    def ganModel(Genmodel,Discmodel):\n        # First define that discriminator model cannot be trained\n        Discmodel.trainable = False\n        Ganmodel = Sequential()\n        # First adding the generator model\n        Ganmodel.add(Genmodel)\n        \"\"\"\n        Next adding the discriminator model \n        without training the parameters\n        \"\"\"\n        Ganmodel.add(Discmodel)\n        \"\"\"\n        Compile the model for loss to optimise the Generator model\n        \"\"\"\n        Ganmodel.compile(loss='binary_crossentropy',\\\n                         optimizer = 'adam')\n        return Ganmodel\n    ```", "```py\n    # Initialise the GAN model\n    gan_model = ganModel(Genmodel,Discmodel)\n    # Print summary of the GAN model\n    gan_model.summary()\n    ```", "```py\n    # Defining the number of epochs\n    nEpochs = 5000\n    ```", "```py\n    Activity7.01.ipynb\n    # Train the GAN network\n    for i in range(nEpochs):\n        \"\"\"\n        Generate samples equal to the batch size \n        from the real distribution\n        \"\"\"\n        x_real = realData(batch)\n        #Generate fake samples using the fake data generator function\n        x_fake = fakedataGenerator(Genmodel,batch,infeats)\n        # Concatenating the real and fake data \n        X = np.concatenate([x_real,x_fake])\n        #Creating the dependent variable and initializing them as '0'\n        Y = np.zeros(batch * 2)\n    The complete code for this step can be found on https://packt.live/3fpobDm\n    ```", "```py\n    Discriminator probability:0.5276428461074829\n    Discriminator probability:0.5038391351699829\n    Discriminator probability:0.47621315717697144\n    Discriminator probability:0.48467564582824707\n    Discriminator probability:0.5270703434944153\n    Discriminator probability:0.5247280597686768\n    Discriminator probability:0.5282968282699585\n    ```", "```py\n     # Images generated after training\n    x_fake = fakedataGenerator(Genmodel,25,infeats)\n    # Displaying the plots\n    for j in range(5*5):\n    pyplot.subplot(5,5,j+1)\n        # turn off axis \n        pyplot.axis('off')\n        pyplot.imshow(x_fake[j,:,:,0],cmap='gray_r')\n    ```"]