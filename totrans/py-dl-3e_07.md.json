["```py\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    # 1) and 2) Compute the alignment scores with scaling\n    scores = (query @ key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    # 3) Compute the attention scores (softmax)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    # 4) Apply the attention scores over the values\n    return p_attn @ value, p_attn\n```", "```py\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"\"\"\n        :param h: number of heads\n        :param d_model: query/key/value vector length\n        \"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        # Create 4 fully connected layers\n        # 3 for the query/key/value projections\n        # 1 to concatenate the outputs of all heads\n        self.fc_layers = clones(\n            torch.nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = torch.nn.Dropout(p=dropout)\n```", "```py\ndef forward(self, query, key, value, mask=None):\n    if mask is not None:\n        # Same mask applied to all h heads.\n        mask = mask.unsqueeze(1)\n    batch_samples = query.size(0)\n    # 1) Do all the linear projections in batch from d_model => h x d_k\n    projections = [\n        l(x).view(batch_samples, -1, self.h, self.d_k)\n        .transpose(1, 2)\n        for l, x in zip(self.fc_layers, (query, key, value))\n    ]\n    query, key, value = projections\n    # 2) Apply attention on all the projected vectors in batch.\n    x, self.attn = attention(\n        query, key, value,\n        mask=mask,\n        dropout=self.dropout)\n    # 3) \"Concat\" using a view and apply a final linear.\n    x = x.transpose(1, 2).contiguous() \\\n        .view(batch_samples, -1, self.h * self.d_k)\n    return self.fc_layers[-1](x)\n```", "```py\nl(x).view(batch_samples, -1, self.h, self.d_k).transpose(1, 2)\n```", "```py\nclass Encoder(torch.nn.Module):\n    def __init__(self, block: EncoderBlock, N: int):\n        super(Encoder, self).__init__()\n        self.blocks = clones(block, N)\n        self.norm = torch.nn.LayerNorm(block.size)\n    def forward(self, x, mask):\n        \"\"\"Iterate over all blocks and normalize\"\"\"\n        for layer in self.blocks:\n            x = layer(x, mask)\n        return self.norm(x)\n```", "```py\nclass EncoderBlock(torch.nn.Module):\n    def __init__(self,\n                 size: int,\n                 self_attn: MultiHeadedAttention,\n                 ffn: PositionwiseFFN,\n                 dropout=0.1):\n        super(EncoderBlock, self).__init__()\n        self.self_attn = self_attn\n        self.ffn = ffn\n        # Create 2 sub-layer connections\n        # 1 for the self-attention\n        # 1 for the FFN\n        self.sublayers = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n    def forward(self, x, mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayers[1](x, self.ffn)\n```", "```py\nclass SublayerConnection(torch.nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = torch.nn.LayerNorm(size)\n        self.dropout = torch.nn.Dropout(dropout)\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n```", "```py\nclass PositionwiseFFN(torch.nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout=0.1):\n        super(PositionwiseFFN, self).__init__()\n        self.w_1 = torch.nn.Linear(d_model, d_ff)\n        self.w_2 = torch.nn.Linear(d_ff, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n    def forward(self, x):\n        return self.w_2(\n            self.dropout(\n                torch.nn.functional.silu(\n                    self.w_1(x)\n                )))\n```", "```py\nclass Decoder(torch.nn.Module):\n    def __init__(self, block: DecoderBlock, N: int, vocab_size: int):\n        super(Decoder, self).__init__()\n        self.blocks = clones(block, N)\n        self.norm = torch.nn.LayerNorm(block.size)\n        self.projection = torch.nn.Linear(block.size, vocab_size)\n    def forward(self, x, encoder_states, source_mask, target_mask):\n        for layer in self.blocks:\n            x = layer(x, encoder_states, source_mask, target_mask)\n        x = self.norm(x)\n        return torch.nn.functional.log_softmax(self.projection(x), dim=-1)\n```", "```py\nclass DecoderBlock(torch.nn.Module):\n    def __init__(self,\n                 size: int,\n                 self_attn: MultiHeadedAttention,\n                 encoder_attn: MultiHeadedAttention,\n                 ffn: PositionwiseFFN,\n                 dropout=0.1):\n        super(DecoderBlock, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.encoder_attn = encoder_attn\n        self.ffn = ffn\n        self.sublayers = clones(SublayerConnection(size,\n                dropout), 3)\n    def forward(self, x, encoder_states, source_mask, target_mask):\n        x = self.sublayers[0](x, lambda x: \\\n                self.self_attn(x, x, x, target_mask))\n        x = self.sublayers[1](x, lambda x: \\\n                self.encoder_attn(x, encoder_states,\n                encoder_states, source_mask))\n        return self.sublayers[2](x, self.ffn)\n```", "```py\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self,\n                 encoder: Encoder,\n                 decoder: Decoder,\n                 source_embeddings: torch.nn.Sequential,\n                 target_embeddings: torch.nn.Sequential):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.source_embeddings = source_embeddings\n        self.target_embeddings = target_embeddings\n    def forward(self, source, target, source_mask, target_mask):\n        encoder_output = self.encoder(\n            x=self.source_embeddings(source),\n            mask=source_mask)\n        return self.decoder(\n            x=self.target_embeddings(target),\n            encoder_states=encoder_output,\n            source_mask=source_mask,\n            target_mask=target_mask)\n```", "```py\ndef build_model(source_vocabulary: int,\n                target_vocabulary: int,\n                N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFFN(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n      encoder=Encoder(\n        EncoderBlock(d_model, c(attn), c(ff), dropout), N),\n        decoder=Decoder(\n            DecoderBlock(d_model, c(attn), c(attn),\n                    c(ff), dropout), N, target_vocabulary),\n        source_embeddings=torch.nn.Sequential(\n            Embeddings(d_model, source_vocabulary), c(position)),\n        target_embeddings=torch.nn.Sequential(\n            Embeddings(d_model, target_vocabulary), c(position)))\n    # Initialize parameters with random weights\n    for p in model.parameters():\n        if p.dim() > 1:\n            torch.nn.init.xavier_uniform_(p)\n    return model\n```", "```py\nsource_embeddings=torch.nn.Sequential(Embeddings(d_model, source_vocabulary), c(position))\n```"]