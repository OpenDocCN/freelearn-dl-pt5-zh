- en: '*Chapter 3*: Automating the Machine Learning Pipeline with AutoKeras'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating the machine learning pipeline involves automating a series of processes
    such as **data exploration**, **data preprocessing**, **feature engineering**,
    **algorithm selection**, **model training**, and **hyperparameter tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explains the standard machine learning pipeline and how to automate
    some of them with **AutoKeras**. We will also describe the main data preparation
    best practices to apply before training a model. The post-data preparation steps
    are performed by AutoKeras and we will see them in depth in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the first chapter, AutoKeras can automate all pipeline modeling
    steps by applying hyperparameter optimization and **Neural Architecture Search**
    (**NAS**), but some data preprocessing before these steps must be done by hand
    or with other tools.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the data representations expected by our model, as well as the
    basic preprocessing techniques that AutoKeras applies. By the end of this chapter,
    you will have learned the main data formats and techniques for feeding your models
    in a suitable and optimal way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics that will be covered are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data to feed deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data into AutoKeras in multiple formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting your dataset for training and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will look at some basic preprocessing techniques and how
    to use the AutoKeras helpers to apply them, but first, let's explain what kind
    of data structures are expected by our model and how are they represented.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the MNIST example, the digit images were stored in NumPy matrices, also called
    tensors. These tensors are the basic data structures for machine learning models.
    Now that we know what fuels our models, let's dig deeper into understanding what
    tensors are and their different types.
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A tensor is basically a multi-dimensional array of numbers, usually floating-point
    numbers of N dimensions (also called *axes*).
  prefs: []
  type: TYPE_NORMAL
- en: 'A tensor is defined by three key attributes: the number of axes or rank, the
    dimension of each axes or shape, and the type of data it contains. Let''s explain
    them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy` nomenclature (`ndim`). For instance, a scalar (a single number) has
    no axes, a vector (a list of numbers) has one, a matrix (a list of vectors) has
    two, and a 3D tensor (a list of matrices) has three. Let''s look at a practical
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the previous code snippet, we created a matrix and printed its rank (`2`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Shape (each axis dimension)**: This is the dimension of each axis and returns
    a tuple with the lengths of the corresponding array dimensions. In the case of
    a matrix, the first item would correspond to the number of rows and the second
    item would correspond to the number of columns, as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This time, we have created a matrix and printed its shape (4 rows, 3 columns).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data type**: The type of data that''s contained in the tensor is usually
    floating-point numbers because the computer works in a more optimal way with this
    type of data. For instance, in the following matrix, the stored items are integers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we've explained the key attributes of a tensor, let's see what types
    of tensors we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Types of tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on their dimensions, we can classify tensors like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalar (N=0)**: A tensor containing just one number is called a scalar; let''s
    create one:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By creating a scalar and printing its rank, we can see its value is 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Vector (N=1)**: A 1D tensor is a called vector. It''s an array of numbers
    of 1 dimension, as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have created a vector of 1 dimension and printed its rank.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`[1, 2, 3]` and the first column is `[1, 4, 7]`, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3D tensors (N=3)**: A 3D tensor is an array of matrices. This tensor is typically
    used to represent images using a 3-matrice array, with each matrix representing
    a color (red, green, or blue) of the pixel. You can imagine it as a cube filled
    with numbers. Let''s create one using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that the rank that's returned for the 3D tensor is 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**4D tensors (N=4)**: 4D tensors are arrays of 3D tensors. This complex structure
    is often used to store video, which is basically a batch of frames, where each
    frame is an image represented by a 3D tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a visual representation of these ranks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A visual representation of tensors with different ranks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – A visual representation of tensors with different ranks
  prefs: []
  type: TYPE_NORMAL
- en: A 3D tensor can store an RGB image or frame, while a 4D tensor can contain a
    video as an array of frames.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data to feed deep learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explained that AutoKeras is a framework that specializes
    in deep learning that uses neural networks as a learning engine. We also learned
    how to create end-to-end classifier/regressor models for the MNIST dataset of
    handwritten digits as input data. This dataset had already been preprocessed to
    be used by the model, which means all the images had the same attributes (same
    size, color, and so on), but this is not always the case.
  prefs: []
  type: TYPE_NORMAL
- en: Once we know what a tensor is, we are ready to learn how to feed our neural
    networks. Most of the data preprocessing techniques are domain-specific, and we
    will explain them in the following chapters when we need to use them in specific
    examples. But first, we will present some fundamentals that are the basis for
    each specific technique.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing operations for neural network models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at some of the operations we can use to transform
    the raw input data into a more appropriate format. This will allow us to feed
    the neural network in order to improve the learning performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main data preprocessing operations are feature engineering, data normalization,
    data vectorization, and missing values processing. Let''s look at them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering**: This is the process of extracting features from raw
    data using the domain knowledge of human experts, in such a way that these extracted
    features improve the performance of our models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In traditional machine learning, function engineering is critical but with deep
    learning, this process is not as important because neural networks can automatically
    extract the relevant characteristics from the raw input data. However, there are
    cases where function engineering is still crucial, such as when we don't have
    a large dataset, the input data is structured, or we have limited resources. In
    these cases, this step is key to achieving our goals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data normalization**: Neural networks work much better with small input values,
    usually between 0 and 1\. It is easier for the model to learn from small numbers
    because the learning algorithms are based on gradient updates being made to its
    weight parameters, in such a way that small values will cause faster updates,
    thus speeding up the process, while larger values will slow it down. Usually,
    the dataset comes with larger values, so before we incorporate them into our model,
    we need to change and scale them to a range of 0 to 1\. This technique is called
    normalization. AutoKeras already does this for us. In the previous digit classification
    example, the dataset contained images encoded as integers from 0 to 255\. However,
    we fed our model without performing normalization because AutoKeras did it for
    us automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MINST` example shown in the previous chapter, the dataset has already been
    vectorized, so this process was not necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing values processing**: Datasets often contain missing values in some
    records. How should your model handle these incomplete records? Generally, for
    deep learning models, initializing missing values to 0 is a common practice, as
    long as 0 is not already a significant value. Once the neural network model learns
    that 0 means a missing value, it will ignore it every time. It is important to
    note that if your model will be exposed to missing values in the real world and
    you trained it without them, it will not learn to ignore them. So, a common practice
    in this case is to artificially generate missing values to force your model to
    learn how to handle them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've learned about the main data structures and their `transform`
    operations, we will look at what data formats are supported by AutoKeras and what
    utilities it has to convert raw data into a more suitable format.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data into AutoKeras in multiple formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, AutoKeras performs normalization automatically.
    However, in the following chapters, you will see that you can create your model
    in a more personalized way by stacking blocks. More specifically, you can use
    special blocks to normalize your data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the different data structures that we can use to feed our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoKeras models accept three types of input:'
  prefs: []
  type: TYPE_NORMAL
- en: A **NumPy array** is an array that's commonly used by **Scikit-Learn** and many
    other Python-based libraries. This is always the fastest option, as long as your
    data fits in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python generators** load batches of data from disk to memory, so this is
    a good option when the entire dataset does not fit in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Dataset** is a high-performance option that is similar to Python
    generators, but it is best suited for deep learning and large datasets. This is
    because data can be streamed from disk or from a distributed filesystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can prepare your data in one of these formats before feeding it to your
    AutoKeras model. If you are working with large datasets and need to train in GPUs,
    the best choice is to use the **TensorFlow Dataset** object, because they have
    many advantages in terms of performance and versatility, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It can perform asynchronous preprocessing and data queuing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides GPU memory data preloading, so that it is available after the GPU
    finishes processing the previous batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides transformation primitives, so that you can apply a function to each
    element of the dataset that's generating a new transformed dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cache that maintains the latest batches that have been read from the dataset
    in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can load from several sources (**NumPy arrays**, **Python generators**,
    **CSV files**, text files, folders, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram represents all the different data sources that can use
    a TensorFlow Dataset object as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – A visual representation of a TensorFlow Dataset object''s input
    sources](img/B16953_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – A visual representation of a TensorFlow Dataset object's input
    sources
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoKeras has very useful utilities to help you convert raw data on disk into
    a TensorFlow Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`autokeras.image_dataset_from_directory` converts image files stored in a directory
    in a specific way into a tagged dataset of image tensors. Let''s learn how to
    process a images directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following directory is well-structured, which means we can feed it to AutoKeras.
    There''s a subfolder for each class of images:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must pass this folder path to the `autokeras` function in order to
    create a dataset from the images directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are several parameters, but only the path directory (`main_directory`)
    is required; the rest of the parameters are set by default. We will explain them
    in more detail in later chapters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`autokeras.text_dataset_from_directory` generates a Tensorflow Dataset from
    text files stored in a directory in a specific way. As we saw previously with
    the images, we have to create a subfolder for every category:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we mentioned previously, with images, only the path directory (`directory`)
    is required; the rest of the parameters, if they haven't been initialized, will
    be set by default. We will explain these in more detail in later chapters as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, AutoKeras can work with CSV files by directly passing the filename
    as a parameter to its structured data models; that is, `autokeras.StructuredDataClassifier`
    and `autokeras.StructuredDataRegressor`. Now that we know what kind of data is
    best suited for AutoKeras and what utilities it has for preprocessing it, we will
    learn how to divide our dataset so that we can properly evaluate and test our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting your dataset for training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate a model, you must divide your dataset into three subsets: a training
    set, a validation set, and a test set. During the training phase, AutoKeras will
    train your model with the training dataset, while using the validation dataset
    to evaluate its performance. Once you are ready, the final evaluation will be
    done using the test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Why you should split your dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a separate test dataset that is not used during training is really important
    to avoid information leaks.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the validation set is used to tune the hyperparameters
    of your model based on the performance of the model, but some information about
    the validation data is filtered into the model. Due to this, you run the risk
    of ending up with a model that works artificially well with the validation data,
    because that's what you trained it for. However, the actual performance of the
    model is due to us using previously unseen data, not validation data, so we must
    use a different and never-before-seen dataset to evaluate our model. This is known
    as the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid information leaks, it is very important that your model has never had
    access to any information about the test set, not even indirectly. This is why
    it's so important to have a separate test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to split your dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the MNIST example in the previous chapter, we did not split the dataset
    explicitly because the `load_data` method did this split for us. However, often,
    these datasets are just one set of records that you will have to split. The following
    is a visual representation of dataset splitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – A visual representation of dataset splitting](img/B16953_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A visual representation of dataset splitting
  prefs: []
  type: TYPE_NORMAL
- en: 'When AutoKeras is training a model, it will reserve 20% of the training set
    for validation by default, but you can always define a different percent using
    `validation_split param` in the `fit` function. In the following code, we are
    using this parameter to split the training data and using the last 15% as validation
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also manually create the validation dataset and pass it as `validation_data
    param: split = 5000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use to split the `train_test_split` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's summarize what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about tensors, the main data structures for networks,
    some data preprocessing operations for neural networks, and the AutoKeras data
    formats that are supported, as well as its data-preprocessing utilities. Finally,
    we learned how to split a dataset in a quick and easy way. Now, you are ready
    to power your AutoKeras models in the most appropriate way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how AutoKeras works with images. We will
    also introduce some techniques we can use to extract specific characteristics
    from images and how to apply them.
  prefs: []
  type: TYPE_NORMAL
