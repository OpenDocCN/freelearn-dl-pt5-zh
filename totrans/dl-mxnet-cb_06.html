<html><head></head><body>
<div id="_idContainer157">
<h1 class="c apter-number" id="_idParaDest-120"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.2.1">Understanding Text with Natural Language Processing</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">NLP</span></strong><span class="koboSpan" id="kobo.6.1">) is </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.7.1">the field of machine learning that deals with the understanding of language, in the form of text data. </span><span class="koboSpan" id="kobo.7.2">It is one of the fields that has seen a strong evolution in the last few years, achieving great results in the areas of sentiment analysis, chatbots, text summarization, and machine translation. </span><span class="koboSpan" id="kobo.7.3">NLP is at the core of the assistants developed by Amazon (Alexa), Google, and Apple (Siri), as well as modern assistants such as ChatGPT and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">Llama 2.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we will learn how to use GluonNLP, an MXNet Gluon library specific to NLP, how to build our own networks, and how to use its Model Zoo API for several applications of </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">pre-trained models.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">NLP networks</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Classifying news highlights with </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">topic modeling</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Analyzing sentiment in </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">movie reviews</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Translating text from Vietnamese </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">to English</span></span></li>
</ul>
<h1 id="_idParaDest-122"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.22.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.23.1">Preface</span></em><span class="koboSpan" id="kobo.24.1">, the following technical requirements apply to </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.26.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.27.1">Recipe 1</span></em><span class="koboSpan" id="kobo.28.1">, </span><em class="italic"><span class="koboSpan" id="kobo.29.1">Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.30.1">, from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.31.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.32.1">, </span><em class="italic"><span class="koboSpan" id="kobo.33.1">Up and Running </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.34.1">with MXNet</span></em></span></li>
<li><span class="koboSpan" id="kobo.35.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.36.1">Recipe 4</span></em><span class="koboSpan" id="kobo.37.1">, </span><em class="italic"><span class="koboSpan" id="kobo.38.1">Toy dataset for text classification: Load, manage, and visualize a spam email dataset</span></em><span class="koboSpan" id="kobo.39.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.40.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.41.1">, </span><em class="italic"><span class="koboSpan" id="kobo.42.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.43.1">and DataLoader</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.44.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06"><span class="No-Break"><span class="koboSpan" id="kobo.46.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06</span></span></a><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06 "/></p>
<p><span class="koboSpan" id="kobo.47.1">Furthermore, you can access each recipe directly from Google Colab; for example, use the following link for the first recipe of this </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">chapter: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">.</span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb "/></p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.51.1">Introducing NLP networks</span></h1>
<p><span class="koboSpan" id="kobo.52.1">In the previous </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.53.1">chapters, we saw how different architectures, such as </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">Multi-Layer Perceptrons</span></strong><span class="koboSpan" id="kobo.55.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.56.1">MLPs</span></strong><span class="koboSpan" id="kobo.57.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">Convolutional Neural Networks</span></strong><span class="koboSpan" id="kobo.59.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.60.1">CNNs</span></strong><span class="koboSpan" id="kobo.61.1">), deal with numerical data and images, respectively. </span><span class="koboSpan" id="kobo.61.2">In this recipe, we will analyze the most important architectures to process natural language expressed as </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">text data.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">The most important characteristic of natural language is that it is a list of words of variable length, and the order of those words matters; it is a sequence. </span><span class="koboSpan" id="kobo.63.2">The previous architectures that we have analyzed are not suited for variable-length data inputs and also do not exploit the relationships among </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">words effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">In this recipe, we will introduce neural networks that have been developed to process sequences </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">of words:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.67.1">We will start by applying the </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.68.1">network introduced in the previous chapter, that is, CNNs</span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.69.1"> for text processing, </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.71.1">TextCNNs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.73.1">Afterward, we will</span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.74.1"> introduce </span><strong class="bold"><span class="koboSpan" id="kobo.75.1">Recurrent Neural Networks</span></strong><span class="koboSpan" id="kobo.76.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.77.1">RNNs</span></strong><span class="koboSpan" id="kobo.78.1">) an</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.79.1">d their vanilla implementation. </span><span class="koboSpan" id="kobo.79.2">Then, we will continue with an improved version known</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.80.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">Long Short-Term </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.82.1">Memory</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.84.1">LSTM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.86.1">Then, as we did with computer vision, we will introduce </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">GluonNLP Model Zoo</span></strong><span class="koboSpan" id="kobo.88.1">, one of </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.89.1">the </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.90.1">most value-adding features of MXNet. </span><span class="koboSpan" id="kobo.90.2">We will leverage these libraries MXNet and GluonNLP to understand and implement </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">transformers</span></strong><span class="koboSpan" id="kobo.92.1"> and </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.93.1">their self-attention mechanisms, and how these networks deal with sequences of </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">variable length.</span></span></li>
</ol>
<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.95.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.96.1">As in previous chapters, in this recipe, we will be using a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">at all.</span></span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.98.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.99.1">In this recipe, we </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.100.1">will be doing </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.102.1">Applying CNNs for text </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">processing (TextCNNs)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.104.1">Introducing RNNs</span></span></li>
<li><span class="koboSpan" id="kobo.105.1">Improving RNNs </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">with LSTM</span></span></li>
<li><span class="koboSpan" id="kobo.107.1">Introducing GluonNLP </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">Model Zoo</span></span></li>
<li><span class="koboSpan" id="kobo.109.1">Paying attention </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">to Transformers</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.111.1">Let’s go through each of these network architectures in </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">detail next.</span></span></p>
<h3><span class="koboSpan" id="kobo.113.1">Applying CNNs for text processing</span></h3>
<p><span class="koboSpan" id="kobo.114.1">CNNs</span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.115.1"> were introduced in the previous chapter and are typically used for working with images. </span><span class="koboSpan" id="kobo.115.2">However, with some slight changes, CNNs can work very efficiently with </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">text data.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">Images are 2D data and, as shown in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.118.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.119.1">, </span><em class="italic"><span class="koboSpan" id="kobo.120.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.121.1">, we worked with two layers on </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">this data:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.123.1">2D </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">convolutions layers</span></span></li>
<li><span class="koboSpan" id="kobo.125.1">Max </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">pooling layers</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.127.1">These operations are slightly modified to work with text data, which can be seen as a 1D sequence. </span><span class="koboSpan" id="kobo.127.2">Therefore, for</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.128.1"> 1D convolution layers, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.130.1"><img alt="Figure 6.1 – 1D convolution" src="image/B16591_06_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.131.1">Figure 6.1 – 1D convolution</span></p>
<p><span class="koboSpan" id="kobo.132.1">As can be</span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.133.1"> seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.135.1">.1</span></em><span class="koboSpan" id="kobo.136.1">, the sequence combined with the kernel varies over time, yielding a new sequence as output. </span><span class="koboSpan" id="kobo.136.2">Please note how the number of words that are analyzed at the same time is the kernel size (3 in the </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">preceding figure).</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">For the max pooling layers, as we only have one dimension, which corresponds to time, these layers are </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.139.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.140.1">max-over-time </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.141.1">pooling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.142.1"> layers:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<span class="koboSpan" id="kobo.143.1"><img alt="Figure 6.2 – Max-over-time pooling" src="image/B16591_06_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.144.1">Figure 6.2 – Max-over-time pooling</span></p>
<p><span class="koboSpan" id="kobo.145.1">As can </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.146.1">be seen from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.147.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.148.1">.2</span></em><span class="koboSpan" id="kobo.149.1">, the maximum value from</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.150.1"> the sequence </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">is selected.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Using MXNet Gluon, we can define the 1D convolution and max-over-time layers </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.154.1">
# TextCNN Components
# 1D Convolution
conv1d = mx.gluon.nn.Conv1D(3, 100, activation='relu')
# Max-Over-Time Pooling
max_over_time_pooling = mx.gluon.nn.GlobalMaxPool1D()</span></pre> <p><span class="koboSpan" id="kobo.155.1">An example of how to work with these layers can be found in the </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">GitHub code.</span></span></p>
<p><span class="koboSpan" id="kobo.157.1">As with the CNN architecture shown in the previous chapter (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.158.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.159.1">.6</span></em><span class="koboSpan" id="kobo.160.1">), typically, after a feature learning phase, we have a classifier. </span><span class="koboSpan" id="kobo.160.2">As an example application, this kind of architecture will help us later with </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">sentiment analysis.</span></span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.162.1">Introducing Recurrent Neural Networks (RNNs)</span></h2>
<p><span class="koboSpan" id="kobo.163.1">As discussed</span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.164.1"> in the recipe introduction, RNNs</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.165.1"> are architectures that deal with sequences of data of variable length. </span><span class="koboSpan" id="kobo.165.2">For NLP, these data points are sentences, composed of words, but they can also be utilized for sequences of images (video), </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">for example.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">RNN’s history is a series of step-by-step attempts to improve the recurrent processing of different inputs of a sequence. </span><span class="koboSpan" id="kobo.167.2">There have been several important contributions, the most notable being Hopfield (1982), Jordan (1986), and </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">Elman (1990).</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">The idea behind RNNs is that once the output is processed from the input, that output is fed again to the model (in a recurrent manner), in combination with the new incoming input. </span><span class="koboSpan" id="kobo.169.2">This mechanism allows the model to have memory (can access past data) and </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.170.1">process the new input, taking into account that past information as well. </span><span class="koboSpan" id="kobo.170.2">This basic architecture is typically referred to as</span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.171.1"> a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.172.1">vanilla RNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.174.1">Please note that, as introduced in </span><em class="italic"><span class="koboSpan" id="kobo.175.1">Recipe 4</span></em><span class="koboSpan" id="kobo.176.1">, </span><em class="italic"><span class="koboSpan" id="kobo.177.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.178.1">, in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.179.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.180.1">, </span><em class="italic"><span class="koboSpan" id="kobo.181.1">Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.182.1">, the inputs to NLP networks, including RNNs, are not the words obtained from the dataset, but numerical representations of those words, such as one-hot encoding or </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">word embeddings.</span></span></p>
<p><span class="koboSpan" id="kobo.184.1">If a sequence of data is modeled as successive inputs, x(1), x(2), .... </span><span class="koboSpan" id="kobo.184.2">x(t), the architecture of an RNN can be visualized </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<span class="koboSpan" id="kobo.186.1"><img alt="Figure 6.3 – RNN architecture" src="image/B16591_06_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.187.1">Figure 6.3 – RNN architecture</span></p>
<p><span class="koboSpan" id="kobo.188.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.189.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.190.1">.3</span></em><span class="koboSpan" id="kobo.191.1">, we</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.192.1"> can see how every input </span><em class="italic"><span class="koboSpan" id="kobo.193.1">x(t)</span></em><span class="koboSpan" id="kobo.194.1"> is processed over time, and how the processed input (hidden state) is looped back for the next iterations. </span><span class="koboSpan" id="kobo.194.2">Let’s take a deeper look at what is happening at each step, as in the preceding figure, activation functions and biases are not shown for simplicity. </span><span class="koboSpan" id="kobo.194.3">The actual equations of a vanilla RNN cell are </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<span class="koboSpan" id="kobo.196.1"><img alt="Figure 6.4 – Equations for a vanilla RNN cell" src="image/B16591_06_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.197.1">Figure 6.4 – Equations for a vanilla RNN cell</span></p>
<p><span class="koboSpan" id="kobo.198.1">As</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.199.1"> can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.200.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.201.1">.4</span></em><span class="koboSpan" id="kobo.202.1">, for each step, the input, </span><em class="italic"><span class="koboSpan" id="kobo.203.1">x(t)</span></em><span class="koboSpan" id="kobo.204.1">, is multiplied by a weight matrix, </span><em class="italic"><span class="koboSpan" id="kobo.205.1">U</span></em><span class="koboSpan" id="kobo.206.1">, and a bias vector, </span><em class="italic"><span class="koboSpan" id="kobo.207.1">b</span></em><span class="koboSpan" id="kobo.208.1">, is added, which yields the value </span><em class="italic"><span class="koboSpan" id="kobo.209.1">a(t)</span></em><span class="koboSpan" id="kobo.210.1">, assuming there was no previous input, </span><em class="italic"><span class="koboSpan" id="kobo.211.1">h(t – 1) = 0</span></em><span class="koboSpan" id="kobo.212.1">. </span><span class="koboSpan" id="kobo.212.2">The state value, </span><em class="italic"><span class="koboSpan" id="kobo.213.1">h(t)</span></em><span class="koboSpan" id="kobo.214.1">, is computed as the output of the activation function, </span><em class="italic"><span class="koboSpan" id="kobo.215.1">tanh</span></em><span class="koboSpan" id="kobo.216.1">, of that value. </span><span class="koboSpan" id="kobo.216.2">When there is a previous input, the previous state value, </span><em class="italic"><span class="koboSpan" id="kobo.217.1">h(t – 1)</span></em><span class="koboSpan" id="kobo.218.1">, is multiplied by a weight matrix, </span><em class="italic"><span class="koboSpan" id="kobo.219.1">W</span></em><span class="koboSpan" id="kobo.220.1">, and added to the computations of values </span><em class="italic"><span class="koboSpan" id="kobo.221.1">a(t)</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.222.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.223.1">h(t)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">The state value, </span><em class="italic"><span class="koboSpan" id="kobo.226.1">h(t)</span></em><span class="koboSpan" id="kobo.227.1">, is then multiplied by a weight matrix, </span><em class="italic"><span class="koboSpan" id="kobo.228.1">V</span></em><span class="koboSpan" id="kobo.229.1">, and a bias vector, </span><em class="italic"><span class="koboSpan" id="kobo.230.1">c</span></em><span class="koboSpan" id="kobo.231.1">, is added which yields the value </span><em class="italic"><span class="koboSpan" id="kobo.232.1">o(t)</span></em><span class="koboSpan" id="kobo.233.1">. </span><span class="koboSpan" id="kobo.233.2">The output of the cell is computed as the output of the activation function, </span><em class="italic"><span class="koboSpan" id="kobo.234.1">softmax</span></em><span class="koboSpan" id="kobo.235.1">, of that value, yielding the final output </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">value </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.237.1">y(t)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.239.1">These cells can be stacked together to produce a </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">complete RNN.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">Using MXNet and Gluon, we can easily create our own custom </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">RNN networks:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.243.1">
# RNNs MXNet Implementation Example
class RNNModel(mx.gluon.Block):
    """
    A basic RNN Model
    """
    def __init__(self, num_hidden, num_layers, embed_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = mx.gluon.rnn.RNN(
            num_hidden,
            num_layers,
            input_size=embed_size)
    def forward(self, inputs, hidden):
        output, hidden = self.rnn(inputs, hidden)
        return output, hidden</span></pre> <p><span class="koboSpan" id="kobo.244.1">We can</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.245.1"> use this code to define a</span><a id="_idIndexMarker616"/> <span class="No-Break"><span class="koboSpan" id="kobo.246.1">custom RNN:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.247.1">
# RNN with 3 hidden cells, 1 layer and expecting inputs with 20 embeddings
rnn = RNNModel(3, 1, 20)
 rnn.collect_params().initialize(mx.init.Xavier(), ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.248.1">Furthermore, we use the following to process a </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">sequential input:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.250.1">
rnn_hidden = hidden_initial
outputs = []
for index in range(3):
    rnn_output, rnn_hidden = rnn(inputs[index], rnn_hidden)
    outputs.append(rnn_output)</span></pre> <p><span class="koboSpan" id="kobo.251.1">The previous code shown runs a custom RNN. </span><span class="koboSpan" id="kobo.251.2">Feel free to play with the notebook available in the GitHub repository accompanying </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">To conclude, one advantage of RNNs is that the information present in previous inputs is kept stored in the states transferred along time steps. </span><span class="koboSpan" id="kobo.253.2">However, this information is constantly multiplied by different weight matrices and passed through non-linear functions (</span><em class="italic"><span class="koboSpan" id="kobo.254.1">tanh</span></em><span class="koboSpan" id="kobo.255.1">), and the outcome is that, after several time steps, the state information is modified and does not work anymore as memory; the information stored has changed too much. </span><span class="koboSpan" id="kobo.255.2">Long-term memory storage is a problem </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">for RNNs.</span></span></p>
<h3><span class="koboSpan" id="kobo.257.1">Training RNNs</span></h3>
<p><span class="koboSpan" id="kobo.258.1">In the </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.259.1">previous chapters, we saw how to train networks using supervised learning, by computing the loss function between the expected outputs, the </span><strong class="bold"><span class="koboSpan" id="kobo.260.1">ground truth</span></strong><span class="koboSpan" id="kobo.261.1">, and the actual outputs of the network. </span><span class="koboSpan" id="kobo.261.2">This error could then be back-propagated from the outer layers of the network to the inner layers of the network and update the weights of </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">these layers.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">With RNNs, at each time step, the network is shown a sequence of inputs and the expected sequence in the outputs. </span><span class="koboSpan" id="kobo.263.2">Errors are computed for each time step and back-propagated from the outer layers of the network to the inner layers of the network. </span><span class="koboSpan" id="kobo.263.3">This variation, suitable for RNNs, is </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.264.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">Back-Propagation Through </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.266.1">Time</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.267.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.268.1">BPTT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">As with other networks, computing the different gradients involves the iterative multiplication of matrices. </span><span class="koboSpan" id="kobo.270.2">This operation is exponential, which means that, after several occurrences, the values will either shrink or blow up. </span><span class="koboSpan" id="kobo.270.3">This leads to a problem we have already</span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.271.1"> discussed: </span><strong class="bold"><span class="koboSpan" id="kobo.272.1">vanishing gradients</span></strong><span class="koboSpan" id="kobo.273.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.274.1">exploding gradients</span></strong><span class="koboSpan" id="kobo.275.1">. </span><span class="koboSpan" id="kobo.275.2">These </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.276.1">issues make the training of RNNs very unstable. </span><span class="koboSpan" id="kobo.276.2">Another important drawback of BPTT is that as it is a sequential computation, it cannot </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">be parallelized.</span></span></p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.278.1">Improving RNNs with Long Short-Term Memory (LSTM)</span></h2>
<p><span class="koboSpan" id="kobo.279.1">LSTMs were </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.280.1">introduced by </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.281.1">Hochreiter and Schmidhuber in 1997, as a mechanism to solve the problems described previously (lack of long-term memory and vanishing/exploding gradients). </span><span class="koboSpan" id="kobo.281.2">In LSTMs, instead of having the previous state multiplied and passed through the non-linear function, the connection is much more straightforward. </span><span class="koboSpan" id="kobo.281.3">To provide this mechanism, each LSTM cell receives two inputs from </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.282.1">the previous</span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.283.1"> cell: the </span><strong class="bold"><span class="koboSpan" id="kobo.284.1">hidden state</span></strong><span class="koboSpan" id="kobo.285.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.286.1">ht</span></strong><span class="koboSpan" id="kobo.287.1">) and the </span><strong class="bold"><span class="koboSpan" id="kobo.288.1">cell </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.289.1">state</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.290.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.291.1">ct</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 6.5 – LSTM network" src="image/B16591_06_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 6.5 – LSTM network</span></p>
<p><span class="koboSpan" id="kobo.295.1">The key </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.296.1">components of LSTMs are called gates, which define how a certain input is modified to become a part of the outputs. </span><span class="koboSpan" id="kobo.296.2">These vectors </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.297.1">have values between 0 and 1 and help activate/deactivate the information from the input. </span><span class="koboSpan" id="kobo.297.2">They are, therefore, a sigmoid operation (depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.298.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.299.1">.5</span></em><span class="koboSpan" id="kobo.300.1"> as </span><em class="italic"><span class="koboSpan" id="kobo.301.1">σ</span></em><span class="koboSpan" id="kobo.302.1">) of a weighted sum of the inputs, followed by a multiplication operation. </span><span class="koboSpan" id="kobo.302.2">To emphasize, each of the three gates present in an LSTM cell allows how much of the input or the state passes through each of the outputs. </span><span class="koboSpan" id="kobo.302.3">Taking this into account, the </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.303.1">following equations define the </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">LSTM behavior:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<span class="koboSpan" id="kobo.305.1"><img alt="Figure 6.6 – Equations for ﻿an LSTM cell" src="image/B16591_06_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.306.1">Figure 6.6 – Equations for an LSTM cell</span></p>
<p><span class="koboSpan" id="kobo.307.1">The </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.308.1">equations in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.309.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.310.1">.6</span></em><span class="koboSpan" id="kobo.311.1"> can be </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.312.1">explained </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">as </span></span><span class="No-Break"><a id="_idIndexMarker630"/></span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.315.1">Input gate (it)</span></strong><span class="koboSpan" id="kobo.316.1">: This is </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.317.1">the gate that decides how the information from the previous state and the current input will </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">be updated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Forget gate (ft)</span></strong><span class="koboSpan" id="kobo.320.1">: This</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.321.1"> is the gate that decides how the information from the previous state and the current input will become part of the long-term memory (cell state). </span><span class="koboSpan" id="kobo.321.2">This is how much of the current step we want </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">to forget.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.323.1">Memory cell candidate (gt)</span></strong><span class="koboSpan" id="kobo.324.1">: This </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.325.1">is the computation that decides how the information from the previous state and the current input will become part of the memory cell. </span><span class="koboSpan" id="kobo.325.2">It must allow for positive and negative values; therefore, </span><em class="italic"><span class="koboSpan" id="kobo.326.1">tanh</span></em><span class="koboSpan" id="kobo.327.1"> is the activation </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">function selected.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.329.1">Output gate (ot)</span></strong><span class="koboSpan" id="kobo.330.1">: This is the </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.331.1">gate that decides how the information from the previous state and the current input will become part of </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">the output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">Memory cell (ct)</span></strong><span class="koboSpan" id="kobo.334.1">: This is</span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.335.1"> the computation that combines the previous state (</span><em class="italic"><span class="koboSpan" id="kobo.336.1">c</span></em><span class="subscript"><span class="koboSpan" id="kobo.337.1">t-1</span></span><span class="koboSpan" id="kobo.338.1">) and the current memory cell candidate (</span><em class="italic"><span class="koboSpan" id="kobo.339.1">g</span></em><span class="subscript"><span class="koboSpan" id="kobo.340.1">t</span></span><span class="koboSpan" id="kobo.341.1">) into the new </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">cell state</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.343.1">Output state (ht)</span></strong><span class="koboSpan" id="kobo.344.1">: This is </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.345.1">the computation that combines the memory cell with the output </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">gate value.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.347.1">Using </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.348.1">MXNet and Gluon, we can</span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.349.1"> easily create our own </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.350.1">custom </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">LSTM networks:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.352.1">
# LSTMs MXNet Implementation Example
class LSTMModel(mx.gluon.Block):
    """
    A basic LSTM Model
    """
    def __init__(self, num_hidden, num_layers, embed_size, **kwargs):
        super(LSTMModel, self).__init__(**kwargs)
        self.lstm = mx.gluon.rnn.LSTM(
            num_hidden,
            num_layers,
            input_size=embed_size)
    def forward(self, inputs, hidden):
        output, hidden = self.lstm(inputs, hidden)
        return output, hidden</span></pre> <p><span class="koboSpan" id="kobo.353.1">We can use the following code to define a </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">custom RNN:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.355.1">
# LSTM with 3 hidden cells, 1 layer and expecting inputs with 20 embeddings
lstm = LSTMModel(3, 1, 20)
 lstm.collect_params().initialize(mx.init.Xavier(), ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.356.1">Furthermore, we</span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.357.1"> can use the following to process a </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">sequential input:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.359.1">
lstm_hidden = [hidden_initial, state_initial]
 outputs = []
for index in range(3):
    lstm_output, lstm_hidden = lstm(inputs[index], lstm_hidden)
    outputs.append(lstm_output)</span></pre> <p><span class="koboSpan" id="kobo.360.1">The</span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.361.1"> code shown here runs a</span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.362.1"> custom LSTM network. </span><span class="koboSpan" id="kobo.362.2">Feel free to play with the notebook available in the GitHub repository accompanying </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.364.1">To conclude, LSTMs allow RNNs to be trained more optimally and have been implemented to solve a large number of tasks in NLP, such as sentiment analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">language modeling.</span></span></p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.366.1">Introducing GluonNLP Model Zoo</span></h2>
<p><span class="koboSpan" id="kobo.367.1">One of the </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.368.1">best features that MXNet GluonCV provides</span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.369.1"> is its large pool of pre-trained models, readily available for its users to use and deploy in their own applications. </span><span class="koboSpan" id="kobo.369.2">This model library is called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.370.1">Model Zoo</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">In Model Zoo, models have been pre-trained for the </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">following tasks:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.374.1">Language models</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.375.1">Sentiment analysis</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.376.1">Machine translation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.377.1">Sentence classification</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.378.1">Question answering</span></span></li>
<li><span class="koboSpan" id="kobo.379.1">Named </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">entity recognition</span></span></li>
<li><span class="koboSpan" id="kobo.381.1">Joint intent classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">slot labeling</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.383.1">In this chapter, we </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.384.1">will examine in detail the pre-trained models included for sentiment analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">machine translation.</span></span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.386.1">Paying attention with Transformers</span></h2>
<p><span class="koboSpan" id="kobo.387.1">Although LSTMs have </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.388.1">been proven to work well for a lot of applications, they also have significant drawbacks, including taking longer and requiring </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.389.1">more memory to train, as well as being sensitive to random initialization. </span><span class="koboSpan" id="kobo.389.2">New architectures have been developed that overcome these limitations. </span><span class="koboSpan" id="kobo.389.3">One of the most important examples </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">is </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.391.1">Transformers</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">Transformers were introduced by Google Brain in 2017. </span><span class="koboSpan" id="kobo.393.2">It is a novel approach of an encoder-decoder architecture (as seen in </span><em class="italic"><span class="koboSpan" id="kobo.394.1">Recipe 4</span></em><span class="koboSpan" id="kobo.395.1">, </span><em class="italic"><span class="koboSpan" id="kobo.396.1">Segmenting objects in images with PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.397.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.398.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.399.1">, </span><em class="italic"><span class="koboSpan" id="kobo.400.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.401.1">) with a repurposed mechanism to process sequences </span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.402.1">of data, called </span><strong class="bold"><span class="koboSpan" id="kobo.403.1">attention</span></strong><span class="koboSpan" id="kobo.404.1">. </span><span class="koboSpan" id="kobo.404.2">The largest improvement of this architecture is that it does not depend on processing the data sequentially. </span><span class="koboSpan" id="kobo.404.3">All the data can be processed in parallel, allowing for faster training and inference. </span><span class="koboSpan" id="kobo.404.4">This improvement allowed a very large amount of text, the </span><strong class="bold"><span class="koboSpan" id="kobo.405.1">corpus</span></strong><span class="koboSpan" id="kobo.406.1">, to </span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.407.1">be </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.408.1">processed, yielding </span><strong class="bold"><span class="koboSpan" id="kobo.409.1">Large Language Models</span></strong><span class="koboSpan" id="kobo.410.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.411.1">LLMs</span></strong><span class="koboSpan" id="kobo.412.1">) such </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.413.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.414.1">Bidirectional Encoder Representations from </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.415.1">Transformers</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.416.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.417.1">BERT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.419.1">The architecture of Transformers can be represented </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<span class="koboSpan" id="kobo.421.1"><img alt="Figure 6.7 – Transformer architecture" src="image/B16591_06_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.422.1">Figure 6.7 – Transformer architecture</span></p>
<p><span class="koboSpan" id="kobo.423.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.424.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.425.1">.7</span></em><span class="koboSpan" id="kobo.426.1">, we can </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.427.1">distinguish </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">several components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.429.1">Input and output preprocessing</span></strong><span class="koboSpan" id="kobo.430.1">: Embeddings </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.431.1">and positional encodings are computed before inputting the data into </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">the network.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.433.1">Encoder-decoder architecture</span></strong><span class="koboSpan" id="kobo.434.1">: The left </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.435.1">part corresponds to the encoder and the right part corresponds to the decoder. </span><span class="koboSpan" id="kobo.435.2">Feed-forward processing, residual connections, and normalization are parts of </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">this component.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.437.1">Attention heads</span></strong><span class="koboSpan" id="kobo.438.1">: The</span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.439.1"> sequence inputs the path from the encoder to the decoder and the sequence outputs are all processed via </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">this mechanism.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.441.1">Let’s see each of these components in </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">more detail.</span></span></p>
<h3><span class="koboSpan" id="kobo.443.1">Input and output preprocessing</span></h3>
<p><span class="koboSpan" id="kobo.444.1">In the original </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.445.1">paper, the inputs and outputs, before being fed into the network, are processed through </span><strong class="bold"><span class="koboSpan" id="kobo.446.1">learned embeddings</span></strong><span class="koboSpan" id="kobo.447.1">. </span><span class="koboSpan" id="kobo.447.2">These </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.448.1">embedding vectors typically have a size of </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">512 elements.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">Furthermore, as we will see afterward, these embeddings are passed through a </span><em class="italic"><span class="koboSpan" id="kobo.451.1">softmax</span></em><span class="koboSpan" id="kobo.452.1"> function, combining several pieces of information into one number, but also losing the positional information in the process. </span><span class="koboSpan" id="kobo.452.2">To maintain the positional information through the whole encoding and decoding, for both inputs and outputs, the input embeddings are added as a vector with information on position. </span><span class="koboSpan" id="kobo.452.3">These </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.453.1">are called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.454.1">positional encodings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.456.1">Encoder-decoder architecture</span></h3>
<p><span class="koboSpan" id="kobo.457.1">As noted in the original </span><em class="italic"><span class="koboSpan" id="kobo.458.1">Google Brain’s original Transformer</span></em><span class="koboSpan" id="kobo.459.1"> paper: </span><a href="https://arxiv.org/pdf/1706.03762.pdf "><span class="koboSpan" id="kobo.460.1">https://arxiv.org/pdf/1706.03762.pdf </span></a><span class="koboSpan" id="kobo.461.1">the encoder and decoder are composed of six identical layers (in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.462.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.463.1">.5</span></em><span class="koboSpan" id="kobo.464.1">, </span><em class="italic"><span class="koboSpan" id="kobo.465.1">N = 6</span></em><span class="koboSpan" id="kobo.466.1"> in the left diagram and right diagram, respectively). </span><span class="koboSpan" id="kobo.466.2">Each encoder layer has two components, a multi-head self-attention mechanism followed by a fully</span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.467.1"> connected feed-forward network. </span><span class="koboSpan" id="kobo.467.2">For each decoded layer, another attention component is added, a masked multi-head self-attention mechanism. </span><span class="koboSpan" id="kobo.467.3">Attention heads will be explained in the </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">following section.</span></span></p>
<p><span class="koboSpan" id="kobo.469.1">Residual connections are also added, similar to what we saw for ResNets in </span><em class="italic"><span class="koboSpan" id="kobo.470.1">Recipe 2</span></em><span class="koboSpan" id="kobo.471.1">, </span><em class="italic"><span class="koboSpan" id="kobo.472.1">Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.473.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.474.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.475.1">, </span><em class="italic"><span class="koboSpan" id="kobo.476.1">Analyzing Images with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.477.1">Computer Vision</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">This information together is normalized using layer normalization, which is similar to batch normalization, introduced in </span><em class="italic"><span class="koboSpan" id="kobo.480.1">Recipe 3</span></em><span class="koboSpan" id="kobo.481.1">, </span><em class="italic"><span class="koboSpan" id="kobo.482.1">Training for regression models</span></em><span class="koboSpan" id="kobo.483.1">, in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.484.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.485.1">, </span><em class="italic"><span class="koboSpan" id="kobo.486.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.487.1">. </span><span class="koboSpan" id="kobo.487.2">The most important difference is that, as described in the paper introducing layer normalization (</span><a href="https://arxiv.org/abs/1607.06450"><span class="koboSpan" id="kobo.488.1">https://arxiv.org/abs/1607.06450</span></a><span class="koboSpan" id="kobo.489.1">), with layer normalization, all the hidden units in a layer share</span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.490.1"> the same normalization terms, but different input data can have different normalization terms. </span><span class="koboSpan" id="kobo.490.2">Layer normalization has been proven to work better than batch normalization for </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">processing sequences.</span></span></p>
<p><span class="koboSpan" id="kobo.492.1">By combining all these layers across the different encoder and decoder steps (including the embeddings), the vectors will all have a dimension </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">of 512.</span></span></p>
<h3><span class="koboSpan" id="kobo.494.1">Attention heads</span></h3>
<p><span class="koboSpan" id="kobo.495.1">In Transformers, how</span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.496.1"> each word in a sequence is connected to each of the </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.497.1">words in another sequence is done via the attention mechanism. </span><span class="koboSpan" id="kobo.497.2">For example, if we have an input sequence with </span><em class="italic"><span class="koboSpan" id="kobo.498.1">N</span></em><span class="koboSpan" id="kobo.499.1"> words in English (</span><em class="italic"><span class="koboSpan" id="kobo.500.1">I love you very much</span></em><span class="koboSpan" id="kobo.501.1">) and its translation to French has </span><em class="italic"><span class="koboSpan" id="kobo.502.1">M</span></em><span class="koboSpan" id="kobo.503.1"> words (</span><em class="italic"><span class="koboSpan" id="kobo.504.1">Je t’aime beaucoup</span></em><span class="koboSpan" id="kobo.505.1">), the attention matrix of weights between these two sequences will have </span><em class="italic"><span class="koboSpan" id="kobo.506.1">NxM</span></em><span class="koboSpan" id="kobo.507.1"> dimensions. </span><span class="koboSpan" id="kobo.507.2">Connecting sequences using this mechanism has a strong advantage over a recurrent mechanism (such as the one used in RNNs), which is parallelization. </span><span class="koboSpan" id="kobo.507.3">Attention is a matrix operation and therefore can be </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">optimally parallelized.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.509.1">I</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.510.1">love</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.511.1">you</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.512.1">very</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.513.1">much</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.514.1">je</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.515.1">0.90</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.516.1">0.02</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.517.1">0.06</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.518.1">0.01</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.519.1">0.01</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.520.1">t’</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.521.1">0.11</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.522.1">0.01</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.523.1">0.80</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.524.1">0.03</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.525.1">0.05</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.526.1">aime</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.527.1">0.03</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.528.1">0.92</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.529.1">0.03</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.530.1">0.01</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.531.1">0.01</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.532.1">beaucoup</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.533.1">0.02</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.534.1">0.02</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.535.1">0.02</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.536.1">0.41</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.537.1">0.53</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.538.1">Figure 6.8 – Example of attention matrix</span></p>
<p><span class="koboSpan" id="kobo.539.1">In the Transformer paper, three of the matrices shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.540.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.541.1">.9 </span></em><span class="koboSpan" id="kobo.542.1">were introduced, </span><em class="italic"><span class="koboSpan" id="kobo.543.1">Query (Q), Key (K), and Value (V).</span></em><span class="koboSpan" id="kobo.544.1"> The following is an explanation of each of </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">these matrices:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.546.1">Query</span></strong><span class="koboSpan" id="kobo.547.1">: This is</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.548.1"> the input representation of each </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">input word</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.550.1">Key and value</span></strong><span class="koboSpan" id="kobo.551.1">: Similar </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.552.1">to a </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">hash map</span></strong><span class="koboSpan" id="kobo.554.1"> that maps keys into values, these matrices are used for indexing (key) and providing a representation (value) of the word (different from </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">the query)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.556.1">The combination </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.557.1">of the operations carried out by these three matrices is called the dot-product attention function and can be described </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<span class="koboSpan" id="kobo.559.1"><img alt="Figure 6.9 – Attention function" src="image/B16591_06_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.560.1">Figure 6.9 – Attention function</span></p>
<p><span class="koboSpan" id="kobo.561.1">When the output is a representation of the input sequence, this mechanism is called a </span><strong class="bold"><span class="koboSpan" id="kobo.562.1">self-attention head</span></strong><span class="koboSpan" id="kobo.563.1">. </span><span class="koboSpan" id="kobo.563.2">In</span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.564.1"> the architecture diagram shown</span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.565.1"> earlier in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.566.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.567.1">.7</span></em><span class="koboSpan" id="kobo.568.1">, the two attention mechanisms closer to the input sequence and the output sequence (lower part of the diagram) are self-attention mechanisms, as the sequence outputted by the function is the same as the sequence that is inputted. </span><span class="koboSpan" id="kobo.568.2">When this is not the case, such as in the attention mechanism that connects the encoder to the </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.569.1">decoder, this is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.570.1">cross-attention head</span></strong><span class="koboSpan" id="kobo.571.1">. </span><span class="koboSpan" id="kobo.571.2">The self-attention head for the output vectors is masked, meaning</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.572.1"> that only past information is available in the training of the network. </span><span class="koboSpan" id="kobo.572.2">This allows Transformer models to generate text from a limited input (auto-regressive models such as GPT-3 </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">or BLOOM).</span></span></p>
<p><span class="koboSpan" id="kobo.574.1">Instead of processing all the input data in parallel, in Google Brain’s Attention is All You Need original paper (</span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.575.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.576.1">), eight attention heads are used in parallel. </span><span class="koboSpan" id="kobo.576.2">As the output is expected to have the same dimensionality (512), each head works with a reduced vector (with a dimensionality of 64). </span><span class="koboSpan" id="kobo.576.3">This is described in the paper </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">as follows:</span></span></p>
<p class="aut or-quote"><span class="koboSpan" id="kobo.578.1">"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions."</span></p>
<p><span class="koboSpan" id="kobo.579.1">Reducing the dimensionality allows for the total computation cost to be similar to using a complete (full-dimensionality) </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">attention head.</span></span></p>
<h3><span class="koboSpan" id="kobo.581.1">Implementation in GluonNLP</span></h3>
<p><span class="koboSpan" id="kobo.582.1">GluonNLP </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.583.1">has its own implementation of a Transformer model and therefore, getting our encoder and decoder is straightforward, </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.585.1">
# Transformers MXNet Implementation Example
# Transformer with 6 layers (encoder and decoder), 2 parallel heads, and expecting inputs with 20 embeddings
transformer_encoder, transformer_decoder, _ = nlp.model.transformer.get_transformer_encoder_decoder(
    num_layers=6,
    num_heads=2,
    units=20)
transformer_encoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)
 transformer_decoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.586.1">Now, we can use the encoder to process the inputs; however, with Transformers, we can process the whole input at the </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">same time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.588.1">
encoded_inputs, _ = transformer_encoder(inputs[0])</span></pre> <p><span class="koboSpan" id="kobo.589.1">Large-scale Transformers are the current state of the art for most NLP tasks, such as topic modeling, sentiment analysis, or question answering, and the encoder and decoder architectures are also used separately for </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">different tasks.</span></span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.591.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.592.1">In this recipe, we have introduced several networks to work with NLP using MXNet, Gluon, </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">and GluonNLP:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.594.1">RNNs</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.595.1">LSTM</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.596.1">Transformers</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.597.1">We have reviewed</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.598.1"> how each of these architectures works and analyzed its advantages and disadvantages, as well as how each one has improved on the previous one, exploring concepts such as sequences, BPTT, memory cells, </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">and attention.</span></span></p>
<p><span class="koboSpan" id="kobo.600.1">In the following recipes, we will explore how to use these architectures to solve practical problems such as topic modeling, sentiment analysis, and </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">machine translation.</span></span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.602.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.603.1">Some of the concepts </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.604.1">explored in this recipe are too advanced to cover in detail in this book. </span><span class="koboSpan" id="kobo.604.2">I strongly suggest taking a look at the following references if you would like to get a </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">deeper understanding:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">TextCNNs (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.607.1">Paper):</span></strong></span><span class="No-Break"> </span><a href="https://aclanthology.org/D14-1181.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.608.1">https://aclanthology.org/D14-1181.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">RNNs (Intuitive explanation): </span></strong><a href="https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740"><span class="koboSpan" id="kobo.610.1">https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740</span></a><strong class="bold"> </strong></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.611.1">RNNs (Backpropagation Through </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.612.1">Time): </span></strong></span><a href="https://d2l.ai/chapter_recurrentneural-networks/bptt.html"><span class="No-Break"><span class="koboSpan" id="kobo.613.1">https://d2l.ai/chapter_recurrentneural-networks/bptt.html</span></span></a><a href="https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740 "/></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.614.1">Vanishing/exploding gradients research papers (Learning Long-Term Dependencies with Gradient Descent is </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.615.1">Difficult):</span></strong></span><span class="No-Break"> </span><a href="http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.616.1">http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.617.1">Vanishing/exploding gradients research papers: (On the difficulty of training Recurrent Neural </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.618.1">Networks): </span></strong></span><a href="https://arxiv.org/pdf/1211.5063.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.619.1">https://arxiv.org/pdf/1211.5063.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.620.1">Vanishing/exploding gradients research papers (The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs,and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.621.1">solutions): </span></strong></span><a href="https://arxiv.org/pdf/1712.05577.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.622.1">https://arxiv.org/pdf/1712.05577.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.623.1">LSTMs (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.624.1">Paper): </span></strong></span><a href="http://www.bioinf.jku.at/publications/older/2604.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.625.1">http://www.bioinf.jku.at/publications/older/2604.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.626.1">LSTMs (Intuitive </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.627.1">explanation): </span></strong></span><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><span class="No-Break"><span class="koboSpan" id="kobo.628.1">http://colah.github.io/posts/2015-08-</span></span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">Understanding-LSTMs/</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.630.1">Transformers (Original paper – Attention Is All You </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.631.1">Need): </span></strong></span><a href="https://arxiv.org/pdf/1706.03762.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.632.1">https://arxiv.org/pdf/1706.03762.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Transformers (Intuitive </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.634.1">explanation): </span></strong></span><a href="https://towardsdatascience.com/transformers-89034557de14"><span class="No-Break"><span class="koboSpan" id="kobo.635.1">https://towardsdatascience.com/transformers-89034557de14</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.636.1">Transformers (Layer </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.637.1">Normalization): </span></strong></span><a href="https://arxiv.org/pdf/1607.06450.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.638.1">https://arxiv.org/pdf/1607.06450.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.639.1">State-of-the-art models in NLP (GPT-3 </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.640.1">paper): </span></strong></span><a href="https://arxiv.org/pdf/2005.14165.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.641.1">https://arxiv.org/pdf/2005.14165.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">State-of-the-art models in NLP (BLOOM (open source </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.643.1">alternative): </span></strong></span><a href="https://huggingface.co/blog/bloom"><span class="No-Break"><span class="koboSpan" id="kobo.644.1">https://huggingface.co/blog/bloom</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.645.1">In this chapter, we</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.646.1"> are going to analyze in detail the following tasks: topic modeling, sentiment analysis, and text translation. </span><span class="koboSpan" id="kobo.646.2">However, MXNet GluonNLP Model Zoo contains lots of models pre-trained for a large number of tasks. </span><span class="koboSpan" id="kobo.646.3">You are encouraged to explore the different examples included </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">at </span></span><a href="https://nlp.gluon.ai/model_zoo/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.648.1">https://nlp.gluon.ai/model_zoo/index.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.649.1">.</span></span></p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.650.1">Classifying news highlights with topic modeling</span></h1>
<p><span class="koboSpan" id="kobo.651.1">In this recipe, we are</span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.652.1"> going to study one of the most interesting tasks in NLP, topic modeling. </span><span class="koboSpan" id="kobo.652.2">In this task, the user must find the number of topics given a set of documents. </span><span class="koboSpan" id="kobo.652.3">Sometimes, the topics (and the number of topics) are </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.653.1">known beforehand and the supervised learning techniques that we have seen in previous chapters can be applied. </span><span class="koboSpan" id="kobo.653.2">However, in a typical scenario, topic modeling datasets do not provide ground truth and are therefore unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">learning problems.</span></span></p>
<p><span class="koboSpan" id="kobo.655.1">To achieve this, we</span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.656.1"> will use a pre-trained model from GluonNLP Model Zoo and apply its word embeddings to feed a clustering algorithm, which will yield the clustered topics. </span><span class="koboSpan" id="kobo.656.2">We will apply this process to a new dataset: </span><em class="italic"><span class="koboSpan" id="kobo.657.1">1 Million </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.658.1">News Headlines</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">.</span></span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.660.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.661.1">As in previous chapters, in this recipe, we will be using a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.663.1">Furthermore, we will be working with text datasets. </span><span class="koboSpan" id="kobo.663.2">Therefore, we will revisit some concepts already seen in </span><em class="italic"><span class="koboSpan" id="kobo.664.1">Recipe 4</span></em><span class="koboSpan" id="kobo.665.1">, </span><em class="italic"><span class="koboSpan" id="kobo.666.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.667.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.668.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.669.1">, </span><em class="italic"><span class="koboSpan" id="kobo.670.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.671.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">.</span></span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.673.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.674.1">In this recipe, we will be carrying out the </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.676.1">Exploring the </span><em class="italic"><span class="koboSpan" id="kobo.677.1">1 Million News </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.678.1">Headlines</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.679.1"> dataset</span></span></li>
<li><span class="koboSpan" id="kobo.680.1">Applying </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">word embeddings</span></span></li>
<li><span class="koboSpan" id="kobo.682.1">Clustering the topics </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">using K-means</span></span></li>
<li><span class="koboSpan" id="kobo.684.1">Putting </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">everything together</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.686.1">Let’s go through each of these steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">following subsections.</span></span></p>
<h3><span class="koboSpan" id="kobo.688.1">Exploring the 1 Million News Headlines dataset</span></h3>
<p><span class="koboSpan" id="kobo.689.1">This </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.690.1">dataset is one of the most well-known datasets used for topic modeling. </span><span class="koboSpan" id="kobo.690.2">It contains 19 years of noteworthy news headlines published by the </span><strong class="bold"><span class="koboSpan" id="kobo.691.1">Australian Broadcasting Corporation</span></strong><span class="koboSpan" id="kobo.692.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.693.1">ABC</span></strong><span class="koboSpan" id="kobo.694.1">) website, from 2003 to 2021 (inclusive). </span><span class="koboSpan" id="kobo.694.2">The topic of each news headline is </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">not included.</span></span></p>
<p><span class="koboSpan" id="kobo.696.1">As</span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.697.1"> expected from a real-world dataset, the corpus contains a large number of words. </span><span class="koboSpan" id="kobo.697.2">Therefore, before going further, we will proceed to clean the data and follow the process described in </span><em class="italic"><span class="koboSpan" id="kobo.698.1">Recipe 4</span></em><span class="koboSpan" id="kobo.699.1">, </span><em class="italic"><span class="koboSpan" id="kobo.700.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.701.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.702.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.703.1">, </span><em class="italic"><span class="koboSpan" id="kobo.704.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.705.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.707.1">Tokenizing</span></span></li>
<li><span class="koboSpan" id="kobo.708.1">Removing </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">stop words</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.710.1">Stemming</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.711.1">Lemmatization</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.712.1">Furthermore, this</span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.713.1"> dataset contains more than 1 million headlines (1.2 million, actually). </span><span class="koboSpan" id="kobo.713.2">In order to be able to process them in a time-efficient manner, we will work with a reduced subset </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">of 5%:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.715.1">
reduced_number_headlines = int(0.05 * number_headlines)
 print(reduced_number_headlines)
62209</span></pre> <p><span class="koboSpan" id="kobo.716.1">If we analyze this subset to compute the number of words each headline has, we can plot the </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">following histogram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<span class="koboSpan" id="kobo.718.1"><img alt="Figure 6.10 – Distribution of headlines by number of words" src="image/B16591_06_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.719.1">Figure 6.10 – Distribution of headlines by number of words</span></p>
<p><span class="koboSpan" id="kobo.720.1">As can be </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.721.1">seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.722.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.723.1">.10</span></em><span class="koboSpan" id="kobo.724.1">, most of</span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.725.1"> the headlines have between 4 and </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">8 words.</span></span></p>
<h3><span class="koboSpan" id="kobo.727.1">Applying word embeddings</span></h3>
<p><span class="koboSpan" id="kobo.728.1">In </span><em class="italic"><span class="koboSpan" id="kobo.729.1">Recipe 4</span></em><span class="koboSpan" id="kobo.730.1">, </span><em class="italic"><span class="koboSpan" id="kobo.731.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.732.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.733.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.734.1">, </span><em class="italic"><span class="koboSpan" id="kobo.735.1">Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.736.1">, we introduced two embedding models from GluonNLP Model Zoo: Google’s </span><strong class="bold"><span class="koboSpan" id="kobo.737.1">word2vec</span></strong><span class="koboSpan" id="kobo.738.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">GloVe</span></strong><span class="koboSpan" id="kobo.740.1"> from Stanford University. </span><span class="koboSpan" id="kobo.740.2">For this use </span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.741.1">case, we are going to work with word2vec, because it was trained on a dataset called Google News, composed of 3 million words and phrases from a corpus of 100 billion words. </span><span class="koboSpan" id="kobo.741.2">As the corpus is composed of news information, this embedding model is very well suited for our </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">use case.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.743.1"><img alt="Figure 6.11 – 2D representation of word2vec embeddings" src="image/B16591_06_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.744.1">Figure 6.11 – 2D representation of word2vec embeddings</span></p>
<p><span class="koboSpan" id="kobo.745.1">By using </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.746.1">this model, each word is transformed into a vector with 300 components. </span><span class="koboSpan" id="kobo.746.2">However, for our application with headlines (full sentences), we are more interested in a representation of the complete headline and not just its independent words. </span><span class="koboSpan" id="kobo.746.3">A simple but effective method to accomplish this for our application is to compute the average vector of each </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">preprocessed word.</span></span></p>
<h3><span class="koboSpan" id="kobo.748.1">Clustering the topics using K-means</span></h3>
<p><span class="koboSpan" id="kobo.749.1">With our </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.750.1">headline embeddings ready, the last step to classify our headlines is to cluster them. </span><span class="koboSpan" id="kobo.750.2">There are many clustering algorithms, such as </span><strong class="bold"><span class="koboSpan" id="kobo.751.1">expectation maximization clustering</span></strong><span class="koboSpan" id="kobo.752.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.753.1">mean shift clustering</span></strong><span class="koboSpan" id="kobo.754.1">. </span><span class="koboSpan" id="kobo.754.2">However, for</span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.755.1"> our application, we will use my favorite </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.756.1">one, </span><strong class="bold"><span class="koboSpan" id="kobo.757.1">K-means</span></strong><span class="koboSpan" id="kobo.758.1">, which is</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.759.1"> implemented in a Python package we have already talked</span><a id="_idIndexMarker688"/> <span class="No-Break"><span class="koboSpan" id="kobo.760.1">about, </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.761.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.763.1"><img alt="Figure 6.12 – K-means visualization" src="image/B16591_06_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.764.1">Figure 6.12 – K-means visualization</span></p>
<p><span class="koboSpan" id="kobo.765.1">The</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.766.1"> intuitive idea behind K-means is that given a number of clusters, </span><em class="italic"><span class="koboSpan" id="kobo.767.1">K</span></em><span class="koboSpan" id="kobo.768.1">, it will assign the clusters’ centroids randomly, and add each newly seen vector to the closest centroid (assignment step). </span><span class="koboSpan" id="kobo.768.2">Then, it will compute the new centroid as the mean of the vectors that belong to the cluster (update step) and iterate. </span><span class="koboSpan" id="kobo.768.3">This process is repeated until there is no significant change in the centroids’ positions. </span><span class="koboSpan" id="kobo.768.4">Therefore, the full dataset can be iterated several times. </span><span class="koboSpan" id="kobo.768.5">For large datasets, other criteria can be added for convergence and </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">stopping criteria.</span></span></p>
<p><span class="koboSpan" id="kobo.770.1">One important </span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.771.1">drawback of K-means is that the number of clusters is an input parameter, and therefore, it requires some intuition or knowledge about the dataset. </span><span class="koboSpan" id="kobo.771.2">In practice, knowing this information beforehand is difficult. </span><span class="koboSpan" id="kobo.771.3">Therefore, I strongly recommend running the algorithm for several different values of </span><em class="italic"><span class="koboSpan" id="kobo.772.1">K</span></em><span class="koboSpan" id="kobo.773.1">. </span><span class="koboSpan" id="kobo.773.2">For our use case, the number of clusters chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">is 4.</span></span></p>
<h3><span class="koboSpan" id="kobo.775.1">Putting everything together</span></h3>
<p><span class="koboSpan" id="kobo.776.1">After </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.777.1">this three-step process (cleaning the data, embedding, and clustering), we are ready to analyze </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">some results.</span></span></p>
<p><span class="koboSpan" id="kobo.779.1">The most interesting output of topic modeling is identifying which headline topics are associated with each cluster. </span><span class="koboSpan" id="kobo.779.2">A helpful approach to doing this is to visualize the most important words for each cluster and come up with the connecting topic. </span><span class="koboSpan" id="kobo.779.3">Therefore, we can plot the following for the first identified cluster (</span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">cluster 0):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.781.1"><img alt="Figure 6.13 – Top 15 words in order of importance for the first cluster" src="image/B16591_06_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.782.1">Figure 6.13 – Top 15 words in order of importance for the first cluster</span></p>
<p><span class="koboSpan" id="kobo.783.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.784.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.785.1">.13</span></em><span class="koboSpan" id="kobo.786.1">, we can see that the most important words are </span><strong class="source-inline"><span class="koboSpan" id="kobo.787.1">win</span></strong><span class="koboSpan" id="kobo.788.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.789.1">world</span></strong><span class="koboSpan" id="kobo.790.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">cup</span></strong><span class="koboSpan" id="kobo.792.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.793.1">final</span></strong><span class="koboSpan" id="kobo.794.1">. </span><span class="koboSpan" id="kobo.794.2">All these words are sports-related, and therefore the topic for cluster 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">is </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.796.1">sports</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.798.1">We can put together the most important words </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">per cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.800.1">
Cluster 0 : ['win', 'world', 'cup', 'final', 'lead', 'set', 'hit', 'face', 'record', 'open', 'year', 'miss', 'test', 'new', 'day']
Cluster 1 : ['govt', 'iraq', 'urg', 'nsw', 'polic', 'continu', 'say', 'australia', 'vic', 'consid', 'qld', 'iraqi', 'forc', 'secur', 'sar']
Cluster 2 : ['plan', 'council', 'new', 'govt', 'fund', 'say', 'boost', 'group', 'water', 'concern', 'health', 'report', 'claim', 'seek', 'warn']
Cluster 3 : ['polic', 'man', 'kill', 'charg', 'court', 'murder', 'crash', 'death', 'attack', 'woman', 'face', 'arrest', 'probe', 'car', 'dead']</span></pre> <p><span class="koboSpan" id="kobo.801.1">From the</span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.802.1"> preceding information, we can conclude the topics for </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">each cluster:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">Cluster </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.805.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">: Sports</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.807.1">Cluster 1</span></strong><span class="koboSpan" id="kobo.808.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">Global affairs</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">Cluster </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">: Economy</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.813.1">Cluster 3</span></strong><span class="koboSpan" id="kobo.814.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">Crime/current happenings</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.816.1">With this information, now we can take any of the headlines that were not used during the clustering process and predict </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">their topic:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.818.1">
import random
random_index = random.randint(0, len(headlines_full)) random_headline = headlines_full["headline_text"][random_index] print(random_index, random_headline)</span></pre> <p><span class="koboSpan" id="kobo.819.1">This piece of code will yield a similar statement to </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.821.1">
771004 waratahs count cost with loss to cheetahs</span></pre> <p><span class="koboSpan" id="kobo.822.1">The preceding sentence is a clear reference to sports; therefore, we would expect the predicted cluster to </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">be </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.826.1">In order to predict the cluster group, we need to follow following steps: Define the function, run it and verify </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">the result.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.828.1">Let’s code a prediction function that will perform the necessary cleaning and preprocessing, embedding, and </span><a id="_idIndexMarker693"/><span class="No-Break"><span class="koboSpan" id="kobo.829.1">clustering steps:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.830.1">
def predict(cluster_km, headline):    """    This function predicts the cluster  of a headline via K-Means
    """
    # Cleaning
    headline_clean = clean_text(headline)
    headline_pre = process_words(headline_clean)
    # Embeddings
    bag_of_words_list = headline_pre.split()
    number_of_words = len(bag_of_words_list)
    # Process 1st word (to be able to concatenate)
    word_embeddings_array = w2v[bag_of_words_list[0]].reshape(1, embedding_features)
    # To manage headlines with just 1 meaningful word
    word_index = -1
    for word_index, word in enumerate(bag_of_words_list[1:]):
        word_embeddings = w2v[word].reshape(1, embedding_features)
        word_embeddings_array = mx.nd.concat(word_embeddings_array,
 word_embeddings, dim=0)
    assert(number_of_words == word_index + 2)
    average_embedding_headline_pre = mx.nd.mean(word_embeddings_array, axis=0).reshape(1, embedding_features)
    # Clustering
    selected_cluster = cluster_km.predict(average_embedding_headline_pre.asnumpy())
    return selected_cluster</span></pre></li> <li><span class="koboSpan" id="kobo.831.1">Let’s run </span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.832.1">this function with the </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.834.1">
predicted_cluster = predict(cluster_km, random_headline)
print(predicted_cluster)</span></pre></li> <li><span class="koboSpan" id="kobo.835.1">The output is the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">predicted cluster:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.837.1">
[0]</span></pre></li> </ol>
<p><span class="No-Break"><span class="koboSpan" id="kobo.838.1">Great work!</span></span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.839.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.840.1">In this recipe, we</span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.841.1"> have explored the NLP task known as topic modeling. </span><span class="koboSpan" id="kobo.841.2">This task tries to come up with the topics associated with a given set of documents. </span><span class="koboSpan" id="kobo.841.3">Typically, no answer is given (no ground truth), and so this task is better solved via unsupervised learning. </span><span class="koboSpan" id="kobo.841.4">We attempted to solve this task with ABC’s </span><em class="italic"><span class="koboSpan" id="kobo.842.1">1 Million News </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.843.1">Headlines</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.844.1"> dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.845.1">We followed a </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">three-step approach:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.847.1">Data processing </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">and cleaning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.849.1">Word embeddings</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.850.1">Clustering</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.851.1">For the first step, we followed a typical pipeline for any </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">NLP problem:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.853.1">Data cleaning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.854.1">Tokenizing</span></span></li>
<li><span class="koboSpan" id="kobo.855.1">Removing </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">stop words</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.857.1">Stemming</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.858.1">Lemmatization</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.859.1">For the</span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.860.1"> second step, we applied Google’s word2vec to compute embeddings for each word, and each headline embedding was computed as the average of the embeddings of each one of </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">its words.</span></span></p>
<p><span class="koboSpan" id="kobo.862.1">In the third step, we explored the unsupervised learning algorithm K-means, selected four clusters, and computed its centroids. </span><span class="koboSpan" id="kobo.862.2">We generated the following topic clusters sports, global affairs, economy and crime, and </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">current happenings.</span></span></p>
<p><span class="koboSpan" id="kobo.864.1">With this information, we selected a random headline and accurately predicted the topic it was </span><span class="No-Break"><span class="koboSpan" id="kobo.865.1">related to.</span></span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.866.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.867.1">Unsupervised learning is a very wide topic and an active field of research. </span><span class="koboSpan" id="kobo.867.2">To learn more, a good starting point is its Wikipedia </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">article: </span></span><a href="https://en.wikipedia.org/wiki/Unsupervised_learning"><span class="No-Break"><span class="koboSpan" id="kobo.869.1">https://en.wikipedia.org/wiki/Unsupervised_learning</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.870.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.871.1">Apart from the </span><em class="italic"><span class="koboSpan" id="kobo.872.1">1 Million News Headlines</span></em><span class="koboSpan" id="kobo.873.1"> dataset, another well-known reference dataset for topic modeling is the 20 Newsgroups dataset. </span><span class="koboSpan" id="kobo.873.2">I recommend working with the larger 6 Newsgroups choice as many Newsgroups had a lot of themes in common. </span><span class="koboSpan" id="kobo.873.3">More information can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">at </span></span><a href="http://qwone.com/~jason/20Newsgroups/"><span class="No-Break"><span class="koboSpan" id="kobo.875.1">http://qwone.com/~jason/20Newsgroups/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.876.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.877.1">One simplification we followed during the processing of embeddings is that the computation of our headline embedding was done by averaging each of the corresponding word embeddings. </span><span class="koboSpan" id="kobo.877.2">However, there are other approaches, known as document embeddings or sentence embeddings, with</span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.878.1"> models such as </span><strong class="bold"><span class="koboSpan" id="kobo.879.1">Doc2Vec</span></strong><span class="koboSpan" id="kobo.880.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.881.1">Sentence-BERT</span></strong><span class="koboSpan" id="kobo.882.1">, which</span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.883.1"> can be more useful for other applications. </span><span class="koboSpan" id="kobo.883.2">An analysis comparing some of these approaches can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">at </span></span><a href="https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"><span class="No-Break"><span class="koboSpan" id="kobo.885.1">https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.886.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.887.1">For a detailed explanation of how the K-means algorithm works, it is suggested you </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">review </span></span><a href="https://towardsdatascience.com/k-means-clustering-explained-4528df86a120"><span class="No-Break"><span class="koboSpan" id="kobo.889.1">https://towardsdatascience.com/k-means-clustering-explained-4528df86a120</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.890.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.891.1">When predicting the topic of a given headline, K-means is equivalent to another algorithm, called 1-nearest neighbor, which is the specific case of K-nearest neighbor with K = 1. </span><span class="koboSpan" id="kobo.891.2">More information regarding this supervised learning algorithm can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.892.1">at </span></span><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><span class="No-Break"><span class="koboSpan" id="kobo.893.1">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.894.1">.</span></span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.895.1">Analyzing sentiment in movie reviews</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.896.1">Sentiment analysis</span></strong><span class="koboSpan" id="kobo.897.1"> is the</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.898.1"> use of several different techniques, including NLP, to identify the emotional state associated with human-generated information, text in our case. </span><span class="koboSpan" id="kobo.898.2">In this recipe, we are going to perform sentiment analysis on real-world movie reviews. </span><span class="koboSpan" id="kobo.898.3">We will classify the reviews into two sentiments: positive </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">or negative.</span></span></p>
<p><span class="koboSpan" id="kobo.900.1">To achieve this, we will use several pre-trained models from GluonNLP Model Zoo, and apply its word embeddings to feed a classifier, which will output the predicted sentiment. </span><span class="koboSpan" id="kobo.900.2">We will apply this process to a new dataset: </span><strong class="bold"><span class="koboSpan" id="kobo.901.1">IMDb </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.902.1">Movie Reviews</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">.</span></span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.904.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.905.1">As in previous chapters, in this recipe, we will be using a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.907.1">Furthermore, we will be classifying text datasets. </span><span class="koboSpan" id="kobo.907.2">Therefore, we will revisit some concepts already seen in </span><em class="italic"><span class="koboSpan" id="kobo.908.1">Recipe 4</span></em><span class="koboSpan" id="kobo.909.1">, </span><em class="italic"><span class="koboSpan" id="kobo.910.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.911.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.912.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.913.1">, </span><em class="italic"><span class="koboSpan" id="kobo.914.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.915.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.917.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.918.1">In this recipe, we will be carrying out the </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.920.1">Exploring the </span><em class="italic"><span class="koboSpan" id="kobo.921.1">IMDb Movie </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">Reviews</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.923.1"> dataset</span></span></li>
<li><span class="koboSpan" id="kobo.924.1">Combining TextCNNs with </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">word embeddings</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.926.1">Introducing BERT</span></span></li>
<li><span class="koboSpan" id="kobo.927.1">Putting </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">everything together</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.929.1">Exploring the IMDb Movie Reviews dataset</span></h3>
<p><span class="koboSpan" id="kobo.930.1">This dataset was </span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.931.1">collected in 2011 by researchers from Stanford University. </span><span class="koboSpan" id="kobo.931.2">It is split into a training set and a test set, with each of the sets having 25,000 reviews. </span><span class="koboSpan" id="kobo.931.3">They included at most 30 reviews per movie. </span><span class="koboSpan" id="kobo.931.4">The sentiments are quite polar, with negative reviews with values between [1, 4] and positive reviews with values between [</span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">7, 10].</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.933.1"><img alt="Figure 6.14 – Histogram of movie reviews (imbalanced dataset)" src="image/B16591_06_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.934.1">Figure 6.14 – Histogram of movie reviews (imbalanced dataset)</span></p>
<p><span class="koboSpan" id="kobo.935.1">For our analysis, we will simplify the sentiment values to a binary sentiment classification task. </span><span class="koboSpan" id="kobo.935.2">Therefore, negative reviews are assigned a 0 value and positive reviews a 1 value. </span><span class="koboSpan" id="kobo.935.3">As a by-product of this simplification, the dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">becomes balanced.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<span class="koboSpan" id="kobo.937.1"><img alt="Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)" src="image/B16591_06_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.938.1">Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)</span></p>
<p><span class="koboSpan" id="kobo.939.1">Another</span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.940.1"> point to take into account, noted by the paper’s authors, is that as nuances in the language can contain information regarding the sentiment, the preprocessing of the reviews must not include usual stop words and stemming. </span><span class="koboSpan" id="kobo.940.2">We took this remark into account in </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">our preprocessing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.942.1">
def process_words_basic(
    text,
    lemmatizer = lemmatizer):
    words = nltk.tokenize.word_tokenize(text)
    filtered_words_post = []
    for word in words:
        if word.isalpha():
            filtered_words_post.append(lemmatizer.lemmatize(word))
    return filtered_words_post</span></pre> <p><span class="koboSpan" id="kobo.943.1">The file can be accessed </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">from </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py"><span class="No-Break"><span class="koboSpan" id="kobo.945.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.946.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">This function is applied to all samples in </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">the dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.949.1">Combining TextCNNs with word embeddings</span></h3>
<p><span class="koboSpan" id="kobo.950.1">After </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.951.1">processing the dataset, we are now ready to use it with any architecture of our choice. </span><span class="koboSpan" id="kobo.951.2">In the first recipe of this chapter, we showed how we can use CNNs with sequences. </span><span class="koboSpan" id="kobo.951.3">In order to provide language information, TextCNNs can use pre-trained token representations as input. </span><span class="koboSpan" id="kobo.951.4">For this recipe, we will use two word embeddings that will generate inputs for </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">our model:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.953.1">word2vec</span></strong><span class="koboSpan" id="kobo.954.1">: These</span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.955.1"> embeddings were introduced in </span><em class="italic"><span class="koboSpan" id="kobo.956.1">Recipe 4</span></em><span class="koboSpan" id="kobo.957.1">, </span><em class="italic"><span class="koboSpan" id="kobo.958.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.959.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.960.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.961.1">, </span><em class="italic"><span class="koboSpan" id="kobo.962.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.963.1">and DataLoader</span></em></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.964.1">BERT</span></strong><span class="koboSpan" id="kobo.965.1">: A </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.966.1">language model introduced by Google </span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">in 2018</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.968.1">Introducing BERT</span></h3>
<p><span class="koboSpan" id="kobo.969.1">RNNs and</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.970.1"> Transformers can work with large sequences of text. </span><span class="koboSpan" id="kobo.970.2">However, one of the largest disadvantages is that the data is processed in a single direction, from left to right. </span><span class="koboSpan" id="kobo.970.3">BERT provides a mechanism so that every word representation (token) can jointly use information from both directions, to the left and to the right of that </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">specific word.</span></span></p>
<p><span class="koboSpan" id="kobo.972.1">Another distinctive characteristic of BERT is that its attention mechanisms are solely based on self-attention layers; no cross-attention layers </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">are used.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<span class="koboSpan" id="kobo.974.1"><img alt="Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach with Transformers, such as GPT-1; left-to-right-only approach" src="image/B16591_06_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.975.1">Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach with Transformers, such as GPT-1; left-to-right-only approach</span></p>
<p><span class="koboSpan" id="kobo.976.1">BERT was </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.977.1">trained in an unsupervised manner, using two </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">task objectives:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.979.1">Masked language model</span></strong><span class="koboSpan" id="kobo.980.1">: The</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.981.1"> word under analysis is not shown, and therefore the model needs to understand its meaning from </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">context alone</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.983.1">Next-sentence prediction</span></strong><span class="koboSpan" id="kobo.984.1">: Given</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.985.1"> two sentences, this task predicts whether they have a connection (one could happen after the other in a longer text) or they are </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">not related</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.987.1">This training methodology, combined with the BERT architecture, proved to be very successful, beating state-of-the-art on 11 NLP tasks at the time the paper </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">was published.</span></span></p>
<p><span class="koboSpan" id="kobo.989.1">GluonNLP provides two pre-trained BERT models with the </span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">following features:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.991.1">BERT_12_768_12</span></strong><span class="koboSpan" id="kobo.992.1">: 12 layers, 768-dimensional embedding vectors, 12 self-attention heads. </span><span class="koboSpan" id="kobo.992.2">This model is </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.993.1">known as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.994.1">BERT base</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.996.1">BERT_24_1024_16</span></strong><span class="koboSpan" id="kobo.997.1">: 24 layers, 1,024-dimensional embedding vectors, 16 self-attention heads. </span><span class="koboSpan" id="kobo.997.2">This model is </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.998.1">known as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.999.1">BERT large</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1001.1">For our experiments, we will use the BERT base model, which can be easily loaded with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">code statement:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1003.1">
bert_model, vocab = nlp.model.get_model(
    'bert_12_768_12',
    dataset_name='book_corpus_wiki_en_uncased',
    use_classifier=False,
    use_decoder=False,
    ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1004.1">With the </span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.1005.1">preceding function, we can easily obtain a BERT model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1006.1">bert_model</span></strong><span class="koboSpan" id="kobo.1007.1">) and its vocabulary (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1008.1">vocab</span></strong><span class="koboSpan" id="kobo.1009.1">), based on the architecture of 12 layers, 768-dimensional embedding vectors, 12 self-attention heads, and a dataset from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">Wikipedia (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1011.1">book_corpus_wiki_en_uncased</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">).</span></span></p>
<h3><span class="koboSpan" id="kobo.1013.1">Putting everything together</span></h3>
<p><span class="koboSpan" id="kobo.1014.1">Let’s summarize </span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.1015.1">all the steps we have seen </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">so far.</span></span></p>
<p><span class="koboSpan" id="kobo.1017.1">Our </span><em class="italic"><span class="koboSpan" id="kobo.1018.1">IMDb Movie Reviews</span></em><span class="koboSpan" id="kobo.1019.1"> dataset is composed of 25,000 training samples and 25,000 test samples. </span><span class="koboSpan" id="kobo.1019.2">For cost and compute optimization purposes, we work with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">following datasets:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1021.1">Training set</span></strong><span class="koboSpan" id="kobo.1022.1">: 5,000 samples (from the original </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">training set)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1024.1">Validation set</span></strong><span class="koboSpan" id="kobo.1025.1">: 1,250 samples (from the original training set; no overlap with our 5,000-sample </span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">training set)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1027.1">Test set</span></strong><span class="koboSpan" id="kobo.1028.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">25,000 samples</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1030.1">We use two embedding models as inputs </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">to TextCNN:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1032.1">word2vec</span></strong><span class="koboSpan" id="kobo.1033.1">: Vectors with </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">300 components</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1035.1">BERT base</span></strong><span class="koboSpan" id="kobo.1036.1">: Vectors with </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">768 components</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1038.1">The TextCNN architecture is very similar for both approaches. </span><span class="koboSpan" id="kobo.1038.2">The kernel sizes for TextCNN are 3, 4, and 5, that is, analyzing 3, 4, and 5 words at the same time, and the number of channels is equivalent to the embedding components. </span><span class="koboSpan" id="kobo.1038.3">Furthermore, as we have a binary output (</span><em class="italic"><span class="koboSpan" id="kobo.1039.1">negative</span></em><span class="koboSpan" id="kobo.1040.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.1041.1">positive</span></em><span class="koboSpan" id="kobo.1042.1">), the classifier is a fully connected layer with one sigmoid output (the sigmoid activation function is included in the loss function due to </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">computational optimizations).</span></span></p>
<p><span class="koboSpan" id="kobo.1044.1">For the training, equivalent </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.1045.1">parameters are used for both </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">embedding models:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1047.1">Optimizer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">: Adam</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1049.1">Learning </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1050.1">rate</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">: 10</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.1052.1">-3</span></span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1053.1">Loss function</span></strong><span class="koboSpan" id="kobo.1054.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">Sigmoid cross-entropy</span></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1056.1">Epochs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">: 5</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1058.1">Batch </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1059.1">size</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">: 4</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1061.1">With these parameters, we have the following results using a word2vec </span><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">embedding model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<span class="koboSpan" id="kobo.1063.1"><img alt="Figure 6.17 – Training loss/validation loss and accuracy using word2vec" src="image/B16591_06_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1064.1">Figure 6.17 – Training loss/validation loss and accuracy using word2vec</span></p>
<p><span class="koboSpan" id="kobo.1065.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1066.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1067.1">.17</span></em><span class="koboSpan" id="kobo.1068.1">, we can see how the training improved with the epochs, yielding the best validation accuracy of 0.89 at the end of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">training process.</span></span></p>
<p><span class="koboSpan" id="kobo.1070.1">We can check the results qualitatively, selecting a movie review from our test set (unseen samples) and seeing the output of our sentiment analysis algorithm. </span><span class="koboSpan" id="kobo.1070.2">The example movie review is from </span><a href="https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset"><span class="koboSpan" id="kobo.1071.1">https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset</span></a><span class="koboSpan" id="kobo.1072.1"> with license CC </span><span class="No-Break"><span class="koboSpan" id="kobo.1073.1">BY 4.0):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1074.1">
I went and saw this movie last night after being coaxed to by a few friends of mine. </span><span class="koboSpan" id="kobo.1074.2">I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. </span><span class="koboSpan" id="kobo.1074.3">I was wrong. </span><span class="koboSpan" id="kobo.1074.4">Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. </span><span class="koboSpan" id="kobo.1074.5">The sign of a good movie is that it can toy with our emotions. </span><span class="koboSpan" id="kobo.1074.6">This one did exactly that. </span><span class="koboSpan" id="kobo.1074.7">The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. </span><span class="koboSpan" id="kobo.1074.8">While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. </span><span class="koboSpan" id="kobo.1074.9">This movie was great, and I suggest that you go see it before you judge.</span></pre> <p><span class="koboSpan" id="kobo.1075.1">We can format</span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.1076.1"> this input as embeddings expected by our </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">TextCNN network:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1078.1">
# Formatting single input as expected for the network
seq_output, _ = process_dataset_sample(test_dataset[0][0])
 seq_output_reshaped = mx.nd.array(seq_output, ctx=ctx).expand_dims(axis=0)</span></pre> <p><span class="koboSpan" id="kobo.1079.1">We can pass it through our best model </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">from training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1081.1">
# Retrieve best model from training
text_cnn.load_parameters(model_file_name)
review_sentiment = text_cnn(seq_output_rehaped)
# We can omit sigmoid processing, outputs of the network
# with positive values are positive reviews
if review_sentiment &gt;= 0:
    print(review_sentiment, "The review is positive")
else:
    print(review_sentiment, "The review is negative")</span></pre> <p><span class="koboSpan" id="kobo.1082.1">These commands yield the </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1084.1">
[[2.5862172]]
 &lt;NDArray 1x1 @gpu(0)&gt; The review is positive</span></pre> <p><span class="koboSpan" id="kobo.1085.1">As can be seen</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.1086.1"> in the preceding output, our algorithm has classified the review correctly </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">as </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1088.1">positive</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1090.1">However, for a more thorough and formal analysis, we can quantitatively process the full test set and compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">final accuracy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1092.1">
Final Test Accuracy: 0.87724</span></pre> <p><span class="koboSpan" id="kobo.1093.1">However, that number alone does not provide the full information about </span><em class="italic"><span class="koboSpan" id="kobo.1094.1">Type I</span></em><span class="koboSpan" id="kobo.1095.1"> and</span><em class="italic"><span class="koboSpan" id="kobo.1096.1"> Type II</span></em><span class="koboSpan" id="kobo.1097.1"> errors. </span><span class="koboSpan" id="kobo.1097.2">Therefore, we can also display the results as a confusion matrix (introduced in </span><em class="italic"><span class="koboSpan" id="kobo.1098.1">Recipe 4</span></em><span class="koboSpan" id="kobo.1099.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1100.1">Evaluating classification models</span></em><span class="koboSpan" id="kobo.1101.1">, in </span><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1102.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.1103.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1104.1">Solving </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1105.1">Classification Problems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<span class="koboSpan" id="kobo.1107.1"><img alt="Figure 6.18 – Confusion matrix using word2vec" src="image/B16591_06_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1108.1">Figure 6.18 – Confusion matrix using word2vec</span></p>
<p><span class="koboSpan" id="kobo.1109.1">When following the same approach, but this time using our BERT model for embeddings, we have the </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">following results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<span class="koboSpan" id="kobo.1111.1"><img alt="Figure 6.19 – Training loss/validation loss and accuracy using BERT" src="image/B16591_06_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1112.1">Figure 6.19 – Training loss/validation loss and accuracy using BERT</span></p>
<p><span class="koboSpan" id="kobo.1113.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1114.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1115.1">.19</span></em><span class="koboSpan" id="kobo.1116.1">, we can</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.1117.1"> see how the training improved with the epochs, yielding the best validation accuracy of 0.91 at the end of the training process. </span><span class="koboSpan" id="kobo.1117.2">This figure is higher than with word2vec, as expected, as BERT is able to build more contextual relationships between the words in </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">the reviews.</span></span></p>
<p><span class="koboSpan" id="kobo.1119.1">We can also pass the same review from the test set through our best model from training, using the same code statements as previously, obtaining the </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1121.1">
[[15.462966]]
 &lt;NDArray 1x1 @gpu(0)&gt; The review is positive</span></pre> <p><span class="koboSpan" id="kobo.1122.1">This experiment produces the same result (positive review) as the previous experiment </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1124.1">word2vec</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1126.1">For the test set accuracy, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1128.1">
Final Test Accuracy: 0.90848</span></pre> <p><span class="koboSpan" id="kobo.1129.1">Compared to the word2vec result, BERT provides a 3% </span><span class="No-Break"><span class="koboSpan" id="kobo.1130.1">higher accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.1131.1">The confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<span class="koboSpan" id="kobo.1133.1"><img alt="Figure 6.20 – Confusion matrix using BERT" src="image/B16591_06_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1134.1">Figure 6.20 – Confusion matrix using BERT</span></p>
<p><span class="koboSpan" id="kobo.1135.1">As we can </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.1136.1">see from these results, BERT clearly outperforms word2vec. </span><span class="koboSpan" id="kobo.1136.2">Another important advantage to mention is that, as Transformers allow for better parallelization, the training process is </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">also faster.</span></span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.1138.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1139.1">In this recipe, we </span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.1140.1">tackled the sentiment analysis problem. </span><span class="koboSpan" id="kobo.1140.2">We analyzed an architecture, TextCNN, to solve this task and explored how it can be applied to different word </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">embedding models.</span></span></p>
<p><span class="koboSpan" id="kobo.1142.1">We explored a new dataset, </span><em class="italic"><span class="koboSpan" id="kobo.1143.1">IMDb Movie Reviews</span></em><span class="koboSpan" id="kobo.1144.1">, and made adequate transformations so that we could work with the dataset in a constrained computation environment and simplify it to a binary </span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">classification task.</span></span></p>
<p><span class="koboSpan" id="kobo.1146.1">We introduced BERT, a new word embedding model introduced by Google in 2018, and compared it to a previously explored model, word2vec, understanding its differences, advantages, and constraints. </span><span class="koboSpan" id="kobo.1146.2">We understood the two most important advantages of BERT: using bidirectional information for each word and masking each word in training so that the information about each word is purely based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1147.1">its context.</span></span></p>
<p><span class="koboSpan" id="kobo.1148.1">We ran experiments to compare these two word embedding approaches and saw that despite both approaches solving the problem rather well (the test accuracy for word2vec and BERT being 88% and 91%, respectively), BERT </span><span class="No-Break"><span class="koboSpan" id="kobo.1149.1">performed better.</span></span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.1150.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1151.1">Sentiment analysis</span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.1152.1"> is a well-researched task in the literature. </span><span class="koboSpan" id="kobo.1152.2">To learn more, it is recommended to read </span><span class="No-Break"><span class="koboSpan" id="kobo.1153.1">this: </span></span><a href="https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/"><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1156.1">The paper introducing the </span><em class="italic"><span class="koboSpan" id="kobo.1157.1">IMDb Movie Reviews</span></em><span class="koboSpan" id="kobo.1158.1"> dataset, which also proposed a model for sentiment analysis, can be found here: </span><em class="italic"><span class="koboSpan" id="kobo.1159.1">Learning Word Vectors for Sentiment </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1160.1">Analysis</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">, </span></span><a href="https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1163.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1164.1">BERT was introduced in the paper </span><a href="https://arxiv.org/pdf/1810.04805.pdf"><span class="koboSpan" id="kobo.1165.1">https://arxiv.org/pdf/1810.04805.pdf</span></a><span class="koboSpan" id="kobo.1166.1">. </span><span class="koboSpan" id="kobo.1166.2">However, a more intuitive explanation can be found here: </span><a href="https://huggingface.co/blog/bert-101"><span class="koboSpan" id="kobo.1167.1">https://huggingface.co/blog/bert-101</span></a><span class="koboSpan" id="kobo.1168.1">. </span><span class="koboSpan" id="kobo.1168.2">Reading the preceding article is strongly encouraged due to its analysis of how data can embed biases in </span><span class="No-Break"><span class="koboSpan" id="kobo.1169.1">our models.</span></span></p>
<p><span class="koboSpan" id="kobo.1170.1">BERT is very powerful and can be complemented with even better language models, such as RoBERTa (improved version) or DistilBERT (smaller model with similar performance), and lots of models fine-tuned for specific tasks. </span><span class="koboSpan" id="kobo.1170.2">A list of the pre-trained models available in MXNet GluonNLP can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">at </span></span><a href="https://nlp.gluon.ai/model_zoo/bert/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.1172.1">https://nlp.gluon.ai/model_zoo/bert/index.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">.</span></span></p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.1174.1">Translating text from Vietnamese to English</span></h1>
<p><span class="koboSpan" id="kobo.1175.1">Translating</span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.1176.1"> text automatically (machine translation) has been a very interesting and useful use case for NLP since its inception, as breaking language barriers has lots of applications, including chatbots and automated subtitles in </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">multiple languages.</span></span></p>
<p><span class="koboSpan" id="kobo.1178.1">Before </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.1179.1">deep learning, machine translation was typically approached as a statistical problem. </span><span class="koboSpan" id="kobo.1179.2">Even after deep learning, it was not until Google, in 2016, applied deep learning to machine translation that the area of </span><strong class="bold"><span class="koboSpan" id="kobo.1180.1">Neural Machine Translation</span></strong><span class="koboSpan" id="kobo.1181.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1182.1">NMT</span></strong><span class="koboSpan" id="kobo.1183.1">) was </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.1184.1">born. </span><span class="koboSpan" id="kobo.1184.2">This model </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.1185.1">set the foundation for translating tasks now available in LLMs, such </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.1186.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.1187.1">OpenAI GPT</span></strong><span class="koboSpan" id="kobo.1188.1"> and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1189.1">Google Bard</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1191.1">In this recipe, we will apply these techniques to translate sentences from Vietnamese to English, using pre-trained models from GluonNLP </span><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">Model Zoo.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.1193.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.1194.1">As in</span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.1195.1"> previous chapters, in this recipe, we will be using a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.1197.1">Furthermore, we </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.1198.1">will be classifying text datasets. </span><span class="koboSpan" id="kobo.1198.2">Therefore, we will revisit some concepts already seen in </span><em class="italic"><span class="koboSpan" id="kobo.1199.1">Recipe 4</span></em><span class="koboSpan" id="kobo.1200.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1201.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></em><span class="koboSpan" id="kobo.1202.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1203.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1204.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1205.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1206.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1207.1">.</span></span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.1208.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.1209.1">In this recipe, we will be carrying out the </span><span class="No-Break"><span class="koboSpan" id="kobo.1210.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1211.1">Exploring the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1212.1">IWSLT2015</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1213.1"> dataset</span></span></li>
<li><span class="koboSpan" id="kobo.1214.1">Evaluating machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1215.1">translators (BLEU)</span></span></li>
<li><span class="koboSpan" id="kobo.1216.1">Introducing the GNMT model and exploring Transformers for </span><span class="No-Break"><span class="koboSpan" id="kobo.1217.1">this task</span></span></li>
<li><span class="koboSpan" id="kobo.1218.1">Putting </span><span class="No-Break"><span class="koboSpan" id="kobo.1219.1">everything together</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1220.1">Let’s look at these steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">detail next.</span></span></p>
<h3><span class="koboSpan" id="kobo.1222.1">Exploring the IWSLT2015 dataset</span></h3>
<p><span class="koboSpan" id="kobo.1223.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1224.1">International Workshop on Spoken Language Translation</span></strong><span class="koboSpan" id="kobo.1225.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1226.1">IWSLT</span></strong><span class="koboSpan" id="kobo.1227.1">) is a yearly</span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.1228.1"> scientific workshop</span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.1229.1"> focused on all forms of translation (not necessarily machine translation). </span><span class="koboSpan" id="kobo.1229.2">They have generated several very important datasets and benchmarks that have helped the field of machine translation evolve. </span><span class="koboSpan" id="kobo.1229.3">In 2015, an English-Vietnamese dataset was published, composed of a training set of 130,000+ sentence pairs and validation/test sets with 1,000+ sentence pairs. </span><span class="koboSpan" id="kobo.1229.4">This dataset is publicly available </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.1230.1">with MXNet GluonNLP and can be easily retrieved, </span><span class="No-Break"><span class="koboSpan" id="kobo.1231.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1232.1">
# IWSLT2015 Dataset (Train, Validation and Test)
# Dataset Parameters
src_lang, tgt_lang = "vi", "en"
src_max_len, tgt_max_len = 50, 50
iwslt_train_text = nlp.data.IWSLT2015("train",
                                      src_lang=src_lang,
                                      tgt_lang=tgt_lang)
iwslt_val_text   = nlp.data.IWSLT2015("val",
                                      src_lang=src_lang,
                                      tgt_lang=tgt_lang)
iwslt_test_text  = nlp.data.IWSLT2015("test",
                                      src_lang=src_lang,
                                      tgt_lang=tgt_lang)
iwslt_src_vocab = iwslt_train_text.src_vocab
iwslt_tgt_vocab = iwslt_train_text.tgt_vocab</span></pre> <p><span class="koboSpan" id="kobo.1233.1">This version</span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.1234.1"> of the dataset provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.1235.1">following data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1236.1">
Length of train set: 133166
Length of val set  : 1553
Length of test set : 1268</span></pre> <p><span class="koboSpan" id="kobo.1237.1">The preprocessing is similar to previous pipelines we have already seen, and includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1239.1">Sentence clipping (to define </span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1">maximum values)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">Tokenizing</span></span></li>
<li><span class="koboSpan" id="kobo.1242.1">Adding </span><strong class="bold"><span class="koboSpan" id="kobo.1243.1">End-of-Sentence</span></strong><span class="koboSpan" id="kobo.1244.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1245.1">EOS</span></strong><span class="koboSpan" id="kobo.1246.1">) tokens</span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.1247.1"> to the source sentence (Vietnamese) and </span><strong class="bold"><span class="koboSpan" id="kobo.1248.1">Beginning-of-Sentence</span></strong><span class="koboSpan" id="kobo.1249.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1250.1">BOS</span></strong><span class="koboSpan" id="kobo.1251.1">) and EOS tokens</span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.1252.1"> to the target </span><span class="No-Break"><span class="koboSpan" id="kobo.1253.1">sentence (English)</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1254.1">Furthermore, to optimize training, a bucketing process is applied, where sentences are grouped by </span><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">similar length:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<span class="koboSpan" id="kobo.1256.1"><img alt="Figure 6.21 – Fixed bucket sampler" src="image/B16591_06_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1257.1">Figure 6.21 – Fixed bucket sampler</span></p>
<p><span class="koboSpan" id="kobo.1258.1">The example</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.1259.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1260.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1261.1">.21</span></em><span class="koboSpan" id="kobo.1262.1"> shows this strategy with 10 buckets, yielding a minimal amount of padding (required so that all sentences in 1 batch can be processed in parallel). </span><span class="koboSpan" id="kobo.1262.2">The size of the buckets is also exponentially increased. </span><span class="koboSpan" id="kobo.1262.3">Use MXNet GluonNLP, </span><span class="No-Break"><span class="koboSpan" id="kobo.1263.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1264.1">
# Bucket scheme
bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)</span></pre> <p><span class="koboSpan" id="kobo.1265.1">In the preceding example, the size (width) of each bucket is augmented by 20% (</span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">1.2 increments).</span></span></p>
<h3><span class="koboSpan" id="kobo.1267.1">Evaluating machine translators (BLEU)</span></h3>
<p><span class="koboSpan" id="kobo.1268.1">Evaluating how successful</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.1269.1"> machine translation systems are is very difficult. </span><span class="koboSpan" id="kobo.1269.2">For example, using a single number to </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.1270.1">measure the quality of a translation is inherently subjective. </span><span class="koboSpan" id="kobo.1270.2">For our use case, we will work with a widely used metric</span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.1271.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.1272.1">BiLingual Evaluation </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1273.1">Understudy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1275.1">BLEU</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.1277.1">With BLEU, several reference translations are provided, and it tries to measure how close the automated translation is to its reference translations. </span><span class="koboSpan" id="kobo.1277.2">To do so, it compares the different N-grams (size 1 to 4) of the automated translation to the N-grams of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">reference translations.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer144">
<span class="koboSpan" id="kobo.1279.1"><img alt="Figure 6.22 – BLEU metric" src="image/B16591_06_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1280.1">Figure 6.22 – BLEU metric</span></p>
<p><span class="koboSpan" id="kobo.1281.1">As can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1282.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1283.1">.22</span></em><span class="koboSpan" id="kobo.1284.1">, BLEU tries to minimize the subjectivity associated </span><span class="No-Break"><span class="koboSpan" id="kobo.1285.1">with translations.</span></span></p>
<p><span class="koboSpan" id="kobo.1286.1">Another</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.1287.1"> metric</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.1288.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.1289.1">Perplexity</span></strong><span class="koboSpan" id="kobo.1290.1">, which defines approximately how “surprised” the model is to see a translated word. </span><span class="koboSpan" id="kobo.1290.2">When the model is not surprised, it means it is performing well; therefore, for Perplexity, a lower value is better. </span><span class="koboSpan" id="kobo.1290.3">Computing Perplexity is much faster than BLEU, and so it is used more as a checking metric during per-batch computations in training, leaving BLEU for </span><span class="No-Break"><span class="koboSpan" id="kobo.1291.1">per-epoch computations.</span></span></p>
<h3><span class="koboSpan" id="kobo.1292.1">Introducing the GNMT model and exploring Transformers for this task</span></h3>
<p><span class="koboSpan" id="kobo.1293.1">As </span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.1294.1">mentioned, the largest improvement in the field of machine translation was introduced by Google in 2016 with </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.1295.1">their </span><strong class="bold"><span class="koboSpan" id="kobo.1296.1">Google Neural Machine Translator</span></strong><span class="koboSpan" id="kobo.1297.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1298.1">GNMT</span></strong><span class="koboSpan" id="kobo.1299.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">model (</span></span><a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html"><span class="No-Break"><span class="koboSpan" id="kobo.1301.1">https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<span class="koboSpan" id="kobo.1303.1"><img alt="Figure 6.23 – GNMT architecture" src="image/B16591_06_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1304.1">Figure 6.23 – GNMT architecture</span></p>
<p><span class="koboSpan" id="kobo.1305.1">GNMT is a</span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.1306.1"> pioneer of transformers </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.1307.1">and makes use of attention with an encoder-decoder architecture as well. </span><span class="koboSpan" id="kobo.1307.2">The encoder and the decoder are LSTM RNNs, with eight layers in the encoder and another eight layers in the decoder. </span><span class="koboSpan" id="kobo.1307.3">The attention mechanism implemented in the model is a </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">cross-attention layer.</span></span></p>
<p><span class="koboSpan" id="kobo.1309.1">At the end of the model, a beam-search sampler is chained to generate new translations to maximize the trained conditional probability of the translations. </span><span class="koboSpan" id="kobo.1309.2">As in the paper, in our implementation, the scoring function includes a length penalty so that all words in the translation </span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">are covered.</span></span></p>
<p><span class="koboSpan" id="kobo.1311.1">We’ll compare </span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.1312.1">GNMT with Transformers</span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.1313.1"> for our use case of Vietnamese-to-English machine translation. </span><span class="koboSpan" id="kobo.1313.2">For our application, these are the most important parameters for </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">each model:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1315.1">GNMT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">:</span></span><ul><li><span class="koboSpan" id="kobo.1317.1">Number of layers for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">encoder: 2</span></span></li><li><span class="koboSpan" id="kobo.1319.1">Number of layers for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">decoder: 2</span></span></li><li><span class="koboSpan" id="kobo.1321.1">Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">units: 512</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1323.1">Transformer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">:</span></span><ul><li><span class="koboSpan" id="kobo.1325.1">Number of layers for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1326.1">encoder: 4</span></span></li><li><span class="koboSpan" id="kobo.1327.1">Number of layers for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1328.1">decoder: 4</span></span></li><li><span class="koboSpan" id="kobo.1329.1">Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">units: 512</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.1331.1">In the next sections, we will compare both architectures for the same </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">translation task.</span></span></p>
<h3><span class="koboSpan" id="kobo.1333.1">Putting everything together</span></h3>
<p><span class="koboSpan" id="kobo.1334.1">Let’s</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.1335.1"> summarize all the steps we have seen </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">so far.</span></span></p>
<p><span class="koboSpan" id="kobo.1337.1">Our </span><em class="italic"><span class="koboSpan" id="kobo.1338.1">IWSLT2015</span></em><span class="koboSpan" id="kobo.1339.1"> Vietnamese-to-English dataset is composed of 133,000+ training samples and 1,000+ validation/test samples. </span><span class="koboSpan" id="kobo.1339.2">We’ll work with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1340.1">complete datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1341.1">We’ll use two models for our machine translation </span><span class="No-Break"><span class="koboSpan" id="kobo.1342.1">use case:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1343.1">GNMT</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">Transformer</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1345.1">For the training, equivalent parameters are used for </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">both architectures:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1347.1">Optimizer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1348.1">: Adam</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1349.1">Learning rate</span></strong><span class="koboSpan" id="kobo.1350.1">: 10</span><span class="superscript"><span class="koboSpan" id="kobo.1351.1">-3</span></span><span class="koboSpan" id="kobo.1352.1">, with a learning rate schedule of a step decay, halving the learning rate every epoch after </span><span class="No-Break"><span class="koboSpan" id="kobo.1353.1">half training</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1354.1">Loss function</span></strong><span class="koboSpan" id="kobo.1355.1">: Masked softmax cross-entropy, similar to the cross-entropy loss functions we have already explored with the added feature that when predictions are longer than their valid length, the redundant words are </span><span class="No-Break"><span class="koboSpan" id="kobo.1356.1">masked out</span></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1357.1">Epochs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">: 12</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1359.1">Batch </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1360.1">size</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1361.1">: 128</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1362.1">With these parameters, we have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1363.1">GNMT model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<span class="koboSpan" id="kobo.1364.1"><img alt="Figure 6.24 – GNMT training evolution (training loss and validation loss, Perplexity, and BLEU)" src="image/B16591_06_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1365.1">Figure 6.24 – GNMT training evolution (training loss and validation loss, Perplexity, and BLEU)</span></p>
<p><span class="koboSpan" id="kobo.1366.1">Furthermore, for</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.1367.1"> the best iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1368.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1369.1">
Best model test Loss=2.3807, test ppl=10.8130, test bleu=23.15</span></pre> <p><span class="koboSpan" id="kobo.1370.1">Current state-of-the-art models can yield above 30 points in their BLEU score, but this score is certainly </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">very high.</span></span></p>
<p><span class="koboSpan" id="kobo.1372.1">Qualitatively, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.1372.2">In our case, we chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.1373.1">I like to read books</span></strong><span class="koboSpan" id="kobo.1374.1">, and this can be verified with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1375.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1376.1">
print("Qualitative Evaluation: Translating from Vietnamese to English")
expected_tgt_seq = "I like to read books."
 </span><span class="koboSpan" id="kobo.1376.2">print("Expected translation:")
 print(expected_tgt_seq)
# From Google Translate
src_seq = "Tôi thích đ</span><span class="koboSpan" id="kobo.1377.1"><img alt="" role="presentation" src="image/01.png"/></span><span class="koboSpan" id="kobo.1378.1">c sách k</span><span class="koboSpan" id="kobo.1379.1"><img alt="" role="presentation" src="image/02.png"/></span><span class="koboSpan" id="kobo.1380.1"> thu</span><span class="koboSpan" id="kobo.1381.1"><img alt="" role="presentation" src="image/03.png"/></span><span class="koboSpan" id="kobo.1382.1">t."
 </span><span class="koboSpan" id="kobo.1382.2">print("In Vietnamese (from Google Translate):")
 print(src_seq)
translation_out = nmt.utils.translate(
    gnmt_translator,
    src_seq,
    iwslt_src_vocab,
    iwslt_tgt_vocab,
    ctx)
print("The English translation is:")
 print(" ".join(translation_out[0]))</span></pre> <p><span class="koboSpan" id="kobo.1383.1">These code</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.1384.1"> statements will give us the </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1386.1">
Qualitative Evaluation: Translating from Vietnamese to English
Expected translation:
 I like to read books.
 </span><span class="koboSpan" id="kobo.1386.2">In Vietnamese (from Google Translate):
 Tôi thích đ</span><span class="koboSpan" id="kobo.1387.1"><img alt="" role="presentation" src="image/01.png"/></span><span class="koboSpan" id="kobo.1388.1">c sách k</span><span class="koboSpan" id="kobo.1389.1"><img alt="" role="presentation" src="image/02.png"/></span><span class="koboSpan" id="kobo.1390.1"> thu</span><span class="koboSpan" id="kobo.1391.1"><img alt="" role="presentation" src="image/03.png"/></span><span class="koboSpan" id="kobo.1392.1">t.
 </span><span class="koboSpan" id="kobo.1392.2">The English translation is:
 I like to read books .</span></pre> <p><span class="koboSpan" id="kobo.1393.1">As can be seen from the results, the text has been correctly translated from Vietnamese </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">to English.</span></span></p>
<p><span class="koboSpan" id="kobo.1395.1">Now, we are going to repeat the same exercises with our Transformer model. </span><span class="koboSpan" id="kobo.1395.2">With the parameters defined previously for its training, we have the following evolution in </span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">the training:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<span class="koboSpan" id="kobo.1397.1"><img alt="Figure 6.25 – Transformer training evolution (training loss and validation loss, Perplexity, and BLEU)" src="image/B16591_06_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1398.1">Figure 6.25 – Transformer training evolution (training loss and validation loss, Perplexity, and BLEU)</span></p>
<p><span class="koboSpan" id="kobo.1399.1">Furthermore, for </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.1400.1">the best iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1401.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1402.1">
Best model test Loss=2.1171, test ppl=8.3067, test bleu=24.16</span></pre> <p><span class="koboSpan" id="kobo.1403.1">As we can see, the Transformer architecture yields around ~0.015 higher BLEU </span><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">score points.</span></span></p>
<p><span class="koboSpan" id="kobo.1405.1">As done for GNMT, we can also check how well our model is performing qualitatively with the same sentence example and code. </span><span class="koboSpan" id="kobo.1405.2">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1407.1">
Qualitative Evaluation: Translating from Vietnamese to English
Expected translation:
 I like to read books.
 </span><span class="koboSpan" id="kobo.1407.2">In Vietnamese (from Google Translate):
 Tôi thích đ</span><span class="koboSpan" id="kobo.1408.1"><img alt="" role="presentation" src="image/01.png"/></span><span class="koboSpan" id="kobo.1409.1">c sách k</span><span class="koboSpan" id="kobo.1410.1"><img alt="" role="presentation" src="image/02.png"/></span><span class="koboSpan" id="kobo.1411.1"> thu</span><span class="koboSpan" id="kobo.1412.1"><img alt="" role="presentation" src="image/03.png"/></span><span class="koboSpan" id="kobo.1413.1">t.
 </span><span class="koboSpan" id="kobo.1413.2">The English translation is:
 I like to read books .</span></pre> <p><span class="koboSpan" id="kobo.1414.1">As can be </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.1415.1">seen from the results, the text has been correctly translated from Vietnamese </span><span class="No-Break"><span class="koboSpan" id="kobo.1416.1">to English.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.1417.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1418.1">In this </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.1419.1">recipe, we solved one of the most useful tasks in NLP, machine translation. </span><span class="koboSpan" id="kobo.1419.2">We introduced a new architecture, GNMT, the precursor of the Transformer, and compared </span><span class="No-Break"><span class="koboSpan" id="kobo.1420.1">both models.</span></span></p>
<p><span class="koboSpan" id="kobo.1421.1">We explored a new dataset, </span><em class="italic"><span class="koboSpan" id="kobo.1422.1">IWSLT2015</span></em><span class="koboSpan" id="kobo.1423.1">, which, among other language pairs, supports translations between Vietnamese and English. </span><span class="koboSpan" id="kobo.1423.2">We introduced the Perplexity and BLEU metrics, which are widely used to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">translation models.</span></span></p>
<p><span class="koboSpan" id="kobo.1425.1">We ran experiments to compare these two models and saw that, despite both approaches solving the problem rather well (the BLEU scores in the test set for GNMT and the Transformer were 23.15 and 24.34, respectively), the Transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">performed better.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.1427.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1428.1">Machine translation is a difficult problem to tackle. </span><span class="koboSpan" id="kobo.1428.2">Two very good official guides from MXNet GluonNLP where this problem is solved are </span><span class="No-Break"><span class="koboSpan" id="kobo.1429.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1430.1">Official machine translation tutorials of MXNet </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1431.1">GluonNLP: </span></strong></span><a href="https://nlp.gluon.ai/examples/machine_translation/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.1432.1">https://nlp.gluon.ai/examples/machine_translation/index.html</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1433.1">AMLC19-GluonNLP:</span></strong></span><span class="No-Break"> </span><a href="https://github.com/eric-haibin-lin/AMLC19-GluonNLP"><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">https://github.com/eric-haibin-lin/AMLC19-GluonNLP</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1435.1">This recipe used code from the previous references. </span><span class="koboSpan" id="kobo.1435.2">I would like to kindly thank </span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">the contributors.</span></span></p>
<p><span class="koboSpan" id="kobo.1437.1">The IWSLT conference takes place every year. </span><span class="koboSpan" id="kobo.1437.2">For more info, please visit their official </span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1">site: </span></span><a href="https://iwslt.org/"><span class="No-Break"><span class="koboSpan" id="kobo.1439.1">https://iwslt.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1440.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1441.1">We introduced two new metrics for translation problems, Perplexity and BLEU. </span><span class="koboSpan" id="kobo.1441.2">Work is actively being carried out to improve these metrics, with new metrics being developed recently, such </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.1442.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.1443.1">SacreBLEU</span></strong><span class="koboSpan" id="kobo.1444.1">. </span><span class="koboSpan" id="kobo.1444.2">Some references that tackle this very important topic are </span><span class="No-Break"><span class="koboSpan" id="kobo.1445.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1446.1">Perplexity</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1447.1">: </span></span><a href="http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/"><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1449.1">BLEU</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">: </span></span><a href="https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1"><span class="No-Break"><span class="koboSpan" id="kobo.1451.1">https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1452.1">Improving BLEU with </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1453.1">SacreBLEU</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1454.1">: </span></span><a href="https://aclanthology.org/W18-6319.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1455.1">https://aclanthology.org/W18-6319.pdf</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1456.1">We also discussed GNMT for the first time, which was one of the first real-world systems that used deep learning for translation (NMT), developed by Google in 2016. </span><span class="koboSpan" id="kobo.1456.2">The blog post where this was announced is worth </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">reading: </span></span><a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html"><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1459.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1460.1">There are many translation models that have used the </span><em class="italic"><span class="koboSpan" id="kobo.1461.1">IWSLT2015</span></em><span class="koboSpan" id="kobo.1462.1"> dataset. </span><span class="koboSpan" id="kobo.1462.2">The results can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">at </span></span><a href="https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1"><span class="No-Break"><span class="koboSpan" id="kobo.1464.1">https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1465.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1466.1">Furthermore, in this recipe, we analyzed language-to-language translation, which has been the de facto approach of the industry for a long time, using English as a bridge language for multilingual translation. </span><span class="koboSpan" id="kobo.1466.2">This is an active area of research, and recently, Meta, formerly known as Facebook, has developed the </span><strong class="bold"><span class="koboSpan" id="kobo.1467.1">No Language Left Behind</span></strong><span class="koboSpan" id="kobo.1468.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1469.1">NLLB-200</span></strong><span class="koboSpan" id="kobo.1470.1">) model. </span><span class="koboSpan" id="kobo.1470.2">More information </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.1471.1">about this breakthrough can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">at </span></span><a href="https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/"><span class="No-Break"><span class="koboSpan" id="kobo.1473.1">https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">.</span></span></p>
</div>
</body></html>