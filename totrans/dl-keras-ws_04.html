<html><head></head><body>
		<div>
			<div id="_idContainer096" class="Content">
			</div>
		</div>
		<div id="_idContainer097" class="Content">
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>4. Evaluating Your Model with Cross-Validation Using Keras Wrappers</h1>
		</div>
		<div id="_idContainer107" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces you to building Keras wrappers with scikit-learn. You will learn to apply cross-validation to evaluate deep learning models, and create user-defined functions to implement deep learning models along with cross-validation. By the end of this chapter, you will be able to build robust models that perform as well on new, unseen data as they do on the trained data.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Introduction</h1>
			<p>In the previous chapter, we experimented with different neural network architectures. We were able to evaluate the performance of the different models by observing the loss and accuracy during the course of the training process. This helped us determine when the model was underfitting or overfitting the training data and how to use techniques such as early stopping to prevent overfitting.</p>
			<p>In this chapter, you will learn about <strong class="bold">cross-validation</strong>. This is a <strong class="bold">resampling technique</strong> that leads to a very accurate and robust estimation of a model's performance, in comparison to the model evaluation approaches we discussed in the previous chapters.</p>
			<p>This chapter starts with an in-depth discussion about why we need to use cross-validation for model evaluation, the underlying basics of cross-validation, its variations, and a comparison between them. Next, we will implement cross-validation on Keras deep learning models. We will also use Keras wrappers with scikit-learn to allow Keras models to be treated as estimators in a scikit-learn workflow. You will then learn how to implement cross-validation in scikit-learn and finally bring it all together and perform cross-validation using scikit-learn on Keras deep learning models.</p>
			<p>Lastly, you will learn about how to use cross-validation to perform more than just model evaluation, and how a cross-validation estimation of model performance can be used to compare different models and select the one that results in the best performance on a particular dataset. You will also use cross-validation to improve the performance of a given model by finding the best set of hyperparameters for it. We will implement the concepts that we will learn about in this chapter in three activities, each involving a real-life dataset.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor086"/>Cross-Validation</h1>
			<p><strong class="source-inline">Resampling techniques</strong> are an important group of techniques in statistical data analysis. They involve repeatedly drawing samples from a dataset to create the training set and the test set. At each repetition, they fit and evaluate the model using the samples drawn from the dataset for the training set and the test set at that repetition.</p>
			<p>Using these techniques can provide us with information about the model that is otherwise not obtainable by fitting and evaluating the model only once, using one training set and one test set. Since resampling methods involve fitting a model to the training data several times, they are computationally expensive. Therefore, when it comes to deep learning, we only implement them in the cases where the dataset and the network are relatively small, and the available computational power allows us to do so.</p>
			<p>In this section, you will learn about a very important resampling method called <strong class="source-inline">cross-validation</strong>. Cross-validation is one of the most important and most commonly used resampling methods. It computes the best estimation of model performance on new, unseen examples when given a limited dataset. We will also explore the basics of cross-validation, its two variations, and a comparison between them.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Drawbacks of Splitting a Dataset Only Once</h2>
			<p>In the previous chapter, we mentioned that evaluating a model on the same dataset that's used to train the model is a methodological mistake. Since the model has been trained to reduce the error on this particular set of examples, its performance on it is highly biased. That is why the error rate on training data is always an underestimation of the error rate on new examples. We learned that one way to solve this problem is to randomly hold out a subset of the data as a test set for evaluation and fit the model on the rest of the data, which is called the training set. An illustration of this approach can be seen in the following image:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B15777_04_01.jpg" alt="Figure 4.1: Overview of training set/test set split&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: Overview of training set/test set split</p>
			<p>As we mentioned previously, assigning the data to either the training set or the test set is completely random. This means that if we repeat this process, different data will be assigned to the test set and training set each time. The test error rate that's reported by this approach can vary a lot, depending on which examples are in the test set and which examples are in the training set.</p>
			<p><strong class="bold">Example</strong></p>
			<p>Let's look at an example. Here, we have built a single-layer neural network for the hepatitis C dataset that you saw in <em class="italic">Activity 3.02</em>, <em class="italic">Advanced Fibrosis Diagnosis with Neural Networks</em> in <em class="italic">Chapter 3</em>, <em class="italic">Deep Learning with Keras</em>. We used the training set/test set approach to compute the test error associated with this model. Instead of splitting and training only once, if we split the data into five separate datasets and repeated this process five times, we might expect five different plots for the test error rates. The test error rates for each of these five experiments can be seen in the following plot:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B15777_04_02.jpg" alt="Figure 4.2: Plot of test error rates with five different training set/test set splits on an example dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Plot of test error rates with five different training set/test set splits on an example dataset</p>
			<p>As you can see, the test error rate is quite different in each experiment. This variation in the models' evaluation results indicates that the simple strategy of splitting the dataset into a training set and a test set only once may not lead to a robust and accurate estimation of the model's performance.</p>
			<p>To summarize, the training set/test set approach that we learned about in the previous chapter has the obvious advantage of being simple, easy to implement, and computationally inexpensive. However, it has drawbacks too, which are as follows:</p>
			<ul>
				<li>The first drawback is that its estimation of the model's error rate strongly depends on exactly which data is assigned to the test set and which data is assigned to the training set.</li>
				<li>The second drawback is that, in this approach, we are only training the model on a subset of the data. Machine learning models tend to perform worse when they are trained using a small amount of data.</li>
			</ul>
			<p>Since the performance of a model can be improved by training it on the entire dataset, we are always looking for ways to include all the available data points in training. Additionally, we are interested in finding a robust estimation of the model's performance by including all the available data points in the evaluation. These objectives can be accomplished with the use of cross-validation techniques. The following are the two methods of cross-validation:</p>
			<ul>
				<li><strong class="bold">K-fold cross-validation</strong></li>
				<li><strong class="bold">Leave-one-out cross-validation</strong></li>
			</ul>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>K-Fold Cross-Validation</h2>
			<p>In <strong class="source-inline">k-fold cross-validation</strong>, instead of dividing the dataset into two subsets, we divide the dataset into <strong class="source-inline">k</strong> approximately equal-sized subsets or folds. In the first iteration of the method, the first fold is considered a test set. The model is trained on the remaining <strong class="source-inline">k-1</strong> folds, and then it is evaluated on the first fold (the first fold is used to estimate the test error rate). </p>
			<p>This process is repeated <strong class="source-inline">k</strong> times, and a different fold is used as the test set in each iteration, while the remaining folds are used as the training set. Eventually, the method results in <strong class="source-inline">k</strong> different test error rates. The final k-fold cross-validation estimate of the model's error rate is computed by averaging these <strong class="source-inline">k</strong> test error rates. </p>
			<p>The following diagram illustrates the dataset splitting process in the <strong class="source-inline">k-fold cross-validation</strong> method:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B15777_04_03.jpg" alt="Figure 4.3: Overview of dataset splitting in the k-fold cross-validation method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Overview of dataset splitting in the k-fold cross-validation method</p>
			<p>In practice, we usually perform <strong class="source-inline">k-fold cross-validation</strong> with <strong class="source-inline">k=5</strong> or <strong class="source-inline">k=10</strong>, and these are the recommended values if you are struggling to select a value for your dataset. Deciding on the number of folds to use is dependent on the number of examples in the dataset and the available computational power. If <strong class="source-inline">k=5</strong>, the model will be trained and evaluated five times, while if <strong class="source-inline">k=10</strong>, this process will be repeated 10 times. The higher the number of folds, the longer it will take to perform k-fold cross-validation.</p>
			<p>In k-fold cross-validation, the assignment of examples to each fold is completely random. However, by looking at the preceding diagram, you will see that, in the end, every single piece of data is used for both training and evaluation. That's why if you repeat k-fold cross-validation many times on the same dataset and the same model, the final reported test error rates will be almost identical. Therefore, k-fold cross-validation does not suffer from high variance in its results, in contrast to the training set/test set approach. Now, we will take a look at the second form of cross-validation: leave-one-out validation.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>Leave-One-Out Cross-Validation</h2>
			<p><strong class="source-inline">Leave-One-Out</strong> <strong class="source-inline">(LOO)</strong> is a variation of the cross-validation technique in which, instead of dividing the dataset into two comparable-sized subsets for the training set and test set, only one single piece of data is used for evaluation. If there are <strong class="source-inline">n</strong> data examples in the entire dataset, at each iteration of <strong class="source-inline">LOO cross-validation</strong>, the model is trained on <strong class="source-inline">n-1</strong> examples and the single remaining example is used to compute the test error rate. </p>
			<p>Using only one example for estimating the test error rate leads to an unbiased but high variance estimation of model performance; it is unbiased because this one example has not been used in training the model, it has high variance because it is computed based on only one data example, and it will vary depending on which exact data example is used. This process is repeated <strong class="source-inline">n</strong> times, and, at each iteration, a different data example is used for evaluation. In the end, the method will result in <strong class="source-inline">n</strong> different test error rates, and the final <strong class="source-inline">LOO cross-validation</strong> test error estimation is computed by averaging these <strong class="source-inline">n</strong> error rates. </p>
			<p>An illustration of the dataset splitting process in the <strong class="source-inline">LOO cross-validation</strong> method can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B15777_04_04.jpg" alt="Figure 4.4: Overview of dataset splitting in the LOO cross-validation method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Overview of dataset splitting in the LOO cross-validation method</p>
			<p>In each iteration of <strong class="source-inline">LOO cross-validation</strong>, almost all the examples in the dataset are used to train the model. On the other hand, in the training set/test set approach, a relatively large subset of data is used for evaluation and not used in training. Therefore, the <strong class="source-inline">LOO</strong> estimation of model performance is much closer to the performance of a model that is trained on the entire dataset, and this is the main advantage of <strong class="source-inline">LOO cross-validation</strong> over the <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> approach.</p>
			<p>Additionally, since in each iteration of <strong class="source-inline">LOO cross-validation</strong> only one unique data example is used for evaluation, and every single data example is used for training as well, there is no randomness associated with this method. Therefore, if you repeat <strong class="source-inline">LOO cross-validation</strong> many times on the same dataset and the same model, the final reported test error rates will be exactly the same each time.</p>
			<p>The drawback of <strong class="source-inline">LOO cross-validation</strong> is that it is computationally expensive. The reason for this is that the model needs to be trained <strong class="source-inline">n</strong> times, and in cases where <strong class="source-inline">n</strong> is large and/or the network is large, it will take a long time to complete. Both <strong class="source-inline">LOO</strong> and <strong class="source-inline">k-fold cross-validation</strong> have their advantages and disadvantages, all of which we will compare in the next section.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>Comparing the K-Fold and LOO Methods</h2>
			<p>By comparing the two preceding diagrams, it is obvious that <strong class="source-inline">LOO cross-validation</strong> is, in fact, a special case of <strong class="source-inline">k-fold cross-validation</strong>, where <strong class="source-inline">k=n</strong>. However, as was mentioned previously, choosing <strong class="source-inline">k=n</strong> is computationally very expensive in comparison to choosing <strong class="source-inline">k=5</strong> or <strong class="source-inline">k=10</strong>. </p>
			<p>Therefore, the first advantage of <strong class="source-inline">k-fold cross-validation</strong> over <strong class="source-inline">LOO cross-validation</strong> is that it is computationally less expensive. The following table compares the <strong class="source-inline">k-fold with low-k</strong>, <strong class="source-inline">k-fold with high-k</strong> and <strong class="source-inline">LOO</strong>, and <strong class="source-inline">no cross-validation</strong> with respect to <strong class="source-inline">bias</strong> and <strong class="source-inline">variance</strong>. The table shows that the highest bias comes with a simple <strong class="source-inline">train-test split approach</strong> and that the highest variance comes with leave-one-put cross-validation. In the middle is <strong class="source-inline">k-fold cross-validation</strong>. This is why k-fold cross-validation is generally the most appropriate choice for most machine learning tasks:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B15777_04_05.jpg" alt="Figure 4.5: Comparing the train-test split, k-fold cross-validation, and LOO &#13;&#10;cross-validation methods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Comparing the train-test split, k-fold cross-validation, and LOO cross-validation methods</p>
			<p>The following plot compares the <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> approach, <strong class="source-inline">k-fold cross-validation</strong>, and <strong class="source-inline">LOO cross-validation</strong> in terms of <strong class="source-inline">bias</strong> and <strong class="source-inline">variance</strong>:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B15777_04_06.jpg" alt="Figure 4.6: Comparing the training set/test set approach, k-fold cross-validation, and LOO cross-validation in terms of bias and variance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: Comparing the training set/test set approach, k-fold cross-validation, and LOO cross-validation in terms of bias and variance</p>
			<p>Generally, in machine learning and data analysis, the most desirable model is the one with the <strong class="source-inline">lowest bias</strong> and the <strong class="source-inline">lowest variance</strong>. As shown in the preceding plot, the region labeled in the middle of the graph, where both <strong class="source-inline">bias</strong> and <strong class="source-inline">variance</strong> are low, is of interest. It turns out that this region is equivalent to <strong class="source-inline">k-fold cross-validation</strong> with <strong class="source-inline">k</strong> between <strong class="source-inline">5</strong> and <strong class="source-inline">10</strong>. In the next section, we will explore how to implement various methods of cross-validation in practice.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/>Cross-Validation for Deep Learning Models</h1>
			<p>In this section, you will learn about using the Keras wrapper with scikit-learn, which is a helpful tool that allows us to use Keras models as part of a scikit-learn workflow. As a result, scikit-learn methods and functions, such as the one for performing cross-validation, can easily be applied to Keras models. </p>
			<p>You will learn, step-by-step, how to implement what you learned about cross-validation in the previous section using scikit-learn. Furthermore, you will learn how to use cross-validation to evaluate Keras deep learning models using the Keras wrapper with scikit-learn. Lastly, you will practice what you have learned by solving a problem involving a real dataset.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Keras Wrapper with scikit-learn</h2>
			<p>When it comes to general machine learning and data analysis, the scikit-learn library is much richer and easier to use than Keras. That is why being able to use scikit-learn methods on Keras models will be of great value. </p>
			<p>Fortunately, Keras comes with a helpful wrapper, <strong class="source-inline">keras.wrappers.scikit_learn</strong>, that allows us to build scikit-learn interfaces for deep learning models that can be used as classification or regression estimators in scikit-learn. There are two types of wrapper: one for classification estimators and one for regression estimators. The following code is used to define these scikit-learn interfaces:</p>
			<p class="source-code">keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)</p>
			<p class="source-code"># wrappers for classification estimators</p>
			<p class="source-code">keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params)</p>
			<p class="source-code"># wrappers for regression estimators</p>
			<p>The <strong class="source-inline">build_fn</strong> argument needs to be a callable function where a Keras sequential model is defined, compiled and returned inside its body.</p>
			<p>The <strong class="source-inline">sk_params</strong> argument can take parameters for building the model (such as activation functions for layers) and parameters for fitting the model (such as the number of epochs and batch size). This will be put into practice in the following exercise, where we will use Keras wrappers for a regression problem.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the activities in this chapter will be developed in a Jupyter notebook. Please download this book's GitHub repository, along with all the prepared templates, which can be found here:</p>
			<p class="callout"> <a href="https://packt.live/3btnjfA">https://packt.live/3btnjfA</a>.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor093"/>Exercise 4.01: Building the Keras Wrapper with scikit-learn for a Regression Problem</h2>
			<p>In this exercise, you will learn the step-by-step process of building the wrapper for a Keras deep learning model so that it can be used in a scikit-learn workflow. First, load in the dataset of <strong class="source-inline">908</strong> data points of a regression problem, where each record describes six attributes of a chemical, and the target is the acute toxicity toward the fish Pimephales promelas, or <strong class="source-inline">LC50</strong>:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Watch out for the slashes in the string below. Remember that the backslashes ( <strong class="source-inline">\</strong> ) are used to split the code across multiple lines, while the forward slashes ( <strong class="source-inline">/</strong> ) are part of the path.</p>
			<p class="source-code"># import data</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">colnames = ['CIC0', 'SM1_Dz(Z)', 'GATS1i', \</p>
			<p class="source-code">            'NdsCH', 'NdssC','MLOGP', 'LC50']</p>
			<p class="source-code">data = pd.read_csv('../data/qsar_fish_toxicity.csv', \</p>
			<p class="source-code">                   sep=';', names=colnames)</p>
			<p class="source-code">X = data.drop('LC50', axis=1)</p>
			<p class="source-code">y = data['LC50']</p>
			<p class="source-code"># Print the sizes of the dataset</p>
			<p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p>
			<p class="source-code">print("Number of Features for each example = ", X.shape[1])</p>
			<p class="source-code"># print output range</p>
			<p class="source-code">print("Output Range = [%f, %f]" %(min(y), max(y)))</p>
			<p>This is the expected output:</p>
			<p class="source-code">Number of Examples in the Dataset =  908</p>
			<p class="source-code">Number of Features for each example =  6</p>
			<p class="source-code">Output Range = [0.053000, 9.612000]</p>
			<p>Since the output in this dataset takes a numerical value, this is a regression problem. The goal is to build a model that predicts the acute toxicity toward the fish <strong class="source-inline">LC50</strong>, given the other attributes of the chemical. Now, let's go through the steps:</p>
			<ol>
				<li>Define a function that builds and returns a Keras model for this regression problem. The Keras model that you define must have a single hidden layer of size <strong class="source-inline">8</strong> with <strong class="source-inline">ReLU activation</strong> functions. Also, use the <strong class="source-inline">Mean Squared Error</strong> (<strong class="source-inline">MSE</strong>) loss function and the <strong class="source-inline">Adam optimizer</strong> to compile the model:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, Activation</p><p class="source-code"># Create the function that returns the keras model</p><p class="source-code">def build_model():</p><p class="source-code">    # build the Keras model</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(8, input_dim=X.shape[1], \</p><p class="source-code">              activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    # return the model</p><p class="source-code">    return model  </p></li>
				<li>Now, use the Keras wrapper with scikit-learn to create the scikit-learn interface for your model. Remember that you need to provide the <strong class="source-inline">epochs</strong>, <strong class="source-inline">batch_size</strong>, and <strong class="source-inline">verbose</strong> arguments here:<p class="source-code"># build the scikit-Learn interface for the keras model</p><p class="source-code">from keras.wrappers.scikit_learn import KerasRegressor</p><p class="source-code">YourModel = KerasRegressor(build_fn= build_model, \</p><p class="source-code">                           epochs=100, \</p><p class="source-code">                           batch_size=20, \</p><p class="source-code">                           verbose=1) </p><p>Now, <strong class="source-inline">YourModel</strong> is ready to be used as a regression estimator in scikit-learn.</p></li>
			</ol>
			<p>In this exercise, we learned how to build a Keras wrapper with scikit-learn for a regression problem by using a simulated dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38nuqVP">https://packt.live/38nuqVP</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31MLgMF">https://packt.live/31MLgMF</a>.</p>
			<p>We will continue implementing cross-validation using this dataset in the rest of the exercises in this chapter.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/>Cross-Validation with scikit-learn</h2>
			<p>In the previous chapter, you learned that you can perform <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> splitting easily in scikit-learn. Let's assume that your original dataset is stored in <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> arrays. You can split them randomly into a training set and a test set using the following commands:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p>
			<p class="source-code">                                   (X, y, test_size=0.3, \</p>
			<p class="source-code">                                    random_state=0)</p>
			<p>The <strong class="source-inline">test_size</strong> argument can be assigned to any number between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, depending on how large you would like the test set to be. By providing an <strong class="source-inline">int</strong> number for a <strong class="source-inline">random_state</strong> argument, you will be able to select the seed for the random number generator.</p>
			<p>The easiest way to perform cross-validation in scikit-learn is by using the <strong class="source-inline">cross_val_score</strong> function. In order to do this, you need to define your estimator first (in our case, the estimator will be a Keras model). Then, you will be able to perform cross-validation on your estimator/model using the following commands: </p>
			<p class="source-code">from sklearn.model_selection import cross_val_score</p>
			<p class="source-code">scores = cross_val_score(YourModel, X, y, cv=5)</p>
			<p>Notice that we provide the Keras model and the original dataset as arguments to the <strong class="source-inline">cross_val_score</strong> function, along with the number of folds (the <strong class="source-inline">cv</strong> argument). Here, we used <strong class="source-inline">cv=5</strong>, so the <strong class="source-inline">cross_val_score</strong> function will randomly split the dataset into five-folds and perform training and fitting on the model five times using five different training and test sets. It will compute the default metric for model evaluation (or the metrics given to the Keras model when defining it) at each iteration/fold and store them in scores. We can print the final cross-validation score as follows:</p>
			<p class="source-code">print(scores.mean())</p>
			<p>Earlier, we mentioned that the score that's returned by the <strong class="source-inline">cross_val_score</strong> function is the default metric for our model or the metric that we determined for it when defining our model. However, it is possible to change the cross-validation metric by providing the desired metric as a <strong class="source-inline">scoring</strong> argument when calling the <strong class="source-inline">cross_val_score</strong> function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can learn more about how to provide the desired metric in the <strong class="source-inline">scoring</strong> argument of the <strong class="source-inline">cross_val_score</strong> function here: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a>.</p>
			<p>By providing an integer number for the <strong class="source-inline">cv</strong> argument of the <strong class="source-inline">cross_val_score</strong> function, we are telling the function to perform k-fold cross-validation on the dataset. However, there are several other iterators available in scikit-learn that we can assign to <strong class="source-inline">cv</strong> to perform other variations of cross-validation on the dataset. For example, the following code block will perform <strong class="source-inline">LOO cross-validation</strong> on the dataset:</p>
			<p class="source-code">from sklearn.model_selection import LeaveOneOut</p>
			<p class="source-code">loo = LeaveOneOut()</p>
			<p class="source-code">scores = cross_val_score(YourModel, X, y, cv=loo)</p>
			<p>In the next section, we will explore k-fold cross-validation in scikit-learn and see how it can be used with Keras models.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>Cross-Validation Iterators in scikit-learn</h2>
			<p>A list of the most commonly used cross-validation iterators available in scikit-learn is provided here, along with a brief description of each of them:</p>
			<ul>
				<li><strong class="source-inline">KFold(n_splits=?)</strong><p>This divides the dataset into k folds or groups. The <strong class="source-inline">n_splits</strong> argument is required to determine how many folds to use. If <strong class="source-inline">n_splits=n</strong>, it will be equivalent to <strong class="source-inline">LOO cross-validation</strong>.</p></li>
				<li><strong class="source-inline">RepeatedKFold(n_splits=?, n_repeats=?, random_state=random_state)</strong><p>This will repeat k-fold cross-validation <strong class="source-inline">n_repeats</strong> times.</p></li>
				<li><strong class="source-inline">LeaveOneOut()</strong><p>This will split the dataset for <strong class="source-inline">LOO cross-validation</strong>.</p></li>
				<li><strong class="source-inline">ShuffleSplit(n_splits=?, test_size=?, random_state= random_state)</strong><p>This will generate an <strong class="source-inline">n_splits</strong> number of random and independent training set/test set dataset splits. It is possible to store the seed for the random number generator using the <strong class="source-inline">random_state</strong> argument; if you do this, the dataset splits will be reproducible.</p></li>
			</ul>
			<p>In addition to the regular iterators, such as the ones mentioned here, there are <strong class="source-inline">stratified</strong> versions as well. Stratified sampling is useful when the number of examples in different classes of a dataset is unbalanced. For example, imagine that we want to design a classifier to predict whether someone will default on their credit card debt, where almost <strong class="source-inline">95%</strong> of the examples in the dataset are in the <strong class="source-inline">negative</strong> class. Stratified sampling makes sure that the relative class frequencies are preserved in each <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> split. It is recommended to use the stratified versions of iterators for such cases.</p>
			<p>Usually, before using a training set to train and evaluate a model, we perform preprocessing on it to scale the examples so that they have a mean equal to <strong class="source-inline">0</strong> and a standard deviation equal to <strong class="source-inline">1</strong>. In the <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> approach, we need to scale the training set and store the transformation. The following code block will do this for us:</p>
			<p class="source-code">from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">scaler = StandardScaler()</p>
			<p class="source-code">X_train = scaler.fit_transform(X_train)</p>
			<p class="source-code">X_test = scaler.transform(X_test)</p>
			<p>Here's an example of performing <strong class="source-inline">stratified k-fold cross-validation</strong> with <strong class="source-inline">k=5</strong> on our <strong class="source-inline">X</strong>, <strong class="source-inline">y</strong> dataset:</p>
			<p class="source-code">from sklearn.model_selection import StratifiedKFold</p>
			<p class="source-code">skf = StratifiedKFold(n_splits=5)</p>
			<p class="source-code">scores = cross_val_score(YourModel, X, y, cv=skf)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can learn more about cross-validation iterators in scikit-learn here: </p>
			<p class="callout"><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators">https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators</a>.</p>
			<p>Now that we understand cross-validation iterators, we can put them into practice in an exercise.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor096"/>Exercise 4.02: Evaluating Deep Neural Networks with Cross-Validation</h2>
			<p>In this exercise, we will bring all the concepts and methods that we have learned about in this topic about cross-validation together. We will go through all the steps one more time, from defining a Keras deep learning model to transferring it to a scikit-learn workflow and performing cross-validation in order to evaluate its performance. In a sense, this exercise is a recap of what we have learned so far, and what is covered here will be extremely helpful for <em class="italic">Activity 4.01</em>, <em class="italic">Model Evaluation Using Cross-Validation for an Advanced Fibrosis Diagnosis Classifier</em>:</p>
			<ol>
				<li value="1">The first step is always to load the dataset that you would like to build the model for. First, load in the dataset of <strong class="source-inline">908</strong> data points of a regression problem, where each record describes six attributes of a chemical and the target is the acute toxicity toward the fish Pimephales promelas, or <strong class="source-inline">LC50</strong>:<p class="source-code"># import data</p><p class="source-code">import pandas as pd</p><p class="source-code">colnames = ['CIC0', 'SM1_Dz(Z)', 'GATS1i', \</p><p class="source-code">            'NdsCH', 'NdssC','MLOGP', 'LC50']</p><p class="source-code">data = pd.read_csv('../data/qsar_fish_toxicity.csv', \</p><p class="source-code">                   sep=';', names=colnames)</p><p class="source-code">X = data.drop('LC50', axis=1)</p><p class="source-code">y = data['LC50']</p><p class="source-code"># Print the sizes of the dataset</p><p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p><p class="source-code">print("Number of Features for each example = ", X.shape[1])</p><p class="source-code"># print output range</p><p class="source-code">print("Output Range = [%f, %f]" %(min(y), max(y)))</p><p>The output is as follows:</p><p class="source-code">Number of Examples in the Dataset =  908</p><p class="source-code">Number of Features for each example =  6</p><p class="source-code">Output Range = [0.053000, 9.612000]</p></li>
				<li>Define the function that returns the Keras model with a single hidden layer of size <strong class="source-inline">8</strong> with <strong class="source-inline">ReLU activation</strong> functions using the <strong class="source-inline">Mean Squared Error</strong> (<strong class="source-inline">MSE</strong>) loss function and the <strong class="source-inline">Adam optimizer</strong>:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, Activation</p><p class="source-code"># Create the function that returns the keras model</p><p class="source-code">def build_model():</p><p class="source-code">    # build the Keras model</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(8, input_dim=X.shape[1], \</p><p class="source-code">              activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    # return the model</p><p class="source-code">    return model</p></li>
				<li>Set the <strong class="source-inline">seed</strong> and use the wrapper to build the scikit-learn interface for the Keras model we defined in the function in <strong class="source-inline">step 2</strong>:<p class="source-code"># build the scikit-Learn interface for the keras model</p><p class="source-code">from keras.wrappers.scikit_learn import KerasRegressor</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">YourModel = KerasRegressor(build_fn= build_model, \</p><p class="source-code">                           epochs=100, batch_size=20, \</p><p class="source-code">                           verbose=1 , shuffle=False)</p></li>
				<li>Define the iterator to use for cross-validation. Let's perform <strong class="source-inline">5-fold cross-validation</strong>:<p class="source-code"># define the iterator to perform 5-fold cross-validation</p><p class="source-code">from sklearn.model_selection import KFold</p><p class="source-code">kf = KFold(n_splits=5)</p></li>
				<li>Call the <strong class="source-inline">cross_val_score</strong> function to perform cross-validation. This step will take a while to complete, depending on the computational power that's available:<p class="source-code"># perform cross-validation on X, y</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">results = cross_val_score(YourModel, X, y, cv=kf) </p></li>
				<li>Once cross-validation has been completed, print the <strong class="source-inline">final cross-validation</strong> estimation of model performance (the default metric for performance will be the test loss):<p class="source-code"># print the result</p><p class="source-code">print(f"Final Cross-Validation Loss = {abs(results.mean()):.4f}")</p><p>Here's an example output:</p><p class="source-code">Final Cross-Validation Loss = 0.9680</p></li>
			</ol>
			<p>The cross-validation loss states that the Keras model that was trained on this dataset is able to predict the <strong class="source-inline">LC50</strong> of the chemicals with an average loss of <strong class="source-inline">0.9680</strong>. We will try to examine this model further in the next exercise.</p>
			<p>These were all the steps that are required in order to evaluate a Keras deep learning model using cross-validation in scikit-learn. Now, we will put them into practice in an activity.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eRTlTM">https://packt.live/3eRTlTM</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31IdVT0">https://packt.live/31IdVT0</a>.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor097"/>Activity 4.01: Model Evaluation Using Cross-Validation for an Advanced Fibrosis Diagnosis Classifier</h2>
			<p>We learned about the hepatitis C dataset in <em class="italic">Activity 3.02,</em> <em class="italic">Advanced Fibrosis Diagnosis with Neural Networks</em> of <em class="italic">Chapter 3</em>, <em class="italic">Deep Learning with Keras</em>. The dataset consists of information for <strong class="source-inline">1385</strong> patients who underwent treatment dosages for hepatitis C. For each patient, <strong class="source-inline">28</strong> different attributes are available, such as age, gender, and BMI, as well as a class label, which can only take two values: <strong class="source-inline">1</strong>, indicating advanced fibrosis, and <strong class="source-inline">0</strong>, indicating no indication of advanced fibrosis. This is a <strong class="source-inline">binary</strong>/<strong class="source-inline">two-class</strong> classification problem with an input dimension equal to <strong class="source-inline">28</strong>.</p>
			<p>In <em class="italic">Chapter 3</em>, <em class="italic">Deep Learning with Keras</em>, we built Keras models to perform classification on this dataset. We trained and evaluated the models using <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> splitting and reported the test error rate. In this activity, we are going to use what we learned in this topic to train and evaluate a deep learning model using <strong class="source-inline">k-fold cross-validation</strong>. We will use the model that resulted in the best test error rate from the previous activity. The goal is to compare the cross-validation error rate with the training set/test set approach error rate:</p>
			<ol>
				<li value="1">Import the necessary libraries. Load the dataset from the <strong class="source-inline">data</strong> subfolder of the <strong class="source-inline">Chapter04</strong> folder from GitHub using <strong class="source-inline">X = pd.read_csv('../data/HCV_feats.csv'), y = pd.read_csv('../data/HCV_target.csv')</strong>. Print the number of examples in the dataset, the number of features available, and the possible values for the class labels.</li>
				<li>Define the function that returns the Keras model. The Keras model will be a deep neural network with two hidden layers, where the <strong class="source-inline">first hidden layer</strong> is of <strong class="source-inline">size 4</strong> and the <strong class="source-inline">second hidden layer</strong> is of <strong class="source-inline">size 2</strong>, and use the <strong class="source-inline">tanh activation</strong> function to perform the classification. Use the following values for the hyperparameters: <p><strong class="source-inline">optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']</strong></p></li>
				<li>Build the scikit-learn interface for the Keras model with <strong class="source-inline">epochs=100</strong>, <strong class="source-inline">batch_size=20</strong>, and <strong class="source-inline">shuffle=False</strong>. Define the cross-validation iterator as <strong class="source-inline">StratifiedKFold</strong> with <strong class="source-inline">k=5</strong>. Perform k-fold cross-validation on the model and store the scores.</li>
				<li>Print the accuracy for each iteration/fold, plus the overall cross-validation accuracy and its associated standard deviation. </li>
				<li>Compare this result with the result from <em class="italic">Activity 3.02</em>, <em class="italic">Advanced Fibrosis Diagnosis with Neural Networks</em> of <em class="italic">Chapter 3</em>, <em class="italic">Deep Learning with Keras</em>.</li>
			</ol>
			<p>After implementing the preceding steps, the expected output will look as follows:</p>
			<p class="source-code">Test accuracy at fold 1 = 0.5198556184768677</p>
			<p class="source-code">Test accuracy at fold 2 = 0.4693140685558319</p>
			<p class="source-code">Test accuracy at fold 3 = 0.512635350227356</p>
			<p class="source-code">Test accuracy at fold 4 = 0.5740072131156921</p>
			<p class="source-code">Test accuracy at fold 5 = 0.5523465871810913</p>
			<p class="source-code">Final Cross Validation Test Accuracy: 0.5256317675113678</p>
			<p class="source-code">Standard Deviation of Final Test Accuracy: 0.03584760640500936</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 381.</p>
			<p>The accuracy we received from training set/test set approach we performed in <em class="italic">Activity 3.02</em>, <em class="italic">Advanced Fibrosis Diagnosis with Neural Networks</em> of <em class="italic">Chapter 3</em>, <em class="italic">Deep Learning with Keras</em>, was <strong class="source-inline">49.819%</strong>, which is lower than the test accuracy we achieved when performing <strong class="source-inline">5-fold cross-validation</strong> on the same deep learning model and the same dataset, but lower than the accuracy on one of the folds. </p>
			<p>The reason for this difference is that the test error rate resulting from the training set/test set approach was computed by only including a subset of the data points in the model's evaluation. On the other hand, the test error rate here is computed by including all the data points in the evaluation, and therefore this estimation of the model's performance is more accurate and more robust, performing better on the unseen test dataset.</p>
			<p>In this activity, we used cross-validation to perform a model evaluation on a problem involving a real dataset. Improving model evaluation is not the only purpose of using cross-validation, and it can be used to select the best model or parameters for a given problem as well.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor098"/>Model Selection with Cross-Validation</h1>
			<p>Cross-validation provides us with robust estimation of model performance on unseen examples. For this reason, it can be used to decide between two models for a particular problem or to decide which model parameters (or hyperparameters) to use for a particular problem. In these cases, we would like to find out which model or which set of model parameters/hyperparameters results in the lowest test error rate. Therefore, we will select that model or that set of parameters/hyperparameters for our problem.</p>
			<p>In this section, you are going to practice using cross-validation for this purpose. You will learn how to define a set of hyperparameters for your deep learning model and then write user-defined functions in order to perform cross-validation on your model for each of the possible combinations of hyperparameters. Then, you will observe which combination of hyperparameters leads to the lowest test error rate, and that combination will be your choice for your final model.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor099"/>Cross-Validation for Model Evaluation versus Model Selection</h2>
			<p>In this section, we are going to go deeper into what it means to use cross-validation for model evaluation versus model selection. So far, we have learned that evaluating a model on the training set results in an underestimation of the model's error rate on unseen examples. Splitting the dataset into a <strong class="source-inline">training set</strong> and a <strong class="source-inline">test set</strong> gives us a more accurate estimation of the model's performance but suffers from high variance. </p>
			<p>Lastly, cross-validation results in a much more robust and accurate estimation of the model's performance on unseen examples. An illustration of the error rate estimations resulting from these three approaches for model evaluation can be seen in the following plot.</p>
			<p>The following plot shows the case where the error rate estimation in the training set/test set approach is slightly lower than the cross-validation estimation. However, it is important to remember that the <strong class="source-inline">training set</strong>/<strong class="source-inline">test set</strong> error rate can be higher than the cross-validation estimation error rate as well, depending on what data is included in the test set (hence the high variance problem). On the other hand, the error rate resulting from performing an evaluation on the training set is always lower than the other two approaches:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B15777_04_07.jpg" alt="Figure 4.7: Illustration of the error rate estimations resulting from the three approaches to model evaluation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: Illustration of the error rate estimations resulting from the three approaches to model evaluation</p>
			<p>We have established that cross-validation leads to the best estimation of a model's performance on independent data examples. Knowing this, we can use cross-validation to decide which model to use for a particular problem. For example, if we have four different models and we would like to decide which one is a better fit for a particular dataset, we can train and evaluate each of the four models using cross-validation and choose the model with the lowest cross-validation error rate as our final model for the dataset. The following plot shows the cross-validation error rate associated with four hypothetical models. From this, we can conclude that <strong class="bold">Model 1</strong> is the best fit for the problem, while <strong class="bold">Model 4</strong> is the worst choice. These four models could be deep neural networks with a different number of hidden layers and a different number of units in their hidden layers:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B15777_04_08.jpg" alt="Figure 4.8: Illustration of cross-validation error rates associated with four &#13;&#10;hypothetical models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8: Illustration of cross-validation error rates associated with four hypothetical models</p>
			<p>After we have found out which model is the best fit for a particular problem, the next step is choosing the best set of parameters or hyperparameters for that model. Previously, we discussed that when building a deep neural network, several hyperparameters need to be selected for the model, and several choices are available for each of these hyperparameters. </p>
			<p>These hyperparameters include the type of activation function, loss function, and optimizer, plus the number of epochs and batch size. We can define the set of possible choices for each of these hyperparameters and then implement the model, along with cross-validation, to find the best combination of hyperparameters. </p>
			<p>An illustration of the cross-validation error rates associated with four different sets of hyperparameters for a hypothetical deep learning model is shown in the following plot. From this, we can conclude that <strong class="source-inline">Set 1</strong> is the best choice for this model since the line corresponding to <strong class="source-inline">Hyperparameters Set 1</strong> has the lowest value for the cross-validation error rate:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B15777_04_09.jpg" alt="Figure 4.9: Illustration of cross-validation error rates associated with four different sets of hyperparameters for a hypothetical deep learning model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: Illustration of cross-validation error rates associated with four different sets of hyperparameters for a hypothetical deep learning model</p>
			<p>In the next exercise, we will learn how to iterate through different model architectures and hyperparameters to find the set that results in an optimal model.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor100"/>Exercise 4.03: Writing User-Defined Functions to Implement Deep Learning Models with Cross-Validation</h2>
			<p>In this exercise, you will learn how to use cross-validation for the purpose of model selection.</p>
			<p>First, load in the dataset of <strong class="source-inline">908</strong> data points of a regression problem, where each record describes six attributes of a chemical and the target is the acute toxicity toward the fish Pimephales promelas, or <strong class="source-inline">LC50</strong>. The goal is to build a model to predict the <strong class="source-inline">LC50</strong> of each chemical, given the chemical attributes:</p>
			<p class="source-code"># import data</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from tensorflow import random</p>
			<p class="source-code">colnames = ['CIC0', 'SM1_Dz(Z)', 'GATS1i', 'NdsCH', \</p>
			<p class="source-code">            'NdssC','MLOGP', 'LC50']</p>
			<p class="source-code">data = pd.read_csv('../data/qsar_fish_toxicity.csv', \</p>
			<p class="source-code">                   sep=';', names=colnames)</p>
			<p class="source-code">X = data.drop('LC50', axis=1)</p>
			<p class="source-code">y = data['LC50']</p>
			<p>Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Define three functions to return three Keras models. The first model should have one hidden layer of <strong class="source-inline">size 4</strong>, the second model should have one hidden layer of <strong class="source-inline">size 8</strong>, and the third model should have two hidden layers, with the first layer of <strong class="source-inline">size 4</strong> and the second layer of <strong class="source-inline">size 2</strong>. Use a <strong class="source-inline">ReLU activation</strong> function for all the hidden layers. The goal is to find out which of these three models leads to the lowest cross-validation error rate:<p class="source-code"># Define the Keras models</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">def build_model_1():</p><p class="source-code">    # build the Keras model_1</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4, input_dim=X.shape[1], \</p><p class="source-code">                    activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    # return the model</p><p class="source-code">    return model</p><p class="source-code">def build_model_2():</p><p class="source-code">    # build the Keras model_2</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(8, input_dim=X.shape[1], \</p><p class="source-code">              activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    # return the model</p><p class="source-code">    return model</p><p class="source-code">def build_model_3():</p><p class="source-code">    # build the Keras model_3</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4, input_dim=X.shape[1], \</p><p class="source-code">                    activation='relu'))</p><p class="source-code">    model.add(Dense(2, activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    # return the model</p><p class="source-code">    return model</p></li>
				<li>Write a loop to build the Keras wrapper and perform <strong class="source-inline">3-fold cross-validation</strong> on the <strong class="source-inline">three</strong> models. Store the scores for each model:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># perform cross-validation on each model</p><p class="source-code">from keras.wrappers.scikit_learn import KerasRegressor</p><p class="source-code">from sklearn.model_selection import KFold</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">results_1 = []</p><p class="source-code">models = [build_model_1, build_model_2, build_model_3]</p><p class="source-code"># loop over three models</p><p class="source-code">for m in range(len(models)):</p><p class="source-code">    model = KerasRegressor(build_fn=models[m], \</p><p class="source-code">                           epochs=100, batch_size=20, \</p><p class="source-code">                           verbose=0, shuffle=False)</p><p class="source-code">    kf = KFold(n_splits=3)</p><p class="source-code">    result = cross_val_score(model, X, y, cv=kf)</p><p class="source-code">    results_1.append(result)</p></li>
				<li>Print the final cross-validation error rate for each of the models to find out which model has a lower error rate:<p class="source-code"># print the cross-validation scores</p><p class="source-code">print("Cross-Validation Loss for Model 1 =", \</p><p class="source-code">      abs(results_1[0].mean()))</p><p class="source-code">print("Cross-Validation Loss for Model 2 =", \</p><p class="source-code">      abs(results_1[1].mean()))</p><p class="source-code">print("Cross-Validation Loss for Model 3 =", \</p><p class="source-code">      abs(results_1[2].mean()))</p><p>Here's an example output:</p><p class="source-code">Cross-Validation Loss for Model 1 = 0.990475798256843</p><p class="source-code">Cross-Validation Loss for Model 2 = 0.926532513151634</p><p class="source-code">Cross-Validation Loss for Model 3 = 0.9735719371528117</p><p><strong class="source-inline">Model 2</strong> results in the lowest error rate, so we will use it in the steps that follow.</p></li>
				<li>Use cross-validation again to determine the number of epochs and batch size for the model that resulted in the lowest cross-validation error rate. Write the code that performs <strong class="source-inline">3-fold cross-validation</strong> on every possible combination of <strong class="source-inline">epochs</strong> and <strong class="source-inline">batch-size</strong> in the ranges <strong class="source-inline">epochs=[100, 150]</strong> and <strong class="source-inline">batch_size=[20, 15]</strong> and store the scores:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">results_2 = []</p><p class="source-code">epochs = [100, 150]</p><p class="source-code">batches = [20, 15]</p><p class="source-code"># Loop over pairs of epochs and batch_size</p><p class="source-code">for e in range(len(epochs)):</p><p class="source-code">    for b in range(len(batches)):</p><p class="source-code">        model = KerasRegressor(build_fn= build_model_3, \</p><p class="source-code">                               epochs= epochs[e], \</p><p class="source-code">                               batch_size= batches[b], \</p><p class="source-code">                               verbose=0, \</p><p class="source-code">                               shuffle=False)</p><p class="source-code">        kf = KFold(n_splits=3)</p><p class="source-code">        result = cross_val_score(model, X, y, cv=kf)</p><p class="source-code">        results_2.append(result)</p><p class="callout-heading">Note</p><p class="callout">The preceding code block uses two <strong class="source-inline">for</strong> loops to perform <strong class="source-inline">3-fold cross-validation</strong> for all possible combinations of <strong class="source-inline">epochs </strong>and <strong class="source-inline">batch_size</strong>. Since there are two choices for each of them, four different pairs are possible and therefore cross-validation will be performed four times.</p></li>
				<li>Print the final cross-validation error rate for each of the <strong class="source-inline">epochs</strong>/<strong class="source-inline">batch_size</strong> pairs to find out which pair has the lowest error rate:<p class="source-code">"""</p><p class="source-code">Print cross-validation score for each possible pair of epochs, batch_size</p><p class="source-code">"""</p><p class="source-code">c = 0</p><p class="source-code">for e in range(len(epochs)):</p><p class="source-code">    for b in range(len(batches)):</p><p class="source-code">        print("batch_size =", batches[b],", \</p><p class="source-code">              epochs =", epochs[e], ", Test Loss =", \</p><p class="source-code">              abs(results_2[c].mean()))</p><p class="source-code">        c += 1</p><p>Here's an example output:</p><p class="source-code">batch_size = 20 , epochs = 100 , Test Loss = 0.9359159401008821</p><p class="source-code">batch_size = 15 , epochs = 100 , Test Loss = 0.9642481369794683</p><p class="source-code">batch_size = 20 , epochs = 150 , Test Loss = 0.9561188386646661</p><p class="source-code">batch_size = 15 , epochs = 150 , Test Loss = 0.9359079093029896</p><p>As you can see, the performance for <strong class="source-inline">epochs=150</strong> and <strong class="source-inline">batch_size=15</strong>, and for <strong class="source-inline">epochs=100</strong> and <strong class="source-inline">batch_size=20</strong>, are almost the same. Therefore, we will choose <strong class="source-inline">epochs=100</strong> and <strong class="source-inline">batch_size=20</strong> in the next step to speed up this process.</p></li>
				<li>Use cross-validation again in order to decide on the activation function for the hidden layers and the optimizer for the model from <strong class="source-inline">activations = ['relu', 'tanh']</strong> and <strong class="source-inline">optimizers = ['sgd', 'adam', 'rmsprop']</strong>. Remember to use the best pair of <strong class="source-inline">batch_size</strong> and <strong class="source-inline">epochs</strong> from the previous step:<p class="source-code"># Modify build_model_2 function</p><p class="source-code">def build_model_2(activation='relu', optimizer='adam'):</p><p class="source-code">    # build the Keras model_2</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(8, input_dim=X.shape[1], \</p><p class="source-code">              activation=activation))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile the model</p><p class="source-code">    model.compile(loss='mean_squared_error', \</p><p class="source-code">                  optimizer=optimizer)</p><p class="source-code">    # return the model</p><p class="source-code">    return model</p><p class="source-code">results_3 = []</p><p class="source-code">activations = ['relu', 'tanh']</p><p class="source-code">optimizers = ['sgd', 'adam', 'rmsprop']</p><p class="source-code">"""</p><p class="source-code">Define a seed for the random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># Loop over pairs of activation and optimizer</p><p class="source-code">for o in range(len(optimizers)):</p><p class="source-code">    for a in range(len(activations)):</p><p class="source-code">        optimizer = optimizers[o]</p><p class="source-code">        activation = activations[a]</p><p class="source-code">        model = KerasRegressor(build_fn= build_model_3, \</p><p class="source-code">                               epochs=100, batch_size=20, \</p><p class="source-code">                               verbose=0, shuffle=False)</p><p class="source-code">        kf = KFold(n_splits=3)</p><p class="source-code">        result = cross_val_score(model, X, y, cv=kf)</p><p class="source-code">        results_3.append(result)</p><p class="callout-heading">Note</p><p class="callout">Notice that we had to modify the <strong class="source-inline">build_model_2</strong> function by passing the <strong class="source-inline">activation</strong>, the <strong class="source-inline">optimizer</strong>, and their default values as arguments of the function. </p></li>
				<li>Print the final cross-validation error rate for each pair of <strong class="source-inline">activation</strong> and <strong class="source-inline">optimizer</strong> to find out which pair has the lower error rate:<p class="source-code">"""</p><p class="source-code">Print cross-validation score for each possible pair of optimizer, activation</p><p class="source-code">"""</p><p class="source-code">c = 0</p><p class="source-code">for o in range(len(optimizers)):</p><p class="source-code">    for a in range(len(activations)):</p><p class="source-code">        print("activation = ", activations[a],", \</p><p class="source-code">              optimizer = ", optimizers[o], ", \</p><p class="source-code">              Test Loss = ", abs(results_3[c].mean()))</p><p class="source-code">        c += 1</p><p>Here's the output:</p><p class="source-code">activation =  relu , optimizer =  sgd , Test Loss =  1.0123592540516995</p><p class="source-code">activation =  tanh , optimizer =  sgd , Test Loss =  3.393908379781118</p><p class="source-code">activation =  relu , optimizer =  adam , Test Loss =  0.9662686089392641</p><p class="source-code">activation =  tanh , optimizer =  adam , Test Loss =  2.1369285960222144</p><p class="source-code">activation =  relu , optimizer =  rmsprop , Test Loss =  2.1892826984214984</p><p class="source-code">activation =  tanh , optimizer =  rmsprop , Test Loss =  2.2029884275363014</p></li>
				<li>The <strong class="source-inline">activation='relu'</strong> and <strong class="source-inline">optimizer='adam'</strong> pair results in the lowest error rate. Also, the result for the <strong class="source-inline">activation='relu'</strong> and <strong class="source-inline">optimizer='sgd'</strong> pair is almost as good. Therefore, we can use either of these optimizers in the final model to predict the aquatic toxicity for this dataset.<p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BYCwbg">https://packt.live/2BYCwbg</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3gofLfP">https://packt.live/3gofLfP</a>.</p></li>
			</ol>
			<p>Now, you are ready to practice model selection using cross-validation on another dataset. In <em class="italic">Activity 4.02</em>, <em class="italic">Model Selection Using Cross-Validation for the Advanced Fibrosis Diagnosis Classifier</em>, you will practice these steps further by implementing them by yourself on a classification problem with the hepatitis C dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><em class="italic">Exercise 4.02</em>, <em class="italic">Evaluating Deep Neural Networks with Cross-Validation</em>, and <em class="italic">Exercise 4.03</em>, <em class="italic">Writing User-Defined Functions to Implement Deep Learning Models with Cross-Validation</em>, involve performing <strong class="source-inline">k-fold cross-validation</strong> several times, so the steps may take several minutes to complete. If they are taking too long to complete, you may want to try speeding up the process by decreasing the number of folds or epochs or increasing the batch sizes. Obviously, if you do so, you will get different results compared to the expected outputs, but the same principles still apply for selecting the model and hyperparameters.</p>
			<h2 id="_idParaDest-100">Acti<a id="_idTextAnchor101"/>vity 4.02: Model Selection Using Cross-Validation for the Advanced Fibrosis Diagnosis Classifier</h2>
			<p>In this activity, we are going to improve our classifier for the hepatitis C dataset by using cross-validation for model selection and hyperparameter selection. Follow these steps:</p>
			<ol>
				<li value="1">Import the required packages. Load the dataset from the <strong class="source-inline">data</strong> subfolder of the <strong class="source-inline">Chapter04</strong> folder from GitHub using <strong class="source-inline">X = pd.read_csv('../data/HCV_feats.csv'), y = pd.read_csv('../data/HCV_target.csv')</strong>.</li>
				<li>Define three functions, each returning a different Keras model. The first Keras model will be a deep neural network with three hidden layers all of <strong class="source-inline">size 4</strong> and <strong class="source-inline">ReLU activation</strong> functions. The second Keras model will be a deep neural network with two hidden layers, the first layer of <strong class="source-inline">size 4</strong> and the second later of <strong class="source-inline">size 2</strong>, and <strong class="source-inline">ReLU activation</strong> functions. The third Keras model will be a deep neural network with two hidden layers, both of <strong class="source-inline">size 8</strong>, and a <strong class="source-inline">ReLU activation</strong> function. Use the following values for the hyperparameters: <p><strong class="source-inline">optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']</strong></p></li>
				<li>Write the code that will loop over the three models and perform <strong class="source-inline">5-fold cross-validation</strong> on each of them (use <strong class="source-inline">epochs=100</strong>, <strong class="source-inline">batch_size=20</strong>, and <strong class="source-inline">shuffle=False</strong> in this step). Store all the cross-validation scores in a list and print the results. Which model results in the best accuracy?<p class="callout-heading">Note</p><p class="callout"><em class="italic">Steps</em> <em class="italic">3</em>, <em class="italic">4</em>, and <em class="italic">5</em> of this activity involve performing <strong class="source-inline">5-fold cross-validation</strong> three, four, and six times, respectively. Therefore, they may take some time to complete.</p></li>
				<li>Write the code that uses the <strong class="source-inline">epochs = [100, 200]</strong> and <strong class="source-inline">batches = [10, 20]</strong> values for <strong class="source-inline">epochs</strong> and <strong class="source-inline">batch_size</strong>. Perform 5-fold cross-validation for each possible pair on the Keras model that resulted in the best accuracy from <em class="italic">step 3</em>. Store all the cross-validation scores in a list and print the results. Which <strong class="source-inline">epochs</strong> and <strong class="source-inline">batch_size</strong> pair results in the best accuracy?</li>
				<li>Write the code that uses the <strong class="source-inline">optimizers = ['rmsprop', 'adam','sgd']</strong> and <strong class="source-inline">activations = ['relu', 'tanh']</strong> values for <strong class="source-inline">optimizer</strong> and <strong class="source-inline">activation</strong>. Perform 5-fold cross-validation for each possible pair on the Keras model that resulted in the best accuracy from <em class="italic">step 3</em>. Use the <strong class="source-inline">batch_size</strong> and <strong class="source-inline">epochs</strong> values that resulted in the best accuracy from <em class="italic">step 4</em>. Store all the cross-validation scores in a list and print the results. Which <strong class="source-inline">optimizer</strong> and <strong class="source-inline">activation</strong> pair results in the best accuracy?<p class="callout-heading">Note</p><p class="callout">Please note that there is randomness associated with initializing weights and biases in a deep neural network, as well as with selecting which examples to include in each fold when performing k-fold cross-validation. Therefore, you might get a completely different result if you run the exact same code twice. For this reason, it is important to set up seeds when building and training neural networks, as well as when performing cross-validation. By doing this, you can make sure that you are repeating the exact same neural network initialization and the exact same training sets and test sets when you rerun the code.</p></li>
			</ol>
			<p>After implementing these steps, the expected output will be as follows:</p>
			<p class="source-code">activation =  relu , optimizer =  rmsprop , Test accuracy =  0.5234657049179077</p>
			<p class="source-code">activation =  tanh , optimizer =  rmsprop , Test accuracy =  0.49602887630462644</p>
			<p class="source-code">activation =  relu , optimizer =  adam , Test accuracy =  0.5039711117744445</p>
			<p class="source-code">activation =  tanh , optimizer =  adam , Test accuracy =  0.4989169597625732</p>
			<p class="source-code">activation =  relu , optimizer =  sgd , Test accuracy =  0.48953068256378174</p>
			<p class="source-code">activation =  tanh , optimizer =  sgd , Test accuracy =  0.5191335678100586</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 384.</p>
			<p>In this activity, you learned how to use cross-validation to evaluate deep neural networks in order to find the model that results in the lowest error rate for a classification problem. You also learned how to improve a given classification model by using cross-validation in order to find the best set of hyperparameters for it. In the next activity, we repeat this activity with a regression task.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor102"/>Activity 4.03: Model Selection Using Cross-validation on a Traffic Volume Dataset</h2>
			<p>In this activity, you are going to practice model selection using cross-validation one more time. Here, we are going to use a simulated dataset that represents a target variable representing the volume of traffic in cars/hour across a city bridge and various normalized features related to traffic data such as the time of day and the traffic volume on the previous day. Our goal is to build a model that predicts the traffic volume across the city bridge given the various features.</p>
			<p>The dataset contains <strong class="source-inline">10000</strong> records, and for each of them, <strong class="source-inline">10</strong> attributes/features are included. The goal is to build a deep neural network that receives the <strong class="source-inline">10</strong> features and predicts the traffic volume across the bridge. Since the output is a number, this is a regression problem. Let's get started:</p>
			<ol>
				<li value="1">Import all the required packages.</li>
				<li>Print the input and output sizes to check the number of examples in the dataset and the number of features for each example. Also, you can print the range of the output (the output in this dataset represents the median value of owner-occupied homes in thousands of dollars).</li>
				<li>Define three functions, each returning a different Keras model. The first Keras model will be a shallow neural network with one hidden layer of <strong class="source-inline">size 10</strong> and a <strong class="source-inline">ReLU activation</strong> function. The second Keras model will be a deep neural network with two hidden layers of <strong class="source-inline">size 10</strong> and a <strong class="source-inline">ReLU activation</strong> function in each layer. The third Keras model will be a deep neural network with three hidden layers of <strong class="source-inline">size 10</strong> and a <strong class="source-inline">ReLU activation</strong> function in each layer.<p>Use the following values as well: </p><p><strong class="source-inline">optimizer = 'adam', loss = 'mean_squared_error'</strong></p><p class="callout-heading">Note</p><p class="callout"><em class="italic">Steps</em> <em class="italic">4</em>, <em class="italic">5</em>, and <em class="italic">6</em> of this activity involve performing <strong class="source-inline">5-fold cross-validation</strong> three, four, and three times, respectively. Therefore, they may take some time to complete.</p></li>
				<li>Write the code to loop over the three models and perform <strong class="source-inline">5-fold cross-validation</strong> on each of them (use <strong class="source-inline">epochs=100</strong>, <strong class="source-inline">batch_size=5</strong>, and <strong class="source-inline">shuffle=False</strong> in this step). Store all the cross-validation scores in a list and print the results. Which model results in the lowest test error rate?</li>
				<li>Write the code that uses the <strong class="source-inline">epochs = [80, 100]</strong> and <strong class="source-inline">batches = [5, 10]</strong> values for <strong class="source-inline">epochs</strong> and <strong class="source-inline">batch_size</strong>. Perform 5-fold cross-validation for each possible pair on the Keras model that resulted in the lowest test error rate from <em class="italic">step 4</em>. Store all the cross-validation scores in a list and print the results. Which <strong class="source-inline">epochs</strong> and <strong class="source-inline">batch_size</strong> pair results in the lowest test error rate?</li>
				<li>Write the code that uses <strong class="source-inline">optimizers = ['rmsprop', 'sgd', 'adam']</strong> and perform <strong class="source-inline">5-fold cross-validation</strong> for each possible optimizer on the Keras model that resulted in the lowest test error rate from <em class="italic">step 4</em>. Use the <strong class="source-inline">batch_size</strong> and <strong class="source-inline">epochs</strong> values that resulted in the lowest test error rate from <em class="italic">step 5</em>. Store all the cross-validation scores in a list and print the results. Which <strong class="source-inline">optimizer</strong> results in the lowest test error rate?</li>
			</ol>
			<p>After implementing these steps, the expected output will be as follows:</p>
			<p class="source-code">optimizer= adam  test error rate =  25.391812739372256</p>
			<p class="source-code">optimizer= sgd  test error rate =  25.140230269432067</p>
			<p class="source-code">optimizer= rmsprop  test error rate =  25.217947859764102</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 391.</p>
			<p>In this activity, you learned how to use cross-validation to evaluate deep neural networks in order to find the model that results in the lowest error rate for a <strong class="source-inline">regression problem</strong>. Also, you learned how to improve a given regression model by using cross-validation in order to find the best set of hyperparameters for it.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor103"/>Summary</h1>
			<p>In this chapter, you learned about cross-validation, which is one of the most important resampling methods. It results in the best estimation of model performance on independent data. This chapter covered the basics of cross-validation and its two different variations, leave-one-out and k-fold, along with a comparison of them.</p>
			<p>Next, we covered the Keras wrapper with scikit-learn, which is a very helpful tool that allows scikit-learn methods and functions that perform cross-validation to be easily applied to Keras models. Following this, you were shown a step-by-step process of implementing cross-validation in order to evaluate Keras deep learning models using the Keras wrapper with scikit-learn.</p>
			<p>Finally, you learned that cross-validation estimations of model performance can be used to decide between different models for a particular problem or to decide which parameters (or hyperparameters) should be used for a particular model. You practiced using cross-validation for this purpose by writing user-defined functions in order to perform cross-validation on different models or different possible combinations of hyperparameters and selecting the model or the set of hyperparameters that leads to the lowest test error rate for your final model.</p>
			<p>In the next chapter, you will learn that what we did here in order to find the best set of hyperparameters for our model is, in fact, a technique called <strong class="source-inline">hyperparameter tuning</strong> or <strong class="source-inline">hyperparameter optimization</strong>. Also, you will learn how to perform hyperparameter tuning in scikit-learn by using a method called <strong class="source-inline">grid search</strong> and without the need to write user-defined functions to loop over possible combinations of hyperparameters.</p>
		</div>
		<div>
			<div id="_idContainer108" class="Content">
			</div>
		</div>
	</body></html>