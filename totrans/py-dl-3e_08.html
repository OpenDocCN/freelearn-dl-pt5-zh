<html><head></head><body>
<div id="_idContainer1016">
<h1 class="chapter-number" id="_idParaDest-146" lang="en-GB"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-147" lang="en-GB"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.2.1">Exploring Large Language Models in Depth</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In recent years, interest in transformers has skyrocketed in the academic world, industry, and even the general public. </span><span class="koboSpan" id="kobo.3.2">The state-of-the-art transformer-based architectures today are</span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.4.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">large language models</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">LLMs</span></strong><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">The most captivating feature is their text-generation capabilities, and the most popular example is</span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.9.1"> ChatGPT (</span><a href="https://chat.openai.com/"><span class="koboSpan" id="kobo.10.1">https://chat.openai.com/</span></a><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">But in their core lies the humble transformer we introduced in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">Luckily, we already have a solid foundation of transformers. </span><span class="koboSpan" id="kobo.13.3">One remarkable aspect of this architecture is that it has changed little in the years since it was introduced. </span><span class="koboSpan" id="kobo.13.4">Instead, the capabilities of LLMs have grown with their size (the name gives it away), lending credibility to the phrase </span><em class="italic"><span class="koboSpan" id="kobo.14.1">quantitative change leads to </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.15.1">qualitative change</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.17.1">The success of LLMs has further fueled the research in the area (or is it the other way around?). </span><span class="koboSpan" id="kobo.17.2">On the one hand, large industrial labs (such as Google, Meta, Microsoft, or OpenAI) invest heavily to push the boundaries for even larger LLMs. </span><span class="koboSpan" id="kobo.17.3">On the other hand, the agile, open source community finds creative ways to achieve a lot with </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">limited resources.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.19.1">In this chapter, we’ll explore the current LLM landscape from theoretical and practical perspectives. </span><span class="koboSpan" id="kobo.19.2">We will survey many of the latest LLMs, their properties, and their training. </span><span class="koboSpan" id="kobo.19.3">Furthermore, we’ll see how to apply them for our purposes with the help of the Hugging Face </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">Transformers library.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.21.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">main topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">Introducing LLMs</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.24.1">LLM architecture</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.25.1">Training LLMs</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.26.1">Emergent abilities </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">of LLMs</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.28.1">Introducing Hugging </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">Face Transformers</span></span></li>
</ul>
<h1 id="_idParaDest-148" lang="en-GB"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.30.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.31.1">We’ll implement the example in this chapter using Python, PyTorch, and the Hugging Face Transformers library (</span><a href="https://github.com/huggingface/transformers"><span class="koboSpan" id="kobo.32.1">https://github.com/huggingface/transformers</span></a><span class="koboSpan" id="kobo.33.1">). </span><span class="koboSpan" id="kobo.33.2">If you don’t have an environment with these tools, fret not—the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.33.3">The code examples are in the book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08"><span class="No-Break"><span class="koboSpan" id="kobo.35.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.36.1">.</span></span></p>
<h1 id="_idParaDest-149" lang="en-GB"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.37.1">Introducing LLMs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.38.1">In this section, we’ll take a more systematic approach and dive deeper into transformer-based architectures. </span><span class="koboSpan" id="kobo.38.2">As we mentioned in the introduction, the transformer block has changed remarkedly little since its introduction in 2017. </span><span class="koboSpan" id="kobo.38.3">Instead, the main advances have come in terms of larger models and larger training sets. </span><span class="koboSpan" id="kobo.38.4">For example, the original GPT model (GPT-1) has 117M parameters, while GPT-3 (</span><em class="italic"><span class="koboSpan" id="kobo.39.1">Language Models are Few-Shot Learners</span></em><span class="koboSpan" id="kobo.40.1">, </span><a href="https://arxiv.org/abs/2005.14165"><span class="koboSpan" id="kobo.41.1">https://arxiv.org/abs/2005.14165</span></a><span class="koboSpan" id="kobo.42.1">) has 175B, a thousandfold increase. </span><span class="koboSpan" id="kobo.42.2">We can distinguish two informal transformer model categories based </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">on size:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.44.1">Pre-trained language models</span></strong><span class="koboSpan" id="kobo.45.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.46.1">PLMs</span></strong><span class="koboSpan" id="kobo.47.1">): Transformers</span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.48.1"> with fewer parameters, such</span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.49.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.51.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.52.1">BERT</span></strong><span class="koboSpan" id="kobo.53.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">generative pre-trained transformers</span></strong><span class="koboSpan" id="kobo.55.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.56.1">GPT</span></strong><span class="koboSpan" id="kobo.57.1">), fall </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.58.1">into this category. </span><span class="koboSpan" id="kobo.58.2">Starting with BERT, these transformers introduced the two-step pre-training/FT paradigm. </span><span class="koboSpan" id="kobo.58.3">The combination of the attention mechanism</span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.59.1"> and unsupervised pre-training (</span><strong class="bold"><span class="koboSpan" id="kobo.60.1">masked language modeling</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.62.1">MLM</span></strong><span class="koboSpan" id="kobo.63.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">next-word prediction</span></strong><span class="koboSpan" id="kobo.65.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.66.1">NWP</span></strong><span class="koboSpan" id="kobo.67.1">) creates </span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.68.1">effective general-purpose semantic features, which we can use for a number of downstream tasks. </span><span class="koboSpan" id="kobo.68.2">Because of this, PLMs</span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.69.1"> perform better than other </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">natural language processing</span></strong><span class="koboSpan" id="kobo.71.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.72.1">NLP</span></strong><span class="koboSpan" id="kobo.73.1">) algorithms, such as </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">RNNs</span></strong><span class="koboSpan" id="kobo.77.1">). </span><span class="koboSpan" id="kobo.77.2">Combined </span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.78.1">with their highly parallelizable architecture, this has inspired a lot of follow-up work on transformers, which produced improved models and eventually led to the </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">next category.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.80.1">LLMs</span></strong><span class="koboSpan" id="kobo.81.1">: These are </span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.82.1">transformer models with billions of parameters. </span><span class="koboSpan" id="kobo.82.2">LLMs differ qualitatively from PLMs in the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">following ways:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.84.1">Emergent capabilities</span></strong><span class="koboSpan" id="kobo.85.1">: They </span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.86.1">can solve a series of complex tasks, which we will discuss in the </span><em class="italic"><span class="koboSpan" id="kobo.87.1">Emergent abilities of </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.88.1">LLMs</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.89.1"> section</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.90.1">Prompting interface</span></strong><span class="koboSpan" id="kobo.91.1">: LLMs </span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.92.1">can interact with humans with natural language instead of </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">special APIs</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.94.1">Fusion of research and engineering</span></strong><span class="koboSpan" id="kobo.95.1">: The scale of LLMs requires researchers to</span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.96.1"> have strong engineering skills in large-scale data processing and </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">parallel training</span></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.98.1">Today, LLMs are almost exclusively decoder-only models because the main applications of the current LLMs revolve around text generation (for example, chatbots such as ChatGPT). </span><span class="koboSpan" id="kobo.98.2">This has happened at the expense of encoder-only and encoder-decoder architectures. </span><span class="koboSpan" id="kobo.98.3">To better understand why, let’s see how a chatbot works. </span><span class="koboSpan" id="kobo.98.4">It starts with a user-generated message (known</span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.99.1"> as a </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">prompt</span></strong><span class="koboSpan" id="kobo.101.1">). </span><span class="koboSpan" id="kobo.101.2">A prompt is the initial input sequence to the decoder-based model, which generates a response one token at a time. </span><span class="koboSpan" id="kobo.101.3">The response is added back to the input sequence. </span><span class="koboSpan" id="kobo.101.4">A special token separates the prompts and the responses. </span><span class="koboSpan" id="kobo.101.5">Once the LLM generates a response, the user may make another prompt. </span><span class="koboSpan" id="kobo.101.6">In this case, we concatenate the new prompt to the existing sequence and task the LLM to create a new response based on the extended sequence. </span><span class="koboSpan" id="kobo.101.7">The LLM has no mechanism for memorizing the existing chat session other than including it as part of the input sequence. </span><span class="koboSpan" id="kobo.101.8">This process can continue indefinitely. </span><span class="koboSpan" id="kobo.101.9">However, once it reaches the maximum length of the context window, it will start truncating the initial parts of the sequence (we can think of this as a </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">sliding window).</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.103.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.104.1">Parts of this chapter are based on the paper </span><em class="italic"><span class="koboSpan" id="kobo.105.1">A Survey of Large Language Models</span></em><span class="koboSpan" id="kobo.106.1"> (</span><a href="https://arxiv.org/abs/2303.18223"><span class="koboSpan" id="kobo.107.1">https://arxiv.org/abs/2303.18223</span></a><span class="koboSpan" id="kobo.108.1">). </span><span class="koboSpan" id="kobo.108.2">We’ll refer to it simply as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.109.1">the survey</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">.</span></span></p>
<h1 id="_idParaDest-150" lang="en-GB"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.111.1">LLM architecture</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.112.1">In </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.113.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.114.1">, we introduced the </span><strong class="bold"><span class="koboSpan" id="kobo.115.1">multi-head attention</span></strong><span class="koboSpan" id="kobo.116.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.117.1">MHA</span></strong><span class="koboSpan" id="kobo.118.1">) mechanism</span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.119.1"> and the three major transformer variants—encoder-decoder, encoder-only, and decoder-only (we used BERT and GPT as prototypical encoder and </span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.120.1">decoder models). </span><span class="koboSpan" id="kobo.120.2">In this section, we’ll discuss various bits and pieces of the LLM architecture. </span><span class="koboSpan" id="kobo.120.3">Let’s start by focusing our attention (yes—it’s the same old joke) on the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">attention mechanism.</span></span></p>
<h2 id="_idParaDest-151" lang="en-GB"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.122.1">LLM attention variants</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.123.1">The attention we discussed so far is</span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.124.1"> known </span><a id="_idIndexMarker1137"/><span class="koboSpan" id="kobo.125.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">global attention</span></strong><span class="koboSpan" id="kobo.127.1">. </span><span class="koboSpan" id="kobo.127.2">The following diagram displays</span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.128.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.129.1">connectivity matrix</span></strong><span class="koboSpan" id="kobo.130.1"> of a bidirectional global self-attention mechanism (context window with </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">size </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.132.1">n=8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer891">
<span class="koboSpan" id="kobo.134.1"><img alt="Figure 8.1 – Global self-attention with a context window with size n=8" src="image/B19627_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.135.1">Figure 8.1 – Global self-attention with a context window with size n=8</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.136.1">Each row and column represent the full input token sequence, </span><span class="koboSpan" id="kobo.137.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/737.png" style="vertical-align:-0.390em;height:1.032em;width:2.997em"/></span><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">The dotted colored diagonal cells represent the current input token (query), </span><span class="koboSpan" id="kobo.139.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/738.png" style="vertical-align:-0.340em;height:0.932em;width:0.575em"/></span><span class="koboSpan" id="kobo.140.1">. </span><span class="koboSpan" id="kobo.140.2">The uninterrupted colored cells of each column represent all tokens (keys) that </span><span class="koboSpan" id="kobo.141.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/338.png" style="vertical-align:-0.340em;height:0.932em;width:0.548em"/></span><span class="koboSpan" id="kobo.142.1"> can attend to. </span><span class="koboSpan" id="kobo.142.2">For example, </span><span class="koboSpan" id="kobo.143.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/740.png" style="vertical-align:-0.340em;height:0.932em;width:0.668em"/></span><span class="koboSpan" id="kobo.144.1"> attends to all preceding tokens, </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.145.1">[</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.146.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.147.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.148.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.149.1">…</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.150.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.151.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.152.1">4</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.153.1">]</span></span><span class="koboSpan" id="kobo.154.1">, </span><br/><span class="koboSpan" id="kobo.155.1">and all succeeding tokens, </span><span class="koboSpan" id="kobo.156.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/741.png" style="vertical-align:-0.390em;height:1.032em;width:2.951em"/></span><span class="koboSpan" id="kobo.157.1">. </span><span class="koboSpan" id="kobo.157.2">The term </span><em class="italic"><span class="koboSpan" id="kobo.158.1">global</span></em><span class="koboSpan" id="kobo.159.1"> implies that </span><span class="koboSpan" id="kobo.160.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/742.png" style="vertical-align:-0.340em;height:0.932em;width:0.538em"/></span><span class="koboSpan" id="kobo.161.1"> attends to all tokens. </span><span class="koboSpan" id="kobo.161.2">Hence all cells are colored. </span><span class="koboSpan" id="kobo.161.3">As we’ll see in the </span><em class="italic"><span class="koboSpan" id="kobo.162.1">Sparse attention</span></em><span class="koboSpan" id="kobo.163.1"> section, there are attention variants where not all tokens participate. </span><span class="koboSpan" id="kobo.163.2">We’ll denote these tokens with transparent cells. </span><span class="koboSpan" id="kobo.163.3">The diagram depicts bidirectional self-attention, as the query can attend to both preceding (down) and succeeding (up) elements. </span><span class="koboSpan" id="kobo.163.4">The query will only attend to the elements below the current input</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.164.1"> token in the unidirectional case. </span><span class="koboSpan" id="kobo.164.2">For example, </span><span class="koboSpan" id="kobo.165.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/740.png" style="vertical-align:-0.340em;height:0.932em;width:0.668em"/></span><span class="koboSpan" id="kobo.166.1"> will only attend </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">to </span></span><span class="No-Break"><span class="koboSpan" id="kobo.168.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/744.png" style="vertical-align:-0.383em;height:1.025em;width:2.997em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.170.1">As we’ll see, one of the main challenges of the attention mechanism is its time and </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">space complexity.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.172.1">Attention complexity</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.173.1">Despite its advantages, the </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.174.1">attention mechanism (particularly global attention) has </span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.175.1">some drawbacks. </span><span class="koboSpan" id="kobo.175.2">One of them is that space and time complexity increase quadratically with the increase of the context window. </span><span class="koboSpan" id="kobo.175.3">That’s because the mechanism is implemented with the help of matrices and </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">matrix multiplication.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.177.1">Matrix multiplication time complexity</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.178.1">The time complexity of the</span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.179.1"> multiplication of two </span><em class="italic"><span class="koboSpan" id="kobo.180.1">n×n</span></em><span class="koboSpan" id="kobo.181.1"> matrices is </span><span class="koboSpan" id="kobo.182.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/745.png" style="vertical-align:-0.062em;height:0.814em;width:2.384em"/></span><span class="koboSpan" id="kobo.183.1"> because the classic implementation uses three nested loops. </span><span class="koboSpan" id="kobo.183.2">In practice, the algorithm is optimized and is less complex. </span><span class="koboSpan" id="kobo.183.3">For the purposes of this section, we’ll use the complexity of the </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">classic implementation.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.185.1">For example, a context window with size </span><em class="italic"><span class="koboSpan" id="kobo.186.1">n=4</span></em><span class="koboSpan" id="kobo.187.1"> results in </span><em class="italic"><span class="koboSpan" id="kobo.188.1">n×n=4x4</span></em> <strong class="bold"><span class="koboSpan" id="kobo.189.1">Q</span></strong><span class="koboSpan" id="kobo.190.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.191.1">V</span></strong><span class="koboSpan" id="kobo.192.1"> matrices with 16 total cells each. </span><span class="koboSpan" id="kobo.192.2">But a context window of </span><em class="italic"><span class="koboSpan" id="kobo.193.1">n=8</span></em><span class="koboSpan" id="kobo.194.1"> results in </span><em class="italic"><span class="koboSpan" id="kobo.195.1">n×n=8x8</span></em> <strong class="bold"><span class="koboSpan" id="kobo.196.1">Q</span></strong><span class="koboSpan" id="kobo.197.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.198.1">V</span></strong><span class="koboSpan" id="kobo.199.1"> matrices with 64 total cells each. </span><span class="koboSpan" id="kobo.199.2">Therefore, a two-times-larger context window requires four times more memory. </span><span class="koboSpan" id="kobo.199.3">Since the time complexity of matrix multiplication is </span><span class="koboSpan" id="kobo.200.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/746.png" style="vertical-align:-0.062em;height:0.814em;width:2.289em"/></span><span class="koboSpan" id="kobo.201.1">, increasing the context window from </span><em class="italic"><span class="koboSpan" id="kobo.202.1">n=4</span></em><span class="koboSpan" id="kobo.203.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.204.1">n=8</span></em><span class="koboSpan" id="kobo.205.1"> would increase the number of operations from </span><span class="koboSpan" id="kobo.206.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;64&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/747.png" style="vertical-align:-0.012em;height:0.715em;width:3.309em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.207.1">to </span></span><span class="No-Break"><span class="koboSpan" id="kobo.208.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;512&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/748.png" style="vertical-align:-0.012em;height:0.715em;width:3.789em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.210.1">Next, let’s </span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.211.1">focus on the transformer block, where we have a </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">feed-forward network</span></strong><span class="koboSpan" id="kobo.213.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.214.1">FFN</span></strong><span class="koboSpan" id="kobo.215.1">), </span><br/><span class="koboSpan" id="kobo.216.1">multi-head </span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.217.1">self-attention, and four linear projections (</span><strong class="bold"><span class="koboSpan" id="kobo.218.1">fully connected</span></strong><span class="koboSpan" id="kobo.219.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.220.1">FC</span></strong><span class="koboSpan" id="kobo.221.1">) layers)—three for the </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">Q</span></strong><span class="koboSpan" id="kobo.223.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.224.1">K</span></strong><span class="koboSpan" id="kobo.225.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.226.1">V</span></strong><span class="koboSpan" id="kobo.227.1"> pre-attention split and one that combines the attention heads’ outputs. </span><span class="koboSpan" id="kobo.227.2">We’ll discuss each component’s relative weight in the block’s computational load. </span><span class="koboSpan" id="kobo.227.3">Let’s denote the embedding size with </span><span class="koboSpan" id="kobo.228.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/693.png" style="vertical-align:-0.340em;height:1.051em;width:1.862em"/></span><span class="koboSpan" id="kobo.229.1">, the key dimension with </span><span class="koboSpan" id="kobo.230.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/680.png" style="vertical-align:-0.340em;height:1.051em;width:0.746em"/></span><span class="koboSpan" id="kobo.231.1">, the value dimension with </span><span class="koboSpan" id="kobo.232.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/695.png" style="vertical-align:-0.340em;height:1.051em;width:0.711em"/></span><span class="koboSpan" id="kobo.233.1"> (</span><span class="koboSpan" id="kobo.234.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/752.png" style="vertical-align:-0.340em;height:1.051em;width:9.429em"/></span><span class="koboSpan" id="kobo.235.1">), the context window size with </span><em class="italic"><span class="koboSpan" id="kobo.236.1">n</span></em><span class="koboSpan" id="kobo.237.1">, the number of heads with </span><em class="italic"><span class="koboSpan" id="kobo.238.1">h</span></em><span class="koboSpan" id="kobo.239.1">, and the size of the hidden layer in the FFN with </span><em class="italic"><span class="koboSpan" id="kobo.240.1">ffn</span></em><span class="koboSpan" id="kobo.241.1"> (the usual convention is </span><em class="italic"><span class="koboSpan" id="kobo.242.1">ffn=4*d</span></em><span class="koboSpan" id="kobo.243.1">). </span><span class="koboSpan" id="kobo.243.2">The time complexity of the different components is </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">shown here:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.245.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/753.png" style="vertical-align:-0.062em;height:0.823em;width:12.388em"/></span><span class="koboSpan" id="kobo.246.1">: The three input linear projections for </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">all heads</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.248.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/754.png" style="vertical-align:-0.062em;height:0.823em;width:10.901em"/></span><span class="koboSpan" id="kobo.249.1">: The </span><em class="italic"><span class="koboSpan" id="kobo.250.1">h</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.251.1">self-attention heads</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.252.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/755.png" style="vertical-align:-0.062em;height:0.823em;width:10.828em"/></span><span class="koboSpan" id="kobo.253.1">: The fourth output linear projection after the </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">self-attention heads</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.255.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/756.png" style="vertical-align:-0.307em;height:1.068em;width:22.270em"/></span><span class="koboSpan" id="kobo.256.1">: The </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">FFN module</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.258.1">The full combined </span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.259.1">complexity of the block is </span><span class="koboSpan" id="kobo.260.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/757.png" style="vertical-align:-0.062em;height:0.823em;width:7.482em"/></span><span class="koboSpan" id="kobo.261.1">. </span><span class="koboSpan" id="kobo.261.2">We can see that it depends on the ratio between the length of the context window, </span><em class="italic"><span class="koboSpan" id="kobo.262.1">n</span></em><span class="koboSpan" id="kobo.263.1">, and the embedding size, </span><em class="italic"><span class="koboSpan" id="kobo.264.1">d</span></em><span class="koboSpan" id="kobo.265.1">. </span><span class="koboSpan" id="kobo.265.2">If </span><em class="italic"><span class="koboSpan" id="kobo.266.1">d&gt;&gt;n</span></em><span class="koboSpan" id="kobo.267.1">, then the computational time of </span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.268.1">the linear projections will overshadow the time of the attention heads and vice versa. </span><span class="koboSpan" id="kobo.268.2">In practice, </span><em class="italic"><span class="koboSpan" id="kobo.269.1">d&gt;&gt;n</span></em><span class="koboSpan" id="kobo.270.1"> is the most common scenario. </span><span class="koboSpan" id="kobo.270.2">But in either case, the attention mechanism has at least quadratic space and time complexity. </span><span class="koboSpan" id="kobo.270.3">Let’s see some solutions to </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">this challenge.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.272.1">Multi-query and grouped-query attention</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.273.1">MHA branches</span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.274.1"> the input data to multiple heads using </span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.275.1">three linear projections per head. </span><span class="koboSpan" id="kobo.275.2">The following diagram shows two optimizations of </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">this configuration:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer913">
<span class="koboSpan" id="kobo.277.1"><img alt="Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query attention (GQA) (inspired by https://arxiv.org/abs/2305.13245)" src="image/B19627_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.278.1">Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query attention (GQA) (inspired by </span><a href="https://arxiv.org/abs/2305.13245"><span class="koboSpan" id="kobo.279.1">https://arxiv.org/abs/2305.13245</span></a><span class="koboSpan" id="kobo.280.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.281.1">Let’s discuss them (apart from MHA, which we introduced in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.282.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.283.1">):</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.284.1">MQA</span></strong><span class="koboSpan" id="kobo.285.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.286.1">Fast transformer decoding: One write-head is all you need</span></em><span class="koboSpan" id="kobo.287.1">, </span><a href="https://arxiv.org/abs/1911.02150):"><span class="koboSpan" id="kobo.288.1">https://arxiv.org/abs/1911.02150):</span></a><span class="koboSpan" id="kobo.289.1"> The different heads share key and value projections, as opposed to unique projections in MHA. </span><span class="koboSpan" id="kobo.289.2">Since the input sequence is the same as well, all </span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.290.1">heads share the same key-value store and only differ in their queries. </span><span class="koboSpan" id="kobo.290.2">This</span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.291.1"> optimization reduces both the memory and computational requirements, with little </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">performance penalty.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.293.1">GQA</span></strong><span class="koboSpan" id="kobo.294.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.295.1">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</span></em><span class="koboSpan" id="kobo.296.1">, </span><a href="https://arxiv.org/abs/2305.13245):"><span class="koboSpan" id="kobo.297.1">https://arxiv.org/abs/2305.13245):</span></a><span class="koboSpan" id="kobo.298.1"> A hybrid between MHA and MQA, which shares single key and value heads for a </span><em class="italic"><span class="koboSpan" id="kobo.299.1">subgroup</span></em><span class="koboSpan" id="kobo.300.1"> of query</span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.301.1"> heads. </span><span class="koboSpan" id="kobo.301.2">The authors</span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.302.1"> show that GQA is almost as fast as MQA and achieves quality close </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">to MHA.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.304.1">In the next section, we’ll discuss attention optimization, which takes into account the specifics of GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">memory management.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.306.1">FlashAttention</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.307.1">In this section, we’ll introduce</span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.308.1"> FlashAttention (</span><em class="italic"><span class="koboSpan" id="kobo.309.1">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</span></em><span class="koboSpan" id="kobo.310.1">, </span><a href="https://arxiv.org/abs/2205.14135;"><span class="koboSpan" id="kobo.311.1">https://arxiv.org/abs/2205.14135;</span></a> <em class="italic"><span class="koboSpan" id="kobo.312.1">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</span></em><span class="koboSpan" id="kobo.313.1">, </span><a href="https://arxiv.org/abs/2307.08691"><span class="koboSpan" id="kobo.314.1">https://arxiv.org/abs/2307.08691</span></a><span class="koboSpan" id="kobo.315.1">). </span><span class="koboSpan" id="kobo.315.2">This is not a new attention </span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.316.1">mechanism but an implementation of global attention, which considers the specifics of the GPU hardware. </span><span class="koboSpan" id="kobo.316.2">A GPU has a large number of computational cores that can perform relatively simple but highly parallelizable operations (such as matrix multiplication). </span><span class="koboSpan" id="kobo.316.3">It has two memory levels: small but fast cache (L1 and L2) and large but relatively slow </span><strong class="bold"><span class="koboSpan" id="kobo.317.1">high bandwidth memory</span></strong><span class="koboSpan" id="kobo.318.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.319.1">HBM</span></strong><span class="koboSpan" id="kobo.320.1">). </span><span class="koboSpan" id="kobo.320.2">To perform an operation, it transfers the necessary data from</span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.321.1"> the HBM to the cache. </span><span class="koboSpan" id="kobo.321.2">The cores use the cache for their calculations. </span><span class="koboSpan" id="kobo.321.3">Once the operation is done, the result is stored back in the HBM. </span><span class="koboSpan" id="kobo.321.4">The main bottleneck in this pipeline is the data transfers rather than the actual computation (the fewer data transfers, </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">the better).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.323.1">Next, let’s focus on the attention block, which has five operations: 1) matrix multiplication (</span><span class="koboSpan" id="kobo.324.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/758.png" style="vertical-align:-0.168em;height:0.886em;width:2.021em"/></span><span class="koboSpan" id="kobo.325.1">), 2) mask, 3) softmax, 4) dropout, and 5) matrix multiplication (</span><strong class="bold"><span class="koboSpan" id="kobo.326.1">V</span></strong><span class="koboSpan" id="kobo.327.1">). </span><span class="koboSpan" id="kobo.327.2">The standard implementation performs the operations sequentially, starting with the first matrix multiplication. </span><span class="koboSpan" id="kobo.327.3">Once it’s done, it proceeds with the mask, and so on. </span><span class="koboSpan" id="kobo.327.4">Each operation involves two-way </span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.328.1">data transfer between the HBM and the cache. </span><span class="koboSpan" id="kobo.328.2">These transfers are unnecessary because the results of operation </span><em class="italic"><span class="koboSpan" id="kobo.329.1">i</span></em><span class="koboSpan" id="kobo.330.1"> are transferred from the cache to the HBM just to be sent back from the HBM to the cache for operation </span><em class="italic"><span class="koboSpan" id="kobo.331.1">i+1</span></em><span class="koboSpan" id="kobo.332.1">. </span><span class="koboSpan" id="kobo.332.2">FlashAttention </span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.333.1">proposes a special </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">fused kernel</span></strong><span class="koboSpan" id="kobo.335.1"> to solve the inefficiency. </span><span class="koboSpan" id="kobo.335.2">It splits the </span><strong class="bold"><span class="koboSpan" id="kobo.336.1">Q</span></strong><span class="koboSpan" id="kobo.337.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.338.1">K</span></strong><span class="koboSpan" id="kobo.339.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.340.1">V</span></strong><span class="koboSpan" id="kobo.341.1"> matrices into smaller</span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.342.1"> blocks that can fit in the cache. </span><span class="koboSpan" id="kobo.342.2">Once these blocks are transferred there, the fused kernel performs all five operations without intermediate data transfers. </span><span class="koboSpan" id="kobo.342.3">Only the final result is sent back to the HBM. </span><span class="koboSpan" id="kobo.342.4">Splitting the matrices into blocks is possible because matrix multiplication is embarrassingly parallel. </span><span class="koboSpan" id="kobo.342.5">But the other innovation of FlashAttention is the ability to split the softmax operation, which isn’t as trivial (we won’t go into details about how it’s implemented). </span><span class="koboSpan" id="kobo.342.6">The operation is done once all matrix blocks pass through </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">this pipeline.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.344.1">Splitting matrix multiplication</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.345.1">Let’s say we want to multiply the matrices </span><strong class="bold"><span class="koboSpan" id="kobo.346.1">A</span></strong><span class="koboSpan" id="kobo.347.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.348.1">B</span></strong><span class="koboSpan" id="kobo.349.1">. </span><span class="koboSpan" id="kobo.349.2">Because of the way matrix multiplication works, we can split </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">B</span></strong><span class="koboSpan" id="kobo.351.1"> by column into two matrices, </span><span class="koboSpan" id="kobo.352.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/759.png" style="vertical-align:-0.333em;height:0.982em;width:0.972em"/></span><span class="koboSpan" id="kobo.353.1"> and </span><span class="koboSpan" id="kobo.354.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/760.png" style="vertical-align:-0.333em;height:0.982em;width:0.973em"/></span><span class="koboSpan" id="kobo.355.1">. </span><span class="koboSpan" id="kobo.355.2">Then, we perform two matrix multiplications on each device: </span><span class="koboSpan" id="kobo.356.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/761.png" style="vertical-align:-0.333em;height:0.997em;width:1.824em"/></span><span class="koboSpan" id="kobo.357.1"> and </span><span class="koboSpan" id="kobo.358.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/762.png" style="vertical-align:-0.333em;height:0.997em;width:1.825em"/></span><span class="koboSpan" id="kobo.359.1">. </span><span class="koboSpan" id="kobo.359.2">Finally, we concatenate the output of the two operations in a single matrix, equivalent to the matrix produced by the original </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">multiplication, </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.361.1">AB</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.363.1">In the next section, we’ll discuss solving the performance issue with new </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">attention mechanisms.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.365.1">Sparse attention</span></h3>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.366.1">Sparse attention</span></strong><span class="koboSpan" id="kobo.367.1"> is a </span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.368.1">class of methods where the output vector attends to </span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.369.1">a subset of all key vectors instead of the entire context window. </span><span class="koboSpan" id="kobo.369.2">For example, if we can attend to four vectors of interest from the entire eight-vector context, we could reduce the necessary </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">computations twice.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.371.1">The following diagram displays three bidirectional sparse </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">attention mechanisms:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer919">
<span class="koboSpan" id="kobo.373.1"><img alt="Figure 8.3 – Left: local attention; center: dilated local attention; right: random attention; context window size n=12" src="image/B19627_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.374.1">Figure 8.3 – Left: local attention; center: dilated local attention; right: random attention; context window size n=12</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.375.1">The mechanisms follow the </span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.376.1">same notation as the ones in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.378.1">.2</span></em><span class="koboSpan" id="kobo.379.1">, with one addition—the</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.380.1"> transparent cells represent tokens (keys), which the query doesn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">attend to.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.382.1">On the left, we</span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.383.1"> have </span><a id="_idIndexMarker1164"/><span class="koboSpan" id="kobo.384.1">bidirectional </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">local attention</span></strong><span class="koboSpan" id="kobo.386.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">sliding window attention</span></strong><span class="koboSpan" id="kobo.388.1">), first introduced in </span><em class="italic"><span class="koboSpan" id="kobo.389.1">Image Transformer</span></em><span class="koboSpan" id="kobo.390.1">, </span><a href="https://arxiv.org/abs/1802.05751"><span class="koboSpan" id="kobo.391.1">https://arxiv.org/abs/1802.05751</span></a><span class="koboSpan" id="kobo.392.1">). </span><span class="koboSpan" id="kobo.392.2">The query attends to a limited context window of the nearest </span><em class="italic"><span class="koboSpan" id="kobo.393.1">w</span></em><span class="koboSpan" id="kobo.394.1"> keys around the current token (½</span><em class="italic"><span class="koboSpan" id="kobo.395.1">w</span></em><span class="koboSpan" id="kobo.396.1"> to the left and ½</span><em class="italic"><span class="koboSpan" id="kobo.397.1">w</span></em><span class="koboSpan" id="kobo.398.1"> to the right). </span><span class="koboSpan" id="kobo.398.2">The self-attention block still takes the full </span><em class="italic"><span class="koboSpan" id="kobo.399.1">n</span></em><span class="koboSpan" id="kobo.400.1">-sized sequence as input, but each token attends to a limited </span><em class="italic"><span class="koboSpan" id="kobo.401.1">w</span></em><span class="koboSpan" id="kobo.402.1">-sized local context. </span><span class="koboSpan" id="kobo.402.2">This way, the memory footprint is the same as global attention, but the time complexity is reduced to </span><span class="koboSpan" id="kobo.403.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/763.png" style="vertical-align:-0.062em;height:0.823em;width:5.269em"/></span><span class="koboSpan" id="kobo.404.1"> instead </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.406.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/764.png" style="vertical-align:-0.062em;height:0.823em;width:5.131em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.408.1">To understand why local attention </span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.409.1">works, let’s return to </span><strong class="bold"><span class="koboSpan" id="kobo.410.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.411.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.412.1">CNNs</span></strong><span class="koboSpan" id="kobo.413.1">). </span><span class="koboSpan" id="kobo.413.2">Recall that the earlier layers of a CNN have small receptive fields and capture smaller, simpler features. </span><span class="koboSpan" id="kobo.413.3">Conversely, the deeper CNN layers have large receptive fields that capture larger and more complex features. </span><span class="koboSpan" id="kobo.413.4">We can apply the same principle to transformers. </span><span class="koboSpan" id="kobo.413.5">Research has shown that the initial transformer blocks learn simple token features and local syntax, while the deeper layers learn more complex context-dependent aspects of token semantics. </span><span class="koboSpan" id="kobo.413.6">Because of this, we can apply local attention to the earlier transformer blocks and reserve global attention for the deeper ones without </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">sacrificing performance.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.415.1">Dilated attention</span></strong><span class="koboSpan" id="kobo.416.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.417.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.418.1">.3</span></em><span class="koboSpan" id="kobo.419.1">, center) is a </span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.420.1">modification of local attention, which works in a similar way to the dilated convolutions we introduced in </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.421.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.422.1">. </span><span class="koboSpan" id="kobo.422.2">Unlike local attention, here, the context window is not continuous. </span><span class="koboSpan" id="kobo.422.3">Instead, there is a gap of </span><em class="italic"><span class="koboSpan" id="kobo.423.1">g</span></em><span class="koboSpan" id="kobo.424.1"> cells (which could be more than one) between each context token. </span><span class="koboSpan" id="kobo.424.2">This makes it possible to attend to a wider context with the same </span><em class="italic"><span class="koboSpan" id="kobo.425.1">n</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.426.1">of computations.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.427.1">Next, we have bidirectional </span><strong class="bold"><span class="koboSpan" id="kobo.428.1">random attention</span></strong><span class="koboSpan" id="kobo.429.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.430.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.431.1">.3</span></em><span class="koboSpan" id="kobo.432.1">, right), where the current query (token) attends to a </span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.433.1">subset of </span><em class="italic"><span class="koboSpan" id="kobo.434.1">r</span></em><span class="koboSpan" id="kobo.435.1"> keys (tokens) from the full context window. </span><span class="koboSpan" id="kobo.435.2">The time complexity is reduced to </span><span class="koboSpan" id="kobo.436.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/765.png" style="vertical-align:-0.062em;height:0.823em;width:4.988em"/></span><span class="koboSpan" id="kobo.437.1"> instead of </span><span class="koboSpan" id="kobo.438.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/766.png" style="vertical-align:-0.062em;height:0.823em;width:5.157em"/></span><span class="koboSpan" id="kobo.439.1">. </span><span class="koboSpan" id="kobo.439.2">The attention pattern can be viewed as a directed graph. </span><span class="koboSpan" id="kobo.439.3">In the case of random attention, this graph is also random. </span><span class="koboSpan" id="kobo.439.4">That is, the information can flow rapidly between any pair of nodes without considering the actual structure of the data, which might </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">be biased.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.441.1">It is also</span><a id="_idIndexMarker1168"/><span class="koboSpan" id="kobo.442.1"> possible to combine global and local attention. </span><span class="koboSpan" id="kobo.442.2">One such example is </span><strong class="bold"><span class="koboSpan" id="kobo.443.1">Longformer</span></strong><span class="koboSpan" id="kobo.444.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.445.1">Longformer: The Long-Document Transformer</span></em><span class="koboSpan" id="kobo.446.1">, </span><a href="https://arxiv.org/abs/2004.05150"><span class="koboSpan" id="kobo.447.1">https://arxiv.org/abs/2004.05150</span></a><span class="koboSpan" id="kobo.448.1">), displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer924">
<span class="koboSpan" id="kobo.450.1"><img alt="Figure 8.4 – Combined local and global attention; left: Longformer block; right: Big Bird block" src="image/B19627_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.451.1">Figure 8.4 – Combined local and global attention; left: Longformer block; right: Big Bird block</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.452.1">It introduces a </span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.453.1">drop-in replacement self-attention block in an otherwise </span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.454.1">unmodified transformer model. </span><span class="koboSpan" id="kobo.454.2">The block represents a combination of global and local (or dilated) attention. </span><span class="koboSpan" id="kobo.454.3">It applies local attention to most input tokens, but a few can use global attention. </span><span class="koboSpan" id="kobo.454.4">The left section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.455.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.456.1">.4</span></em><span class="koboSpan" id="kobo.457.1"> shows the combined self-attention block and one example of input tokens that apply local and global attention. </span><span class="koboSpan" id="kobo.457.2">More specifically, the authors use the Longformer block in a unidirectional BERT-style model to solve MLM</span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.458.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.459.1">question-answering</span></strong><span class="koboSpan" id="kobo.460.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.461.1">QA</span></strong><span class="koboSpan" id="kobo.462.1">) tasks (</span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.463.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.464.1">). </span><span class="koboSpan" id="kobo.464.2">They only apply global attention to special tokens such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">[CLS]</span></strong><span class="koboSpan" id="kobo.466.1"> in MLM tasks. </span><span class="koboSpan" id="kobo.466.2">As the diagram shows, global attention works in both directions. </span><span class="koboSpan" id="kobo.466.3">The special token can attend to all other tokens, but the other tokens can also attend to the special token in addition to their local attention context. </span><span class="koboSpan" id="kobo.466.4">In the case of autoregressive language modeling (unidirectional model), they apply only dilated local attention, as there are no tokens with special significance. </span><span class="koboSpan" id="kobo.466.5">The full Longformer model uses dilated attention with a larger context window and </span><em class="italic"><span class="koboSpan" id="kobo.467.1">g</span></em><span class="koboSpan" id="kobo.468.1"> in the deeper layers, leaving the earlier ones with only </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">local attention.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.470.1">Big Bird</span></strong><span class="koboSpan" id="kobo.471.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.472.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.473.1">.4</span></em><span class="koboSpan" id="kobo.474.1">, right; </span><em class="italic"><span class="koboSpan" id="kobo.475.1">Big Bird: Transformers for Longer Sequences</span></em><span class="koboSpan" id="kobo.476.1">, </span><a href="https://arxiv.org/abs/2007.14062"><span class="koboSpan" id="kobo.477.1">https://arxiv.org/abs/2007.14062</span></a><span class="koboSpan" id="kobo.478.1">) is similar to </span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.479.1">Longformer but adds </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">random attention.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.481.1">Next, let’s discuss the </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">sparse transformer</span></strong><span class="koboSpan" id="kobo.483.1"> attention</span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.484.1"> developed by OpenAI (</span><em class="italic"><span class="koboSpan" id="kobo.485.1">Generating Long Sequences with Sparse Transformers</span></em><span class="koboSpan" id="kobo.486.1">, </span><a href="https://arxiv.org/abs/1904.10509"><span class="koboSpan" id="kobo.487.1">https://arxiv.org/abs/1904.10509</span></a><span class="koboSpan" id="kobo.488.1">). </span><span class="koboSpan" id="kobo.488.2">A sparse transformer introduces unidirectional strided and fixed attention schemes, displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer925">
<span class="koboSpan" id="kobo.490.1"><img alt="Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse attention; input image size 4×4; sequence length n=12 (inspired by https://arxiv.org/abs/1904.10509)" src="image/B19627_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.491.1">Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse attention; input image size 4×4; sequence length n=12 (inspired by </span><a href="https://arxiv.org/abs/1904.10509"><span class="koboSpan" id="kobo.492.1">https://arxiv.org/abs/1904.10509</span></a><span class="koboSpan" id="kobo.493.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.494.1">To</span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.495.1"> understand how they work, let’s discuss the context of the </span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.496.1">paper. </span><span class="koboSpan" id="kobo.496.2">It proposes a unified decoder-only model to generate new images, text, or audio. </span><span class="koboSpan" id="kobo.496.3">Depending on the use case, the input and output data can be a two-dimensional image tensor (we’ll omit the color dimension for simplicity). </span><span class="koboSpan" id="kobo.496.4">However, the transformer accepts as input a one-dimensional sequence. </span><span class="koboSpan" id="kobo.496.5">We can solve this by concatenating the rows of the image in a single one-dimensional tensor. </span><span class="koboSpan" id="kobo.496.6">Once done, we can treat the image like a regular sequence and feed it to the model. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.497.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.498.1">.5</span></em><span class="koboSpan" id="kobo.499.1"> displays a strided (left) and fixed attention (right) connectivity matrix for a two-dimensional image (top) and its equivalent concatenated one-dimensional sequence (bottom). </span><span class="koboSpan" id="kobo.499.2">Let’s note that the bottom expanded sequence doesn’t match the dimensions of the top image—it should be with length </span><em class="italic"><span class="koboSpan" id="kobo.500.1">n=16</span></em><span class="koboSpan" id="kobo.501.1">, which reflects the 4×4 image, instead of </span><em class="italic"><span class="koboSpan" id="kobo.502.1">n=12</span></em><span class="koboSpan" id="kobo.503.1"> as it is now. </span><span class="koboSpan" id="kobo.503.2">Since this is a generative decoder-only model, it uses unidirectional attention, even though the concept of direction doesn’t exist in the same way in images as </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">in text.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.505.1">Next, let’s discuss the two attention schemes. </span><span class="koboSpan" id="kobo.505.2">We’ll start with strided attention, where the current token attends to the preceding row and column of the input image. </span><span class="koboSpan" id="kobo.505.3">These are two separate mechanisms split between different </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">attention heads:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.507.1">Row head</span></strong><span class="koboSpan" id="kobo.508.1">: Equivalent </span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.509.1">to unidirectional local attention, which attends to the previous </span><span class="koboSpan" id="kobo.510.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/767.png" style="vertical-align:-0.052em;height:0.763em;width:2.953em"/></span><span class="koboSpan" id="kobo.511.1"> tokens, where </span><span class="koboSpan" id="kobo.512.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msqrt&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/768.png" style="vertical-align:-0.052em;height:0.670em;width:1.158em"/></span><span class="koboSpan" id="kobo.513.1"> is the length of one entire row of the 2D input image. </span><span class="koboSpan" id="kobo.513.2">Let’s denote the index of the current input token with </span><em class="italic"><span class="koboSpan" id="kobo.514.1">i</span></em><span class="koboSpan" id="kobo.515.1"> and the tokens it attends to with </span><em class="italic"><span class="koboSpan" id="kobo.516.1">j</span></em><span class="koboSpan" id="kobo.517.1">. </span><span class="koboSpan" id="kobo.517.2">We can summarize the row mechanism in the </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">following way:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.519.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/769.png" style="vertical-align:-0.307em;height:1.018em;width:4.071em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.520.1">Column head</span></strong><span class="koboSpan" id="kobo.521.1">: Equivalent to</span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.522.1"> unidirectional dilated attention with a stride (gap) of </span><span class="koboSpan" id="kobo.523.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/770.png" style="vertical-align:-0.052em;height:0.763em;width:2.902em"/></span><span class="koboSpan" id="kobo.524.1"> (the same as the row head). </span><span class="koboSpan" id="kobo.524.2">Assuming that the input image is square, the column head jumps the equivalent of one row (</span><span class="koboSpan" id="kobo.525.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msqrt&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/768.png" style="vertical-align:-0.052em;height:0.670em;width:1.154em"/></span><span class="koboSpan" id="kobo.526.1">) and attends to a location representing the previous cell in a virtual column of the one-dimensional sequence. </span><span class="koboSpan" id="kobo.526.2">We can summarize column strided attention in the </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">following way:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.528.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mtext&gt;mod&lt;/mtext&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/772.png" style="vertical-align:-0.307em;height:1.018em;width:6.895em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.529.1">This scheme performs best for 2D input data, such as images, because the row/column split reflects the underlying data structure. </span><span class="koboSpan" id="kobo.529.2">The time complexity of this scheme </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">is </span></span><span class="No-Break"><span class="koboSpan" id="kobo.531.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/773.png" style="vertical-align:-0.102em;height:0.863em;width:12.153em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.533.1">Next, we have </span><strong class="bold"><span class="koboSpan" id="kobo.534.1">fixed attention</span></strong><span class="koboSpan" id="kobo.535.1">, which </span><a id="_idIndexMarker1178"/><span class="koboSpan" id="kobo.536.1">attends to a fixed column and the elements after the latest column element. </span><span class="koboSpan" id="kobo.536.2">It performs better on non-periodic data, such as text. </span><span class="koboSpan" id="kobo.536.3">Once again, this is a combination of two separate mechanisms split between </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">different heads:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.538.1">Column head</span></strong><span class="koboSpan" id="kobo.539.1">: Attends to a</span><a id="_idIndexMarker1179"/><span class="koboSpan" id="kobo.540.1"> fixed column, which doesn’t necessarily match the column of the current input token, </span><span class="koboSpan" id="kobo.541.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/774.png" style="vertical-align:-0.340em;height:0.932em;width:0.515em"/></span><span class="koboSpan" id="kobo.542.1">. </span><span class="koboSpan" id="kobo.542.2">Multiple input tokens attend to the same column, which makes it possible to attend to the entire length of the sequence. </span><span class="koboSpan" id="kobo.542.3">We can summarize the column mechanism in the </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">following way:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.544.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;mod&lt;/mtext&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/775.png" style="vertical-align:-0.257em;height:0.968em;width:7.868em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.545.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.546.1">c</span></em><span class="koboSpan" id="kobo.547.1"> is a parameter (8, 16, or 32). </span><span class="koboSpan" id="kobo.547.2">For example, if </span><em class="italic"><span class="koboSpan" id="kobo.548.1">l=64</span></em><span class="koboSpan" id="kobo.549.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.550.1">c=16</span></em><span class="koboSpan" id="kobo.551.1">, then all positions greater than 64 can attend to positions 48-64, all positions greater than 128 can attend to 112-128, and </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">so on.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.553.1">Row head</span></strong><span class="koboSpan" id="kobo.554.1">: The first </span><a id="_idIndexMarker1180"/><span class="koboSpan" id="kobo.555.1">head is similar to the row head in strided attention. </span><span class="koboSpan" id="kobo.555.2">But instead of attending to the length of one entire row, it only attends to the location of the current column head. </span><span class="koboSpan" id="kobo.555.3">The row head provides local context. </span><span class="koboSpan" id="kobo.555.4">We can summarize it in the </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">following way:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.557.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/776.png" style="vertical-align:-0.563em;height:1.806em;width:8.198em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.558.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.559.1">floor</span></em><span class="koboSpan" id="kobo.560.1"> rounds down the result of the division to the nearest </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">whole number.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.562.1">Next, let’s focus our attention (I can’t stop myself) on a special case of decoder-only architecture and various aspects of </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">LLM architecture.</span></span></p>
<h2 id="_idParaDest-152" lang="en-GB"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.564.1">Prefix decoder</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.565.1">In this </span><a id="_idIndexMarker1181"/><span class="koboSpan" id="kobo.566.1">section, we’ll </span><a id="_idIndexMarker1182"/><span class="koboSpan" id="kobo.567.1">introduce </span><strong class="bold"><span class="koboSpan" id="kobo.568.1">prefix</span></strong><span class="koboSpan" id="kobo.569.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.570.1">non-causal</span></strong><span class="koboSpan" id="kobo.571.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.572.1">decoder</span></strong><span class="koboSpan" id="kobo.573.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.574.1">Unified Language Model Pre-training for Natural Language Understanding and Generation</span></em><span class="koboSpan" id="kobo.575.1">, </span><a href="https://arxiv.org/abs/1905.03197"><span class="koboSpan" id="kobo.576.1">https://arxiv.org/abs/1905.03197</span></a><span class="koboSpan" id="kobo.577.1">). </span><span class="koboSpan" id="kobo.577.2">This is a decoder-only model that introduces a new type of attention pattern, displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">following diagram:</span></span></p>
<p class="IMG---Figure" lang="en-GB"><span class="koboSpan" id="kobo.579.1"><img alt="Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)" src="image/B19627_08_6.png"/></span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.580.1">Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.581.1">We split the</span><a id="_idIndexMarker1183"/><span class="koboSpan" id="kobo.582.1"> input sequence into two segments—</span><span class="koboSpan" id="kobo.583.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/777.png" style="vertical-align:-0.333em;height:0.925em;width:0.684em"/></span><span class="koboSpan" id="kobo.584.1"> through </span><span class="koboSpan" id="kobo.585.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/778.png" style="vertical-align:-0.333em;height:0.925em;width:0.684em"/></span><span class="koboSpan" id="kobo.586.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.587.1">source</span></strong><span class="koboSpan" id="kobo.588.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">prefix</span></strong><span class="koboSpan" id="kobo.590.1">), and </span><span class="koboSpan" id="kobo.591.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/779.png" style="vertical-align:-0.340em;height:0.932em;width:0.684em"/></span><span class="koboSpan" id="kobo.592.1"> through </span><span class="koboSpan" id="kobo.593.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/780.png" style="vertical-align:-0.340em;height:0.932em;width:0.679em"/></span><span class="koboSpan" id="kobo.594.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.595.1">target</span></strong><span class="koboSpan" id="kobo.596.1">). </span><span class="koboSpan" id="kobo.596.2">The tokens of the source</span><a id="_idIndexMarker1184"/><span class="koboSpan" id="kobo.597.1"> segment have bidirectional access to all other tokens of that segment. </span><span class="koboSpan" id="kobo.597.2">However, the target segment tokens have unidirectional access to the preceding tokens of the whole (source and target) input sequence. </span><span class="koboSpan" id="kobo.597.3">For example, </span><span class="koboSpan" id="kobo.598.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/781.png" style="vertical-align:-0.340em;height:0.932em;width:0.682em"/></span><span class="koboSpan" id="kobo.599.1"> is part of the source segment and can attend to </span><br/><span class="koboSpan" id="kobo.600.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/782.png" style="vertical-align:-0.333em;height:0.925em;width:0.679em"/></span><span class="koboSpan" id="kobo.601.1">, </span><span class="koboSpan" id="kobo.602.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/783.png" style="vertical-align:-0.333em;height:0.925em;width:0.679em"/></span><span class="koboSpan" id="kobo.603.1">, and </span><span class="koboSpan" id="kobo.604.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/784.png" style="vertical-align:-0.333em;height:0.925em;width:0.679em"/></span><span class="koboSpan" id="kobo.605.1">. </span><span class="koboSpan" id="kobo.605.2">Conversely, </span><span class="koboSpan" id="kobo.606.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;7&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/785.png" style="vertical-align:-0.340em;height:0.932em;width:0.679em"/></span><span class="koboSpan" id="kobo.607.1"> is part of the target and can only attend to tokens </span><span class="koboSpan" id="kobo.608.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/782.png" style="vertical-align:-0.333em;height:0.925em;width:0.679em"/></span><span class="koboSpan" id="kobo.609.1"> through </span><span class="koboSpan" id="kobo.610.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/787.png" style="vertical-align:-0.340em;height:0.932em;width:0.680em"/></span><span class="koboSpan" id="kobo.611.1"> (but not </span><span class="koboSpan" id="kobo.612.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/788.png" style="vertical-align:-0.340em;height:0.932em;width:0.655em"/></span><span class="koboSpan" id="kobo.613.1">). </span><br/><span class="koboSpan" id="kobo.614.1">The prefix decoder is a hybrid between encoder-decoder and decoder models. </span><span class="koboSpan" id="kobo.614.2">The source segment acts as an encoder, and the target acts as a decoder, yet the underlying architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">is decoder-based.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.616.1">We can use the prefix </span><a id="_idIndexMarker1185"/><span class="koboSpan" id="kobo.617.1">decoder for </span><strong class="bold"><span class="koboSpan" id="kobo.618.1">sequence-to-sequence</span></strong><span class="koboSpan" id="kobo.619.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.620.1">seq2seq</span></strong><span class="koboSpan" id="kobo.621.1">) tasks, such as machine translation or text summarization, for which we would normally use a full encoder-decoder. </span><span class="koboSpan" id="kobo.621.2">To do so, we concatenate the input and output sequence using special start-of-sequence (</span><strong class="source-inline"><span class="koboSpan" id="kobo.622.1">[SOS]</span></strong><span class="koboSpan" id="kobo.623.1">) and end-of-sequence (</span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">[EOS]</span></strong><span class="koboSpan" id="kobo.625.1">) tokens. </span><span class="koboSpan" id="kobo.625.2">For example, let’s take the text summarization task. </span><span class="koboSpan" id="kobo.625.3">We represent the text sequence to summarize (</span><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">S1</span></strong><span class="koboSpan" id="kobo.627.1">) and its summarization (</span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">S2</span></strong><span class="koboSpan" id="kobo.629.1">) as a single sequence: </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">[[SOS],S1,[EOS],S2,[EOS]]</span></strong><span class="koboSpan" id="kobo.631.1">. </span><span class="koboSpan" id="kobo.631.2">The source sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.632.1">[[SOS],S1,[EOS]]</span></strong><span class="koboSpan" id="kobo.633.1">, falls within the bidirectional part of the attention pattern, and the target sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">[S2,[EOS]]</span></strong><span class="koboSpan" id="kobo.635.1">, falls within the unidirectional one. </span><span class="koboSpan" id="kobo.635.2">We pre-train the model with the help of MLM, where we mask random tokens from the full sequence. </span><span class="koboSpan" id="kobo.635.3">We fine-tune the model by randomly masking some tokens in the target sequence and learning to recover the masked words. </span><span class="koboSpan" id="kobo.635.4">Let’s note that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">[EOS]</span></strong><span class="koboSpan" id="kobo.637.1"> token can also participate in the masking. </span><span class="koboSpan" id="kobo.637.2">In this way, the model learns when to generate </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">[EOS]</span></strong><span class="koboSpan" id="kobo.639.1"> tokens and terminate the generation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">target sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.641.1">Next, let’s get into more details about various aspects of the </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">LLM architecture.</span></span></p>
<h2 id="_idParaDest-153" lang="en-GB"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.643.1">Transformer nuts and bolts</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.644.1">The following</span><a id="_idIndexMarker1186"/><span class="koboSpan" id="kobo.645.1"> table provides a detailed summary of the main transformer network configurations and </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">their variants:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer949">
<span class="koboSpan" id="kobo.647.1"><img alt="Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)" src="image/B19627_08_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.648.1">Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.649.1">We’re already familiar with many of these—we introduced the three different normalization positions in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.650.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.651.1">. </span><span class="koboSpan" id="kobo.651.2">We also introduced two of the three normalization methods </span><a id="_idIndexMarker1187"/><span class="koboSpan" id="kobo.652.1">in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.653.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.654.1">. </span><span class="koboSpan" id="kobo.654.2">By default, most transformers use </span><strong class="bold"><span class="koboSpan" id="kobo.655.1">layer normalization</span></strong><span class="koboSpan" id="kobo.656.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.657.1">LN</span></strong><span class="koboSpan" id="kobo.658.1">). </span><span class="koboSpan" id="kobo.658.2">However, some models</span><a id="_idIndexMarker1188"/><span class="koboSpan" id="kobo.659.1"> use </span><strong class="bold"><span class="koboSpan" id="kobo.660.1">RMSNorm</span></strong><span class="koboSpan" id="kobo.661.1"> because of </span><a id="_idIndexMarker1189"/><span class="koboSpan" id="kobo.662.1">its superior training speed and performance. </span><span class="koboSpan" id="kobo.662.2">Last but not least, </span><strong class="bold"><span class="koboSpan" id="kobo.663.1">DeepNorm</span></strong><span class="koboSpan" id="kobo.664.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.665.1">DeepNet: Scaling Transformers to 1,000 Layers</span></em><span class="koboSpan" id="kobo.666.1">, </span><a href="https://arxiv.org/abs/2203.00555"><span class="koboSpan" id="kobo.667.1">https://arxiv.org/abs/2203.00555</span></a><span class="koboSpan" id="kobo.668.1">) is new to us. </span><span class="koboSpan" id="kobo.668.2">As the paper’s name suggests, this normalization helped build a 1,000-layer transformer. </span><span class="koboSpan" id="kobo.668.3">The authors argue that in pre-</span><strong class="bold"><span class="koboSpan" id="kobo.669.1">layer normalization</span></strong><span class="koboSpan" id="kobo.670.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.671.1">pre-ln</span></strong><span class="koboSpan" id="kobo.672.1">) architectures, the</span><a id="_idIndexMarker1190"/><span class="koboSpan" id="kobo.673.1"> gradients at the bottom layers tend to be larger than the ones at the top layers, degrading the performance</span><a id="_idIndexMarker1191"/><span class="koboSpan" id="kobo.674.1"> compared to </span><strong class="bold"><span class="koboSpan" id="kobo.675.1">post-layer normalization</span></strong><span class="koboSpan" id="kobo.676.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.677.1">post-ln</span></strong><span class="koboSpan" id="kobo.678.1">) models. </span><span class="koboSpan" id="kobo.678.2">On the other hand, post-ln models are unstable due to exploding gradients. </span><span class="koboSpan" id="kobo.678.3">To overcome this, they propose a simple yet effective normalization of the </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">residual connections:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.680.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;LayerNorm&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mtext&gt;SubLayer&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/789.png" style="vertical-align:-0.307em;height:1.067em;width:12.301em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.681.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.682.1">α</span></em><span class="koboSpan" id="kobo.683.1"> is a constant applied at the output of the residual connection. </span><span class="koboSpan" id="kobo.683.2">Its value depends on the transformer type (encoder or decoder) and the model depth (number of blocks). </span><span class="koboSpan" id="kobo.683.3">The theoretical justification of DeepNorm is that it bounds the model update by </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">that constant.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.685.1">Next, let’s discuss </span><a id="_idIndexMarker1192"/><span class="koboSpan" id="kobo.686.1">the activation functions. </span><span class="koboSpan" id="kobo.686.2">More specifically, we’ll discuss the activation function (</span><strong class="source-inline"><span class="koboSpan" id="kobo.687.1">ActivationFunc</span></strong><span class="koboSpan" id="kobo.688.1">) of the first layer of the </span><strong class="bold"><span class="koboSpan" id="kobo.689.1">feed-forward network</span></strong><span class="koboSpan" id="kobo.690.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.691.1">FFN</span></strong><span class="koboSpan" id="kobo.692.1">) sublayer, as this is the only explicit activation in the transformer block. </span><span class="koboSpan" id="kobo.692.2">As a reminder, we can define the original FFN </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.694.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;FFN&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;Activation&lt;/mml:mtext&gt;&lt;mml:mtext&gt;Func&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/790.png" style="vertical-align:-0.383em;height:1.144em;width:18.570em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.695.1">We</span><a id="_idIndexMarker1193"/><span class="koboSpan" id="kobo.696.1"> discussed </span><a id="_idIndexMarker1194"/><span class="koboSpan" id="kobo.697.1">most activations in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.698.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.699.1">, except for </span><strong class="bold"><span class="koboSpan" id="kobo.700.1">SwiGLU</span></strong><span class="koboSpan" id="kobo.701.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.702.1">GeGLU</span></strong><span class="koboSpan" id="kobo.703.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.704.1">GLU Variants Improve Transformer</span></em><span class="koboSpan" id="kobo.705.1">, </span><a href="https://arxiv.org/abs/2002.05202"><span class="koboSpan" id="kobo.706.1">https://arxiv.org/abs/2002.05202</span></a><span class="koboSpan" id="kobo.707.1">). </span><span class="koboSpan" id="kobo.707.2">They are variations</span><a id="_idIndexMarker1195"/><span class="koboSpan" id="kobo.708.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.709.1">Gated Linear Unit</span></strong><span class="koboSpan" id="kobo.710.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.711.1">GLU</span></strong><span class="koboSpan" id="kobo.712.1">, </span><em class="italic"><span class="koboSpan" id="kobo.713.1">Language Modeling with Gated Convolutional Networks</span></em><span class="koboSpan" id="kobo.714.1">, </span><a href="https://arxiv.org/abs/1612.08083"><span class="koboSpan" id="kobo.715.1">https://arxiv.org/abs/1612.08083</span></a><span class="koboSpan" id="kobo.716.1">), which is more of a fusion between layer and activation function rather than pure activation. </span><span class="koboSpan" id="kobo.716.2">We can define GLU </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.718.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mtext&gt;GLU&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;F&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;c&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⊗&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/791.png" style="vertical-align:-0.134em;height:0.894em;width:23.158em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.719.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.720.1">ActivationFunc</span></em><span class="koboSpan" id="kobo.721.1"> is a specific activation function (Swish for </span><em class="italic"><span class="koboSpan" id="kobo.722.1">Swi</span></em><span class="koboSpan" id="kobo.723.1">GLU and </span><em class="italic"><span class="koboSpan" id="kobo.724.1">Ge</span></em><span class="koboSpan" id="kobo.725.1">LU for </span><em class="italic"><span class="koboSpan" id="kobo.726.1">Ge</span></em><span class="koboSpan" id="kobo.727.1">GLU), ⊗ is the element-wise product of two vectors, and </span><strong class="bold"><span class="koboSpan" id="kobo.728.1">W</span></strong><span class="koboSpan" id="kobo.729.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.730.1">V</span></strong><span class="koboSpan" id="kobo.731.1"> are weight matrices, which represent linear projections (that is, FC layers). </span><span class="koboSpan" id="kobo.731.2">GLU introduces an additional linear projection, </span><strong class="bold"><span class="koboSpan" id="kobo.732.1">V</span></strong><span class="koboSpan" id="kobo.733.1">, parallel to the original path of the network, </span><strong class="bold"><span class="koboSpan" id="kobo.734.1">W</span></strong><span class="koboSpan" id="kobo.735.1">. </span><span class="koboSpan" id="kobo.735.2">Thanks to the element-wise product, the path with activation, </span><strong class="bold"><span class="koboSpan" id="kobo.736.1">W</span></strong><span class="koboSpan" id="kobo.737.1">, acts as a gate to the signal coming from the </span><strong class="bold"><span class="koboSpan" id="kobo.738.1">V</span></strong><span class="koboSpan" id="kobo.739.1"> path. </span><span class="koboSpan" id="kobo.739.2">This is</span><a id="_idIndexMarker1196"/><span class="koboSpan" id="kobo.740.1"> like </span><strong class="bold"><span class="koboSpan" id="kobo.741.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.742.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.743.1">LSTM</span></strong><span class="koboSpan" id="kobo.744.1">) gates. </span><span class="koboSpan" id="kobo.744.2">We can now define the FFN with </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">GLU activation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.746.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;FFN&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;Activation&lt;/mml:mtext&gt;&lt;mml:mtext&gt;GLU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;F&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;c&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⊗&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/792.png" style="vertical-align:-0.433em;height:1.182em;width:21.697em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.747.1">Let’s note </span><a id="_idIndexMarker1197"/><span class="koboSpan" id="kobo.748.1">that the authors have excluded the bias from the modified FFN. </span><span class="koboSpan" id="kobo.748.2">This is also a good place to mention that different LLMs have different bias configurations, </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">listed next:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.750.1">Use bias in both the linear projections and the attention </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">blocks themselves</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.752.1">Use bias in the linear projections but not in the </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">attention blocks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.754.1">Don’t use bias in either the linear projections or the </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">attention blocks</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.756.1">According to some experiments, the lack of biases stabilizes </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">the training.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.758.1">Next, let’s focus on the various types of positional embeddings we haven’t mentioned so far. </span><span class="koboSpan" id="kobo.758.2">Unfortunately (or fortunately), discussing them in detail goes beyond the scope of this book. </span><span class="koboSpan" id="kobo.758.3">But the important thing to remember is that we have either absolute (static) or relative (dynamic) positional encodings. </span><span class="koboSpan" id="kobo.758.4">In the first case, we modify the input token embedding vectors. </span><span class="koboSpan" id="kobo.758.5">In the second case, we modify the </span><strong class="bold"><span class="koboSpan" id="kobo.759.1">K</span></strong><span class="koboSpan" id="kobo.760.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.761.1">V</span></strong><span class="koboSpan" id="kobo.762.1"> attention matrices relative to their position of the current </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">input token.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.764.1">The survey summarizes the suggestions from existing literature for detailed transformer configuration. </span><span class="koboSpan" id="kobo.764.2">For stronger generalization, it suggests using pre-RMSNorm normalization, and SwiGLU or GeGLU activation functions. </span><span class="koboSpan" id="kobo.764.3">In addition, using LN immediately after embedding layers is likely to incur </span><a id="_idIndexMarker1198"/><span class="koboSpan" id="kobo.765.1">performance degradation. </span><span class="koboSpan" id="kobo.765.2">As for position embeddings, </span><strong class="bold"><span class="koboSpan" id="kobo.766.1">Rotary Positional Embedding</span></strong><span class="koboSpan" id="kobo.767.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.768.1">RoPE</span></strong><span class="koboSpan" id="kobo.769.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.770.1">Attention with Linear Biases</span></strong><span class="koboSpan" id="kobo.771.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.772.1">AliBi</span></strong><span class="koboSpan" id="kobo.773.1">) perform better on long sequences than </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">other</span></span><span class="No-Break"><a id="_idIndexMarker1199"/></span><span class="No-Break"><span class="koboSpan" id="kobo.775.1"> methods.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.776.1">Now that we’re familiar with the architecture properties of LLMs, let’s discuss specific </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">model instances.</span></span></p>
<h2 id="_idParaDest-154" lang="en-GB"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.778.1">Models</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.779.1">The following</span><a id="_idIndexMarker1200"/><span class="koboSpan" id="kobo.780.1"> table represents a summary of some of the popular </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">recent LLMs:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer954">
<span class="koboSpan" id="kobo.782.1"><img alt="Figure 8.8 – Model cards of recent LLMs with public configuration details (modified from https://arxiv.org/abs/2303.18223p)" src="image/B19627_08_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.783.1">Figure 8.8 – Model cards of recent LLMs with public configuration details (modified from https://arxiv.org/abs/2303.18223p)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.784.1">Here, </span><strong class="bold"><span class="koboSpan" id="kobo.785.1">PE</span></strong><span class="koboSpan" id="kobo.786.1"> denotes position </span><a id="_idIndexMarker1201"/><span class="koboSpan" id="kobo.787.1">embedding, </span><strong class="bold"><span class="koboSpan" id="kobo.788.1">#L</span></strong><span class="koboSpan" id="kobo.789.1"> denotes the number of transformer layers, </span><strong class="bold"><span class="koboSpan" id="kobo.790.1">#H</span></strong><span class="koboSpan" id="kobo.791.1"> denotes the number of attention heads per layer, </span><span class="koboSpan" id="kobo.792.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/707.png" style="vertical-align:-0.340em;height:1.051em;width:1.916em"/></span><span class="koboSpan" id="kobo.793.1"> denotes the size of hidden states, and </span><strong class="bold"><span class="koboSpan" id="kobo.794.1">MCL</span></strong><span class="koboSpan" id="kobo.795.1"> denotes the maximum context length </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">during training.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.797.1">We’ll start with the GPT series of models (developed by OpenAI), which is outlined in the </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer956">
<span class="koboSpan" id="kobo.799.1"><img alt="Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)" src="image/B19627_08_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.800.1">Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.801.1">We’re already familiar with GPT-1, so let’s move on </span><a id="_idIndexMarker1202"/><span class="koboSpan" id="kobo.802.1">to </span><strong class="bold"><span class="koboSpan" id="kobo.803.1">GPT-2</span></strong><span class="koboSpan" id="kobo.804.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.805.1">Language Models are Unsupervised Multitask Learners</span></em><span class="koboSpan" id="kobo.806.1">, </span><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"><span class="koboSpan" id="kobo.807.1">https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf</span></a><span class="koboSpan" id="kobo.808.1">). </span><span class="koboSpan" id="kobo.808.2">As a recurring theme of this chapter, it is similar to GPT-1 (except that it uses pre-normalization), but it’s much larger (1.5B versus 117M parameters). </span><span class="koboSpan" id="kobo.808.3">It has a larger </span><a id="_idIndexMarker1203"/><span class="koboSpan" id="kobo.809.1">token vocabulary and requires a larger training dataset size. </span><span class="koboSpan" id="kobo.809.2">GPT-1 and GPT-2 are the only open source models of the series. </span><span class="koboSpan" id="kobo.809.3">Next, we have the </span><strong class="bold"><span class="koboSpan" id="kobo.810.1">GPT-3</span></strong><span class="koboSpan" id="kobo.811.1"> collection </span><a id="_idIndexMarker1204"/><span class="koboSpan" id="kobo.812.1">of eight variants (closed source), ranging from 125M to 175B parameters (GPT-3 refers to the largest model). </span><span class="koboSpan" id="kobo.812.2">It uses the same architecture as GPT-2 but adds alternating sparse and MHA layers. </span><span class="koboSpan" id="kobo.812.3">This is the first model of the series that falls into the LLM category. </span><span class="koboSpan" id="kobo.812.4">Thanks to its size and sophisticated training, it exhibits </span><a id="_idIndexMarker1205"/><span class="koboSpan" id="kobo.813.1">so-called </span><strong class="bold"><span class="koboSpan" id="kobo.814.1">emergent abilities</span></strong><span class="koboSpan" id="kobo.815.1"> (we will discuss them in their namesake section) and serves as a base for the next models. </span><span class="koboSpan" id="kobo.815.2">The first derivative </span><a id="_idIndexMarker1206"/><span class="koboSpan" id="kobo.816.1">model is </span><strong class="bold"><span class="koboSpan" id="kobo.817.1">Codex</span></strong><span class="koboSpan" id="kobo.818.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.819.1">Evaluating Large Language Models Trained on Code</span></em><span class="koboSpan" id="kobo.820.1">, https://arxiv.org/abs/2107.03374), which can generate source code from Python docstrings prompts. </span><span class="koboSpan" id="kobo.820.2">To do this, the model is fine-tuned on the publicly available GitHub source code data. </span><span class="koboSpan" id="kobo.820.3">Initially, GitHub Copilot was based on Codex. </span><span class="koboSpan" id="kobo.820.4">Next, we have the GPT-3.5 collection of models. </span><span class="koboSpan" id="kobo.820.5">It is not public and has no official paper, so we can only speculate about its properties, but we’ll assume it’s similar to GPT-3. </span><span class="koboSpan" id="kobo.820.6">As with Codex, it has the ability to generate code based on natural language task descriptions. </span><span class="koboSpan" id="kobo.820.7">It also uses </span><strong class="bold"><span class="koboSpan" id="kobo.821.1">reinforcement learning with human feedback</span></strong><span class="koboSpan" id="kobo.822.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.823.1">RLHF</span></strong><span class="koboSpan" id="kobo.824.1">) for</span><a id="_idIndexMarker1207"/><span class="koboSpan" id="kobo.825.1"> FT (again, we’ll discuss it in its namesake section), which improves the model’s responses. </span><span class="koboSpan" id="kobo.825.2">GPT-3.5 is available through OpenAI’s API in two subvariants—</span><strong class="source-inline"><span class="koboSpan" id="kobo.826.1">gpt-3.5-turbo</span></strong><span class="koboSpan" id="kobo.827.1"> with a context length of 4,096 tokens and </span><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">gpt-3.5-turbo-16k</span></strong><span class="koboSpan" id="kobo.829.1"> with 16,384 tokens. </span><span class="koboSpan" id="kobo.829.2">The current version of Copilot is based on GPT-3.5. </span><span class="koboSpan" id="kobo.829.3">The newest model, GPT-4, accepts multimodal inputs (images and text) but outputs text only. </span><span class="koboSpan" id="kobo.829.4">It is also closed, but it might have more than 1T parameters. </span><span class="koboSpan" id="kobo.829.5">According to Sam Altman, CEO of OpenAI, training GPT-4 has cost more than $100 million (https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/). </span><span class="koboSpan" id="kobo.829.6">GPT-4 is also available through OpenAI’s API with two subvariants—</span><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">gpt-4</span></strong><span class="koboSpan" id="kobo.831.1"> with a context length of 8,192 tokens and </span><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">gpt-4-32k</span></strong><span class="koboSpan" id="kobo.833.1"> with </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">32,768 tokens.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.835.1">Next, let’s discuss the </span><strong class="bold"><span class="koboSpan" id="kobo.836.1">LlaMa</span></strong><span class="koboSpan" id="kobo.837.1"> series</span><a id="_idIndexMarker1208"/><span class="koboSpan" id="kobo.838.1"> of pre-trained (and not fine-tuned) models released by Meta. </span><span class="koboSpan" id="kobo.838.2">The first version (</span><em class="italic"><span class="koboSpan" id="kobo.839.1">LLaMA: Open and Efficient Foundation Language Models</span></em><span class="koboSpan" id="kobo.840.1">, </span><a href="https://arxiv.org/abs/2302.13971"><span class="koboSpan" id="kobo.841.1">https://arxiv.org/abs/2302.13971</span></a><span class="koboSpan" id="kobo.842.1">) has four variants, ranging from 6B to 65B parameters. </span><span class="koboSpan" id="kobo.842.2">This is one of the most popular LLMs in the open source community because Meta has also released its weights (although they are not licensed for commercial use). </span><span class="koboSpan" id="kobo.842.3">This way, the company has done the heavy lifting of pre-training the model. </span><span class="koboSpan" id="kobo.842.4">The open source community uses it as </span><a id="_idIndexMarker1209"/><span class="koboSpan" id="kobo.843.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.844.1">foundation model</span></strong><span class="koboSpan" id="kobo.845.1"> because it can be fine-tuned with relatively little compute. </span><span class="koboSpan" id="kobo.845.2">Recently, Meta </span><a id="_idIndexMarker1210"/><span class="koboSpan" id="kobo.846.1">released </span><strong class="bold"><span class="koboSpan" id="kobo.847.1">Llama 2</span></strong><span class="koboSpan" id="kobo.848.1">—an updated version of Llama (</span><em class="italic"><span class="koboSpan" id="kobo.849.1">Llama 2: Open Foundation and Fine-Tuned Chat Models</span></em><span class="koboSpan" id="kobo.850.1">, </span><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models"><span class="koboSpan" id="kobo.851.1">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models</span></a><span class="koboSpan" id="kobo.852.1">). </span><span class="koboSpan" id="kobo.852.2">It </span><a id="_idIndexMarker1211"/><span class="koboSpan" id="kobo.853.1">has three variants with 7B, 13B, and 70B parameters. </span><span class="koboSpan" id="kobo.853.2">Llama 2 uses GQA and 40% more pre-training data than Llama 1. </span><span class="koboSpan" id="kobo.853.3">In addition, each variant also has a version fine-tuned using RLHF. </span><span class="koboSpan" id="kobo.853.4">The model’s license allows commercial use (with </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">some limitations).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.855.1">This concludes our survey on the architecture of LLMs. </span><span class="koboSpan" id="kobo.855.2">Next, let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">their training.</span></span></p>
<h1 id="_idParaDest-155" lang="en-GB"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.857.1">Training LLMs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.858.1">Since most LLMs</span><a id="_idIndexMarker1212"/><span class="koboSpan" id="kobo.859.1"> are decoder-only, the most common LLM pre-training task is NWP. </span><span class="koboSpan" id="kobo.859.2">The large number of model parameters (up to hundreds of billions) requires comparatively large training datasets to prevent overfitting and realize the full capabilities of the models. </span><span class="koboSpan" id="kobo.859.3">This requirement poses two significant challenges: ensuring training data quality and the ability to process large volumes of data. </span><span class="koboSpan" id="kobo.859.4">In the following sections, we’ll discuss various aspects of the LLM training pipeline, starting from the </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">training datasets.</span></span></p>
<h2 id="_idParaDest-156" lang="en-GB"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.861.1">Training datasets</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.862.1">We can categorize the training data into</span><a id="_idIndexMarker1213"/><span class="koboSpan" id="kobo.863.1"> two </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">broad categories:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.865.1">General</span></strong><span class="koboSpan" id="kobo.866.1">: Examples include web pages, books, or conversational text. </span><span class="koboSpan" id="kobo.866.2">LLMs almost always train on general data because it’s widely available and diverse, improving the language modeling and generalization capabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">of LLMs.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.868.1">Specialized</span></strong><span class="koboSpan" id="kobo.869.1">: Code, scientific articles, textbooks, or multilingual data for providing LLMs with </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">task-specific capabilities.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.871.1">The following table lists the most popular language </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">modeling datasets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer957">
<span class="koboSpan" id="kobo.873.1"><img alt="Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)" src="image/B19627_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.874.1">Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.875.1">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">discuss</span></span><span class="No-Break"><a id="_idIndexMarker1214"/></span><span class="No-Break"><span class="koboSpan" id="kobo.877.1"> them:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.878.1">Books</span></strong><span class="koboSpan" id="kobo.879.1">: We’ll focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">two datasets:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.881.1">BookCorpus</span></strong><span class="koboSpan" id="kobo.882.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.883.1">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</span></em><span class="koboSpan" id="kobo.884.1">, </span><a href="https://arxiv.org/abs/1506.06724"><span class="koboSpan" id="kobo.885.1">https://arxiv.org/abs/1506.06724</span></a><span class="koboSpan" id="kobo.886.1">): Includes 11,000 fictional books with close to </span><a id="_idIndexMarker1215"/><span class="koboSpan" id="kobo.887.1">1B words (released </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">in 2015).</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.889.1">Project Gutenberg</span></strong><span class="koboSpan" id="kobo.890.1"> (</span><a href="https://www.gutenberg.org/"><span class="koboSpan" id="kobo.891.1">https://www.gutenberg.org/</span></a><span class="koboSpan" id="kobo.892.1">): Includes 70,000 </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">fictional</span></span><span class="No-Break"><a id="_idIndexMarker1216"/></span><span class="No-Break"><span class="koboSpan" id="kobo.894.1"> books.</span></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.895.1">Common Crawl</span></strong><span class="koboSpan" id="kobo.896.1"> (</span><a href="https://commoncrawl.org/"><span class="koboSpan" id="kobo.897.1">https://commoncrawl.org/</span></a><span class="koboSpan" id="kobo.898.1">): Petabyte-sized web crawling database. </span><span class="koboSpan" id="kobo.898.2">The data is</span><a id="_idIndexMarker1217"/><span class="koboSpan" id="kobo.899.1"> split by the date obtained, starting from 2008. </span><span class="koboSpan" id="kobo.899.2">The latest archive contains 3.1B web pages (390 TiB of uncompressed content), scraped from 44 million hosts or 35 million registered domains. </span><span class="koboSpan" id="kobo.899.3">It contains a lot of low-quality data, but there are multiple subsets with </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">higher-quality data:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.901.1">Colossal, cleaned version of Common Crawl</span></strong><span class="koboSpan" id="kobo.902.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.903.1">C4</span></strong><span class="koboSpan" id="kobo.904.1">): An 800 GiB dataset developed by </span><a id="_idIndexMarker1218"/><span class="koboSpan" id="kobo.905.1">Google. </span><span class="koboSpan" id="kobo.905.2">The original dataset is unavailable for download, but Google has published the tools to recreate it from the Common Crawl database. </span><span class="koboSpan" id="kobo.905.3">In 2019, the </span><strong class="bold"><span class="koboSpan" id="kobo.906.1">Allen Institute for AI</span></strong><span class="koboSpan" id="kobo.907.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.908.1">AI2</span></strong><span class="koboSpan" id="kobo.909.1">, https://allenai.org/) released a recreation, available</span><a id="_idIndexMarker1219"/><span class="koboSpan" id="kobo.910.1"> at </span><a href="https://huggingface.co/datasets/allenai/c4"><span class="koboSpan" id="kobo.911.1">https://huggingface.co/datasets/allenai/c4</span></a><span class="koboSpan" id="kobo.912.1">. </span><span class="koboSpan" id="kobo.912.2">Its most popular sub-variant is the </span><em class="italic"><span class="koboSpan" id="kobo.913.1">en</span></em><span class="koboSpan" id="kobo.914.1"> version, which removes all documents that contain words from the so-called </span><em class="italic"><span class="koboSpan" id="kobo.915.1">badwords filter</span></em><span class="koboSpan" id="kobo.916.1"> (a list of bad words is available </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">at </span></span><a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words"><span class="No-Break"><span class="koboSpan" id="kobo.918.1">https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.919.1">).</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.920.1">CC-News</span></strong><span class="koboSpan" id="kobo.921.1">: Articles</span><a id="_idIndexMarker1220"/><span class="koboSpan" id="kobo.922.1"> from news sites all over </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">the </span></span><span class="No-Break"><a id="_idIndexMarker1221"/></span><span class="No-Break"><span class="koboSpan" id="kobo.924.1">world.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.925.1">RealNews</span></strong><span class="koboSpan" id="kobo.926.1">: News</span><a id="_idIndexMarker1222"/><span class="koboSpan" id="kobo.927.1"> articles extracted from the 5,000 news domains indexed by </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.928.1">Google News</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.930.1">CC-Stories-R</span></strong><span class="koboSpan" id="kobo.931.1">: A dataset for common sense reasoning and language modeling. </span><span class="koboSpan" id="kobo.931.2">It consists </span><a id="_idIndexMarker1223"/><span class="koboSpan" id="kobo.932.1">of Common Crawl documents with the most overlapping </span><em class="italic"><span class="koboSpan" id="kobo.933.1">n</span></em><span class="koboSpan" id="kobo.934.1">-grams with the questions in common sense reasoning tasks. </span><span class="koboSpan" id="kobo.934.2">The new training corpus represents the top 1.0% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.935.1">highest-ranked documents.</span></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.936.1">Reddit links</span></strong><span class="koboSpan" id="kobo.937.1">: One </span><a id="_idIndexMarker1224"/><span class="koboSpan" id="kobo.938.1">way to overcome the low signal-to-noise ratio of Common Crawl is to rely on human-curated content. </span><span class="koboSpan" id="kobo.938.2">Enter Reddit, where users can post textual content or links, and other users can upvote these submissions (the upvotes are known as </span><em class="italic"><span class="koboSpan" id="kobo.939.1">karma</span></em><span class="koboSpan" id="kobo.940.1">). </span><span class="koboSpan" id="kobo.940.2">We’ll mention two </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">Reddit-based datasets:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.942.1">WebText</span></strong><span class="koboSpan" id="kobo.943.1"> (released alongside the GPT-2 model): Contains a subset of 45 million Reddit-submitted links with a karma of three or more. </span><span class="koboSpan" id="kobo.943.2">The documents behind</span><a id="_idIndexMarker1225"/><span class="koboSpan" id="kobo.944.1"> these links form the LLM training data. </span><span class="koboSpan" id="kobo.944.2">WebText is not publicly available, but</span><a id="_idIndexMarker1226"/><span class="koboSpan" id="kobo.945.1"> there is an open source version called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.946.1">OpenWebText</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.947.1"> (</span></span><a href="https://github.com/jcpeterson/openwebtext"><span class="No-Break"><span class="koboSpan" id="kobo.948.1">https://github.com/jcpeterson/openwebtext</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.949.1">).</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.950.1">Pushshift</span></strong><span class="koboSpan" id="kobo.951.1"> (</span><a href="https://arxiv.org/abs/2001.08435"><span class="koboSpan" id="kobo.952.1">https://arxiv.org/abs/2001.08435</span></a><span class="koboSpan" id="kobo.953.1">): Contains all link submissions and</span><a id="_idIndexMarker1227"/><span class="koboSpan" id="kobo.954.1"> comments posted </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">on Reddit.</span></span></li></ul></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.956.1">Reddit API pricing controversy</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.957.1">The rise of LLMs has made Reddit data much more valuable than before. </span><span class="koboSpan" id="kobo.957.2">Because of this, the company has decided to introduce fees for access to its previously free API. </span><span class="koboSpan" id="kobo.957.3">This measure mainly targets AI companies that plan to train their LLMs using the data. </span><span class="koboSpan" id="kobo.957.4">However, the proposal has led many of the site’s voluntary moderators (Reddit relies on them) to announce a strike by temporarily closing the previously open communities they moderate. </span><span class="koboSpan" id="kobo.957.5">At the time of writing, the disagreement is </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">still ongoing.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.959.1">The Pile</span></strong><span class="koboSpan" id="kobo.960.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.961.1">An 800GB Dataset of Diverse Text for Language Modeling</span></em><span class="koboSpan" id="kobo.962.1">, </span><a href="https://arxiv.org/abs/2101.00027"><span class="koboSpan" id="kobo.963.1">https://arxiv.org/abs/2101.00027</span></a><span class="koboSpan" id="kobo.964.1">): Composed of 22 diverse and high-quality datasets derived from</span><a id="_idIndexMarker1228"/><span class="koboSpan" id="kobo.965.1"> various sources, including PubMed, arXiv, GitHub, Stack Exchange, Hacker News, YouTube, and others. </span><span class="koboSpan" id="kobo.965.2">The Pile also introduces the OpenWebText2 and BookCorpus2 extensions of the original OpenWebText and </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">BookCorpus </span></span><span class="No-Break"><a id="_idIndexMarker1229"/></span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">datasets.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.968.1">ROOTS</span></strong><span class="koboSpan" id="kobo.969.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.970.1">The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset</span></em><span class="koboSpan" id="kobo.971.1">, </span><a href="https://arxiv.org/abs/2303.03915"><span class="koboSpan" id="kobo.972.1">https://arxiv.org/abs/2303.03915</span></a><span class="koboSpan" id="kobo.973.1">): A web-scale curated dataset covering 46 </span><a id="_idIndexMarker1230"/><span class="koboSpan" id="kobo.974.1">natural languages and 13 </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">programming languages.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.976.1">Wikimedia</span></strong><span class="koboSpan" id="kobo.977.1"> (</span><a href="https://dumps.wikimedia.org/"><span class="koboSpan" id="kobo.978.1">https://dumps.wikimedia.org/</span></a><span class="koboSpan" id="kobo.979.1">): Because of its high-quality content, this is an </span><a id="_idIndexMarker1231"/><span class="koboSpan" id="kobo.980.1">excellent source of </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">training data.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.982.1">Stack Exchange</span></strong><span class="koboSpan" id="kobo.983.1"> (</span><a href="https://archive.org/details/stackexchange"><span class="koboSpan" id="kobo.984.1">https://archive.org/details/stackexchange</span></a><span class="koboSpan" id="kobo.985.1">): A network of QA topic sites with a </span><a id="_idIndexMarker1232"/><span class="koboSpan" id="kobo.986.1">rating system. </span><span class="koboSpan" id="kobo.986.2">The most popular representative is </span><strong class="bold"><span class="koboSpan" id="kobo.987.1">Stack Overflow</span></strong><span class="koboSpan" id="kobo.988.1">. </span><span class="koboSpan" id="kobo.988.2">It releases a tri-monthly anonymized data dump with all </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">user-contributed content.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.990.1">arXiv</span></strong><span class="koboSpan" id="kobo.991.1"> (https://www.kaggle.com/datasets/Cornell-University/arxiv): The primary scientific </span><a id="_idIndexMarker1233"/><span class="koboSpan" id="kobo.992.1">data source, which contains more than 2.2B </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">scientific articles.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.994.1">GitHub</span></strong><span class="koboSpan" id="kobo.995.1">: The </span><a id="_idIndexMarker1234"/><span class="koboSpan" id="kobo.996.1">GH Archive project (</span><a href="https://www.gharchive.org/"><span class="koboSpan" id="kobo.997.1">https://www.gharchive.org/</span></a><span class="koboSpan" id="kobo.998.1">) records, archives, and </span><a id="_idIndexMarker1235"/><span class="koboSpan" id="kobo.999.1">provides access to the public </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">GitHub timeline.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1001.1">In practice, the LLM pre-training step uses a mix of several datasets. </span><span class="koboSpan" id="kobo.1001.2">The following screenshot shows the distribution of the sources of pre-training data for several </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">representative LLMs:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer958">
<span class="koboSpan" id="kobo.1003.1"><img alt="Figure 8.11 – Ratios of various data sources in the pre-training data for existing LLMs (source: https://arxiv.org/abs/2303.18223)" src="image/B19627_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1004.1">Figure 8.11 – Ratios of various data sources in the pre-training data for existing LLMs (source: https://arxiv.org/abs/2303.18223)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1005.1">Mixing datasets is </span><a id="_idIndexMarker1236"/><span class="koboSpan" id="kobo.1006.1">not a trivial process and requires several processing steps. </span><span class="koboSpan" id="kobo.1006.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">discuss them:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1008.1">Remove low-quality or irrelevant data</span></strong><span class="koboSpan" id="kobo.1009.1">: For example, web pages contain large amounts of HTML tags, JavaScript, or </span><strong class="bold"><span class="koboSpan" id="kobo.1010.1">cascading style sheets</span></strong><span class="koboSpan" id="kobo.1011.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1012.1">CSS</span></strong><span class="koboSpan" id="kobo.1013.1">). </span><span class="koboSpan" id="kobo.1013.2">Yet, we’re only interested in </span><br/><span class="koboSpan" id="kobo.1014.1">human-readable</span><a id="_idIndexMarker1237"/><span class="koboSpan" id="kobo.1015.1"> text (except when we want to train the model explicitly to understand HTML). </span><span class="koboSpan" id="kobo.1015.2">In this case, we’ll have to remove the HTML and JavaScript and only leave </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">the text.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1017.1">Remove personally identifiable information (PII)</span></strong><span class="koboSpan" id="kobo.1018.1">: Data is often extracted from web pages, which might contain personal information. </span><span class="koboSpan" id="kobo.1018.2">This step aims to remove such data from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">training set.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1020.1">Tokenization</span></strong><span class="koboSpan" id="kobo.1021.1">: We discussed tokenization in depth in </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1022.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1023.1">, and we won’t discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">it here.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1025.1">Finally, let’s introduce a practical transformer scaling law (</span><em class="italic"><span class="koboSpan" id="kobo.1026.1">Scaling Laws for Neural Language Models</span></em><span class="koboSpan" id="kobo.1027.1">, https://arxiv.org/abs/2001.08361). </span><span class="koboSpan" id="kobo.1027.2">Because of their scale, training LLMs can be expensive. </span><span class="koboSpan" id="kobo.1027.3">Therefore, it is important not to train the model more (or less) than necessary. </span><span class="koboSpan" id="kobo.1027.4">Based on empirical experiments, the scaling law proposes an optimal ratio between the amount of training compute (expressed in </span><strong class="bold"><span class="koboSpan" id="kobo.1028.1">floating-point operations per second</span></strong><span class="koboSpan" id="kobo.1029.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.1030.1">FLOPS</span></strong><span class="koboSpan" id="kobo.1031.1">), </span><em class="italic"><span class="koboSpan" id="kobo.1032.1">C</span></em><span class="koboSpan" id="kobo.1033.1">, the model size (number of parameters), </span><em class="italic"><span class="koboSpan" id="kobo.1034.1">N</span></em><span class="koboSpan" id="kobo.1035.1">, and the training dataset</span><a id="_idIndexMarker1238"/><span class="koboSpan" id="kobo.1036.1"> size (number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">tokens), </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1038.1">D</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1040.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/794.png" style="vertical-align:-0.015em;height:0.679em;width:4.073em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1041.1">Now that we know the steps to build the training set, let’s focus on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">actual pre-training.</span></span></p>
<h2 id="_idParaDest-157" lang="en-GB"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.1043.1">Pre-training properties</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1044.1">Similar to </span><a id="_idIndexMarker1239"/><span class="koboSpan" id="kobo.1045.1">other </span><strong class="bold"><span class="koboSpan" id="kobo.1046.1">neural networks</span></strong><span class="koboSpan" id="kobo.1047.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1048.1">NNs</span></strong><span class="koboSpan" id="kobo.1049.1">), pre-training of LLMs works with gradient</span><a id="_idIndexMarker1240"/><span class="koboSpan" id="kobo.1050.1"> descent and backpropagation. </span><span class="koboSpan" id="kobo.1050.2">But because of their size, the training has specific properties, which we’ll discuss in </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">this section.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1052.1">Adam optimizer</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1053.1">Most LLMs use </span><a id="_idIndexMarker1241"/><span class="koboSpan" id="kobo.1054.1">Adam (</span><em class="italic"><span class="koboSpan" id="kobo.1055.1">Adam: A Method for Stochastic Optimization</span></em><span class="koboSpan" id="kobo.1056.1">, </span><a href="https://arxiv.org/abs/1412.6980"><span class="koboSpan" id="kobo.1057.1">https://arxiv.org/abs/1412.6980</span></a><span class="koboSpan" id="kobo.1058.1">) or one of its</span><a id="_idIndexMarker1242"/><span class="koboSpan" id="kobo.1059.1"> modifications. </span><span class="koboSpan" id="kobo.1059.2">Although we’ve used it in many examples so far, we haven’t discussed it in detail. </span><span class="koboSpan" id="kobo.1059.3">Time to remedy </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">this omission.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1061.1">A reminder of the weight update formula</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1062.1">In </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1063.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1064.1">, we learned that we </span><a id="_idIndexMarker1243"/><span class="koboSpan" id="kobo.1065.1">use backpropagation to compute the gradient (first derivative) of the loss function, </span><em class="italic"><span class="koboSpan" id="kobo.1066.1">J(θ)</span></em><span class="koboSpan" id="kobo.1067.1">, with respect to every parameter, </span><span class="koboSpan" id="kobo.1068.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/795.png" style="vertical-align:-0.483em;height:1.194em;width:0.653em"/></span><span class="koboSpan" id="kobo.1069.1">: </span><span class="koboSpan" id="kobo.1070.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/796.png" style="vertical-align:-0.483em;height:1.243em;width:3.739em"/></span><span class="koboSpan" id="kobo.1071.1">. </span><span class="koboSpan" id="kobo.1071.2">Once we have the gradient, we can perform the weight update with the formula </span><span class="koboSpan" id="kobo.1072.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/797.png" style="vertical-align:-0.483em;height:1.243em;width:7.933em"/></span><span class="koboSpan" id="kobo.1073.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1074.1">η</span></em><span class="koboSpan" id="kobo.1075.1"> is the learning rate. </span><span class="koboSpan" id="kobo.1075.2">We can add momentum (or velocity) to that formula. </span><span class="koboSpan" id="kobo.1075.3">To do so, we’ll assume we are at step </span><em class="italic"><span class="koboSpan" id="kobo.1076.1">t</span></em><span class="koboSpan" id="kobo.1077.1"> of the training process. </span><span class="koboSpan" id="kobo.1077.2">Then, we can calculate the momentum of the current update based on the momentum of the update at step </span><em class="italic"><span class="koboSpan" id="kobo.1078.1">t-1</span></em><span class="koboSpan" id="kobo.1079.1">: </span><span class="koboSpan" id="kobo.1080.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/798.png" style="vertical-align:-0.483em;height:1.243em;width:9.244em"/></span><span class="koboSpan" id="kobo.1081.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1082.1">µ</span></em><span class="koboSpan" id="kobo.1083.1"> is a momentum rate in the [0:1] range. </span><span class="koboSpan" id="kobo.1083.2">In addition, we can add L2 regularization (or weight decay; see </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1084.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1085.1">): </span><span class="koboSpan" id="kobo.1086.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/799.png" style="vertical-align:-0.583em;height:1.444em;width:13.984em"/></span><span class="koboSpan" id="kobo.1087.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1088.1">λ</span></em><span class="koboSpan" id="kobo.1089.1"> is the weight decay coefficient. </span><span class="koboSpan" id="kobo.1089.2">Finally, we can perform the weight </span><span class="No-Break"><span class="koboSpan" id="kobo.1090.1">update: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/800.png" style="vertical-align:-0.483em;height:1.194em;width:4.482em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1093.1">Adam calculates individual and adaptive learning rates for every weight based on previous weight updates (momentum). </span><span class="koboSpan" id="kobo.1093.2">Let’s see how </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">that works:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1095.1">Compute the first moment (or mean) and the second moment (or variance) of </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">the gradient:</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1097.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/801.png" style="vertical-align:-0.913em;height:2.108em;width:11.268em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1098.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/802.png" style="vertical-align:-0.963em;height:2.394em;width:12.115em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1099.1">Here, </span><span class="koboSpan" id="kobo.1100.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/803.png" style="vertical-align:-0.333em;height:1.044em;width:0.745em"/></span><span class="koboSpan" id="kobo.1101.1"> and </span><span class="koboSpan" id="kobo.1102.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/804.png" style="vertical-align:-0.333em;height:1.044em;width:0.745em"/></span><span class="koboSpan" id="kobo.1103.1"> are hyperparameters with default values of 0.9 and 0.95, respectively. </span><span class="koboSpan" id="kobo.1103.2">The two formulas are very similar to the momentum one. </span><span class="koboSpan" id="kobo.1103.3">The relationship between </span><span class="koboSpan" id="kobo.1104.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/805.png" style="vertical-align:-0.340em;height:0.788em;width:1.046em"/></span> <em class="italic"><span class="koboSpan" id="kobo.1105.1">(</span></em><em class="italic"><span class="koboSpan" id="kobo.1106.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/806.png" style="vertical-align:-0.340em;height:0.788em;width:1.698em"/></span></em><em class="italic"><span class="koboSpan" id="kobo.1107.1">)</span></em><span class="koboSpan" id="kobo.1108.1"> and </span><br/><span class="koboSpan" id="kobo.1109.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/807.png" style="vertical-align:-0.340em;height:0.788em;width:0.684em"/></span><em class="italic"><span class="koboSpan" id="kobo.1110.1"> (</span></em><em class="italic"><span class="koboSpan" id="kobo.1111.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/808.png" style="vertical-align:-0.340em;height:0.788em;width:1.331em"/></span></em><em class="italic"><span class="koboSpan" id="kobo.1112.1">)</span></em><span class="koboSpan" id="kobo.1113.1"> acts as a simulation of a moving average. </span><span class="koboSpan" id="kobo.1113.2">But instead of averaging across multiple previous values, we take the latest previous value, </span><span class="koboSpan" id="kobo.1114.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/809.png" style="vertical-align:-0.340em;height:0.788em;width:1.575em"/></span><em class="italic"><span class="koboSpan" id="kobo.1115.1"> (</span></em><em class="italic"><span class="koboSpan" id="kobo.1116.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/810.png" style="vertical-align:-0.340em;height:0.788em;width:1.235em"/></span></em><em class="italic"><span class="koboSpan" id="kobo.1117.1">)</span></em><span class="koboSpan" id="kobo.1118.1">, and assign it a weight coefficient, </span><span class="koboSpan" id="kobo.1119.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/803.png" style="vertical-align:-0.333em;height:1.044em;width:0.745em"/></span><span class="koboSpan" id="kobo.1120.1"> (</span><span class="koboSpan" id="kobo.1121.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/804.png" style="vertical-align:-0.333em;height:1.044em;width:0.745em"/></span><span class="koboSpan" id="kobo.1122.1">).</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1123.1">2.	</span><span class="koboSpan" id="kobo.1123.2">The initial values of </span><span class="koboSpan" id="kobo.1124.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/813.png" style="vertical-align:-0.340em;height:0.788em;width:0.954em"/></span><span class="koboSpan" id="kobo.1125.1"> and </span><span class="koboSpan" id="kobo.1126.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/187.png" style="vertical-align:-0.340em;height:0.788em;width:0.614em"/></span><span class="koboSpan" id="kobo.1127.1"> are 0, so they will have a bias toward 0 in the initial phase of the training. </span><span class="koboSpan" id="kobo.1127.2">To understand why this could be a problem, let’s assume that at </span><em class="italic"><span class="koboSpan" id="kobo.1128.1">t=1</span></em><span class="koboSpan" id="kobo.1129.1">, </span><span class="koboSpan" id="kobo.1130.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.9&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/815.png" style="vertical-align:-0.333em;height:1.044em;width:3.553em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.1131.1">and</span></span><br/> <span class="koboSpan" id="kobo.1132.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/816.png" style="vertical-align:-0.483em;height:1.243em;width:5.076em"/></span><span class="koboSpan" id="kobo.1133.1">. </span><span class="koboSpan" id="kobo.1133.2">Then, </span><span class="koboSpan" id="kobo.1134.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.9&lt;/mml:mn&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;0.9&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/817.png" style="vertical-align:-0.333em;height:1.017em;width:13.066em"/></span><span class="koboSpan" id="kobo.1135.1"> is</span><a id="_idIndexMarker1244"/><span class="koboSpan" id="kobo.1136.1"> much less than the actual gradient of 5. </span><span class="koboSpan" id="kobo.1136.2">We can compensate for </span><a id="_idIndexMarker1245"/><span class="koboSpan" id="kobo.1137.1">this bias with bias-corrected versions of </span><span class="koboSpan" id="kobo.1138.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/813.png" style="vertical-align:-0.340em;height:0.788em;width:0.954em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.1139.1">and </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1140.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/187.png" style="vertical-align:-0.340em;height:0.788em;width:0.614em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1142.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/820.png" style="vertical-align:-0.786em;height:1.881em;width:4.616em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1143.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/821.png" style="vertical-align:-0.786em;height:1.881em;width:4.276em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1144.1">3.	</span><span class="koboSpan" id="kobo.1144.2">Perform the weight update with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1146.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mfrac&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/822.png" style="vertical-align:-0.970em;height:2.289em;width:6.893em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1147.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1148.1">ε</span></em><span class="koboSpan" id="kobo.1149.1"> is some small value to prevent division </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">by 0.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1151.1">AdamW</span></strong><span class="koboSpan" id="kobo.1152.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1153.1">Decoupled Weight Decay Regularization</span></em><span class="koboSpan" id="kobo.1154.1">, </span><a href="https://arxiv.org/abs/1711.05101"><span class="koboSpan" id="kobo.1155.1">https://arxiv.org/abs/1711.05101</span></a><span class="koboSpan" id="kobo.1156.1">) improves </span><a id="_idIndexMarker1246"/><span class="koboSpan" id="kobo.1157.1">Adam with decoupled </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">weight decay:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1159.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msqrt&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/823.png" style="vertical-align:-1.020em;height:2.389em;width:10.673em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1160.1">Recall that the L2 regularization participates in the loss function and then, through the derivative process, is transferred (as weight decay) to the weight update formula. </span><span class="koboSpan" id="kobo.1160.2">In this case, the regularization will pass through all the transformations of the cost function and will be subject to them. </span><span class="koboSpan" id="kobo.1160.3">As the name suggests, decoupled weight decay bypasses all these</span><a id="_idIndexMarker1247"/><span class="koboSpan" id="kobo.1161.1"> transformations and participates directly in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">preceding formula.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1163.1">One issue with </span><a id="_idIndexMarker1248"/><span class="koboSpan" id="kobo.1164.1">Adam and </span><a id="_idIndexMarker1249"/><span class="koboSpan" id="kobo.1165.1">AdamW is the increased memory consumption—the optimizer stores at least two additional values (</span><span class="koboSpan" id="kobo.1166.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/813.png" style="vertical-align:-0.340em;height:0.788em;width:0.954em"/></span><span class="koboSpan" id="kobo.1167.1"> and </span><span class="koboSpan" id="kobo.1168.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/187.png" style="vertical-align:-0.340em;height:0.788em;width:0.614em"/></span><span class="koboSpan" id="kobo.1169.1">) for every </span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">model parameter.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1171.1">Parallel processing</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1172.1">The scale of LLMs </span><a id="_idIndexMarker1250"/><span class="koboSpan" id="kobo.1173.1">necessitates special steps for efficient training. </span><span class="koboSpan" id="kobo.1173.2">First, we’ll discuss how to train LLMs across multiple devices. </span><span class="koboSpan" id="kobo.1173.3">More specifically, we’ll discuss a combination of three different types of parallelism (also referred to as </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">3D parallelism):</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1175.1">Data parallelism</span></strong><span class="koboSpan" id="kobo.1176.1">: It works when the model is small enough to fit on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">single device:</span></span><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.1178.1">Create identical copies of the entire model and its optimizer states (including the random seeds) across </span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">all devices.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.1180.1">Split each batch of the training set into unique subsets (shards) and distribute them across </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">all devices.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.1182.1">Each device computes its gradient based on its unique subset of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1183.1">input batch.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.1184.1">Aggregate the gradients of all devices into a single </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">gradient update.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.1186.1">Distribute the aggregated updates across the devices and perform weight updates on each device. </span><span class="koboSpan" id="kobo.1186.2">This way, we start and end each training step with </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">identical models.</span></span></li></ol></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1188.1">Model (or pipeline)</span></strong><span class="koboSpan" id="kobo.1189.1">: Split the model across multiple devices on an operation (layer) level. </span><span class="koboSpan" id="kobo.1189.2">For example, if our model has 9 layers, we can send layers 1 through 6 to one device and layers 7 through 9 to another. </span><span class="koboSpan" id="kobo.1189.3">In this way, we can train models that don’t fit in the memory of a single device. </span><span class="koboSpan" id="kobo.1189.4">Not only that, but we can apply this method even on a single device. </span><span class="koboSpan" id="kobo.1189.5">In this case, we’ll load the first set of operations (1-6) and compute their output. </span><span class="koboSpan" id="kobo.1189.6">Then, we’ll unload them and load the following subset (7-9). </span><span class="koboSpan" id="kobo.1189.7">The output of the first set will serve as input for the second. </span><span class="koboSpan" id="kobo.1189.8">Backpropagation works in the same way but in the opposite direction. </span><span class="koboSpan" id="kobo.1189.9">One issue with model parallelism is that if we use multiple devices, the second one will idle until the first </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">produces output.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1191.1">Tensor (or horizontal)</span></strong><span class="koboSpan" id="kobo.1192.1">: Split the model across different devices on the tensor level, which solves the idling problem of model parallelism. </span><span class="koboSpan" id="kobo.1192.2">To understand how this works, let’s recall that matrix multiplication is the most computationally intensive operation of contemporary NNs. </span><span class="koboSpan" id="kobo.1192.3">But, as we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1193.1">FlashAttention</span></em><span class="koboSpan" id="kobo.1194.1"> section, it is </span><a id="_idIndexMarker1251"/><span class="koboSpan" id="kobo.1195.1">also embarrassingly parallel. </span><span class="koboSpan" id="kobo.1195.2">Therefore, we can split it </span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1">across devices.</span></span></li>
</ul>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1197.1">Zero redundancy optimizer</span></h3>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1198.1">Zero redundancy optimizer</span></strong><span class="koboSpan" id="kobo.1199.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1200.1">ZeRO</span></strong><span class="koboSpan" id="kobo.1201.1">) (</span><em class="italic"><span class="koboSpan" id="kobo.1202.1">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</span></em><span class="koboSpan" id="kobo.1203.1">, https://arxiv.org/abs/1910.02054) is a hybrid between data and model </span><a id="_idIndexMarker1252"/><span class="koboSpan" id="kobo.1204.1">parallelism. </span><span class="koboSpan" id="kobo.1204.2">The following diagram illustrates </span><a id="_idIndexMarker1253"/><span class="koboSpan" id="kobo.1205.1">the three stages </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">of ZeRO:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer991">
<span class="koboSpan" id="kobo.1207.1"><img alt="Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)" src="image/B19627_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1208.1">Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1209.1">The first line represents a case of a data-parallel system. </span><span class="koboSpan" id="kobo.1209.2">Each GPU receives a unique shard of the input mini-batch. </span><span class="koboSpan" id="kobo.1209.3">It also holds an identical copy of the model parameters (first colored rectangle of the GPUi block), gradients (second rectangle), and optimizer states (third rectangle). </span><span class="koboSpan" id="kobo.1209.4">The</span><a id="_idIndexMarker1254"/><span class="koboSpan" id="kobo.1210.1"> size of the optimizer states that they take up most of the memory during training (for example, Adam stores multiple values per model parameter). </span><span class="koboSpan" id="kobo.1210.2">The following three lines</span><a id="_idIndexMarker1255"/><span class="koboSpan" id="kobo.1211.1"> represent the three stages </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">of ZeRO:</span></span></p>
<ol>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1213.1">Optimizer state partitioning</span></strong><span class="koboSpan" id="kobo.1214.1"> (</span><span class="koboSpan" id="kobo.1215.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;os&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/826.png" style="vertical-align:-0.340em;height:0.989em;width:1.179em"/></span><span class="koboSpan" id="kobo.1216.1">): Each GPU holds an identical copy of the entire model parameters and its gradients, but the optimizer states are partitioned across the GPUs, and each holds only </span><span class="No-Break"><span class="koboSpan" id="kobo.1217.1">a portion.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1218.1">Add gradient partitioning</span></strong><span class="koboSpan" id="kobo.1219.1"> (</span><span class="koboSpan" id="kobo.1220.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;os+g&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/827.png" style="vertical-align:-0.483em;height:1.132em;width:1.832em"/></span><span class="koboSpan" id="kobo.1221.1">): Each GPU holds an identical copy of the entire model parameters, but the gradients and the optimizer states </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">are partitioned.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1223.1">Add model parameters</span></strong><span class="koboSpan" id="kobo.1224.1"> (</span><span class="koboSpan" id="kobo.1225.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;os+g+p&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/828.png" style="vertical-align:-0.483em;height:1.132em;width:2.378em"/></span><span class="koboSpan" id="kobo.1226.1">): Each GPU holds a portion of </span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1">all components.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1228.1">To understand how the algorithm works, we’ll assume we use </span><span class="koboSpan" id="kobo.1229.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;os+g+p&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/829.png" style="vertical-align:-0.483em;height:1.132em;width:2.433em"/></span><span class="koboSpan" id="kobo.1230.1">, a model with </span><em class="italic"><span class="koboSpan" id="kobo.1231.1">N</span></em><span class="koboSpan" id="kobo.1232.1"> layers and </span><em class="italic"><span class="koboSpan" id="kobo.1233.1">N</span></em><span class="koboSpan" id="kobo.1234.1"> GPUs. </span><span class="koboSpan" id="kobo.1234.2">Each layer is distributed on one GPU—the first layer is on </span><span class="koboSpan" id="kobo.1235.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/830.png" style="vertical-align:-0.340em;height:1.004em;width:2.319em"/></span><span class="koboSpan" id="kobo.1236.1">, the second layer is on </span><span class="koboSpan" id="kobo.1237.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/831.png" style="vertical-align:-0.333em;height:0.997em;width:2.332em"/></span><span class="koboSpan" id="kobo.1238.1">, </span><br/><span class="koboSpan" id="kobo.1239.1">and so on. </span><span class="koboSpan" id="kobo.1239.2">Let’s start with the forward phase. </span><span class="koboSpan" id="kobo.1239.3">First, </span><span class="koboSpan" id="kobo.1240.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/832.png" style="vertical-align:-0.340em;height:1.004em;width:2.343em"/></span><span class="koboSpan" id="kobo.1241.1"> receives </span><span class="koboSpan" id="kobo.1242.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;Data&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/833.png" style="vertical-align:-0.340em;height:0.989em;width:2.240em"/></span><span class="koboSpan" id="kobo.1243.1">. </span><span class="koboSpan" id="kobo.1243.2">Since it holds the first layer of the model, it can feed it the input and calculate its activations independently. </span><span class="koboSpan" id="kobo.1243.3">At the same time, </span><span class="koboSpan" id="kobo.1244.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/834.png" style="vertical-align:-0.340em;height:1.004em;width:2.380em"/></span><span class="koboSpan" id="kobo.1245.1"> broadcasts the parameters of the first layer to all other GPUs. </span><span class="koboSpan" id="kobo.1245.2">Each GPU now holds the first layer parameters in addition to its own portion of the model parameters. </span><span class="koboSpan" id="kobo.1245.3">In this way, </span><span class="koboSpan" id="kobo.1246.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;i&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/835.png" style="vertical-align:-0.333em;height:0.997em;width:2.162em"/></span><span class="koboSpan" id="kobo.1247.1"> can process its own input, </span><span class="koboSpan" id="kobo.1248.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;Data&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;i&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/836.png" style="vertical-align:-0.333em;height:0.982em;width:2.117em"/></span><span class="koboSpan" id="kobo.1249.1">, through the first layer, just as </span><span class="koboSpan" id="kobo.1250.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/832.png" style="vertical-align:-0.340em;height:1.004em;width:2.343em"/></span><span class="koboSpan" id="kobo.1251.1"> did. </span><span class="koboSpan" id="kobo.1251.2">Once a GPU computes the activations of the first layer, it deletes its parameters from its memory (except </span><span class="koboSpan" id="kobo.1252.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/838.png" style="vertical-align:-0.340em;height:1.004em;width:2.334em"/></span><span class="koboSpan" id="kobo.1253.1">, which preserves them). </span><span class="koboSpan" id="kobo.1253.2">We repeat the same steps, this time with the second layer. </span><span class="koboSpan" id="kobo.1254.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/839.png" style="vertical-align:-0.333em;height:0.997em;width:2.289em"/></span><span class="koboSpan" id="kobo.1255.1"> broadcasts its parameters so that all GPUs can continue with the forward phase. </span><span class="koboSpan" id="kobo.1255.2">After this, all but </span><span class="koboSpan" id="kobo.1256.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;GPU&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/840.png" style="vertical-align:-0.333em;height:0.997em;width:2.309em"/></span><span class="koboSpan" id="kobo.1257.1"> delete the second layer parameters. </span><span class="koboSpan" id="kobo.1257.2">This process continues until all GPUs produce model output. </span><span class="koboSpan" id="kobo.1257.3">Then, the loss function is aggregated across all GPUs. </span><span class="koboSpan" id="kobo.1257.4">Next, the backward phase starts, which works in the same way as the forward one, but this time the GPUs broadcast both the gradients and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">optimizer states.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1259.1">Mixed precision training</span></h3>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1260.1">Mixed precision training</span></strong><span class="koboSpan" id="kobo.1261.1"> (</span><a href="https://arxiv.org/abs/1710.03740"><span class="koboSpan" id="kobo.1262.1">https://arxiv.org/abs/1710.03740</span></a><span class="koboSpan" id="kobo.1263.1">) is the idea that not all values have to be stored </span><a id="_idIndexMarker1256"/><span class="koboSpan" id="kobo.1264.1">with 32-bit (double or full) floating-point</span><a id="_idIndexMarker1257"/><span class="koboSpan" id="kobo.1265.1"> precision (</span><strong class="bold"><span class="koboSpan" id="kobo.1266.1">FP32</span></strong><span class="koboSpan" id="kobo.1267.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.1268.1">Float32</span></strong><span class="koboSpan" id="kobo.1269.1"> data format). </span><span class="koboSpan" id="kobo.1269.2">Research has shown that storing some values into</span><a id="_idIndexMarker1258"/><span class="koboSpan" id="kobo.1270.1"> lower 16-bit (single or half) floating-point</span><a id="_idIndexMarker1259"/><span class="koboSpan" id="kobo.1271.1"> precision (</span><strong class="bold"><span class="koboSpan" id="kobo.1272.1">FP16</span></strong><span class="koboSpan" id="kobo.1273.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.1274.1">Float16</span></strong><span class="koboSpan" id="kobo.1275.1">) doesn’t degrade the model performance. </span><span class="koboSpan" id="kobo.1275.2">The weights, activations, gradients, and optimizer states are stored as FP16. </span><span class="koboSpan" id="kobo.1275.3">In addition, the model retains an FP32 master copy of the weights. </span><span class="koboSpan" id="kobo.1275.4">The forward and backward passes use FP16 weights, but the results are optimal when the weight update operation uses the FP32 master copy. </span><span class="koboSpan" id="kobo.1275.5">One possible explanation is that the weight update formula uses the gradients multiplied by the learning rate, and the result might become too small to be represented in FP16. </span><span class="koboSpan" id="kobo.1275.6">Another explanation is that the ratio of the weight value to the weight update is very large, which could lead to the weight update </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">becoming zero.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1277.1">Bfloat16 and TensorFloat32</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1278.1">The Google Brain division developed the </span><strong class="bold"><span class="koboSpan" id="kobo.1279.1">brain floating-point</span></strong><span class="koboSpan" id="kobo.1280.1"> format (hence the name, </span><strong class="bold"><span class="koboSpan" id="kobo.1281.1">bfloat</span></strong><span class="koboSpan" id="kobo.1282.1">) for </span><strong class="bold"><span class="koboSpan" id="kobo.1283.1">machine learning</span></strong><span class="koboSpan" id="kobo.1284.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1285.1">ML</span></strong><span class="koboSpan" id="kobo.1286.1">) applications. </span><span class="koboSpan" id="kobo.1286.2">The standard FP16 format has one sign bit, five </span><a id="_idIndexMarker1260"/><span class="koboSpan" id="kobo.1287.1">exponent bits, and ten mantissa bits. </span><span class="koboSpan" id="kobo.1287.2">In contrast, bfloat16 has eight exponent and seven mantissa bits. </span><span class="koboSpan" id="kobo.1287.3">The exponent bits are the same as FP32. </span><span class="koboSpan" id="kobo.1287.4">Bfloat16 comes close to FP32 in terms of performance on ML tasks. </span><span class="koboSpan" id="kobo.1287.5">We also have </span><strong class="bold"><span class="koboSpan" id="kobo.1288.1">TensorFloat-32</span></strong><span class="koboSpan" id="kobo.1289.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1290.1">TF32</span></strong><span class="koboSpan" id="kobo.1291.1">)—a 19-bit</span><a id="_idIndexMarker1261"/><span class="koboSpan" id="kobo.1292.1"> format developed by NVIDIA for ML purposes with 8 exponent and 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">mantissa bits.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1294.1">Pre-training peculiarities and summary</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1295.1">In this section, we’ll discuss some LLM </span><a id="_idIndexMarker1262"/><span class="koboSpan" id="kobo.1296.1">pre-training peculiarities. </span><span class="koboSpan" id="kobo.1296.2">Let’s start with the mini-batch size. </span><span class="koboSpan" id="kobo.1296.3">Ideally, we would compute the gradients over the entire training dataset and only then perform one weight update. </span><span class="koboSpan" id="kobo.1296.4">However, large datasets and models make this computationally infeasible. </span><span class="koboSpan" id="kobo.1296.5">The opposite extreme is to perform one weight update per training sample. </span><span class="koboSpan" id="kobo.1296.6">But then, the training would be susceptible to outlier samples, which might steer the loss function to suboptimal local minima. </span><span class="koboSpan" id="kobo.1296.7">Mini-batch training is a compromise that makes it possible to fit within the available computational resources and avoid the influence of outlier samples. </span><span class="koboSpan" id="kobo.1296.8">But in theory, the larger the mini-batch size, the better. </span><span class="koboSpan" id="kobo.1296.9">LLM training is distributed across multiple devices, which makes it possible (and even desirable) to use large batch sizes. </span><span class="koboSpan" id="kobo.1296.10">The batch size can vary between 32K to 8.25M tokens depending on the model. </span><span class="koboSpan" id="kobo.1296.11">In addition, it can be dynamic and increase as the training progresses. </span><span class="koboSpan" id="kobo.1296.12">Empirical experiments have demonstrated that this technique stabilizes </span><span class="No-Break"><span class="koboSpan" id="kobo.1297.1">the training.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1298.1">Next, let’s focus on the learning rate, </span><em class="italic"><span class="koboSpan" id="kobo.1299.1">η</span></em><span class="koboSpan" id="kobo.1300.1">. </span><span class="koboSpan" id="kobo.1300.2">Although Adam implements an adaptive learning rate, most LLMs start with</span><a id="_idIndexMarker1263"/><span class="koboSpan" id="kobo.1301.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.1302.1">warmup phase</span></strong><span class="koboSpan" id="kobo.1303.1"> to stabilize the training. </span><span class="koboSpan" id="kobo.1303.2">More specifically, during the first 0.1% to 0.5% of the training steps, the learning rate gradually increases from around </span><span class="koboSpan" id="kobo.1304.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/841.png" style="vertical-align:-0.257em;height:0.963em;width:4.542em"/></span><span class="koboSpan" id="kobo.1305.1"> to </span><span class="koboSpan" id="kobo.1306.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/842.png" style="vertical-align:-0.257em;height:0.953em;width:4.529em"/></span><span class="koboSpan" id="kobo.1307.1">. </span><br/><span class="koboSpan" id="kobo.1308.1">Then, the learning rate gradually decreases to around 10% of its maximum value following a cosine (or linear) </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">decay strategy.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1310.1">LLM training also </span><a id="_idIndexMarker1264"/><span class="koboSpan" id="kobo.1311.1">uses </span><strong class="bold"><span class="koboSpan" id="kobo.1312.1">gradient clipping</span></strong><span class="koboSpan" id="kobo.1313.1">—a technique that prevents the exploding gradients problem. </span><span class="koboSpan" id="kobo.1313.2">One way to implement it is to clip </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">by value:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1315.1">If l</span><strong class="bold"><span class="koboSpan" id="kobo.1316.1">g</span></strong><span class="koboSpan" id="kobo.1317.1">l ≥ </span><em class="italic"><span class="koboSpan" id="kobo.1318.1">max_threshold</span></em><span class="koboSpan" id="kobo.1319.1"> or l</span><strong class="bold"><span class="koboSpan" id="kobo.1320.1">g</span></strong><span class="koboSpan" id="kobo.1321.1">l ≤ </span><em class="italic"><span class="koboSpan" id="kobo.1322.1">min_threshold</span></em><span class="koboSpan" id="kobo.1323.1"> then </span><strong class="bold"><span class="koboSpan" id="kobo.1324.1">g</span></strong><span class="koboSpan" id="kobo.1325.1"> ← </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1326.1">relevant_threshold</span></em></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1327.1">Here, </span><strong class="bold"><span class="koboSpan" id="kobo.1328.1">g</span></strong><span class="koboSpan" id="kobo.1329.1"> is a vector with all gradient values (l</span><strong class="bold"><span class="koboSpan" id="kobo.1330.1">g</span></strong><span class="koboSpan" id="kobo.1331.1">l is the norm or the vector or its absolute value). </span><span class="koboSpan" id="kobo.1331.2">First, we select </span><em class="italic"><span class="koboSpan" id="kobo.1332.1">min_threshold</span></em><span class="koboSpan" id="kobo.1333.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1334.1">max_threshold</span></em><span class="koboSpan" id="kobo.1335.1"> values. </span><span class="koboSpan" id="kobo.1335.2">Then, we clip the value of the gradient to the threshold in the weight update formula if it exceeds </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">these boundaries.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1337.1">Another option is to clip </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">by norm:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1339.1">If l</span><strong class="bold"><span class="koboSpan" id="kobo.1340.1">g</span></strong><span class="koboSpan" id="kobo.1341.1">l ≥ </span><em class="italic"><span class="koboSpan" id="kobo.1342.1">threshold</span></em><span class="koboSpan" id="kobo.1343.1">, then </span><strong class="bold"><span class="koboSpan" id="kobo.1344.1">g</span></strong><span class="koboSpan" id="kobo.1345.1"> ← </span><em class="italic"><span class="koboSpan" id="kobo.1346.1">threshold</span></em><span class="koboSpan" id="kobo.1347.1"> * </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1348.1">g</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">/l</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1350.1">g</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1351.1">l</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1352.1">Here, </span><strong class="bold"><span class="koboSpan" id="kobo.1353.1">g</span></strong><span class="koboSpan" id="kobo.1354.1">/l</span><strong class="bold"><span class="koboSpan" id="kobo.1355.1">g</span></strong><span class="koboSpan" id="kobo.1356.1">l is a unit vector. </span><span class="koboSpan" id="kobo.1356.2">It has the same direction as the original, but its length is 1. </span><span class="koboSpan" id="kobo.1356.3">The value of every element in the unit vector is in the [0:1] range. </span><span class="koboSpan" id="kobo.1356.4">By multiplying it by the </span><em class="italic"><span class="koboSpan" id="kobo.1357.1">threshold</span></em><span class="koboSpan" id="kobo.1358.1">, every element lies within the [0: threshold] range. </span><span class="koboSpan" id="kobo.1358.2">In this way, norm clipping scales the gradients within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">pre-defined threshold.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1360.1">The following table </span><a id="_idIndexMarker1265"/><span class="koboSpan" id="kobo.1361.1">summarizes the training properties of some </span><span class="No-Break"><span class="koboSpan" id="kobo.1362.1">popular LLMs:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1009">
<span class="koboSpan" id="kobo.1363.1"><img alt="Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)" src="image/B19627_08_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1364.1">Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1365.1">This concludes our introduction to pre-training LLMs. </span><span class="koboSpan" id="kobo.1365.2">Next, let’s focus on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">FT phase.</span></span></p>
<h2 id="_idParaDest-158" lang="en-GB"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.1367.1">FT with RLHF</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1368.1">So far, we have focused </span><a id="_idIndexMarker1266"/><span class="koboSpan" id="kobo.1369.1">on the pre-training phase of LLMs. </span><span class="koboSpan" id="kobo.1369.2">The pre-training objective of an LLM is to predict the next token based on (primarily) a web page training dataset. </span><span class="koboSpan" id="kobo.1369.3">However, pre-trained models can express undesirable behaviors. </span><span class="koboSpan" id="kobo.1369.4">They can often make up facts, generate biased or toxic text, or simply not follow user instructions. </span><span class="koboSpan" id="kobo.1369.5">Yet, their purpose is to interact with humans in a </span><em class="italic"><span class="koboSpan" id="kobo.1370.1">helpful</span></em><span class="koboSpan" id="kobo.1371.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1372.1">honest</span></em><span class="koboSpan" id="kobo.1373.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1374.1">harmless</span></em><span class="koboSpan" id="kobo.1375.1"> way. </span><span class="koboSpan" id="kobo.1375.2">In this section, we’ll discuss the technique of RLHF, which makes it possible to fine-tune the LLM for better alignment with human values (also known </span><a id="_idIndexMarker1267"/><span class="koboSpan" id="kobo.1376.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.1377.1">alignment tuning</span></strong><span class="koboSpan" id="kobo.1378.1">). </span><span class="koboSpan" id="kobo.1378.2">More specifically, we’ll focus on the technique described in </span><em class="italic"><span class="koboSpan" id="kobo.1379.1">Training language models to follow instructions with human feedback</span></em><span class="koboSpan" id="kobo.1380.1"> (</span><a href="https://arxiv.org/abs/2203.02155"><span class="koboSpan" id="kobo.1381.1">https://arxiv.org/abs/2203.02155</span></a><span class="koboSpan" id="kobo.1382.1">) by OpenAI. </span><span class="koboSpan" id="kobo.1382.2">They apply RLHF on a GPT-3 model to produce the GPT-3.5 family of models. </span><span class="koboSpan" id="kobo.1382.3">This is part of the secret sauce that makes ChatGPT so good at interacting </span><span class="No-Break"><span class="koboSpan" id="kobo.1383.1">with users.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1384.1">The FT starts where the pre-training ends—with the pre-trained LLM. </span><span class="koboSpan" id="kobo.1384.2">The following diagram shows the three steps of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">RLHF process:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1010">
<span class="koboSpan" id="kobo.1386.1"><img alt="Figure 8.14 – Left: supervised FT; middle: reward model training; right: LLM RLHF (inspired by https://arxiv.org/abs/2203.02155)" src="image/B19627_08_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1387.1">Figure 8.14 – Left: supervised FT; middle: reward model training; right: LLM RLHF (inspired by https://arxiv.org/abs/2203.02155)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1388.1">First </span><a id="_idIndexMarker1268"/><span class="koboSpan" id="kobo.1389.1">is </span><strong class="bold"><span class="koboSpan" id="kobo.1390.1">supervised FT</span></strong><span class="koboSpan" id="kobo.1391.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1392.1">SFT</span></strong><span class="koboSpan" id="kobo.1393.1">, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1394.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1395.1">.14</span></em><span class="koboSpan" id="kobo.1396.1">—left). </span><span class="koboSpan" id="kobo.1396.2">It uses human labelers to create a dataset of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1397.1">[prompt: response]</span></strong><span class="koboSpan" id="kobo.1398.1"> samples, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.1399.1">prompt</span></strong><span class="koboSpan" id="kobo.1400.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1401.1">response</span></strong><span class="koboSpan" id="kobo.1402.1"> are source and target token sequences, respectively. </span><span class="koboSpan" id="kobo.1402.2">This dataset serves to fine-tune the LLM using the same target as pre-training—to predict the next token of the response, given the </span><a id="_idIndexMarker1269"/><span class="koboSpan" id="kobo.1403.1">prompt. </span><span class="koboSpan" id="kobo.1403.2">The fine-tuned LLM serves as a base for the next </span><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">two steps.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1405.1">On the need for pre-training and three-step FT</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1406.1">The SFT step implicitly answers an unasked question—why do we need pre-training and three-step FT to train our model? </span><span class="koboSpan" id="kobo.1406.2">The reason is that generating a human-labeled training dataset is not scalable and represents a major bottleneck. </span><span class="koboSpan" id="kobo.1406.3">For example, the pre-training dataset can have over a trillion tokens; generating prompts and their respective responses of such magnitude with human labelers is infeasible. </span><span class="koboSpan" id="kobo.1406.4">Therefore, we need pre-training to provide the LLM with a solid foundation, which we can fine-tune with a much </span><span class="No-Break"><span class="koboSpan" id="kobo.1407.1">smaller dataset.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1408.1">The second </span><a id="_idIndexMarker1270"/><span class="koboSpan" id="kobo.1409.1">step is </span><strong class="bold"><span class="koboSpan" id="kobo.1410.1">reward model</span></strong><span class="koboSpan" id="kobo.1411.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1412.1">RM</span></strong><span class="koboSpan" id="kobo.1413.1">) training (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1414.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1415.1">.14</span></em><span class="koboSpan" id="kobo.1416.1">—center). </span><span class="koboSpan" id="kobo.1416.2">We start with a dataset of prompts, and we use the fine-tuned LLM to generate several responses for each prompt. </span><span class="koboSpan" id="kobo.1416.3">Then, a human labeler assigns scalar scores to the responses according to their suitability to the prompt, using their own judgment. </span><span class="koboSpan" id="kobo.1416.4">This step aims to train the RM to assign a score, </span><span class="koboSpan" id="kobo.1417.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/843.png" style="vertical-align:-0.340em;height:0.788em;width:0.645em"/></span><span class="koboSpan" id="kobo.1418.1">, to a response like a human labeler. </span><span class="koboSpan" id="kobo.1418.2">But to do so, the labeler also compares the responses and ranks (orders) them from best to worst, regardless of their scalar score. </span><span class="koboSpan" id="kobo.1418.3">This is necessary because different labelers can give inconsistent scores of the same response, but ranking the responses is much more consistent. </span><span class="koboSpan" id="kobo.1418.4">The prompts and the ranked responses form a new dataset, where each pair of winner and loser responses creates one training sample. </span><span class="koboSpan" id="kobo.1418.5">For example, the responses A &gt; B &gt; C will form the pairs </span><strong class="source-inline"><span class="koboSpan" id="kobo.1419.1">[(A, B), (A, C), (B, C)]</span></strong><span class="koboSpan" id="kobo.1420.1">. </span><span class="koboSpan" id="kobo.1420.2">This dataset trains the RM, which is based on the fine-tuned LLM. </span><span class="koboSpan" id="kobo.1420.3">Its output next-token classifier is replaced with a randomly initialized regression layer, which outputs the predicted scalar score of a given response. </span><span class="koboSpan" id="kobo.1420.4">The RM computes the scores of both responses of each pair. </span><span class="koboSpan" id="kobo.1420.5">The difference between them participates in the loss function. </span><span class="koboSpan" id="kobo.1420.6">This is an example </span><a id="_idIndexMarker1271"/><span class="koboSpan" id="kobo.1421.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.1422.1">transfer learning</span></strong><span class="koboSpan" id="kobo.1423.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1424.1">TL</span></strong><span class="koboSpan" id="kobo.1425.1">), which aims to train the new regression layer on top of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">original LLM.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1427.1">The third step </span><a id="_idIndexMarker1272"/><span class="koboSpan" id="kobo.1428.1">is to train the</span><a id="_idIndexMarker1273"/><span class="koboSpan" id="kobo.1429.1"> LLM using RL with the RM and </span><strong class="bold"><span class="koboSpan" id="kobo.1430.1">proximal policy optimization</span></strong><span class="koboSpan" id="kobo.1431.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1432.1">PPO</span></strong><span class="koboSpan" id="kobo.1433.1">) (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1434.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1435.1">.14</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">—right).</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1437.1">A recap of RL</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1438.1">To understand the third step, let’s recap our introduction to RL from </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1439.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.1440.1">. </span><span class="koboSpan" id="kobo.1440.2">We have a system of environment and an agent. </span><span class="koboSpan" id="kobo.1440.3">The agent can take one of a number of actions that change the state of the environment. </span><span class="koboSpan" id="kobo.1440.4">The environment reacts to the agent’s actions and provides its modified state and reward (or penalty) signals that help the agent to decide its next action. </span><span class="koboSpan" id="kobo.1440.5">The decision-making algorithm of the agent is called a policy. </span><span class="koboSpan" id="kobo.1440.6">The agent’s goal is to maximize the total rewards received throughout the </span><span class="No-Break"><span class="koboSpan" id="kobo.1441.1">training episodes.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1442.1">In this scenario, the policy of the agent is represented by the fine-tuned LLM. </span><span class="koboSpan" id="kobo.1442.2">The token vocabulary represents the actions it can take—that is, the agent’s action is to select the next token in the sequence. </span><span class="koboSpan" id="kobo.1442.3">The environment presents the LLM with a random prompt, and the agent (LLM) generates a response. </span><span class="koboSpan" id="kobo.1442.4">Then, the RM, part of the environment, scores the generated response. </span><span class="koboSpan" id="kobo.1442.5">The RM score is the reward sent to the agent and serves to update </span><span class="No-Break"><span class="koboSpan" id="kobo.1443.1">its</span></span><span class="No-Break"><a id="_idIndexMarker1274"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1444.1"> parameters.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1445.1">In the next section, we’ll discuss what makes LLMs different from </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">other models.</span></span></p>
<h1 id="_idParaDest-159" lang="en-GB"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.1447.1">Emergent abilities of LLMs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1448.1">In this section, we’ll </span><a id="_idIndexMarker1275"/><span class="koboSpan" id="kobo.1449.1">discuss the phenomenon of </span><strong class="bold"><span class="koboSpan" id="kobo.1450.1">emergent abilities</span></strong><span class="koboSpan" id="kobo.1451.1"> of LLMs, first summarized in </span><a href="https://arxiv.org/abs/2206.07682"><span class="koboSpan" id="kobo.1452.1">https://arxiv.org/abs/2206.07682</span></a><span class="koboSpan" id="kobo.1453.1">. </span><span class="koboSpan" id="kobo.1453.2">The paper defines emergent abilities </span><span class="No-Break"><span class="koboSpan" id="kobo.1454.1">as follows:</span></span></p>
<p class="author-quote" lang="en-GB"><span class="koboSpan" id="kobo.1455.1">An ability is emergent if it is not present in smaller models but is present in larger models.</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1456.1">These abilities represent a qualitative difference between large and small language models, which cannot be predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">by extrapolation.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1458.1">We’ll start with the ability known</span><a id="_idIndexMarker1276"/><span class="koboSpan" id="kobo.1459.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.1460.1">few-shot prompting</span></strong><span class="koboSpan" id="kobo.1461.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1462.1">in-context learning</span></strong><span class="koboSpan" id="kobo.1463.1">), popularized </span><a id="_idIndexMarker1277"/><span class="koboSpan" id="kobo.1464.1">by GPT-3. </span><span class="koboSpan" id="kobo.1464.2">Here, the initial user prompt is an instruction the LLM has to follow through its response without any additional training. </span><span class="koboSpan" id="kobo.1464.3">The prompt itself may describe with natural text one or more training examples (hence, the term </span><em class="italic"><span class="koboSpan" id="kobo.1465.1">few-shot</span></em><span class="koboSpan" id="kobo.1466.1">). </span><span class="koboSpan" id="kobo.1466.2">This is the only context that the LLM can use for training before generating its response. </span><span class="koboSpan" id="kobo.1466.3">The following diagram shows an example of a </span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1">few-shot prompt:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1012">
<span class="koboSpan" id="kobo.1468.1"><img alt="Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)" src="image/B19627_08_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1469.1">Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1470.1">Next, let’s discuss the ability of LLMs to </span><a id="_idIndexMarker1278"/><span class="koboSpan" id="kobo.1471.1">solve complex multi-step reasoning tasks with the help of а </span><strong class="bold"><span class="koboSpan" id="kobo.1472.1">chain-of-thought</span></strong><span class="koboSpan" id="kobo.1473.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1474.1">CoT</span></strong><span class="koboSpan" id="kobo.1475.1">) prompting strategy (</span><em class="italic"><span class="koboSpan" id="kobo.1476.1">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</span></em><span class="koboSpan" id="kobo.1477.1">, https://arxiv.org/abs/2201.11903). </span><span class="koboSpan" id="kobo.1477.2">This type of prompt presents the LLM with a series of intermediate steps that can guide the model to reach the final task answer. </span><span class="koboSpan" id="kobo.1477.3">The following diagram shows a comparison between regular and </span><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">CoT prompts:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1013">
<span class="koboSpan" id="kobo.1479.1"><img alt="Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired by https://arxiv.org/abs/2201.11903)" src="image/B19627_08_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1480.1">Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired by https://arxiv.org/abs/2201.11903)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1481.1">It is speculated that this </span><a id="_idIndexMarker1279"/><span class="koboSpan" id="kobo.1482.1">ability is obtained by including source code in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1483.1">training data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1484.1">Let’s also note that the alignment tuning we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1485.1">FT with RLHF</span></em><span class="koboSpan" id="kobo.1486.1"> section is also an emergent ability, as it only improves the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">large models.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1488.1">The following diagram shows how the performance on various tasks significantly improves with the scale of </span><span class="No-Break"><span class="koboSpan" id="kobo.1489.1">the model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1014">
<span class="koboSpan" id="kobo.1490.1"><img alt="Figure 8.17 – Emergent abilities are only present in large-scale models (source: https://arxiv.org/abs/2206.07682)" src="image/B19627_08_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1491.1">Figure 8.17 – Emergent abilities are only present in large-scale models (source: </span><a href="https://arxiv.org/abs/2206.07682"><span class="koboSpan" id="kobo.1492.1">https://arxiv.org/abs/2206.07682</span></a><span class="koboSpan" id="kobo.1493.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1494.1">The </span><em class="italic"><span class="koboSpan" id="kobo.1495.1">x</span></em><span class="koboSpan" id="kobo.1496.1"> axis shows the</span><a id="_idIndexMarker1280"/><span class="koboSpan" id="kobo.1497.1"> training computational time for each model (measured in FLOPS), and the </span><em class="italic"><span class="koboSpan" id="kobo.1498.1">y</span></em><span class="koboSpan" id="kobo.1499.1"> axis shows the model accuracy. </span><span class="koboSpan" id="kobo.1499.2">The graphs show the model accuracy on three </span><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">different benchmarks:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1501.1">An arithmetic benchmark that tests multiplication of 2-digit numbers, as well as the addition and subtraction of </span><span class="No-Break"><span class="koboSpan" id="kobo.1502.1">3-digit numbers</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1503.1">57 college-level tests covering a range of topics, including math, history, law, </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">and more</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1505.1">Chain-of-thought </span><a id="_idIndexMarker1281"/><span class="koboSpan" id="kobo.1506.1">versus regular prompt on math word problems, like the one described in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1507.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1508.1">.16</span></em></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1509.1">This concludes our theoretical introduction to LLMs. </span><span class="koboSpan" id="kobo.1509.2">Next, let’s see how to use them </span><span class="No-Break"><span class="koboSpan" id="kobo.1510.1">in practice.</span></span></p>
<h1 id="_idParaDest-160" lang="en-GB"><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.1511.1">Introducing Hugging Face Transformers</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1512.1">So far, we have discussed </span><a id="_idIndexMarker1282"/><span class="koboSpan" id="kobo.1513.1">in depth the architecture and training properties of LLMs. </span><span class="koboSpan" id="kobo.1513.2">But the sad truth is that these models are so large it is unlikely that you or I would build one from scratch. </span><span class="koboSpan" id="kobo.1513.3">Instead, we’ll probably use a pre-trained model. </span><span class="koboSpan" id="kobo.1513.4">In this section, we’ll see how to do that with the Hugging Face Transformers</span><a id="_idIndexMarker1283"/><span class="koboSpan" id="kobo.1514.1"> library (</span><a href="https://github.com/huggingface/transformers"><span class="koboSpan" id="kobo.1515.1">https://github.com/huggingface/transformers</span></a><span class="koboSpan" id="kobo.1516.1">). </span><span class="koboSpan" id="kobo.1516.2">As the name suggests, its focus is the transformer architecture. </span><span class="koboSpan" id="kobo.1516.3">It supports three different backends—PyTorch, TensorFlow, and JAX (as usual, we’ll focus on PyTorch). </span><span class="koboSpan" id="kobo.1516.4">It is open source and available for commercial use. </span><span class="koboSpan" id="kobo.1516.5">The company behind it, Hugging Face, also develops the Hugging Face Hub—a complementary service to the library cloud-based platform. </span><span class="koboSpan" id="kobo.1516.6">It supports hosting and/or running Git repositories (such as GitHub), transformer models, datasets, and web applications (intended for </span><strong class="bold"><span class="koboSpan" id="kobo.1517.1">proof-of-concept</span></strong><span class="koboSpan" id="kobo.1518.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1519.1">POC</span></strong><span class="koboSpan" id="kobo.1520.1">) demos of</span><a id="_idIndexMarker1284"/><span class="koboSpan" id="kobo.1521.1"> ML applications). </span><span class="koboSpan" id="kobo.1521.2">With that, let’s proceed with our </span><span class="No-Break"><span class="koboSpan" id="kobo.1522.1">first example.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1523.1">We’ll start with </span><a id="_idIndexMarker1285"/><span class="koboSpan" id="kobo.1524.1">a basic use case—we’ll load a pre-trained Llama 2 chat 7B model and use it to generate a response to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1525.1">user’s prompt:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1526.1">First, we </span><a id="_idIndexMarker1286"/><span class="koboSpan" id="kobo.1527.1">add in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1528.1">import</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1529.1"> statements:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1530.1">
import torch
from transformers import AutoTokenizer, pipeline</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1531.1">Then, we define the model’s name in </span><span class="No-Break"><span class="koboSpan" id="kobo.1532.1">a variable:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1533.1">
model = "meta-llama/Llama-2-7b-chat-hf"</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1534.1">Every transformer model has a unique identifier, which works for the Hugging Face model hub. </span><span class="koboSpan" id="kobo.1534.2">The Hub hosts all models, and the library can automatically download the model weights behind the scenes. </span><span class="koboSpan" id="kobo.1534.3">In this case, we use the smallest Llama 2 7B RLHF-optimized model to preserve </span><span class="No-Break"><span class="koboSpan" id="kobo.1535.1">computational resources.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1536.1">Next, let’s load the </span><span class="No-Break"><span class="koboSpan" id="kobo.1537.1">model tokenizer:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1538.1">
tokenizer = AutoTokenizer.from_pretrained(model)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1539.1">The various LLM models use different tokenizers. </span><span class="koboSpan" id="kobo.1539.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1540.1">AutoTokenizer</span></strong><span class="koboSpan" id="kobo.1541.1"> instance can select the right one based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">model identifier.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1543.1">Let’s see the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1544.1">tokenizer</span></strong><span class="koboSpan" id="kobo.1545.1"> properties by printing it </span><span class="No-Break"><span class="koboSpan" id="kobo.1546.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1547.1">print(tokenizer)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1548.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1549.1">
LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken("&lt;s&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("&lt;/s&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("&lt;unk&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1550.1">The tokenizer is based on </span><a id="_idIndexMarker1287"/><span class="koboSpan" id="kobo.1551.1">a byte-level </span><strong class="bold"><span class="koboSpan" id="kobo.1552.1">byte-pair encoding</span></strong><span class="koboSpan" id="kobo.1553.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1554.1">BPE</span></strong><span class="koboSpan" id="kobo.1555.1">). </span><span class="koboSpan" id="kobo.1555.2">This output gives us useful information about the token vocabulary size, special tokens, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1556.1">other properties.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1557.1">Then, we </span><a id="_idIndexMarker1288"/><span class="koboSpan" id="kobo.1558.1">create a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1559.1">pipeline</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1560.1"> instance:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1561.1">
text_gen_pipeline = pipeline(
    task='text-generation',
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map='auto',
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1562.1">The pipeline abstraction makes it easy to use the models for inference. </span><span class="koboSpan" id="kobo.1562.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1563.1">task</span></strong><span class="koboSpan" id="kobo.1564.1"> parameter determines the type of task to solve. </span><span class="koboSpan" id="kobo.1564.2">The library supports multiple tasks, also covering images and audio. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1565.1">pipeline</span></strong><span class="koboSpan" id="kobo.1566.1"> will return different objects, depending on the task. </span><span class="koboSpan" id="kobo.1566.2">It also takes care of downloading and initializing the model. </span><span class="koboSpan" id="kobo.1566.3">In addition, we set the datatype to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1567.1">torch.bfloat16</span></strong><span class="koboSpan" id="kobo.1568.1"> to reduce the memory footprint. </span><span class="koboSpan" id="kobo.1568.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1569.1">device_map='auto'</span></strong><span class="koboSpan" id="kobo.1570.1"> parameter allows the Accelerate library (</span><a href="https://github.com/huggingface/accelerate"><span class="koboSpan" id="kobo.1571.1">https://github.com/huggingface/accelerate</span></a><span class="koboSpan" id="kobo.1572.1">) to run the model across any distributed </span><span class="No-Break"><span class="koboSpan" id="kobo.1573.1">configuration automatically.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1574.1">We can see the model definition with the following command: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1575.1">print(text_gen_pipeline.model)</span></strong><span class="koboSpan" id="kobo.1576.1">. </span><span class="koboSpan" id="kobo.1576.2">For example, the command output for the largest 70B Llama 2 model, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1577.1">Llama-2-70b-hf</span></strong><span class="koboSpan" id="kobo.1578.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1579.1">is this:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1580.1">
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 8192)
    (layers): ModuleList(
      (0-79): 80 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in=8192, out=8192)
          (k_proj): Linear(in=8192, out=1024)
          (v_proj): Linear(in=8192, out=1024)
          (o_proj): Linear(in=8192, out=8192)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in=8192, out=28672)
          (up_proj): Linear(in=8192, out=28672)
          (down_proj): Linear(in=28672, out=8192)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in=8192, out=32000)
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1581.1">To fit the page line</span><a id="_idIndexMarker1289"/><span class="koboSpan" id="kobo.1582.1"> length, I have modified the original output: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1583.1">in</span></strong><span class="koboSpan" id="kobo.1584.1"> stands for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1585.1">in_features</span></strong><span class="koboSpan" id="kobo.1586.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1587.1">out</span></strong><span class="koboSpan" id="kobo.1588.1"> stands for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1589.1">out_features</span></strong><span class="koboSpan" id="kobo.1590.1">, and all linear layers have an additional </span><strong class="source-inline"><span class="koboSpan" id="kobo.1591.1">bias=False</span></strong><span class="koboSpan" id="kobo.1592.1"> parameter. </span><span class="koboSpan" id="kobo.1592.2">The token vocabulary size is 32,000, and the embedding size (</span><span class="koboSpan" id="kobo.1593.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/844.png" style="vertical-align:-0.340em;height:1.051em;width:1.886em"/></span><span class="koboSpan" id="kobo.1594.1">) is 8,192. </span><span class="koboSpan" id="kobo.1594.2">The model has 80 identical decoder blocks (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1595.1">LlamaDecoderLayer</span></strong><span class="koboSpan" id="kobo.1596.1">). </span><span class="koboSpan" id="kobo.1596.2">Each block contains a </span><br/><span class="koboSpan" id="kobo.1597.1">self-attention sublayer (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1598.1">*_proj</span></strong><span class="koboSpan" id="kobo.1599.1"> are the projections), a FFN with a single hidden layer (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1600.1">LlamaMLP</span></strong><span class="koboSpan" id="kobo.1601.1">), rotary</span><a id="_idIndexMarker1290"/><span class="koboSpan" id="kobo.1602.1"> embeddings (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1603.1">LlamaRotaryEmbedding</span></strong><span class="koboSpan" id="kobo.1604.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.1605.1">root mean square</span></strong><span class="koboSpan" id="kobo.1606.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1607.1">RMS</span></strong><span class="koboSpan" id="kobo.1608.1">) normalization (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1609.1">LlamaRMSNorm</span></strong><span class="koboSpan" id="kobo.1610.1">), and SiLU activation (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1611.1">SiLUActivation</span></strong><span class="koboSpan" id="kobo.1612.1">). </span><span class="koboSpan" id="kobo.1612.2">Let’s note that the </span><a id="_idIndexMarker1291"/><span class="koboSpan" id="kobo.1613.1">activation differs from the SwiGLU activation defined in </span><span class="No-Break"><span class="koboSpan" id="kobo.1614.1">the paper.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1615.1">Then, we run </span><span class="No-Break"><span class="koboSpan" id="kobo.1616.1">the inference:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1617.1">
sequences = text_gen_pipeline(
    text_inputs='What is the answer to the ultimate question of life, the universe, and everything?',
    max_new_tokens=200,
    num_beams=2,
    top_k=10,
    top_p=0.9,
    do_sample=True,
    num_return_sequences=2,
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1618.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1619.1">text_inputs</span></strong><span class="koboSpan" id="kobo.1620.1"> is the user prompt, which serves as the initial input sequence. </span><span class="koboSpan" id="kobo.1620.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1621.1">num_return_sequences=2</span></strong><span class="koboSpan" id="kobo.1622.1"> parameter indicates that model will generate two separate responses (more on that later). </span><span class="koboSpan" id="kobo.1622.2">Here’s the </span><span class="No-Break"><span class="koboSpan" id="kobo.1623.1">first response:</span></span></p><pre class="source-code" lang="en-GB">
<strong class="bold"><span class="koboSpan" id="kobo.1624.1">Answer: The answer to the ultimate question of life, the universe, and everything is 42.</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.1625.1">Explanation:</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.1626.1">The answer 42 is a humorous and satirical response to the idea that there is a single, definitive answer to the ultimate questions of life, the universe, and everything. </span><span class="koboSpan" id="kobo.1626.2">The answer was first proposed by Douglas Adams in his science fiction series "The Hitchhiker's Guide to the Galaxy," where</span></strong></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1627.1">As we can</span><a id="_idIndexMarker1292"/><span class="koboSpan" id="kobo.1628.1"> see, the response is factually correct, but it was interrupted because of the limit set by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1629.1">max_new_tokens=200</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1630.1"> parameter.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1631.1">Let’s analyze the rest of the arguments of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1632.1">text_gen_pipeline</span></strong><span class="koboSpan" id="kobo.1633.1"> call, as they all relate to the strategy of generating new tokens. </span><span class="koboSpan" id="kobo.1633.2">The LLM ends with a softmax operation, which outputs a probability distribution over all tokens of the vocabulary. </span><span class="koboSpan" id="kobo.1633.3">The simplest way to select the next token is a greedy strategy, which always takes the one with the highest probability. </span><span class="koboSpan" id="kobo.1633.4">However, this is often suboptimal because it can hide high-probability words behind low-probability ones. </span><span class="koboSpan" id="kobo.1633.5">To clarify, a token might be assigned a low probability at the current state of the generated sequence, and another token would be selected in its place. </span><span class="koboSpan" id="kobo.1633.6">This means that the potential sequence, which includes the current low-probability token, will not exist. </span><span class="koboSpan" id="kobo.1633.7">Therefore, even if it had high-probability tokens down the line, we would never know because the low-probability token blocks it from further exploration. </span><span class="koboSpan" id="kobo.1633.8">One way to solve this is with</span><a id="_idIndexMarker1293"/><span class="koboSpan" id="kobo.1634.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.1635.1">beam search</span></strong><span class="koboSpan" id="kobo.1636.1"> strategy by setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.1637.1">do_sample=True</span></strong><span class="koboSpan" id="kobo.1638.1">. </span><span class="koboSpan" id="kobo.1638.2">In this case, the algorithm takes the probability of the entire current sequence rather than just the probability of the latest token. </span><span class="koboSpan" id="kobo.1638.3">Therefore, the new token will be the one that maximizes the overall probability of the sequence instead of its local probability. </span><span class="koboSpan" id="kobo.1638.4">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1639.1">num_beams=2</span></strong><span class="koboSpan" id="kobo.1640.1"> parameter indicates that the algorithm always keeps the two sequences (beams) with the highest probability. </span><span class="koboSpan" id="kobo.1640.2">Since we can have more than one output sequence, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1641.1">num_return_sequences=2</span></strong><span class="koboSpan" id="kobo.1642.1"> parameter indicates the number of sequences to return. </span><span class="koboSpan" id="kobo.1642.2">For example, if </span><strong class="source-inline"><span class="koboSpan" id="kobo.1643.1">num_beams=5</span></strong><span class="koboSpan" id="kobo.1644.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1645.1">num_return_sequences=3</span></strong><span class="koboSpan" id="kobo.1646.1">, the algorithm will return the three highest-probability sequences out of all five available (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1647.1">num_return_sequences &gt; num_beams</span></strong><span class="koboSpan" id="kobo.1648.1"> are invalid arguments). </span><span class="koboSpan" id="kobo.1648.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1649.1">early_stopping=True</span></strong><span class="koboSpan" id="kobo.1650.1"> parameter indicates that the generation is finished when all beam hypotheses reach the end-of-sequence (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1651.1">[EOS]</span></strong><span class="koboSpan" id="kobo.1652.1">) token. </span><span class="koboSpan" id="kobo.1652.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1653.1">top_k=10</span></strong><span class="koboSpan" id="kobo.1654.1"> parameter indicates that the algorithm will only sample the top 10 highest-probability tokens, regardless of their sequence probabilities. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1655.1">top_p=0.9</span></strong><span class="koboSpan" id="kobo.1656.1"> is like </span><strong class="source-inline"><span class="koboSpan" id="kobo.1657.1">top_k</span></strong><span class="koboSpan" id="kobo.1658.1">, but instead of sampling only from the most likely </span><em class="italic"><span class="koboSpan" id="kobo.1659.1">k</span></em><span class="koboSpan" id="kobo.1660.1"> tokens, it selects from the smallest possible set of tokens whose combined probability exceeds the </span><span class="No-Break"><span class="koboSpan" id="kobo.1661.1">probability </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1662.1">p</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1663.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1664.1">This concludes our introduction to the Transformers library and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1665.1">whole chapter.</span></span></p>
<h1 id="_idParaDest-161" lang="en-GB"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.1666.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1667.1">LLMs are very large transformers with various modifications to accommodate the large size. </span><span class="koboSpan" id="kobo.1667.2">In this chapter, we discussed these modifications, as well as the qualitative differences between LLMs and regular transformers. </span><span class="koboSpan" id="kobo.1667.3">First, we focused on their architecture, including more efficient attention mechanisms such as sparse attention and prefix decoders. </span><span class="koboSpan" id="kobo.1667.4">We also discussed the nuts and bolts of the LLM architecture. </span><span class="koboSpan" id="kobo.1667.5">Next, we surveyed the latest LLM architectures with special attention given to the GPT and LlaMa series of models. </span><span class="koboSpan" id="kobo.1667.6">Then, we discussed LLM training, including training datasets, the Adam optimization algorithm, and various performance improvements. </span><span class="koboSpan" id="kobo.1667.7">We also discussed the RLHF technique and the emergent abilities of LLMs. </span><span class="koboSpan" id="kobo.1667.8">Finally, we introduced the Hugging Face </span><span class="No-Break"><span class="koboSpan" id="kobo.1668.1">Transformers library.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1669.1">In the next chapter, we’ll discuss transformers for </span><strong class="bold"><span class="koboSpan" id="kobo.1670.1">computer vision</span></strong><span class="koboSpan" id="kobo.1671.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1672.1">CV</span></strong><span class="koboSpan" id="kobo.1673.1">), multimodal transformers, and we’ll continue our introduction to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1674.1">Transformers library.</span></span></p>
</div>
</body></html>