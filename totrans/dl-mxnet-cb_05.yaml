- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Analyzing Images with Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用计算机视觉分析图像
- en: Computer vision is one of the fields in which deep learning has progressed enormously,
    surpassing human-level performance in several tasks such as image classification
    and object recognition. Furthermore, the field has moved from academia to real-world
    applications, and the industry is recognizing its practitioners as adding high
    value to businesses.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习取得巨大进展的领域之一，在多个任务中超过了人类水平的表现，例如图像分类和物体识别。此外，计算机视觉已从学术领域走向现实世界的应用，行业也开始认识到其从业者对企业的高价值贡献。
- en: In this chapter, we will learn how to use GluonCV, a MXNet Gluon library specific
    to computer vision, how to build our own networks, and how to use GluonCV’s model
    zoo to use pretrained models for several applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用GluonCV，这是一个专门用于计算机视觉的MXNet Gluon库，如何构建我们自己的网络，并如何使用GluonCV的模型库来使用预训练模型进行多个应用。
- en: 'Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将讨论以下主题：
- en: Understanding convolutional neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解卷积神经网络
- en: Classifying images with AlexNet and ResNet
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AlexNet和ResNet对图像进行分类
- en: Detecting objects with Faster R-CNN and YOLO
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN和YOLO检测物体
- en: Segmenting objects in images with PSPNet and DeepLab-v3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PSPNet和DeepLab-v3对图像中的物体进行分割
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在*前言*中指定的技术要求外，本章还适用以下技术要求：
- en: Ensure that you have completed *Installing MXNet, Gluon, GluonCV and GluonNLP*,
    the first recipe from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and
    Running* *with MXNet*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请确保您已完成[*安装MXNet、Gluon、GluonCV和GluonNLP*](B16591_01.xhtml#_idTextAnchor016)，这是[*第1章*](B16591_01.xhtml#_idTextAnchor016)中的第一个食谱，*使用MXNet快速上手*
- en: 'Ensure that you have completed *A toy dataset for regression – load, manage,
    and visualize a house sales dataset*, the first recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请确保您已完成[*回归的玩具数据集 - 加载、管理和可视化房屋销售数据集*](B16591_02.xhtml#_idTextAnchor029)，这是[*第2章*](B16591_02.xhtml#_idTextAnchor029)中的第一个食谱，*使用MXNet并可视化数据集：Gluon*和*DataLoader*
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub网址找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05)。
- en: 'Furthermore, you can access each recipe directly from Google Colab – for example,
    for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以直接从Google Colab访问每个食谱——例如，本章的第一个食谱：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb)。
- en: Understanding convolutional neural networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解卷积神经网络
- en: In the previous chapters, we have used *fully connected* **Multi-Layer Perceptron**
    (**MLP**) networks to solve our regression and classification problem. However,
    as we will see, these networks are not optimal for solving image-related problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用了**全连接**的**多层感知器**（**MLP**）网络来解决回归和分类问题。然而，正如我们将看到的，这些网络并不是解决图像相关问题的最优选择。
- en: Images are highly dimensional entities – for example, each pixel in a color
    image has three features (red, green, and blue values), and a 1,024x1,024 image
    has more than 1 million pixels (a 1 megapixel image) and, therefore, more than
    3 million features (3 * 106). If we connect all these points in the input layer,
    to a second layer of 100 neurons for a *fully connected* network, we will require
    more than 108 parameters, and that would be only for the first layer. Processing
    images is, therefore, a time-intensive operation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是高维实体——例如，彩色图像中的每个像素有三个特征（红色、绿色和蓝色值），而一张1,024x1,024的图像有超过100万个像素（即1百万像素图像），因此具有超过300万个特征（3
    * 10^6）。如果我们将这些输入层的所有点连接到第二层100个神经元的*全连接*网络中，我们将需要超过10^8个参数，而这仅仅是第一层的需求。因此，处理图像是一个时间密集型操作。
- en: Furthermore, imagine that we are trying to detect eyes in faces; if a pixel
    belongs to an eye, the likelihood of nearby pixels belonging to the eye is very
    high (think of the pixels that make up the iris, for example). When inputting
    all our pixels directly into our network, all the information connected to pixel
    location is lost.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设我们正在尝试在人脸中检测眼睛；如果一个像素属于眼睛，那么附近的像素属于眼睛的可能性非常高（例如，考虑构成虹膜的像素）。当我们将所有像素直接输入到网络中时，所有与像素位置相关的信息都丢失了。
- en: An architecture called a **Convolutional Neural Network** (**CNN**) was developed
    to tackle these problems, and we will analyze the most important features of CNNs
    and how to implement them in this recipe to solve image-related problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为**卷积神经网络**（**CNN**）的架构被开发出来以解决这些问题，我们将在本教程中分析CNN的最重要特性，并了解如何在图像相关问题中实现它们。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As with the previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在这个教程中，我们将使用一些矩阵运算和线性代数，但不会太难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will take the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将执行以下步骤：
- en: Introduce convolutional layer equations.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入卷积层方程。
- en: Understand the convolution parameters and receptive field.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解卷积参数和感受野。
- en: Run a convolutional layer example with MXNet.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MXNet运行卷积层示例。
- en: Introduce pooling layer equations.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入池化层方程。
- en: Run a pooling layer example with MXNet.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MXNet运行池化层示例。
- en: Summarize CNNs.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结卷积神经网络（CNN）。
- en: Introducing convolutional layer equations
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入卷积层方程
- en: The location problems described in the recipe introduction are formally known
    as **translation invariance** and **locality**. In CNNs, these problems is solved
    by using the convolution/cross-correlation operation in the so-called convolutional
    layers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程介绍的定位问题在正式术语中称为**平移不变性**和**局部性**。在卷积神经网络（CNN）中，这些问题通过在所谓的卷积层中使用卷积/互相关操作来解决。
- en: 'In the convolution, we have an input image (or **feature map** – see the following
    *Important note* relating to convolutional layer equations) that is combined with
    a **kernel**, which are the learnable parameters of this layer. The simplest way
    to see how this operation works is with an example – if we had a 3x3 input and
    we wanted to combine it with a 2x2 kernel, it would look like a sliding window,
    as shown in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积操作中，我们有一个输入图像（或**特征图** – 请参见下面与卷积层方程相关的*重要说明*），它与**卷积核**结合，卷积核是该层的可学习参数。查看这个操作如何工作的最简单方式是通过一个示例——如果我们有一个3x3的输入，想将其与一个2x2的卷积核结合，那么它看起来就像一个滑动窗口，如下图所示：
- en: '![Figure 5.1 – The convolutional layer](img/B16591_05_1.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 卷积层](img/B16591_05_1.jpg)'
- en: Figure 5.1 – The convolutional layer
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 卷积层
- en: As shown in *Figure 5**.1*, to compute 1 pixel of the output, we can intuitively
    place the kernel over the input and compute multiplications, and then add all
    these values to obtain the final result. This helps a network learn features from
    the image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.1*所示，为了计算输出的一个像素，我们可以直观地将卷积核放置在输入图像上，进行乘法计算，然后将这些值加起来得到最终结果。这有助于网络从图像中学习特征。
- en: Furthermore, this operation is less computing intensive than the *fully connected*
    layers, addressing the computing problem we identified.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个操作的计算量比*全连接*层要小，解决了我们所识别的计算问题。
- en: Important note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: When using a convolution layer as the first step (typical in CNNs), the input
    is the full image. Moreover, the output can be understood as another image of
    a lower dimension with certain properties, given by the kernel. As kernels are
    learned to highlight certain features of the image, these outputs are known as
    *feature maps*. In the layer close to the input, each pixel of these feature maps
    is a combination of a small number of pixels of the image (for example, horizontal
    or vertical lines). As data travels through convolutional layers, these feature
    maps represent higher levels of abstraction (for example, eyes on a face).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当卷积层作为第一步使用时（在CNN中通常如此），输入是完整的图像。此外，输出可以理解为具有某些特性的低维度图像，这些特性由卷积核给出。随着卷积核学习突出图像的某些特征，这些输出被称为*特征图*。在靠近输入的层中，这些特征图的每个像素都是图像中少量像素的组合（例如，水平或垂直线条）。随着数据通过卷积层流动，这些特征图代表更高层次的抽象（例如，脸上的眼睛）。
- en: Understanding the convolution parameters and receptive field
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解卷积参数和感受野
- en: The example shown in *Figure 5**.1* is quite simple, for illustrative purposes;
    the input size is 6x6 and the kernel size is 3x3\. These sizes are variable and
    depend on the network architecture. However, there are three parameters that are
    very important to calculate the output size; these are padding, stride, and dilation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.1*中展示的例子非常简单，仅用于说明；输入大小是 6x6，卷积核大小是 3x3。这些大小是可变的，取决于网络架构。然而，有三个参数对于计算输出大小非常重要；它们是填充、步幅和膨胀。
- en: Padding is the number of zero-valued pixels (rows/columns) that are added to
    the input. The larger the padding, the larger the output, and effectively, this
    increases the input size. In the example, padding is `1` (represented as *p*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是指添加到输入中的零值像素（行/列）的数量。填充越大，输出越大，实际上这会增加输入的大小。在这个例子中，填充是 `1`（表示为 *p*）。
- en: As mentioned, we can intuitively place the kernel over the input, compute multiplications,
    and then add all these results to obtain the final value. For the next value,
    we need to move the kernel to a different position, as shown in the following
    diagram.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以直观地将卷积核放置在输入上，计算乘积，然后将所有这些结果相加，得到最终的值。对于下一个值，我们需要将卷积核移到不同的位置，如下图所示。
- en: '![Figure 5.2 – The stride parameter](img/B16591_05_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 步幅参数](img/B16591_05_2.jpg)'
- en: Figure 5.2 – The stride parameter
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 步幅参数
- en: The number of spaces the kernel moves is defined by the stride. In *Figure 5**.2*,
    we can see an example of the stride being 2, with each 3x3 kernel separated by
    2 values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核移动的步长由步幅定义。在*图 5.2*中，我们可以看到步幅为 2 的例子，每个 3x3 的卷积核之间相隔 2 个值。
- en: Dilation is defined by how separated each input value that is used in the convolution
    with the kernel is.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀定义了在与卷积核进行卷积时，每个输入值之间的间隔。
- en: '![Figure 5.3 – The dilation parameter](img/B16591_05_3.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 膨胀参数](img/B16591_05_3.jpg)'
- en: Figure 5.3 – The dilation parameter
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 膨胀参数
- en: As we can see in *Figure 5**.3*, different dilation parameters combine elements
    of the input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图 5.3*中看到的，不同的膨胀参数组合了输入的元素。
- en: These parameters define what is known as the **receptive field**, which is the
    size of the region of the input that produces an activation. It is an important
    parameter because only a feature present in the receptive field of our model will
    be represented in the output.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数定义了所谓的**感受野**，即输入区域的大小，该区域生成激活。它是一个重要的参数，因为只有出现在我们模型感受野中的特征才能在输出中得到表示。
- en: In the example in *Figure 5**.1*, a 6x6 input, combined with a 3x3 kernel, with
    a stride of 2, padding of 1, and dilation of 1, yields a 3x3 output and has a
    receptive field of the full input (all pixels in the input are at least used once).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.1*中的例子中，一个 6x6 的输入，结合 3x3 的卷积核，步幅为 2，填充为 1，膨胀为 1，得到一个 3x3 的输出，并且其感受野覆盖了整个输入（输入中的所有像素至少使用一次）。
- en: 'Another very interesting property of these parameters is that given the right
    combinations, you can have an output that is the same size as the input. The equation
    that gives the dimensions of the output is as follows (the equation needs to be
    applied to the height and the width, respectively):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的另一个非常有趣的性质是，给定正确的组合，您可以获得与输入相同大小的输出。给出输出维度的公式如下（该公式分别应用于高度和宽度）：
- en: o = [i + 2 * p − k − (k − 1)*(d − 1)] / s + 1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: o = [i + 2 * p − k − (k − 1)*(d − 1)] / s + 1
- en: In the preceding equation, *o* is the output dimension (height or width if working
    with 2D images), *i* is the input dimension (height/width), *p* is padding, *k*
    is the kernel size, *d* is dilation, and *s* is stride. There are different combinations
    that will give the input and the output the same size, one of which is *p = 1,
    k = 3, d = 1, s =* *1*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*o* 是输出维度（如果处理 2D 图像，则为高度或宽度），*i* 是输入维度（高度/宽度），*p* 是填充，*k* 是卷积核大小，*d*
    是膨胀，*s* 是步幅。有多种组合可以使输入和输出的大小相同，其中之一是 *p = 1, k = 3, d = 1, s = 1*。
- en: Running a convolutional layer example with MXNet
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 MXNet 的卷积层示例
- en: 'We can implement the following example using MXNet capabilities (note that
    padding is 0, stride is 1, and dilation is 1):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 MXNet 的功能实现以下示例（注意填充为 0，步幅为 1，膨胀为 1）：
- en: '![Figure 5.4 – A convolution example](img/B16591_05_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 卷积示例](img/B16591_05_4.jpg)'
- en: Figure 5.4 – A convolution example
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 卷积示例
- en: 'If we want to follow the example depicted in *Figure 5**.4*, applying the convolution
    operation to a 3x3 matrix with a 2x2 kernel, we can use the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要按照*图5.4*中所示的示例，将卷积操作应用于一个3x3的矩阵，并使用一个2x2的卷积核，我们可以使用以下代码：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These operations give the following as a result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的结果如下：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is the expected result for the defined convolution step (taking into account
    the given padding, stride, and dilation parameters).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是定义的卷积步长的预期结果（考虑给定的填充、步幅和扩张参数）。
- en: Introducing the pooling layer equations
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入池化层方程
- en: As described earlier, a desirable property of neural network models when working
    with images is that as we traverse a network, we can increasingly process higher-level
    features, or equivalently, each pixel in the deep feature maps has a higher receptive
    field from the input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当处理图像时，神经网络模型的一个理想特性是，随着我们遍历网络，我们可以处理更高层次的特征，或者等效地说，深层特征图中的每个像素都具有来自输入的更大感受野。
- en: The operation performed in these layers is similar to the convolutional layers,
    in the sense that we take a kernel of constant dimensions and apply a sliding
    window. However, in this case, the kernel parameters are constant and, therefore,
    not learned during the training of the network. This kernel is seen as an operation
    (a function) and is typically either the max function (the **max pooling layer**)
    or the average function (the **average** **pooling layer**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层执行的操作类似于卷积层，从某种意义上来说，我们采用具有恒定维度的卷积核并应用滑动窗口。然而，在这种情况下，卷积核的参数是恒定的，因此在训练网络时不会被学习。这个卷积核被视为一种操作（一个函数），通常是最大值函数（**最大池化层**）或平均值函数（**平均池化层**）。
- en: '![Figure 5.5 – The max pooling layer](img/B16591_05_5.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 最大池化层](img/B16591_05_5.jpg)'
- en: Figure 5.5 – The max pooling layer
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 最大池化层
- en: Furthermore, by combining pixels that are nearby, we also achieve local invariance,
    another desirable property to process images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过组合邻近的像素，我们还实现了局部不变性，这是处理图像时的另一个理想特性。
- en: Running a pooling layer example with MXNet
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MXNet运行池化层示例
- en: 'We can implement the example shown in *Figure 5**.5* using MXNet''s capabilities,
    as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MXNet的功能实现*图5.5*中显示的示例，如下所示：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These operations give the following as a result:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的结果如下：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the expected result for the defined 2x2 max pooling step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是定义的2x2最大池化步长的预期结果。
- en: Summarizing CNNs
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结CNN
- en: 'A typical CNN for **image classification** has, therefore, two different parts:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个典型的CNN用于**图像分类**，它有两个不同的部分：
- en: '**Feature extraction**: This is also known as the network backbone. It is built
    with combinations of the convolutional and pooling layers seen in this recipe.
    The input to each layer is feature maps (an image and all its channels in the
    input layer), and the output is feature maps of reduced dimensions but with a
    larger number of channels. Layers are typically stacked – a convolutional layer,
    an activation function (typically, **Rectified Linear Unit** (**ReLU**)), and
    a max pooling layer.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这也被称为网络的骨干。它是由本教程中看到的卷积层和池化层的组合构建的。每一层的输入是特征图（输入层中的图像及其所有通道），输出是减少维度但具有更多通道的特征图。层通常是堆叠的—一个卷积层，一个激活函数（通常是**修正线性单元**（**ReLU**）），和一个最大池化层。'
- en: '`softmax` function as the activation. The number of layers in the classifier
    depends on the specific problem.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`softmax`函数作为激活函数。分类器中的层数取决于具体问题。'
- en: 'Therefore, a CNN architecture can be the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个CNN架构可以是以下形式：
- en: '![Figure 5.6 – CNN architecture](img/B16591_05_6.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – CNN架构](img/B16591_05_6.jpg)'
- en: Figure 5.6 – CNN architecture
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – CNN架构
- en: In *Figure 5**.6*, we can see a CNN architecture for *image classification*,
    composed of two stages for feature extraction, each stage combining a convolution
    layer (with the ReLU activation function) and a max pooling layer. Then, for the
    classifier, the remaining feature map is flattened into a vector and passed through
    a *fully connected* layer to finally provide the output with the *softmax* activation
    function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.6*中，我们可以看到一个用于*图像分类*的CNN架构，包含两个特征提取阶段，每个阶段结合了一个卷积层（具有ReLU激活函数）和一个最大池化层。然后，分类器将剩余的特征图展平为一个向量，并通过一个*全连接*层，最终通过*softmax*激活函数提供输出。
- en: How it works…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we have introduced CNNs. This architecture has been developed
    since early the 2000s and is responsible for the revolution in **computer vision**
    applications, and making **deep learning** become the spotlight in most data-oriented
    tasks including **natural language processing**, **speech recognition**, **image
    generation**, and so on, reaching state-of-the-art in all of them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了CNNs。这种架构自2000年代初期开始发展，并推动了**计算机视觉**应用的革命，使得**深度学习**成为大多数数据驱动任务中的焦点，包括**自然语言处理**、**语音识别**、**图像生成**等，并在所有领域达到了最先进的水平。
- en: We have understood how CNNs work internally, exploring the concepts of *feature
    maps*, *receptive field*, and the mathematical concepts behind the main layers
    of these architectures, the *convolutional* and *max pooling* layers, and how
    they are combined to build a complete CNN model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了CNNs的内部工作原理，探索了*特征图*、*感受野*以及这些架构主要层（*卷积*层和*最大池化*层）背后的数学概念，并了解了它们如何组合来构建一个完整的CNN模型。
- en: There’s more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'CNNs have evolved rapidly; in 1998, one of the first CNNs was published, solving
    a practical problem: [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 发展迅速；1998年，首个CNN之一被发布，解决了一个实际问题：[http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)。
- en: 'After that, it was not until 2012, with AlexNet, that CNNs gained worldwide
    attention, and since then, progress quickly developed, until they surpassed human-level
    performance. For more information on the history of CNNs, refer to this article:
    [https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f](https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，直到2012年，随着AlexNet的问世，CNNs才获得了全球关注，从那时起，进展迅速发展，直到它们超过了人类的表现。欲了解CNNs历史的更多信息，请参阅这篇文章：[https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f](https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f)。
- en: We briefly touched on the topics of translation invariance and locality. For
    more information on these topics, visit [https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html](https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要讨论了平移不变性和局部性的主题。欲了解更多信息，请访问 [https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html](https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html)。
- en: 'The relationship between convolution and cross-correlation is discussed here:
    [https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5](https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论了卷积与互相关的关系：[https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5](https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5)。
- en: 'For a better understanding of matrix dimensions, padding, stride, dilation,
    and receptive field, a good explanation is provided here: [https://theaisummer.com/receptive-field/](https://theaisummer.com/receptive-field/).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解矩阵维度、填充、步幅、扩张和感受野，这里提供了一个很好的解释：[https://theaisummer.com/receptive-field/](https://theaisummer.com/receptive-field/)。
- en: 'CNNs have been state-of-the-art for image classification until quite recently;
    in October 2020, **Transformers** were applied to computer vision tasks by Google
    Brain with **ViT**: [https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 在图像分类领域一直处于最前沿，直到最近；2020年10月，**Transformers** 被谷歌大脑应用于计算机视觉任务，并推出了**ViT**：[https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)。
- en: In a nutshell, instead of forcing a network with the locality principle, Transformer
    architectures allow the same model to decide which features matter most at any
    layer, local or global. This behavior is called **self-attention**. Transformers
    are state-of-the-art for image classification at the time of writing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，与强制网络遵循局部性原则不同，Transformer架构允许相同的模型在任何层决定哪些特征最重要，无论是局部的还是全局的。这种行为被称为**自注意力**。在本文写作时，Transformers
    已经是图像分类领域的最先进技术。
- en: In this chapter, we are going to analyze in detail the following tasks – **image
    classification**, **object detection**, and **image segmentation**. However, *MXNet
    GluonCV Model Zoo* contains lots of models pre-trained for a large number of tasks.
    You are encouraged to explore the different examples provided at [https://cv.gluon.ai/model_zoo/index.html](https://cv.gluon.ai/model_zoo/index.html).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细分析以下任务——**图像分类**、**目标检测**和**图像分割**。然而，*MXNet GluonCV 模型库*包含了大量预训练模型，涵盖了许多任务。鼓励您探索在[https://cv.gluon.ai/model_zoo/index.html](https://cv.gluon.ai/model_zoo/index.html)提供的不同示例。
- en: Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MXNet 进行图像分类——GluonCV 模型库、AlexNet 和 ResNet。
- en: MXNet provides a variety of tools to compose custom deep learning models. In
    this recipe, we will see how to use MXNet to build a model from scratch, train
    it, and use it to classify images from a dataset. We will also see that although
    this approach works fine, it is time-consuming.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 提供了多种工具来构建自定义深度学习模型。在本节中，我们将看到如何使用 MXNet 从零开始构建模型、训练它，并使用它对数据集中的图像进行分类。我们还将看到，尽管这种方法有效，但它是耗时的。
- en: Another option, and one of the highest value features that MXNet and GluonCV
    provide, is their **Model Zoo**. GluonCV Model Zoo is a set of pre-trained, ready-to-go
    models, for use with your own applications. We will see how to use Model Zoo with
    two very important models for image classification – **AlexNet** and **ResNet**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择，也是 MXNet 和 GluonCV 提供的最有价值的功能之一，就是它们的**模型库**。GluonCV 模型库是一个预训练的、即插即用的模型集合，可以用于您的应用程序。我们将看到如何使用模型库，特别是用于图像分类的两个非常重要的模型——**AlexNet**
    和 **ResNet**。
- en: In this recipe, we will analyze and compare these approaches to classify images
    on a reduced version of the *Dogs vs.* *Cats* dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析并比较这些方法，在简化版的 *Dogs vs.* *Cats* 数据集上进行图像分类。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作。
- en: As with previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在本节中，我们将使用一些矩阵运算和线性代数，但不会太困难。
- en: 'Furthermore, we will be classifying image datasets; therefore, we will revisit
    some concepts that we''ve already seen:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将对图像数据集进行分类，因此我们将重新审视一些我们之前已经学习过的概念。
- en: '*Understanding image datasets- load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解图像数据集——加载、管理和可视化 Fashion MNIST 数据集*，这是[*第 2 章*](B16591_02.xhtml#_idTextAnchor029)的第三个食谱，*使用
    MXNet 和可视化数据集：Gluon* *和 DataLoader*。'
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 4 章*](B16591_04.xhtml#_idTextAnchor075)，*解决* *分类问题*。'
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will take the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将采取以下步骤：
- en: Explore a reduced version of the *Dogs vs.* *Cats* dataset.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索简化版的 *Dogs vs.* *Cats* 数据集。
- en: Create an AlexNet custom model from scratch.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始创建一个 AlexNet 自定义模型。
- en: Train an *AlexNet* custom model.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 *AlexNet* 自定义模型。
- en: Evaluate an *AlexNet* custom model.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 *AlexNet* 自定义模型。
- en: Introduce Model Zoo.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍模型库。
- en: Introduce ImageNet pre-trained models.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 ImageNet 预训练模型。
- en: Load an *AlexNet* pre-trained model from *Model Zoo*.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Model Zoo* 加载 *AlexNet* 预训练模型。
- en: Evaluate an *AlexNet* pre-trained model from *Model Zoo*.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自 *Model Zoo* 的 *AlexNet* 预训练模型。
- en: Load a ResNet pre-trained model from *Model Zoo*.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Model Zoo* 加载 ResNet 预训练模型。
- en: Evaluate a *ResNet* pre-trained model from *Model Zoo*.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自 *Model Zoo* 的 *ResNet* 预训练模型。
- en: Exploring the reduced version of the dataset Dogs vs. Cats
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索简化版的 Dogs vs. Cats 数据集。
- en: For our image classification experiments, we will work with a new dataset, *Dogs
    vs. Cats*. This is a Kaggle dataset ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats))
    that can be downloaded manually. In this recipe, we will work with a reduced version
    of this dataset that can be downloaded from **Zenodo** ([https://zenodo.org/records/5226945](https://zenodo.org/records/5226945))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的图像分类实验，我们将使用一个新的数据集，*Dogs vs. Cats*。这是一个 Kaggle 数据集（[https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)），可以手动下载。在本节中，我们将使用该数据集的简化版本，该版本可以从**Zenodo**（[https://zenodo.org/records/5226945](https://zenodo.org/records/5226945)）下载。
- en: From the set of images in the dataset (either a cat or a dog is depicted), our
    model will need to correctly classify these images. In the first step, as we saw
    in previous chapters, we are going to do some **Exploratory Data** **Analysis**
    (**EDA**).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中的一组图像中（无论是猫还是狗），我们的模型需要正确地对这些图像进行分类。在第一步，正如我们在前几章中看到的，我们将进行一些**探索性数据**
    **分析**（**EDA**）。
- en: '![Figure 5.7 – The Dogs vs. Cats dataset](img/B16591_05_7.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 狗与猫数据集](img/B16591_05_7.jpg)'
- en: Figure 5.7 – The Dogs vs. Cats dataset
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 狗与猫数据集
- en: As can be seen in *Figure 5**.7*, each image in the dataset is in color, and
    they are resized to 224 px by 224 px (width and height). There are 1,000 images
    in the training and validation set and 400 images in the test set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*图 5.7*中所示，数据集中的每张图像都是彩色的，并且它们的大小被调整为224 px * 224 px（宽度和高度）大小。训练集和验证集共有1,000张图像，测试集有400张图像。
- en: 'As we did in *Understanding image datasets – load, manage, and visualize the
    Fashion MNIST dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we can compute
    some visualizations using the dimensionality reduction techniques – **Principal
    Component Analysis** (**PCA**), **t-distributed stochastic neighbor embedding**
    (**t-SNE**), and **Uniform Manifold Approximation and** **Projection** (**UMAP**):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](B16591_02.xhtml#_idTextAnchor029)中所做的那样，*理解图像数据集——加载、管理和可视化Fashion
    MNIST数据集*，第三个食谱，*使用MXNet和可视化数据集：Gluon和DataLoader*，我们可以使用降维技术来计算一些可视化结果——**主成分分析**（**PCA**）、**t-分布随机邻域嵌入**（**t-SNE**）和**均匀流形近似与投影**（**UMAP**）：
- en: '![Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP](img/1.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 狗与猫的可视化 – PCA, t-SNE 和 UMAP](img/1.jpg)'
- en: Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 狗与猫的可视化 – PCA, t-SNE 和 UMAP
- en: In *Figure 5**.8*, there is no clear boundary region to separate dogs versus
    cats. However, as we will see in the following sections, an architecture introduced
    in the previous chapter, CNNs, will achieve very good results on the task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.8*中，并没有明确的边界区域来区分狗与猫。然而，正如我们将在接下来的章节中看到的，上一章介绍的架构——卷积神经网络（CNNs），将在这个任务中取得非常好的结果。
- en: Creating an AlexNet custom model from scratch
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始创建一个AlexNet自定义模型
- en: AlexNet was a deep neural network that was developed by Alex Krizhevsky, Ilya
    Sutskever, and Geoffrey Hinton in 2012\. It was designed to compete in the **ImageNet
    Large Scale Visual Recognition Challenge** (**ILSVRC**) in 2012 and was the first
    CNN-based model to win this competition.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton于2012年开发的深度神经网络。它是为参加2012年的**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）而设计的，并且是第一个基于CNN的模型，赢得了这一竞赛。
- en: '![Figure 5.9 – AlexNet](img/B16591_05_9.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – AlexNet](img/B16591_05_9.jpg)'
- en: Figure 5.9 – AlexNet
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – AlexNet
- en: The network uses five convolutional layers and three *fully connected* layers.
    The activation function used is the ReLU, and it contains about 63 million trainable
    parameters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用了五个卷积层和三个*全连接*层。所使用的激活函数是ReLU，包含大约6300万个可训练参数。
- en: 'To generate this network from scratch with MXNet, we can use the following
    code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从头开始使用MXNet生成此网络，我们可以使用以下代码：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code uses MXNet functions to add the corresponding 2D *convolutional*,
    *max-pooling*, and *fully connected* layers with their corresponding activation
    functions, generating an AlexNet architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用MXNet函数添加相应的2D*卷积*、*最大池化*和*全连接*层，并附加其相应的激活函数，生成一个AlexNet架构。
- en: Training an AlexNet custom model
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练AlexNet自定义模型
- en: The task we are dealing with is an image classification task, which is a classification
    problem where input data is images, and therefore, we can use the training loop
    we saw in [*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving Classification
    Problems* – slightly modified for this task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的任务是图像分类任务，这是一种分类问题，输入数据是图像，因此我们可以使用在[*第4章*](B16591_04.xhtml#_idTextAnchor075)中看到的训练循环——稍作修改以适应此任务。
- en: 'The parameters chosen are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的参数如下：
- en: '**Number of** **epochs**: 20'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期数**: 20'
- en: '**Batch size**: 16 samples'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**: 16个样本'
- en: '**Optimizer**: Adam'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**: Adam'
- en: '**Learning** **rate**: 0.0001'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习** **率**: 0.0001'
- en: 'With these parameters, we obtain the following results (the best model was
    achieved on epoch 11):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们获得了以下结果（最佳模型是在第11个周期达到的）：
- en: '**Training** **loss**: 0.36'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练** **损失**: 0.36'
- en: '**Training** **accuracy**: 0.83'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练** **准确率**: 0.83'
- en: '**Validation** **loss**: 0.55'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证** **损失**: 0.55'
- en: '**Validation** **accuracy**: 0.785'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证** **准确率**：0.785'
- en: Evaluating an AlexNet custom model
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估一个自定义的AlexNet模型
- en: 'The accuracy obtained with the best model (in this case, the one corresponding
    to the last training iteration) on the test set is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳模型（在此情况下为最后一次训练迭代对应的模型）在测试集上获得的准确率如下：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That’s quite a decent number for just five epochs of training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅经过五次训练周期而言，这个结果相当不错。
- en: 'Moreover, the confusion matrix computed is shown in the following figure (class
    **0** corresponds to cats and **1** to dogs):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，计算出的混淆矩阵如下图所示（**0**类对应猫，**1**类对应狗）：
- en: '![Figure 5.10 – A trained custom AlexNet confusion matrix](img/B16591_05_10.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 一个训练过的自定义AlexNet混淆矩阵](img/B16591_05_10.jpg)'
- en: Figure 5.10 – A trained custom AlexNet confusion matrix
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 一个训练过的自定义AlexNet混淆矩阵
- en: 'As shown in *Figure 5**.10*, the model mostly predicts accurately the expected
    classes, with the following per-class errors:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.10*所示，该模型大多能够准确预测期望的类别，以下是每个类别的错误率：
- en: '**Cats detected as dogs**: **85**/200 (43% of cat images were misclassified)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**猫被误识为狗**：**85**/200（43%的猫图像被错误分类）'
- en: '**Dogs detected as cats**: **24**/200 (12% of dog images were misclassified)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**狗被误识为猫**：**24**/200（12%的狗图像被错误分类）'
- en: Let’s move on to the next heading.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一部分内容。
- en: Introducing Model Zoo
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍模型库
- en: One of the best features that MXNet and GluonCV provide is their large pool
    of pre-trained models, readily available for its users to use and deploy in their
    own applications. This model library is called **Model Zoo**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet和GluonCV提供的最优秀的功能之一是它们庞大的预训练模型库，用户可以方便地在自己的应用中使用和部署这些模型。这个模型库被称为**模型库**。
- en: 'Moreover, depending on the task at hand, MXNet has some very interesting charts
    that compare the different pre-trained models optimized for tasks. For image classification
    (based on ImageNet), we have the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据手头的任务，MXNet提供了一些非常有趣的图表，用于比较针对不同任务优化的预训练模型。在图像分类（基于ImageNet）方面，我们有以下内容：
- en: '![Figure 5.11 – Model Zoo for image classification (ImageNet)](img/B16591_05_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 用于图像分类的模型库（ImageNet）](img/B16591_05_11.jpg)'
- en: Figure 5.11 – Model Zoo for image classification (ImageNet)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 用于图像分类的模型库（ImageNet）
- en: Note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Source: [https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html)
- en: '*Figure 5**.11* displays the most important pre-trained models in **GluonCV
    Model Zoo**, according to accuracy (the vertical axis) and inference performance
    (the samples per second and horizontal axis). There are no models (yet) in the
    top-right quadrant, meaning that, currently, we need to balance these characteristics.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.11* 显示了**GluonCV模型库**中最重要的预训练模型，根据准确率（纵轴）和推理性能（每秒样本数与横轴）。目前，右上角的象限没有模型，这意味着我们目前需要平衡这些特性。'
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our reduced *Dogs vs. Cats* dataset
    in the following steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自GluonCV模型库的模型仅需几行代码，我们将在接下来的步骤中探索此路径，以解决我们简化的*狗与猫*数据集。
- en: ImageNet pre-trained models
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageNet预训练模型
- en: Models from the **GluonCV Model Zoo** for **image classification** tasks have
    been pre-trained in the *ImageNet* dataset. This dataset is one of the most well-known
    datasets in computer vision. It was the first large-scale image dataset and was
    part of the deep learning revolution when, in 2012, AlexNet won the ILSVRC.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**GluonCV模型库**中的模型已经在*ImageNet*数据集上进行了预训练，用于**图像分类**任务。这个数据集是计算机视觉领域最著名的数据集之一。它是第一个大规模的图像数据集，并且在2012年AlexNet赢得ILSVRC时，成为深度学习革命的一部分。'
- en: 'This dataset has two variants:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有两个变体：
- en: '**Full dataset**: More than 20,000 classes in about 14 million images'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整数据集**：超过20,000个类别，约1,400万张图像'
- en: '**ImageNet-1k**: 1,000 classes in about 1 million images'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ImageNet-1k**：大约1000个类别，约100万张图像'
- en: Due to the size and large number of classes, the full dataset is rarely used
    on benchmarks, with *ImageNet1k* the de facto ImageNet dataset (unless otherwise
    noted in the research papers, articles, and so on). Images in the dataset are
    in color and have a size of 224px by 224px (width and height).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的庞大和类别的数量，完整的数据集在基准测试中很少使用，通常*ImageNet1k*被视为标准的ImageNet数据集（除非在研究论文、文章等中另有说明）。数据集中的图像为彩色图像，尺寸为224px×224px（宽×高）。
- en: All image classification pre-trained models in GluonCV Model Zoo have been pre-trained
    with ImageNet-1k, and therefore, they have 1,000 outputs. The outputs will be
    post-processed so that all ImageNet classes corresponding to cats point to class
    0, all ImageNet classes corresponding to dogs point to class 1, and all other
    outputs point to class 2, which we will consider as unknown.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Loading an AlexNet pre-trained model from Model Zoo
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To compare the advantages and disadvantages of using a custom-trained model
    and a pre-trained model from Model Zoo, in the following sections, we will work
    with a version of the AlexNet architecture that has been pre-trained on the *ImageNet*
    dataset, acquired from GluonCV Model Zoo.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading a pre-trained model is very easy and can be done with a single line
    of code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `get_model` GluonCV function receives three parameters:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '`alexnet`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False`, only the uninitialized architecture will be loaded)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mx.cpu()` or `mx.gpu()`, if available'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This call will download the chosen model and, if required, its pre-trained weights
    and biases.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating an AlexNet pre-trained model from Model Zoo
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the loaded model from the previous section, we can now evaluate and compare
    our previous results, such as for `accuracy`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, this number is slightly lower than the accuracy we achieved with
    our custom-trained AlexNet model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the confusion matrix, we obtain the following values:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – A pre-trained AlexNet Confusion Matrix](img/B16591_05_12.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – A pre-trained AlexNet Confusion Matrix
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: When we analyze *Figure 5**.12*, the most significant difference is that our
    previous confusion matrix was a 2x2 matrix (with the options of **0** or **1**
    for true labels and **0** or **1** for predicted labels). However, with our pre-trained
    model, we have obtained a 3x3 confusion matrix. This is because, as mentioned
    previously, pre-trained models have been trained in ImageNet, and they output
    1,000 classes (instead of the two required for our dataset). These outputs have
    been post-processed, so all *ImageNet* classes corresponding to cats point to
    class **0**, all ImageNet classes corresponding to dogs point to class **1**,
    and all other outputs point to class **2**, which we will consider as unknown.
    When taking into account this unknown class **2**, the 3x3 matrix is computed.
    Please note how there are no images that produce a true label of **2**; the last
    row is all zeros.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'The model mostly behaves accurately with the expected classes, with the following
    per-class errors (we need to add numbers from the two wrong columns):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '**Cats not detected as cats**: **96**/200 (48% of cats images were misclassified)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dogs not detected as dogs**: **14**/200 (7% of dogs images were misclassified)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a significant difference in the per-class results, and this is due
    to the pre-trained dataset used, *ImageNet*, because it has a large number of
    classes associated with dog breeds and, therefore, has been trained more extensively
    on dog images.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Loading a ResNet pre-trained model from Model Zoo
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Looking at AlexNet and later models with a higher depth, such as VGGNet, it
    became clear that deeper layers could help when classifying images. However, when
    training these deep networks, by using *backpropagation* and the *chain rule*,
    the training algorithm starts computing smaller and smaller values for the gradients
    because of the large number of multiplications of small numbers (the activation
    function outputs are in the [0, 1] range), and therefore, when the gradients for
    the early layers are computed, the updated weights are seldom modified. This is
    known as the **vanishing gradient** problem, and different **ResNet** architectures
    were developed to avoid it. Specifically, ResNet models use residual blocks that
    add direct lines to connect layers, providing shortcuts that can be leveraged
    during training to avoid vanishing gradients.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – A ResNet residual block](img/B16591_05_13.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – A ResNet residual block
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The architecture shown in *Figure 5**.13* allows you to stack layers in a more
    scalable way, with known architectures with 18, 50, 101, and 152 layers. This
    approach proved very successful, and ResNet152 won the ILSVRC in 2015.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will load the `v1d` version of `resNet50`, with a single line
    of code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model then downloads successfully.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a ResNet pre-trained model from Model Zoo
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the loaded model from the previous section, we can now evaluate and compare
    with our previous results, such as for `accuracy`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see, this number is significantly higher than previous models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the confusion matrix, we obtain the following values:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.14 – A pre-trained ResNet \uFEFFconfusion \uFEFFmatrix](img/B16591_05_14.jpg)"
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – A pre-trained ResNet confusion matrix
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'According to *Figure 5**.14*, the per-class error rate is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '**Cats not detected as cats**: **29**/200 (14.5% of cat images were misclassified)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dogs not detected as dogs**: **1**/200 (0.5% of dog images were misclassified)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a significant difference in the per-class results, and this is again
    due to the pre-trained dataset used, *ImageNet*, because it has a large number
    of classes associated with dog breeds and, therefore, has been trained more extensively
    on dog images.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we compared two approaches to using computer vision models
    for image classification:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Training a custom model from scratch
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pre-trained models from GluonCV Model Zoo
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We applied both approaches with AlexNet architecture and compared the results
    with the **ResNet-101 Model** **Zoo** version.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Both approaches have advantages and disadvantages. Training from scratch provides
    us direct control on the number of output classes, and we can fully handle the
    training process and the evolution of loss and accuracy for both the training
    and validation datasets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to train a model, we need sufficient data, and that might
    not be always available. Furthermore, adjusting the training hyperparameters (epochs,
    batch size, optimizer, and learning rate) and the training itself are time-consuming
    processes that, if not done properly, can yield suboptimal accuracy values (or
    other metrics).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we used a reduced version of the *Dogs vs. Cats* dataset from
    Kaggle, and we used pre-trained models on *ImageNet*. The Kaggle dataset contains
    25,000 images (more than 10 times more) and the reader is encouraged to try the
    proposed solution on that dataset (all helper functions have also been tested
    with the full dataset).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the choice of the *ImageNet* dataset was not casual; *ImageNet*
    has dog classes and cat classes, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    `dataset` class. However, when this is not possible, and we apply pre-trained
    models from a dataset to another dataset, the data probability distribution will
    typically be very different; hence, the accuracy obtained can be very low. This
    is known as the **domain gap** or **domain adaptation problem** between the source
    dataset (the data the model has been pre-trained on) and the target dataset (the
    data the model is evaluated on).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: We finalized the recipe evaluating our two pre-trained models, *AlexNet* and
    *ResNet*, and saw how CNN models have evolved through the years, increasing the
    accuracy obtained.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we used *ImageNet* pre-trained models; for more information
    about *ImageNet*, and the ILSVRC, I suggest this article: [https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Although primarily about the ILSVRC, this previous link also includes some history
    regarding CNNs, including *AlexNet*, VGGNet, and *ResNet*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'However, computer vision datasets have been under strict scrutiny recently
    regarding data quality, and ImageNet is no exception, as this article describes:
    [https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/](https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.11* shows a static image corresponding to the accuracy versus samples
    per second graph for the Model Zoo for image classification (on *ImageNet*). A
    snapshot of a dynamic version is available at this link and is worth taking a
    look at: [https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: At this link, results from different models available in GluonCV Model Zoo are
    included, and it is suggested that you reproduce these results, as it is an interesting
    exercise.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Apart from ImageNet, GluonCV Model Zoo provides models pre-trained on **CIFAR10**.
    A list of these models can be found at [https://cv.gluon.ai/model_zoo/classification.html#cifar10.](https://cv.gluon.ai/model_zoo/classification.html#cifar10)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper explanation of the vanishing gradient problem, Wikipedia provides
    a good start: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, regarding ResNet and its current research relevance, in a recently
    published paper, it was shown that ResNet can still achieve **State-of-the-Art**
    (**SOTA**) results when the latest researched training techniques are applied
    to it, highlighting the importance of datasets and a training algorithm (versus
    optimization only for model architecture): [https://gdude.de/blog/2021-03-15/Revisiting-Resnets](https://gdude.de/blog/2021-03-15/Revisiting-Resnets).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects with MXNet – Faster R-CNN and YOLO
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use MXNet and GluonCV on a pre-trained model
    to detect objects from a dataset. We will see how to use GluonCV Model Zoo with
    two very important models for **object detection** – **Faster R-CNN** and **YOLOv3**.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will compare the performance of these two pre-trained models
    to detect objects on the *Penn-Fudan* *Pedestrians* dataset.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for previous chapters, in this recipe, we will be using a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will unpack in this recipe, object detection combines classification
    and regression, and therefore, chapters and recipes where we explored the foundations
    of these topics are recommended to revisit. Furthermore, we will be detecting
    objects on image datasets. This recipe will combine what we learned in the following
    chapters:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding image datasets: load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving* *Regression Problems*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will take the following steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Introduce object detection.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate object detectors.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare *Single-stage* and *Two-stage* object detectors.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the *Penn-Fudan* *Pedestrians* dataset.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduce the Object Detection Model Zoo.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worke with *MS COCO* pre-trained models.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a Faster R-CNN pre-trained model from *Model Zoo*.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a *Faster R-CNN* pre-trained model from *Model Zoo*.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a YOLOv3 pre-trained model from *Model Zoo*.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a *YOLOv3* pre-trained model from *Model Zoo*.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclude what we have learned.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing object detection
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In some of the previous chapters and recipes, we analyzed image classification
    problems, where the task of our models was to take an image and define the class
    most likely associated with it. In **object detection**, however, there can be
    multiple objects per image, corresponding to different classes, and in different
    locations of the image, and therefore, the output is now two lists, one providing
    the most likely class of each detected object and another indicating the estimated
    location of the object. The class output can be modeled as a classification problem,
    and the bounding box output can be modeled as a regression problem. Typically,
    locations are represented with what is called a bounding box. An example of bounding
    boxes is shown as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Bounding box examples](img/B16591_05_15.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Bounding box examples
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.15*, we can see two examples of bounding boxes for two different
    classes – `person` and `dog`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating object detectors
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In image classification, we defined a correct classification if the class identified
    in one image was the right one. However, in object detection, there are two parameters
    – the class and the bounding box. Intuitively, we can define a correct classification
    if, for each object that should be detected, there is a *similar enough* bounding
    box that has been classified properly. To define what *similar enough* means,
    we compute **Intersection over Union** (**IoU**), the ratio of the intersection
    of the area of the bounding boxes over the area of the union of the bounding boxes.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – IoU](img/B16591_05_16.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – IoU
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: A graphical interpretation of IoU can be seen in *Figure 5**.16*. When the IoU
    is above a determined threshold, the bounding boxes are said to match. By using
    IoU and its threshold (typically 0.5), a detection can be classified as correct
    (given the object was also correctly classified), and metrics such as accuracy,
    precision, and recall can be computed (per class).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, in [*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving Classification
    Problems*, we discussed several options to evaluate *classification* problems.
    We introduced **Area Under the Curve**(**AUC**), and we saw how changing the threshold
    had an influence on **precision** and **recall**. When plotting precision and
    recall together (the **PR curve**), we can see the effect of the threshold, in
    the same way as we did for AUC. If we calculate the area covered between the curve
    (the *x* axis, *y = 0 axis*, and *y = 1 axis*), we obtain a parameter that is
    not dependent on the threshold value; it defines the performance of our model
    with the given data:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – The PR curve](img/B16591_05_17.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – The PR curve
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: One of the characteristics of this curve that we can see clearly in *Figure
    5**.17* is its zig-zag pattern. As we decrease the threshold, the curve goes down
    with false positives and goes up again with true positives.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in order to be able to compare different models easily, instead of
    comparing the PR curves for each class, a single number metric was developed,
    the **mean Average Precision** (**mAP**). In short, it is the mean of all the
    areas under the PR curves for a model:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: mAP =  1 _ N  ∑ i=1 N A P i
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: To compute *mAP*, the first step is to calculate the average precision for each
    class, which is the area under the PR curve and then compute its arithmetic mean.
    This value provides a single number where object detection models evaluated on
    the same dataset can be compared.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Comparing single-stage and two-stage object detectors
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can think of object detectors as cropping a specific part of an image and
    passing that cropped image through an image classifier, similar to what we did
    with full images in the previous recipe. Using this approach, our object detector
    will have two steps:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Region proposal network**: This is the module that will indicate the regions
    where an object could be located.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Object classifier**: The regions will be classified by the model, with the
    regions previously cropped and resized to match the model input constraints.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach is known as **two-stage object detection**, and its most important
    characteristic is its accuracy, although it is slow due to its complex architecture,
    and fully accurate (non-approximate) training cannot be done end to end.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Faster **Region-based Convolutional Neural Network** (**R-CNN**) is one of the
    models that follow this approach. One of the most important differences from previous
    versions of the model (**R-CNN** and **Fast R-CNN**) is that in order to provide
    faster computations, it uses pre-computed bounding boxes called *anchor boxes*,
    where the scale and the aspect ratio of the bounding box are pre-defined. This
    approach allows the networks to be modeled to compute the *offset* related to
    an anchor box, instead of the full bounding box coordinates, which simplifies
    the regression problem.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Another algorithm to improve computation times is **Non-Maximum Suppression**
    (**NMS**). Typically, thousands of regions will be proposed for the next step
    of the object detection pipeline. Many of these regions overlap with each other,
    and NMS is the algorithm that takes into account the confidence of the prediction,
    removing all overlapping regions over an IoU threshold.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach for object detectors is to design architectures that make
    predictions of bounding boxes and class probabilities together, allowing end-to-end
    training in one step. Architectures following this approach are known as **single-stage
    object detectors**. These architectures also make use of anchor boxes and NMS
    to improve the regression task. The two most famous architectures using this approach
    are as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '**You Only Look Once (YOLO**): The image is processed just once using a custom
    CNN architecture (a combination of convolutional and max-pooling layers), ending
    with two fully connected layers. These architectures have been developed continuously,
    with *YOLOv3* being one of the most popular.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Shot Detector (SSD)**: The image is processed using a CNN backbone
    architecture (such as VGG-16) to compute feature maps, and the generated multi-scale
    feature maps are then classified. SSD512 (using *VGG-16* as the backbone) is one
    of the models that follow this architecture.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLOv3 model is the fastest and yields reasonable accuracy metrics, the SSD512
    model is a good trade-off between speed and accuracy, and the Faster R-CNN model
    has the highest accuracy and is the slowest of the three.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Penn-Fudan Pedestrians dataset
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our object detection experiments, we will work with a new dataset – *Penn-Fudan
    Pedestrians*. This is a publicly available dataset ([https://www.cis.upenn.edu/~jshi/ped_html/](https://www.cis.upenn.edu/~jshi/ped_html/))
    and is a collaboration between the universities of Pennsylvania and Fudan, and
    it must be downloaded manually.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has 423 pedestrians annotated from 170 images; 345 pedestrians were
    annotated for the release of the dataset (2007) and 78 pedestrians were added
    later, as the previous ones were either small or occluded.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: From the set of images of the datasets, our models will need to correctly detect
    the pedestrians present in the images and localize them, using bounding boxes.
    To understand the problem better, as we saw in previous chapters, we are going
    to do some EDA.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – The Penn-Fudan Pedestrians dataset](img/B16591_05_18.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – The Penn-Fudan Pedestrians dataset
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in *Figure 5**.18*, each image can have one or more pedestrians,
    with their corresponding bounding boxes. Each image in the dataset is in color,
    and they have a variable width and height, which are later resized depending on
    the model requirements. This figure was computed using the GluonCV visualization
    utils package (the `plot_bbox` function):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As for this datase,t there are no different classes to be classified, and no
    further visualizations are computed.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Introducing object detection Model Zoo
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GluonCV also provides pre-trained models for object detection in its Model
    Zoo. For the *MS COCO* dataset, this is the accuracy (mAP) versus performance
    (samples per second) chart:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Model Zoo for object detection (MS COCO)](img/B16591_05_19.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Model Zoo for object detection (MS COCO)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Image adapted from the following source: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.19* displays the most important pre-trained models in GluonCV Model
    Zoo, comparing accuracy (mAP on the vertical axis) and inference performance (samples
    per second on the horizontal axis). There are no models (yet) in the top-right
    quadrant, meaning that, currently, we need to balance between both characteristics.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our *Penn-Fudan Pedestrians* dataset
    in the following steps.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Working with MS COCO pre-trained models
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Models from the GluonCV Model Zoo for object detection tasks have been pre-trained
    in the *MS COCO* dataset. This dataset is one of the most popular object detection
    datasets in computer vision. It was developed by Microsoft in 2015 and was updated
    until 2017\. In its most recent update, it contains 80 classes (plus background)
    and is composed of the following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '**Training/validation sets**: 118,000/5,000 images'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A test set**: 41,000 images'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several object detection pre-trained models in GluonCV Model Zoo have been
    pre-trained with *MS COCO*, and therefore, each object detected will be classified
    among 80 classes. As there can be several objects in each image, in MXNet GluonCV
    implementations, the outputs of an object detection model are structured as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '**An array of indices**: For each object detected, this array gives the index
    of the class of the detected object. The shape of this array is *BxNx1*, where
    *B* is the batch size and *N* is the number of objects detected per image (depending
    on the model).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An array of probabilities**: For each object detected, this array gives the
    probability associated with the detected object of being the detected class in
    the **array of indices**. The shape of this array is BxNx1, where B is the batch
    size and N is the number of objects detected per image (depending on the model).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An array of bounding boxes**: For each object detected, this array gives
    the bounding box coordinates associated with the detected object. The shape of
    this array is *BxNx4*, where *B* is the batch size, N is the number of objects
    detected per image (depending on the model), and 4 is the coordinates in the format
    *[x-min, y-min,* *x-max, y-max]*.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at *Figure 5**.19*, two separated groups can be seen – the Faster R-CNN
    family, which has the highest accuracy but is slow, and the YOLO family, which
    is very fast but has a lower accuracy. For our experiments, we have selected two
    of the most popular models, each of them corresponding to a different Faster R-CNN
    (the backbone of ResNet-101 with the FPN version) and *YOLOv3* (the backbone of
    Darknet53, a 53 CNN).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, MS COCO contains the `person` class, and therefore, models pre-trained
    in MS COCO are well suited for the *Penn-Fudan* *Pedestrian* dataset.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Loading a Faster R-CNN pre-trained model from Model Zoo
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Faster R-CNN is a two-stage object detection architecture, meaning that, first,
    it provides regions where objects could be located, and second, by analyzing those
    regions, it provides the classes and locations of the detected objects. It was
    developed by Ren et al. (Microsoft Research) in 2014.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: It is the third iteration of a series of architectures that have evolved from
    each other – R-CNN, Fast R-CNN, and Faster R-CNN.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'From the research papers (There''s more section), we can see an R-CNN architecture:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – The high-level architecture of R-CNN](img/B16591_05_20.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – The high-level architecture of R-CNN
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we can see the Fast R-CNN architecture:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – The high-level architecture of Fast R-CNN](img/B16591_05_21.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – The high-level architecture of Fast R-CNN
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can see the Faster R-CNN architecture:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – The high-level architecture of Faster R-CNN](img/B16591_05_22.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – The high-level architecture of Faster R-CNN
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '**R-CNN**: This uses the selective search algorithm to provide 2,000 region
    proposals. Each of these regions is then fed into a CNN, which generates a feature
    vector for each object of 4,096 features and the four-coordinate’s bounding box.
    These feature vectors are the input to the **Support Vector Machine** (**SVM**)
    classifier.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast R-CNN**: In this iteration, instead of feeding each region to a CNN,
    the whole image is fed once, and a single feature map of the full image is computed,
    accelerating the process. Then, **Regions Of Interest** (**ROIs**) are computed
    over this feature map (instead of the image), obtained similarly with the *selective
    search* algorithm. These are then passed through an *ROI pooling* layer, where
    each object in a proposed region is assigned a feature map of the same shape.
    This is an efficient method in which the output can now be fed into the two networks
    for the regressor and classifier, which provide the location and class of each
    object respectively. These two networks are based on fully connected layers. The
    regressor computes offsets from the ROIs, and for the classifier, the output activation
    function is *softmax*.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faster R-CNN**: In this last iteration, three changes are introduced. Firstly,
    a backbone CNN is also used to compute the feature map of the image; however,
    instead of using the selective search algorithm to propose regions, some *fully
    convolutional* layers are added on top of the backbone CNN called a **Region Proposal
    Network** (**RPN**), yielding a much smaller computation time. Secondly, these
    region proposals are computed as offsets associated with anchor boxes. Lastly,
    to reduce the number of regions to process, **Non-Maximum Suppression** (**NMS**)
    is used. These three changes provide faster inference and higher accuracy.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our experiments, we will use as the backbone the `v1d` version of weights
    from a ResNet-101 network, with a **feature pyramid network** as the RPN. We can
    load the model with a single line of code:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The model then downloads successfully.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The GluonCV implementation of this model is capable of detecting 80,000 distinct
    objects.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a Faster R-CNN pre-trained model from Model Zoo
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the *Penn-Fudan Pedestrian* dataset, we can now perform qualitative and
    quantitative evaluation of the loaded model from the previous section.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN](img/B16591_05_23.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 5**.23*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth, as well as
    strong confidence (+99%) and perfect class accuracy from the model. This figure
    was computed using the GluonCV visualization `utils` package (the `plot_bbox`
    function).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitatively, we can perform a mAP evaluation and the runtime spent computing
    this metric:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The computed mAP for this model for *MS COCO* (see the object detection model
    zoo) is 40.7; therefore, given the value of 0.67 for our model, we can conclude
    our model performs the task accurately. However, it did take some time to complete
    (~250 seconds), which is expected for Faster R-CNN architectures.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Loading a YOLOv3 pre-trained model from Model Zoo
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**You Only Look Once Version 3** (**YOLOv3**) is a single-stage object detection
    architecture, meaning it uses an end-to-end approach that makes predictions of
    bounding boxes and class probabilities in a single step. It was developed by Redmon
    et al. (at the University of Washington) from 2016 (YOLO) to 2018 (YOLOv3).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: It is the third iteration of a series of architectures that have evolved from
    each other – YOLO, YOLOv2, and YOLOv3.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'From the research papers, we can see the YOLO architecture:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – The architecture of YOLOv1](img/B16591_05_24.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – The architecture of YOLOv1
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we can see the YOLOv2 architecture:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – The architecture of YOLOv2](img/B16591_05_25.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – The architecture of YOLOv2
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, we can see the YOLOv3 architecture:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – The architecture of YOLOv3](img/B16591_05_26.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – The architecture of YOLOv3
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '**YOLO**: The initial model decomposes each image into grid cells of equal
    size. Each cell is responsible for detecting an object if the location of the
    center of the object is within the cell. Each cell can predict two bounding boxes,
    their class, and their confidence score, but only one object (with a different
    size and location). All the predictions are made simultaneously, using a CNN composed
    of 24 convolutional layers and 2 fully connected layers. There is a large number
    of overlapping bounding boxes, and NMS is used to reach the final output.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv2**: YOLO architecture struggles to detect small objects in groups.
    To solve this issue, on this iteration, a number of changes are introduced – batch
    normalization to improve training accuracy, anchor boxes for regression, increased
    detection capabilities to five bounding boxes per cell, and a new backbone network,
    DarkNet-19, with 19 convolutional layers and 5 max pooling layers, with 11 more
    layers for detection.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv3**: In this last iteration, three changes are introduced. Firstly,
    to increase further the detection accuracy of small objects, the backbone network
    is updated to DarkNet-53, with 53 convolutional layers and 53 layers for the detection
    head, allowing for predictions on three different scales. Secondly, the number
    of bounding boxes per cell is reduced from five to three; however, taking into
    account the three different scales, this provides nine anchor boxes.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our experiments, we will use DarkNet-53 as a backbone. We can load the
    model with a single line of code:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The model then downloads successfully.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The GluonCV implementation of this model is capable of detecting 100 distinct
    objects.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a YOLOv3 pre-trained model from Model Zoo
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the *Penn-Fudan Pedestrian* dataset, we can now perform qualitative and
    quantitative evaluation of the loaded model from the previous section.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Comparing predictions and ground-truth for YOLOv3](img/B16591_05_27.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Comparing predictions and ground-truth for YOLOv3
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 5**.27*, we can see a very strong correlation between the bounding
    boxes of the expected results from the ground-truth and the actual outputs of
    the model, as well as strong confidence (+95%) and perfect class accuracy from
    the model. This figure was computed using the GluonCV visualization utils package
    (the `plot_bbox` function).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitatively, we can perform a mAP evaluation and the runtime spent computing
    this metric:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The computed mAP for this model for *MS COCO* (see the object detection model
    zoo) is 36.0; therefore, given the value of `0.53` for our model, we can conclude
    that it is performing the task accurately. It took `113` seconds to complete.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we tackled the object detection problem. We analyzed the differences
    between image classification problem and object detection problem for evaluation,
    network architectures, and datasets.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we used a publicly available dataset, the *Penn-Fudan Pedestrians*
    dataset, and we used pre-trained models on MS COCO. This choice was not casual;
    *MS COCO* has a `person` class, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    dataset class. However, as mentioned in the previous recipe, when this is not
    possible, and we apply pre-trained models from a dataset to another dataset, the
    data probability distribution will typically be very different; hence, the accuracy
    obtained can be very low. This is known as the domain gap or domain adaptation
    problem between the source dataset (the images that a model has been pre-trained
    on) and the target dataset (the images that the model is evaluated on).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)*.*
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: We ended the recipe by evaluating our two pre-trained models, Faster R-CNN and
    YOLOv3, and we were able to confirm that our Faster R-CNN model was very accurate
    but slow, while YOLOv3 was much faster (2x) with a slightly lower accuracy (~20%
    decrease).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 5**.19* showed a static image corresponding to the mAP versus samples
    per second graph for the Model Zoo for object detection (on *MS COCO*). There
    is also a dynamic version at this link that is worth taking a look at: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html).
    On this page, results from different models available in *GluonCV Model Zoo* are
    included; I suggest that you reproduce these result,s as it is an interesting
    exercise.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: To understand more about the *MS COCO* dataset, the original paper is available
    at https://arxiv.org/pdf/1405.0312.pdf.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, it is very interesting to read the original research papers and
    see how object detectors have evolved from an academic point of view:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '**R-CNN**: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast** **R-CNN**: [https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faster** **R-CNN**: [https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLO**: [https://arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv2**: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv3**: [https://arxiv.org/pdf/1804.02767.pdf](https://arxiv.org/pdf/1804.02767.pdf)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we have seen how Faster R-CNN was more accurate but slower,
    and YOLOv3 was faster but less accurate. To balance the trade-off between accuracy
    and inference time for object detection problems, there are different possibilities.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'One option is to estimate the difficulty of an image and apply a different
    object detector. If it is a challenging image, use the Faster R-CNN family; if
    it is simpler, use YOLOv3\. This approach is explored in detail in this paper:
    [https://arxiv.org/pdf/1803.08707.pdf](https://arxiv.org/pdf/1803.08707.pdf);
    however, *fine-tuning* a fast model such as YOLOv3 is recommended as an initial
    approach to this issue.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting objects in images with MXNet – PSPNet and DeepLab-v3
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use MXNet and GluonCV on a pre-trained model,
    segmenting objects in images from a dataset. This means that we will be able to
    split objects into different classes, such as `person`, `cat`, and `dog`. When
    framing the problem as segmentation, the expected output is an image of the same
    size as the input image, with each pixel value being the classified label (we
    will analyze how this works in the following sections). We will see how to use
    GluonCV Model Zoo with two very important models for **semantic segmentation**
    – **PSPNet** and **DeepLab-v3**.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will compare the performance of these two pre-trained models
    to segment objects semantically on the dataset introduced in the previous chapter,
    *Penn-Fudan Pedestrians*, as its ground-truth also includes segmentation masks.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will unpack in this recipe, semantic segmentation is similar to classification
    and object detection problems, and therefore, chapters and recipes where we explored
    the foundations of these topics are recommended to revisit. Furthermore, we will
    be working on image datasets. This recipe will combine what we learned in the
    following chapters:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding image datasets: load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will take the following steps:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Introduce semantic segmentation.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate segmentation models.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare network architectures for semantic segmentation.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the *Penn-Fudan Pedestrians* dataset with segmentation ground-truth.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduce Semantic segmentation Model Zoo.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a PSPNet pre-trained model from Model Zoo.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a PSPNet pre-trained model from Model Zoo.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a DeepLab-v3 pre-trained model from Model Zoo.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a DeepLab-v3 pre-trained model from Model Zoo.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing semantic segmentation
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In some of the previous chapters and recipes, we analyzed image classification
    problems where the task of our models was to take an image and define the class
    most likely associated with it. In semantic segmentation, however, there can be
    multiple objects per image, corresponding to different classes, and in different
    locations of the image. In object detection, the generated output to solve this
    problem was two lists, one providing the most likely class of each detected object
    and another indicating the estimated location of the object. For semantic segmentation,
    the output for each image is a set of binary images, one per class expected to
    be detected (dataset classes), where each pixel can have a value of 1 (active)
    if that pixel has been classified with that label, or 0 (inactive) otherwise.
    Each of these images is a **binary** **segmentation mask**:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16591_05_28(a).jpg)![](img/B16591_05_28(b).jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Binary segmentation masks
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'The person''s image has been taken as an example here from the following source:
    *azerbaijan_stockers* on Freepik: [https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm](https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm)#query=person%20in%20the%20street&position=13'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '&from_view=search&track=ais&uuid=c3458125-63b6-4899-96e5-df07c307fb46The *masks*
    shown in *Figure 5**.28* can be seen as one-hot embeddings of the classes, and
    they can be combined by associating each class with a different number (its class
    index, for example) to form a new image:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Semantic segmentation](img/B16591_05_29.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – Semantic segmentation
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The output of a semantic segmentation model is, therefore, the different binary
    segmentation masks (see an example in *Figure 5**.29*), and the number depends
    on the number of classes that the model has been trained on. Therefore, for each
    image input to the model with the shape *[H, W]* (*H* being the height and *W*
    being the *width*), the output array will have a shape of *[N, H, W]* (with *N*
    being the number of classes).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating segmentation models
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An intuitive approach to evaluate models for a semantic segmentation task is
    to report the percentage of pixels that have been correctly classified. This metric
    is commonly used and is known as *pixel accuracy*.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel Accuracy =  #TP + #TN  __________________  #TP + #TN + #FP + #FN'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: However, pixel accuracy has a problem; when objects to be segmented are small
    in comparison to the image, this metric emphasizes the large number of pixels
    that have been correctly classified as not being the object (the inactive detection).
    For example, in a 1,000x1,000 image, we have a 100x100 object, and our model classifies
    the image as the background for all the pixels, with a pixel accuracy of 99%.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: To fix these issues, we can use another metric to evaluate semantic segmentation
    models, **mean Intersection over** **Union** (**mIoU**).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.30 – IoU for segmentation masks](img/B16591_05_30.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – IoU for segmentation masks
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: The computation of this metric is similar to the IoU we saw in the previous
    recipe for object detection, in *Figure 5**.16*. However, for object detection,
    the analysis was based on bounding boxes, whereas for semantic segmentation, as
    shown in *Figure 5**.30*, it evaluates the number of pixels common (the intersection)
    between the target and the predicted masks, divided by the total number of pixels
    present across both masks (the union), and then its arithmetic mean is computed
    for all classes in the dataset.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Comparing network architectures for semantic segmentation
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semantic segmentation models’ output is the different segmentation masks, which
    are the same size as the image input. To reach this objective, several architectures
    have been proposed, with the main difference with CNNs being that there are no
    fully connected layers. Appropriately, this network architecture is named **Fully
    Connected Networks** (**FCNs**). Several models evolved from this initial architecture
    and were state-of-the-art when they were first proposed:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder–decoder**: The computation of feature maps by the convolutional and
    max pooling layers of the CNNs can be seen as an encoder, as image information
    is encoded in a multidimensional entity (feature maps). In this architecture,
    after the CNN feature maps, a series of upsampling layers (the decoder) are cascaded
    until images of the same size are computed. **U-Net** is an example of this architecture.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial pyramid pooling**: One problem with FCNs is that the encoder does
    not provide enough global scene cues to the downstream layers, resulting in objects
    being misclassified due to a lack of global context (for example, boats labeled
    as cars in water-based images, where boats are expected and not cars). In this
    type of architecture, the feature map is aggregated to the output, along with
    feature maps at different grid scales computed by different modules. **PSPNet**
    is an example of this architecture.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context modules**: Another option to capture multi-scale information is to
    add extra modules cascaded on top of the original network. **DeepLab-v3** can
    be seen as a combination of this type of network and spatial pyramid pooling networks.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.31 – Network architecture for semantic segmentation](img/B16591_05_31.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – Network architecture for semantic segmentation
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: For our experiments in this recipe, we will use pre-trained versions of PSPNet
    and DeepLab-v3.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Penn-Fudan Pedestrians dataset with segmentation ground-truth
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dataset is the one that we worked with in the previous recipe. However,
    the ground-truth we will use in this recipe is not the bounding boxes required
    for object detection but, instead, the masks required for semantic segmentation.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth](img/B16591_05_32.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 5**.32*, each image can have one or more pedestrians,
    with their corresponding masks. This figure was computed using the GluonCV visualization
    utils package (the `plot_mask` function).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: In this dataset, there are no different classes to be classified, and no further
    visualizations are computed.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Semantic Segmentation Model Zoo
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GluonCV also provides pre-trained models for semantic segmentation in its Model
    Zoo. For the *MS COCO* dataset, this is the accuracy (mIoU) versus performance
    (samples per second) chart:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)](img/B16591_05_33.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.33* displays the most important pre-trained models in GluonCV Model
    Zoo, comparing accuracy (mIoU on the vertical axis) and inference performance
    (samples per second on the horizontal axis). There are no models (yet) in the
    top-right quadrant, meaning that, currently, we need to balance between both characteristics.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our *Penn-Fudan Pedestrians* dataset
    for the segmentation task in the following steps.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Loading PSPNet pre-trained model from Model Zoo
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pyramid Scene Parsing Network** (**PSPNet**) is a spatial pyramid pooling
    semantic segmentation architecture, which means that a pyramid pooling module
    is added to provide global cues. It was developed by Zhao et al. (the Chinese
    University of Hong Kong) in 2016\. It achieved first place in the 2016 **ILSVRC
    Scene** **Parsing Challenge**.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the global pooling module, it differentiates from FCNs by using dilated
    convolutions.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – The PSPNet architecture](img/B16591_05_34.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 – The PSPNet architecture
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://github.com/hszhao/PSPNet/issues/101](https://github.com/hszhao/PSPNet/issues/101)'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5**.34*, we can see the overall architecture of PSPNet:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Map**: A ResNet-based CNN architecture, with dilated convolutions,
    is used to compute a feature map with 1/8 the size of the original image.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pyramid Pooling Module**: Using a four-level pyramid (whole, half, quarter,
    and eighth), global context is computed. It is concatenated with the original
    feature map.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Prediction**: A final computation using a convolutional layer is done
    to generate the final predictions.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our experiments, we will use a ResNet-101 network as a backbone. We can
    load the model with a single line of code:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The model then downloads successfully.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a PSPNet pre-trained model from Model Zoo
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using the *Penn-Fudan Pedestrian* dataset for a segmentation task, we can
    now perform qualitative and quantitative evaluation of the loaded model from the
    previous section.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet](img/B16591_05_35.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 5**.35*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth. This figure
    was computed using the GluonCV visualization utils package (the `plot_mask` function).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitatively, we can perform an mIoU evaluation and the runtime spent computing
    this metric:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The computed mIoU for this model for *MS COCO* (see *Semantic Segmentation Model
    Zoo*) is `0.70`; therefore, given the value of `0.56` for our model, we can conclude
    that our model performs the task accurately. However, it did take some time to
    complete it (~340 seconds).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Loading a DeepLab-v3 pre-trained model from Model Zoo
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**DeepLab-v3** is a semantic segmentation architecture that combines both *encoder-decoder*
    and **spatial pyramid pooling** architectures. It was developed by Chen et al.
    (Google) from 2015 (**DeepLab-v1**) to 2017 (**DeepLab-v3**).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: It is the third iteration of a series of architectures that have evolved from
    each other – DeepLab-v1, DeepLab-v2, and DeepLab-v3.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'From the research papers, also included in the There’s more section we can
    see the DeepLab-v1 architecture:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16591_05_36.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
- en: Figure 5.36 – The architecture of DeepLab-v1
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we can see the DeepLab- v2 architecture:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16591_05_37.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
- en: Figure 5.37 – The architecture of DeepLab-v2
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, we can see the DeepLab-v3+ architecture:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16591_05_38.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
- en: Figure 5.38 – The architecture of DeepLab-v3+
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepLab-v1**: This architecture evolves from the original FCN and uses a
    VGG-16 backbone network as well. The most important innovation is the usage of
    atrous or diluted convolutions instead of standard convolutions. We discussed
    this convolution parameter when we first introduced *convolutional layers* in
    [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression Problems*,
    showing how it increased the receptive field. This architecture also uses fully
    connected **Conditional Random Fields** (**CRFs**) for post-processing to polish
    the final segmentation masks, although it is very slow and cannot be trained end
    to end.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab-v2**: The most important innovation for this model is a new module
    called **Atrous Spatial Pyramid Pooling** (**ASPP**). With this module, on one
    hand, the network can encode multi-scale features into a fixed-size feature map
    (which is flexible to different input sizes), and on the ther hand, by using atrous
    convolutions, it increases the receptive field, optimizing the computation cost.
    In the original implementation, 4 to 6 scales were used. Furthermore, it uses
    ResNet as the backbone network.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab-v3**: In this last iteration, several changes are introduced. Firstly,
    the network is modified to use batch normalization and dropout. Secondly, the
    ASPP module is modified to add a new scale in a separate channel for global image
    pooling, to use fine-grained details. Lastly, the multi-scale part of the training
    is removed and is only applied to inference. A by-product of these small improvements
    is that the CRF step is no longer needed, providing much faster results.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our experiments, we will use a ResNet-152 network as a backbone. We can
    load the model with a single line of code:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The model then downloads successfully.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a DeepLab-v3 pre-trained model from Model Zoo
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the *Penn-Fudan Pedestrian* dataset for the segmentation task, we can
    now perform qualitative and quantitative evaluation of the loaded model from the
    previous section.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16591_05_39.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
- en: Figure 5.39 – Comparing predicted masks and ground-truth for DeepLab-v3
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 5**.39*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth. This figure
    was computed using the GluonCV visualization utils package (the `plot_mask` function).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitatively, we can perform mIoU evaluation and the runtime spent computing
    this metric:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The computed mIoU for this model for *MS COCO* (see *Semantic Segmentation Model
    Zoo*) is `0.715`; therefore, given the value of `0.56` for our model, we can conclude
    our model performs the task accurately, with a similar value to PSPNet. However,
    it is much faster (~70 secs).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we tackled the semantic segmentation problem. We analyzed the
    differences between image classification and object detection for evaluation,
    network architectures, and datasets.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we used a publicly available dataset, the *Penn-Fudan Pedestrians*
    dataset, and we used pre-trained models on *MS COCO*. This choice was not casual;
    *MS COCO* has a `person` class, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    dataset class.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: However, as mentioned in the previous recipe, when this is not possible, and
    we apply pre-trained models from a dataset to another dataset, the data probability
    distribution will typically be very different; hence, the accuracy obtained can
    be very low. This is known as the **domain gap** or **domain adaptation problem**
    between the source dataset (the images that the model has been pre-trained on)
    and the target dataset (the images that the model is evaluated on).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in a later chapter.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: We ended the recipe by evaluating our two pre-trained models, PSPNet and DeepLab-v3,
    and we were able to compare qualitatively and quantitatively their results for
    accuracy and computation speed, verifying how both models yield a similar pixel
    accuracy (`0.46)` and mIoU (`0.56`), although DeepLab-v3 was faster (`~4.5x`).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 5**.33* showed a static image corresponding to the mIoU versus samples
    per second graph for the Model Zoo for Semantic Segmentation (on MS COCO); there
    is a dynamic version at this link that is worth taking a look at: https://cv.gluon.ai/model_zoo/segmentation.html.
    On this page, results from different models available in GluonCV Model Zoo are
    included; I suggest that you reproduce these results, as it is an interesting
    exercise.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'During our discussion of the evolution of network architectures, most notably
    DeepLab, we mentioned how dilated/atrous convolutions help with multi-scale context
    aggregation. This research paper explores this topic in depth: [https://arxiv.org/pdf/1511.07122.pdf](https://arxiv.org/pdf/1511.07122.pdf).'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, it is very interesting to read the original research papers and
    see how the segmentation task has evolved from an academic point of view:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '**FCN**: [https://arxiv.org/pdf/1605.06211v1.pdf](https://arxiv.org/pdf/1605.06211v1.pdf)'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**U-Net**: [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PSPNet**: [https://arxiv.org/pdf/1612.01105.pdf](https://arxiv.org/pdf/1612.01105.pdf)'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab-v1**: [https://arxiv.org/pdf/1412.7062.pdf](https://arxiv.org/pdf/1412.7062.pdf)'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab-v2**: [https://arxiv.org/pdf/1606.00915v2.pdf](https://arxiv.org/pdf/1606.00915v2.pdf)'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab-v3**: [https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf)'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Semantic segmentation is an active area of research and, as such, is constantly
    evolving, with new networks appearing and redefining the state of the art, such
    as the following:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepLab-v3+**: The next step from DeepLab-v3, developed by Chen et al. (Google,
    2018): [https://arxiv.org/pdf/1802.02611.pdf](https://arxiv.org/pdf/1802.02611.pdf)'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swin-V2G**: Using Transformers, developed by Liu et al. (Microsoft, 2021):
    [https://arxiv.org/pdf/2111.09883v1.pdf](https://arxiv.org/pdf/2111.09883v1.pdf)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
