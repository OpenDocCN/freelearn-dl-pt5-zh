["```py\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import torch\n    from tqdm import tqdm_notebook\n    import evaluate\n    import syllables\n    from audiomentations import (AddBackgroundNoise, AddGaussianNoise,\n                                 AddGaussianSNR, LoudnessNormalization, PitchShift,\n                                 Shift, TimeStretch)\n    from datasets import load_dataset\n    from transformers import (Speech2TextForConditionalGeneration,\n                              Speech2TextProcessor)\n    ```", "```py\n    device = torch.device(\"cuda\")\n    model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n    processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n    model.to(device)\n    ```", "```py\n    ds = load_dataset(\"google/fleurs\", 'en_us', split=\"validation\")\n    ```", "```py\n    wer = evaluate.load(\"wer\")\n    all_gender = np.array(ds['gender'])\n    gender_map = {'female':1, 'male':0}\n    def get_wer_scores(dataset, transcriptions=None, sampling_rates=None, is_hg_ds=False):\n        all_wer_score = []\n        for idx, audio_data in tqdm_notebook(enumerate(dataset), total=len(dataset)):\n            inputs = processor(\n                audio_data[\"audio\"][\"array\"] if is_hg_ds else audio_data,\n                sampling_rate=audio_data[\"audio\"][\"sampling_rate\"] if is_hg_ds else sampling_rates[idx],\n                return_tensors=\"pt\"\n            )\n            generated_ids = model.generate(\n                inputs[\"input_features\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device)\n            )\n            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n            wer_score = wer.compute(\n                predictions=transcription,\n                references=[audio_data['transcription'] if is_hg_ds else transcriptions[idx]]\n            )\n            all_wer_score.append(wer_score)\n        all_wer_score = np.array(all_wer_score)\n        wer_score_results = {}\n        for gender in gender_map.keys():\n            gender_idx  = np.where(all_gender == gender_map[gender])[0]\n            wer_score_results[gender + '_wer_score'] = all_wer_score[gender_idx].mean()\n        wer_score_results['wer_score'] = all_wer_score.mean()\n        return wer_score_results\n    ```", "```py\n    def get_augmented_samples_wer_results(\n        all_baseline_samples, augment, transcriptions, all_sampling_rates\n    ):\n        all_augmented_samples = []\n        for idx, audio_sample in enumerate(all_baseline_samples):\n            augmented_samples = augment(samples=audio_sample, sample_rate=all_sampling_rates[idx])\n            all_augmented_samples.append(augmented_samples)\n        results = get_wer_scores(\n            all_augmented_samples, transcriptions, sampling_rates=all_sampling_rates, is_hg_ds=False\n        )\n        return results\n    ```", "```py\n    all_syllables_per_second = []\n    for audio_data in ds:\n        num_syllables = syllables.estimate(audio_data['transcription'])\n        syllables_per_second = num_syllables / (audio_data['num_samples'] / audio_data['audio']['sampling_rate'])\n        all_syllables_per_second.append(syllables_per_second)\n    average_syllables_per_second = np.mean(all_syllables_per_second)\n    ```", "```py\n    all_baseline_speed_audio_samples = []\n    transcriptions = []\n    all_sampling_rates = []\n    for idx, audio_data in tqdm_notebook(enumerate(ds), total=len(ds)):\n        rate = average_syllables_per_second / all_syllables_per_second[idx]\n        augment = TimeStretch(min_rate=rate, max_rate=rate, p=1.0)\n        augmented_samples = augment(\n            samples=audio_data['audio']['array'], sample_rate=audio_data['audio']['sampling_rate']\n        )\n        transcriptions.append(audio_data['transcription'])\n        all_sampling_rates.append(audio_data['audio']['sampling_rate'])\n        all_baseline_speed_audio_samples.append(augmented_samples)\n    ```", "```py\n    rates = np.linspace(0.1, 1, 9).tolist() + list(range(1, 11))\n    wer_results_by_rate = []\n    for rate_to_change in tqdm_notebook(rates):\n        augment = TimeStretch(min_rate=rate_to_change, max_rate=rate_to_change, p=1.0)\n        results = get_augmented_samples_wer_results(\n            all_baseline_speed_audio_samples, augment, transcriptions, all_sampling_rates\n        )\n        wer_results_by_rate.append(results)\n    ```", "```py\n    labels = [\"female\", \"male\", \"overall\"]\n    plt.xlabel(\"Syllables per second\")\n    plt.ylabel(\"WER\")\n    for idx, gender in enumerate([\"female_\", \"male_\", \"\"]):\n        plt.plot(\n            [average_syllables_per_second * i for i in rates],\n            [wr[gender + 'wer_score'] for wr in wer_results_by_rate],\n            label=labels[idx]\n        )\n    plt.legend()\n    ```", "```py\n    baseline_samples = [audio_data['audio']['array'] for audio_data in ds]\n    snr_rates = np.linspace(1, 100, 25)\n    wer_results_by_snr = []\n    for snr_rate in tqdm_notebook(snr_rates):\n        all_augmented_samples = []\n        augment = AddGaussianSNR(\n            min_snr_in_db=snr_rate,\n            max_snr_in_db=snr_rate,\n            p=1.0\n        )\n        results = get_augmented_samples_wer_results(\n            baseline_samples, augment, transcriptions, all_sampling_rates\n        )\n        wer_results_by_snr.append(results)\n    ```", "```py\n    plt.xlabel(\"SNR (dB)\")\n    plt.ylabel(\"WER\")\n    for idx, gender in enumerate([\"female_\", \"male_\", \"\"]):\n        plt.plot(\n            snr_rates,\n            [wr[gender + 'wer_score'] for wr in wer_results_by_snr],\n            label=labels[idx]\n        )\n    plt.legend()\n    ```", "```py\n    Wer_results_by_loudness = []\n    loudness_db = np.linspace(-31, 100, 25)\n    for db in tqdm_notebook(loudness_db):\n        augment = LoudnessNormalization(\n            min_lufs_in_db=db,\n            max_lufs_in_db=db,\n            p=1.0\n        )\n        results = get_augmented_samples_wer_results(baseline_samples, augment, transcriptions, all_sampling_rates)\n        wer_results_by_loudness.append(results)\n    ```", "```py\n    labels = [\"female\", \"male\", \"overall\"]\n    plt.xlabel(\"SNR (dB)\")\n    plt.ylabel(\"WER\")\n    for idx, gender in enumerate([\"female_\", \"male_\", \"\"]):\n        plt.plot(\n            loudness_db,\n            [wr[gender + 'wer_score'] for wr in wer_results_by_loudness],\n            label=labels[idx]\n        )\n    plt.legend()\n    ```", "```py\n    snrs = np.linspace(-50, 50, 20)\n    wer_results_by_background_noise_snr = []\n    for snr in tqdm_notebook(snrs):\n        augment = AddBackgroundNoise(\n            sounds_path=\"motorbikes\",\n            min_snr_in_db=snr,\n            max_snr_in_db=snr,\n            p=1.0\n        )\n        results = get_augmented_samples_wer_results(baseline_samples, augment, transcriptions, all_sampling_rates)\n        wer_results_by_background_noise_snr.append(results)\n    ```", "```py\n    plt.xlabel(\"SNR (dB)\")\n    plt.ylabel(\"WER\")\n    for idx, gender in enumerate([\"female_\", \"male_\", \"\"]):\n        plt.plot(\n            snrs,\n            [wr[gender + 'wer_score'] for wr in wer_results_by_background_noise_snr],\n            label=labels[idx]\n        )\n    plt.legend()\n    ```", "```py\n    import os\n    import albumentations as albu\n    import numpy as np\n    import pandas as pd\n    import torch\n    import torch.nn as nn\n    from albumentations.pytorch.transforms import ToTensorV2\n    from PIL import Image\n    from sklearn.model_selection import StratifiedShuffleSplit\n    from torchvision import models\n    from tqdm import tqdm_notebook\n    import evaluate\n    from art.attacks.evasion import HopSkipJump\n    from art.estimators.classification import BlackBoxClassifier\n    import matplotlib.pyplot as plt\n    from catalyst.contrib.layers import ArcFace\n    ```", "```py\n    device = torch.device(\"cuda\")\n    class ArcResNet50(nn.Module):\n        def __init__(self, num_classes):\n            super(ArcResNet50, self).__init__()\n            self.model =  models.resnet50(pretrained=True)\n            self.model.fc = nn.Linear(self.model.fc.in_features, self.model.fc.in_features)\n            self.head = ArcFace(self.model.fc.out_features, num_classes, s=13, m=0.15)\n        def forward(self, x, targets=None):\n            output = self.model(x)\n            outputs = self.head(output, targets)\n            return outputs\n    num_classes = 10000\n    model = ArcResNet50(num_classes=num_classes)\n    model.to(device)\n    model_path = '../CHAPTER_13/experiments/face_modelv10'\n    state_dict = torch.load(os.path.join(model_path, \"model.last.pth\"))\n    model.load_state_dict(state_dict)\n    ```", "```py\n    train = pd.read_csv('../CHAPTER_13/face_age_gender.csv')\n    image_path = train['image_path'].values\n    targets = train['target'].values\n    name2class = {name: idx for idx, name in enumerate(sorted(set(targets)))}\n    id_targets = np.array([name2class[target] for target in targets])\n    ```", "```py\n    transforms = albu.Compose([\n        albu.Resize(224, 224),\n        albu.Normalize(),\n        ToTensorV2()\n    ])\n    def predict(x):\n        if len(x.shape) == 3:\n            img = transforms(image=x)[\"image\"].unsqueeze(0).to(device)\n        else:\n            batch_img = []\n            for img in x:\n                img = transforms(image=img)[\"image\"].to(device)\n                batch_img.append(img)\n            img = torch.stack(batch_img)\n        with torch.inference_mode():\n            output = model(img)\n        return output.cpu().numpy()\n    ```", "```py\n    target_image = np.array(Image.open(os.path.join(\"../CHAPTER_13\", image_path[1])).resize((224, 224))).astype(np.float32)\n    classifier = BlackBoxClassifier(\n    predict, target_image.shape, num_classes, clip_values=(0, 255))\n    ```", "```py\n    attack = HopSkipJump(classifier=classifier, targeted=False, max_iter=10, max_eval=1000, init_eval=10)\n    ```", "```py\n    x_adv = None\n    for i in range(3):\n        x_adv = attack.generate(x=np.array([target_image]), x_adv_init=x_adv)\n        print(\"class label %d.\" % np.argmax(classifier.predict(x_adv)[0]))\n        plt.imshow(x_adv[0].astype(np.uint8))\n        plt.show(block=False)\n        print(np.min(x_adv[0] - target_image), np.max(x_adv[0] - target_image))\n    ```", "```py\n    plt.imshow(target_image.astype(np.uint8))\n    print(np.argmax(classifier.predict(np.array([target_image.astype(np.float32)]))[0]))\n    print(id_targets[1])\n    ```", "```py\n    splitter = StratifiedShuffleSplit(test_size=.02, n_splits=2, random_state = 7)\n    split = splitter.split(image_path, targets)\n    _, val_inds = next(split)\n    val_inds = val_inds[:1000]\n    ```", "```py\n    all_adversarial_hsj_predicted_class = []\n    all_predicted_class = []\n    for idx, path in tqdm_notebook(enumerate(image_path[val_inds]), total=len(val_inds)):\n        img = np.array(Image.open(os.path.join(\"../CHAPTER_13\", path))).astype(np.float32)\n        classifier = BlackBoxClassifier(\n            predict, img.shape, num_classes, clip_values=(0, 255))\n        label = id_targets[idx]\n        predicted_class = np.argmax(classifier.predict(np.array([img]))[0])\n        attack = HopSkipJump(\n            classifier=classifier, targeted=False, max_iter=10, max_eval=1000, init_eval=10, verbose=False)\n        x_adv = None\n        for i in range(3):\n            x_adv = attack.generate(x=np.array([img]), x_adv_init=x_adv)\n        adversarial_hsj_predicted_class = np.argmax(classifier.predict(x_adv)[0])\n        if (np.min(x_adv - img) >= 10.0 or np.max(x_adv - img) >= 10.0):\n            adversarial_hsj_predicted_class = predicted_class\n        all_predicted_class.append(predicted_class)\n    all_adversarial_hsj_predicted_class.append(adversarial_hsj_predicted_class)\n    ```", "```py\n    accuracy_metric = evaluate.load(\"accuracy\")\n    print(accuracy_metric.compute(references=id_targets[val_inds],\n        predictions=all_predicted_class)\n    print(accuracy_metric.compute(references=id_targets[val_inds],\n        predictions=all_adversarial_ba_predicted_class)\n    ```", "```py\nSupervised Use Case: Sentiment Analysis\nPrediction Row: {\"Text\": \"I love this product!\", \"Sentiment\": \"Positive\"}\nAdversarial Example: {\"Text\": \"I l0ve this product!\", \"Sentiment\": \"Negative\"}\n```", "```py\nSupervised Use Case: Spam/Malicious Email Detection\nPrediction Row: {\"Email Content\": \"Click the link below to claim your free iPhone.\", \"Label\": \"Spam\"}\nAdversarial Example: {\"Email Content\": \"Hey, I found this article about getting a new iPhone at a great discount. Here's the link to check it out.\", \"Label\": \"Not Spam\"}\n```"]