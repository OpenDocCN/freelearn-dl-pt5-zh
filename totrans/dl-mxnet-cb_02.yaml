- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with MXNet and Visualizing Datasets – Gluon and DataLoader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to set up MXNet. We also verified how
    MXNet could leverage our hardware to provide maximum performance. Before applying
    **deep learning** (**DL**) to solve specific problems, we need to understand how
    to load, manage, and visualize the datasets we will be working with. In this chapter,
    we will start using MXNet to analyze some toy datasets in the domains of numerical
    regression, data classification, image classification, and text classification.
    To manage those tasks efficiently, we will see new MXNet libraries and functions
    such as Gluon (an API for DL) and DataLoader.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression datasets – loading, managing, and visualizing the *House*
    *Sales* dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding classification datasets – loading, managing, and visualizing the
    Iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding image datasets – loading, managing, and visualizing the Fashion-MNIST
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding text datasets – loading, managing, and visualizing the Enron Email
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from the technical requirements specified in the *Preface*, no other requirements
    apply to this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch02](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch02)'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you can directly access each recipe from Google Colab; for example,
    for the first recipe of this chapter, visit [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch02/2_1_Toy_Dataset_for_Regression_Load_Manage_and_Visualize_House_Sales_Dataset.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch02/2_1_Toy_Dataset_for_Regression_Load_Manage_and_Visualize_House_Sales_Dataset.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression datasets – loading, managing, and visualizing the House
    Sales dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training process of **machine learning** (**ML**) models can be divided
    into three main sub-groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning (SL)**: The expected outputs are known for at least some
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning (UL)**: The expected outputs are not known but the
    data has some features that could help with understanding its internal distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning (RL)**: An agent explores the environment and makes
    decisions based on the inputs acquired from the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also an approach that falls in between the first two sub-groups called
    **weakly SL**, where there are not enough known outputs to follow an SL approach
    for one of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs are inaccurate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only some of the output features are known (incomplete)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are not exactly the expected outputs but are connected/related to the task
    we intend to achieve (inexact)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With SL, one of the most common problem types is **regression**. In regression
    problems, we want to estimate numerical outputs given a variable number of input
    features. In this recipe, we will analyze a toy regression dataset from Kaggle:
    *House Sales in King* *County, USA*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The House Sales dataset presents the problem of estimating the price of a house
    (in $) given the following 19 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Date` of the home sale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number` `of` `bedrooms`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number` `of` `bathrooms`, where `0.5` accounts for a room with a toilet but
    no shower'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_living`: Square feet of the apartment’s interior living space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_lot`: Square feet of the land space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number` `of` `floors`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Waterfront` view or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An index from 0 to 4 of how good the view of the property is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An index from 1 to 5 on the condition of the apartment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Grade`: An index from 1 to 13, with 1 being the worst and 13 the best'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_above`: Square feet of the interior housing space that is above ground
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_basement`: Square feet of the interior housing space that is below ground
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Yr_built`: The year the house was initially built'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Yr_renovated`: The year of the house’s last renovation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Zipcode` area the house is in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latitude (`Lat`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longitude (`Long`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_living15`: Square feet of interior housing living space for the nearest
    15 neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sqft_lot15`: Square feet of the land lots of the nearest 15 neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These data features are provided for *21,613* houses along with the price (value
    to be estimated).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following dataset is provided under the *CC0 Public Domain* license and
    can be downloaded from [https://www.kaggle.com/harlfoxem/housesalesprediction](https://www.kaggle.com/harlfoxem/housesalesprediction).
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the data, we are going to use a very well-known library to manage data,
    `pandas`, and we will use the most common data structure for the library, `matplotlib`,
    `pyplot`, and `seaborn` libraries. Therefore, we must run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have these libraries installed, they can be easily installed
    with the following terminal commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, to load the data, we can simply retrieve the file containing the
    dataset (available in the GitHub repository for the book) and process it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to start working with our regression dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will run an **exploratory data analysis** (**EDA**) that
    will help us understand which features are important (and which are not) to predict
    the price of a house:'
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Living square feet analysis, square feet above ground level analysis, and neighbors’
    living square feet analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grade analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rooms (bedrooms and bathrooms) analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Views analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Year-built and year-renovated analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s analyze what our data looks like. For this, we will use common operations
    on `pandas` DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output, we can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is complete (all columns have 21,613 values, as expected).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no `NULL` values (the data is clean!).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the features described previously, there is a feature called `id`.
    This feature is not needed as the index already allows us to uniquely identify
    each property.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to grasp what the values look like, let’s display the first five properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have had a look at the features. Now, let’s take a look at the price
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands will display a histogram of prices that shows how many houses
    in the dataset have a certain price (column selected in the previous commands).
    Histograms work in ranges (also known as *buckets* or *bins*); in our case, we
    have chosen 24\. As the maximum price is $8M, when applying 24 ranges, we have
    3 ranges per million dollars, specifically (all values in millions of $): [0 –
    0.33), [0.33 - 0.66), [0.66 - 1), ... until [7.66 - 8].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Price distribution](img/B16591_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Price distribution
  prefs: []
  type: TYPE_NORMAL
- en: Correlation study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will analyze how each feature correlates with each other and, most
    importantly, how each feature correlates with the price.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, as previously discussed, we are going to remove the `id` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compute the pairwise correlation diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To easily visualize the calculated correlations, we will plot a heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'These code statements yield the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – House features correlation matrix](img/B16591_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – House features correlation matrix
  prefs: []
  type: TYPE_NORMAL
- en: Note in *Figure 2**.2* that the darker the cell is, the larger the correlation
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To emphasize the first row (which is the most important as it shows the relationship
    between price and the input features), we will run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And we have the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – House features: price correlation](img/B16591_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3 – House features: price correlation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following conclusions can be drawn from *Figures 2.2* and *2.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Living square feet` and `grade` are the most highly correlated features with
    price (0.7 and 0.67 respectively)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Above square feet` and `neighbors'' living square feet` are very correlated
    with `living square feet` (0.88 and 0.76, respectively, which points to a degree
    of redundancy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of each type of room has the following correlation coefficients:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of** **bathrooms**: 0.53'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of** **bedrooms**: 0.31'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of** **floors**: 0.26'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`View`, `waterfront`, and `renovation year` have some correlation with `price`
    (0.4, 0.27, and 0.13)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Location` is correlated with `price` as well, with `latitude` being the most
    important location feature (0.31)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the features seem not to make a large contribution to the price
    of the property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, from an initial analysis, the most correlated features with *price*
    are, by order of importance: `living square feet`, `grade`, `number of bathrooms`,
    `view`, and `latitude`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will confirm these initial conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Square feet analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the correlation diagram, we identified a strong correlation between *living
    square feet* and *price* (as expected), and a potential redundancy with *above
    square feet* and *neighbors’ living square feet*. To analyze this in more detail,
    let’s plot each variable versus price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Price compared with several features: a) living square feet,
    b) above square feet, c) neighbors’ living square feet](img/B16591_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4 – Price compared with several features: a) living square feet, b)
    above square feet, c) neighbors’ living square feet'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the plots are very similar, which indicates a high correlation
    (and redundancy) among these variables. Furthermore, we can observe the largest
    density of data points occurs for prices less than $3M and less than 5,000 square
    feet. As most of our data lies in these areas, we can consider houses outside
    these ranges as outliers and remove them.
  prefs: []
  type: TYPE_NORMAL
- en: Grade analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, we can compare the *grade* feature against the price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – House grade versus price](img/B16591_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – House grade versus price
  prefs: []
  type: TYPE_NORMAL
- en: There is a clear direct correlation between the grade and the price; the higher
    the grade, the higher the price. It is also noteworthy that the highest values
    of grade are much less frequent.
  prefs: []
  type: TYPE_NORMAL
- en: Rooms analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s display in more detail the relationship between the price and the number
    of *floors*, *bedrooms*, and *bathrooms*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Price compared with several features: a) number of floors, b)
    number of bedrooms, c) number of bathrooms](img/B16591_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6 – Price compared with several features: a) number of floors, b)
    number of bedrooms, c) number of bathrooms'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, you can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.6 (a)*, we can see for a small number of floors (1-3), there
    is a direct correlation between the price of the house and this number. However,
    from the fourth floor, this correlation disappears, indicating a lack of data
    on this segment (houses with four or more floors are much less common).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.6 (b)*, the comparison with the number of bedrooms, is a similar
    scenario to the previous chart comparing the number of floors. We can see how
    for a small number of bedrooms, there is a direct correlation between the price
    of the house and this number. However, from four bedrooms up, this correlation
    disappears, and other features need to be taken into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When looking carefully at the data, you will realize that in the row with index
    `15870`, there is an outlier; it is a house with 33 bedrooms. I do not know if
    this is the actual number of bedrooms of the house (I expect not!), but to properly
    analyze the dataset, this house, an outlier, was removed from it. See the code
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.6 (c)*, we can see there is a direct correlation between the
    number of bathrooms; nevertheless, there is also some uncertainty (the chart grows
    wider as we increase the number of bathrooms).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Views analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will take a look in more detail at how *view* quality and
    a *waterfront view* (whether the house has this view or not) have a connection
    with the price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – View quality (a) and waterfront view (b) versus price](img/B16591_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – View quality (a) and waterfront view (b) versus price
  prefs: []
  type: TYPE_NORMAL
- en: From these plots individually, it is a little bit more difficult to draw conclusions.
    Other variables seem to be needed to see a clear connection between the view quality
    and the price, and similarly with the waterfront view.
  prefs: []
  type: TYPE_NORMAL
- en: Year-built and year-renovated analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following plots show how the features of which year a house was built and
    if and when a house was renovated are correlated with price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Price compared with construction year (a) and renovation (b)](img/B16591_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Price compared with construction year (a) and renovation (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, you can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.8 (a)*, we can see a slight linear increase in the price, suggesting
    that the more recently the house was built, the more expensive it is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For *Figure 2**.8 (b)*, instead of analyzing the year, we split the dataset
    into two categories – houses that had been renovated and those that had not –
    and we plotted these two categories against price. Regardless, it is a little
    bit more difficult to draw conclusions. Other variables seem to be needed to see
    a clear connection between the renovation year and the price.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will take a look in more detail at how latitude and longitude
    are connected to price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Location versus price](img/B16591_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Location versus price
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 2**.9*, we can conclude that location plays an important role in
    the price of a house. Very clearly, the northern area of King County is more valued
    than the southern area. And there is a particular central region where houses
    are significantly more expensive than other nearby regions.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression problems are one of the most common problems where SL approaches
    can be applied. By studying in depth a classic regression dataset, *King County
    House Price Prediction*, we can discover the most important connections between
    the input features (square feet, grade, and number of bathrooms) and the output
    feature (price). This analysis will help us build a model to predict the price
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we focused on an individual analysis of each feature against
    price. However, some features are better understood when combined with others
    or preprocessed. We did a simple exploration of this topic, by combining the houses
    that have been renovated in one category and comparing this with the category
    of non-renovated houses. Furthermore, for the location analysis, we used a 2D
    map to plot the latitude and longitude to discover patterns.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are plenty of relationships and analyses to be done, and I suggest
    you explore the dataset by yourself, create your own hypothese or hunches, and
    analyze the data to discover new insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, there are many other regression datasets to play with; a small
    suggestion can be found here: [https://www.kaggle.com/rtatman/datasets-for-regression-analysis](https://www.kaggle.com/rtatman/datasets-for-regression-analysis).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classification datasets – loading, managing, and visualizing the
    Iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we studied one of the most common problem types in
    SL: regression. In this recipe, we will take a closer look at another of these
    problem types: **classification**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification problems, we want to estimate a categorial output, a class,
    from a set of given classes, using a variable number of input features. In this
    recipe, we will analyze a toy classification dataset from Kaggle: the Iris dataset,
    one of the most renowned classification datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Iris dataset presents the problem of estimating the `iris` class of the
    flower of plants, from three classes (iris setosa, iris versicolor, and iris virginica)
    with the help of the following four features:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These data features are provided for 150 flowers, with 50 instances for each
    of the 3 classes (making it a balanced dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This dataset is provided under the *CC0 Public Domain* license and can be downloaded
    from [https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris).
  prefs: []
  type: TYPE_NORMAL
- en: 'To read, manage, and visualize the data, we are going to follow a similar approach
    to the toy regression dataset in the previous recipe. We will use `pandas` to
    manage the data, and we will use the most common data structure for the library,
    DataFrames. Moreover, in order to plot the data and several visualizations we
    will compute, we will use the `matplotlib`, `pyplot`, and `seaborn` libraries.
    Therefore, we must run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the data, we will introduce a new library that is very useful for managing
    datasets, called `scikit-learn`. This library comes pre-installed with a set of
    datasets, including the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have the previously mentioned libraries installed, they can be
    easily installed with the following terminal commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, to load the data, we can simply read the dataset by making use of
    `scikit-learn` library functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to start working with our classification dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will run an EDA that will help us understand which features
    are important (and which are not) to predict the iris class of a flower by completing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-versus-one comparison (pair plots)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Violin plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s analyze what our data looks like. For this, we will use common operations
    on `pandas` DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output, we can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is complete (all columns have 150 values, as expected)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no `NULL` values (the data is clean!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to grasp what the values look like, let’s display the first five properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have looked at what the features look like. Now, let’s take a look
    at what the iris class distribution looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to confirm that there are 50 instances per class, we can run the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, `0` corresponds to `setosa`, `1` to `versicolor`, and `2` to `virginica`.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will analyze how each feature correlates with each other and, most
    importantly, how each feature correlates with the iris class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute a pairwise correlation diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To easily visualize the calculated correlations, we will plot a heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'These code statements yield the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Flower features correlation matrix](img/B16591_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Flower features correlation matrix
  prefs: []
  type: TYPE_NORMAL
- en: Let’s note in *Figure 2**.10* that the darker the cell is, the larger the correlation
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To emphasize the first row (most important as it shows the relationship between
    the iris class and the input features), we will run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And we have the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Flower features: iris class correlation](img/B16591_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11 – Flower features: iris class correlation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following conclusions can be drawn from *Figures 2.10* and *2.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: Petal measurements (length and width) are highly correlated; analyzing and training
    both might not yield any additional information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal measurements are the most highly correlated features with the iris class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal length and width are highly correlated as well but in opposite ways (sepal
    length is positively correlated while sepal width is negatively correlated).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-versus-one comparison (pair plots)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In classification problems, hue/brightness can be used to indicate the class
    of a plot. Moreover, as in this dataset we are able to work with a limited set
    of features (four), a pair plot diagram will be very useful to compare all features
    in a single plot. The code to plot this diagram is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s the displayed diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Flower features pair plot](img/B16591_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Flower features pair plot
  prefs: []
  type: TYPE_NORMAL
- en: 'From this set of plots, we can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: Setosa iris is easily differentiated using any of the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal features overlap among the different iris classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal features have a direct relationship with the iris class; that is, the
    smallest numbers point to **setosa**, medium numbers point to **versicolor**,
    and the largest numbers point to **virginica**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a region on the boundaries of **versicolor** and **virginica** where
    both groups overlap, for a petal length larger than ~5 cm and a petal width larger
    than ~1.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Violin plot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another plot that might help with understanding the relationships between features
    and the iris class is a violin plot. The code to generate this plot is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s the displayed diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Flower features violin plot](img/B16591_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Flower features violin plot
  prefs: []
  type: TYPE_NORMAL
- en: In these plots, the conclusions we found are even clearer, with the setosa (`0`)
    iris class clearly separable and where there is substantial overlap for versicolor
    (`1`) and virginica (`2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The violin plot also provides an indication of the distribution of the values
    in our case (starting with 0 to match the indexes of the classes in the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setosa**: Values are more likely to be found around the mean (~1.5 cm for
    petal length and ~0.25 cm for petal width).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versicolor**: Normal distribution with mean values ~4.25 cm and ~1.3 cm,
    and standard distribution values ~0.5 cm and ~0.2 cm (for petal length and petal
    width respectively).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virginica**: Uniform distribution between [~5.1, ~5.9] cm and [~1.8, ~2.3]
    cm (for petal length and petal width respectively).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification problems are one of the most common problems where **supervised
    ML** (**SML**) approaches can be applied. By studying a classic classification
    dataset, Iris Class in depth we can discover the connections between the input
    features (petal length and petal width) and the output feature (iris class). This
    analysis will help us build a model to predict the class in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we focused on an individual analysis of each feature against
    the iris class. This was similar in principle to the analysis done for regression
    datasets, with the added information about the hue/brightness for each plot. A
    similar suggestion to the reader follows, to continue and deepen the analysis
    by themselves to discover new insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned the Iris dataset is one of the classical classification datasets;
    nonetheless, it dates back to 1936! Original reference: [https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x](https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x).'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as we will explore in the next chapter, classification problems
    can be seen as a special case of regression problems. In the regression recipe,
    we studied the prices of houses, and presented them in a way that would allow
    a buyer to determine which are affordable, by taking the price and comparing it
    with a threshold, which could be, for example, our budget limit. Therefore, we
    can use that threshold to classify houses that are below it as affordable and
    above it as not affordable. We will explore this connection in depth in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, there are many other classification datasets to play with; a small
    selection can be found here: [https://www.kaggle.com/search?q=classification+tags%3Aclassification](https://www.kaggle.com/search?q=classification+tags%3Aclassification).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding image datasets – loading, managing, and visualizing the Fashion-MNIST
    dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the fields that has grown considerably in DL in the last years has been
    **computer vision** (**CV**). Since the AlexNet revolution in 2012, CV has expanded
    from lab research to surpassing human performance in real-world datasets (known
    as “in the wild”).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will explore the simplest CV task: **image classification**.
    Given a set of images, our task is to correctly classify that image among a given
    set of labels (classes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most classic image classification datasets is the **MNIST** (which
    stands for the **Modified National Institute of Standards and Technology**) database.
    Similarly sized, but more suited for current CV analysis, is the *Fashion-MNIST
    dataset*. This dataset is a multi-label image classification dataset, with a training
    set of 60k examples and a test set of 10k examples, with each example belonging
    to 1 of these 10 categories (starting with 0 to match the indexes of the classes
    in the code):'
  prefs: []
  type: TYPE_NORMAL
- en: T-shirt/top
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trouser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pullover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sandal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shirt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sneaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ankle boot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each image is grayscale with 28x28 pixel dimensions. This can be seen as each
    data point having 784 features. The dataset is composed of 6k images per class
    in the training set and 1k images per class in the test set (balanced dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset is provided under the *MIT* license and can be downloaded from
    the following URL: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is directly available from MXNet Gluon, and therefore we will
    use this library to access it. Moreover, as this dataset is significantly larger
    than the others we have explored so far, to handle the data efficiently, we will
    use the Gluon DataLoader functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Gluon is installed with MXNet; no further steps are required.
  prefs: []
  type: TYPE_NORMAL
- en: This is all we need to start working with the Fashion-MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, data needs to be modified (transformed) for some operations. This
    can be done by defining a `transform` function and passing it as a parameter (`transform=<function_name>`).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will run an EDA that will help us understand which features
    are important (and which are not) to predict the category of a garment, with the
    help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the data structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describing examples per class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding dimensionality reduction techniques
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing **Principal Component** **Analysis** (**PCA**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing **t-distributed Stochastic Neighbor** **Embedding** (**t-SNE**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing **Uniform Manifold Approximation and** **Projection** (**UMAP**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing **Python Minimum-Distortion** **Embedding** (**PyMDE**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifying the data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to optimize memory usage to work with large-scale datasets, instead
    of loading the full dataset in memory, datasets are usually accessed through **batches**,
    which are smaller packets of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gluon has its own way of generating batches, while also applying `128`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader does not return a data structure but an iterator. Therefore, to access
    the data we need to iterate upon it, we use constructs such as `for` loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s verify the data structure is as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Gluon loads grayscale images as images with one channel, and the dimension for
    each batch is (batch size, height, width, number of channels); in our example,
    (128, 28, 28, 1).
  prefs: []
  type: TYPE_NORMAL
- en: Describing examples per class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Fashion-MNIST dataset is a balanced dataset with 6k examples per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Fashion-MNIST dataset labels](img/B16591_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Fashion-MNIST dataset labels
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at what each class looks like. In order to do this, we can
    plot 10 examples per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Fashion-MNIST dataset](img/B16591_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Fashion-MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 2**.15*, all instances can be differentiated fairly
    well by a human, except for the `T-shirt`/`top`, `Pullover`, `Coat`, and `Shirt`
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dimensionality reduction techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from the large number of data points that our dataset contains, the number
    of features in the image (that is, the number of pixels per image) is very high.
    In our toy dataset, each image has 784 features, which can be seen as 1 point
    in 784-dimensional space. In this space, it is extremely difficult to analyze
    relationships among features (for example, correlation, as we explored in previous
    datasets). Furthermore, it is not rare to work with higher-quality images, with
    resolutions over 1 MP (more than 1 million features). For a 4k image, the number
    of features is ~8 million.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this subsection and the next ones (on *PCA*, *t-SNE*, *UMAP*,
    and *PyMDE*), we will work with techniques known as **dimensionality reduction**
    techniques. The idea behind these techniques is to be able to visualize high-dimensional
    features easily, typically in 2D or 3D, which are the kinds of visualizations
    humans are used to working with. These embeddings have two or three components
    that can be plotted in 2D or 3D. These representations are dataset-dependent;
    they are *learned* representations.
  prefs: []
  type: TYPE_NORMAL
- en: Each technique described has a different way of achieving this result. In this
    book, we will not deepen our knowledge of how each technique works, but the interested
    reader can find more information in the *There’s* *more...* section.
  prefs: []
  type: TYPE_NORMAL
- en: Please also note that although each technique is different, all of them require
    a vector as an input (feature vector). This means that there is some spatial information
    that is lost. In our example, from 28x28 images, we will input 784 feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As expected, we can see some large clusters (**Sneaker** and **Ankle boot**),
    and others are mostly overlapping (**T-shirt**, **Pullover** and **Coat**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Fashion-MNIST 2D PCA](img/B16591_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Fashion-MNIST 2D PCA
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing t-SNE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another technique for dimensionality reduction is t-SNE. This technique is
    based on the idea of computing a probability distribution that represents similarities
    among neighbors. A recommended preliminary step is to compute PCA for 50 features
    and then pass these 50 feature vectors to the t-SNE algorithm. This is what we
    did to generate the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Fashion-MNIST 2D t-SNE](img/B16591_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Fashion-MNIST 2D t-SNE
  prefs: []
  type: TYPE_NORMAL
- en: In this plot, we can see in a clearer way how easily distinguishable objects
    are clustered in isolation (**Trouser**, on the lower right, and **Bag**, on the
    upper left).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'For PCA and t-SNE, we can choose three components instead of two, which will
    yield a 3D plot. For the code, visit the GitHub repository of the book: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing UMAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another method for dimensionality reduction is **UMAP**. UMAP allows us to
    play with different parameters, such as the *number of neighbors*, which helps
    the visualization of how to balance a local versus global structure. For an example
    with five neighbors, this is the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Fashion-MNIST UMA](img/B16591_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Fashion-MNIST UMA
  prefs: []
  type: TYPE_NORMAL
- en: In this visualization, we can observe similar trends as in previous plots; that
    is, **Bag** is clustered in the upper-center region, and **Trouser** in the lower-center
    region. However, in this visualization, we can also note that there is a cluster
    on the left that contains data for **Ankle boot**, **Sneaker**, and **Sandal**,
    and another important cluster on the right for **Shirt**, **Coat**, **Dress**,
    and **T-shirt/top**, and we can see how these clusters overlap with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install UMAP, please run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing PyMDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another popular technique that provides insightful visualizations is **PyMDE**.
    PyMDE allows two main approaches, to preserve neighbors (local structure of the
    data is preserved) and to preserve distances. This preserves relationship attributes
    such as pairwise distances in the data. The approach to preserve neighbors is
    similar to the plots we are seeing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Fashion-MNIST PyMDE](img/B16591_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Fashion-MNIST PyMDE
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 2**.19*, very similar conclusions to UMAP can be drawn
    from a PyMDE visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install UMAP, please run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand an image dataset, we need to understand the underlying connections
    among images in that dataset. One useful method to achieve this is with different
    visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we have learned how to discover patterns in our image datasets.
    We selected a well-researched dataset, Fashion-MNIST, and learned one of the most
    important approaches for working with large-scale datasets: **batching**.'
  prefs: []
  type: TYPE_NORMAL
- en: We analyzed our dataset by taking a look at its internal structure and what
    the actual images looked like and tried to foresee where potential classification
    algorithms could have issues (such as similarities between coats and shirts and
    ankle boots and sneakers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Every pixel is a dimension/feature of each image, and, therefore, to work with
    them, we learned about some dimensionality reduction techniques: PCA, t-SNE, UMAP,
    and PyMDE. With these visualizations, we were able to verify and extend our knowledge
    of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many resources for MNIST and Fashion-MNIST, as these are well-researched
    datasets. I personally recommend the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MNIST** **database:** [https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**zalandoresearch/fashion-mnist:** [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We introduced some dimensionality reduction techniques, but we did not deepen
    our knowledge of them. If you want to understand better how each of these techniques
    works, I suggest the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA (from** **Caltech)**: [http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/PrincipalComponentAnalysis.pdf](http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/PrincipalComponentAnalysis.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-SNE**: [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UMAP**: [https://umap-learn.readthedocs.io/](https://umap-learn.readthedocs.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyMDE**: [https://pymde.org/](https://pymde.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the code, you can find how to obtain the visualizations included. Furthermore,
    for PCA and t-SNE, as the number of components is a variable, 3D plots for both
    of them are included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for readers interested in learning more about DL and its history,
    I recommend the following link: [https://www.skynettoday.com/overviews/neural-net-history](https://www.skynettoday.com/overviews/neural-net-history).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding text datasets – loading, managing, and visualizing the Enron Email
    dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another field that has grown considerably in DL in recent years is **natural
    language processing** (**NLP**). Similarly to CV, this field aims to surpass human
    performance in real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will explore one of the simplest NLP tasks: **text classification**.
    Given a set of sentences and paragraphs, our task is to correctly classify that
    text among a given set of labels (classes).'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most classic text classification tasks is to distinguish whether
    received email is spam or not (ham). These datasets are binary text classification
    datasets (only two labels to assign, `0` and `1`, or `ham` and `spam`).
  prefs: []
  type: TYPE_NORMAL
- en: In our specific scenario, we will use a real-world email dataset. This set of
    emails was made public during the investigation of the Enron scandal in the early
    2000s by the US Government. This dataset was first published in 2004 and is composed
    of emails from ~150 users, mostly senior management at Enron. Only a subset (known
    as `enron1`) is used in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'It contains 5,171 emails, with no training/test split (labels are provided
    for all examples). Being a real-world dataset, emails vary heavily with respect
    to subjects, content length, word count, and word length, and out of the box,
    the dataset only contains two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0` corresponds to `Ham` and `1` to `Spam`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: Includes the subject and the body of the email'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is composed of 3,672 examples of ham email (~70%) and 1,499 examples
    of spam email (~30%); it is a highly imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This dataset is provided under the *CC0 Public Domain* license and can be downloaded
    from [https://www.kaggle.com/venky73/spam-mails-dataset](https://www.kaggle.com/venky73/spam-mails-dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the data, we are going to follow a similar approach as seen in the
    recipe for regression tasks. We are going to load the data from a CSV file, and
    we are going to work with the data using very well-known Python libraries: `pandas`,
    `pyplot`, and `seaborn`. Therefore, we must run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, to load the data, we can simply read the file containing it (the
    file can be found in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to start working with our spam email dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will run an EDA that will help us understand which features
    are important (and which are not) to predict whether an email is spam or not,
    the following are not worded as steps. please either reword this to a more suitable
    lead-in or reword the following as steps. if the latter, please change the circular
    bullets to numbering:'
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples per class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Content analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word processing (tokenizing, stop words, stemming, and lemmatization)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word clouds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word embeddings (word2vec and **Global Vectors for Word** **Representation**
    (**GloVe**))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA and t-SNE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step we will carry out will be to reformat the dataset for our purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After these modifications, the shape of our email DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Examples per class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now take a look at each class distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Spam emails dataset](img/B16591_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Spam emails dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the dataset is highly imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: Content analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we are going to analyze the emails’ length and their distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Emails’ length](img/B16591_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – Emails’ length
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a large outlier set corresponding to emails with more than 5,000 characters.
    Let’s zoom in to the area where most of the emails lie and graph the length and
    the word count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Emails’ length (detailed)](img/B16591_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – Emails’ length (detailed)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 – Emails’ word count](img/B16591_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 – Emails’ word count
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.23*, we have defined a word without any semantic or dictionary
    approach, simply by specifying that each space-separated entity constitutes a
    word. This approach has disadvantages that we will analyze further in this recipe
    and in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098).
  prefs: []
  type: TYPE_NORMAL
- en: By looking at this graph, we can conclude that in terms of emails’ length and
    word count, there is no significant difference between spam and legitimate emails.
    We will need to understand more about the words, their meaning, and their relationships
    to improve our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, let’s start by looking at which words are most frequent in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24 – Most frequent words](img/B16591_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.24 – Most frequent words
  prefs: []
  type: TYPE_NORMAL
- en: The first and most important conclusion after seeing *Figure 2**.24* is that
    our initial space-separation approach to differentiate words was not enough when
    dealing with real-world datasets. Punctuation errors and typos are very common,
    and furthermore, as expected, very common words such as “the” and “to” yield no
    real benefit in understanding the differences between spam and legitimate emails.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s get rid of some common issues when working with real-world text datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trailing characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clarifications” (text between square brackets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words containing numbers and links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word *subject* (specific to our email dataset structure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After processing our corpus (text data specific to our problem) through our
    cleaning function, the results are more similar to our expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25 – Most frequent words (clean)](img/B16591_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.25 – Most frequent words (clean)
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.25*, we can see how the new word corpus we are analyzing contains
    real words. However, it is very clear that the most frequent words don't help
    to distinguish between spam and legitimate emails; words such as “the” and “to”
    are too common in the English language to be used properly for this classification.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'N-grams from a corpus in NLP are a set of *N* co-occurring words in the corpus.
    Typically in NLP, the most common N-grams are *unigrams* (one word), *bigrams*
    (two words), and *trigrams* (three words). Plotting the most frequent N-grams
    helps us understand relationships among words and classes (spam or not). A unigram
    is simply the most frequent word graph, as plotted in the previous section in
    *Figure 2**.25*. For bigrams (per class), see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26 – Most frequent bigrams in ham (a) and spam (b)](img/B16591_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.26 – Most frequent bigrams in ham (a) and spam (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'For trigrams (per class), see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27 – Most frequent trigrams in ham (a) and spam (b)](img/B16591_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.27 – Most frequent trigrams in ham (a) and spam (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'In these graphs, we can start to grasp the underlying differences between the
    two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: If there is a mention of *Enron Corp*, it is very likely to be a legitimate
    email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a polite call to action (“please let me know”), it is very likely
    to be a legitimate email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a link, it is very likely to be spam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are typos (*hou* instead of *how*, *ect* instead of *etc*, and so on),
    it is very likely to be a legitimate email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *pills* are mentioned, it is very likely to be spam (plus a bonus for repetitiveness)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have also discovered that there are some nuances related to how the emails
    have been coded: `nbsp` (for non-breaking space). It is very likely that the email
    parser found some kind of unstructured spaces in the text and has redacted them
    with the `nbsp` keyword. Coincidentally, these types of parsing nuances are much
    more common in spam emails than in legitimate ones, which will help us in our
    analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Word processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Processing words in text is typically composed of four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop-words filtering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps have their own individual complexity, and therefore we will use
    libraries available to run these steps, such as the **Natural Language Toolkit**
    (**NLTK**). To install it, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**Tokenizing** is the step that processes a text and returns a list of **tokens**.
    Each word is a token, but if there are also punctuation marks, these would become
    separate tokens. Nevertheless, for our corpus, these have been removed in a previous
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that in this step, we have moved from a list of sentences and paragraphs
    per email, the corpus, to what is known as **bag of words** or **BOW**, which
    is directly connected to the vocabulary used in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: After we have each word as an entity, we can remove those common words we had
    already identified such as “the” or “to." These are known as stop words, and NLTK
    contains a set of these stop words for several languages. We will use this available
    set to filter our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is the process of reducing derived (and inflected, if we want to be
    formal) words to their root, known as the stem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lemmatization is the process of grouping together several different forms that
    can be analyzed as a single item, identified by the word’s lemma, or dictionary
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28 – Stemming and lemmatization](img/B16591_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.28 – Stemming and lemmatization
  prefs: []
  type: TYPE_NORMAL
- en: 'After processing our BOW through these steps, we have reduced the number of
    words we are working with to ~10% of the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Word clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With our postprocessed BOW, we can generate one of the most impactful and popular
    visualizations of a text corpus, a word cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29 – Word clouds for (a) ham and (b) spam](img/B16591_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.29 – Word clouds for (a) ham and (b) spam
  prefs: []
  type: TYPE_NORMAL
- en: In these visualizations we can clearly see how *Enron*, *please*, and *let know*
    are relevant for legitimate emails, whereas *new*, *nbsp*, *compani*, *market*,
    and *product* are typically connected to spam emails.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have taken a look at how individual words behave (frequency and length)
    and their connections to other words in terms of the most frequent N-grams (bigrams
    and trigrams). However, we have not connected the meaning of the words among them.
    For example, one would expect that *Enron*, *corp*, and *company* (*compani*,
    its stemmed counterpart) are close from a semantic point of view. Therefore, we
    would like to have a representation where words with a similar meaning would have
    a similar representation. Furthermore, if that representation had a constant number
    of dimensions, we could easily make comparisons (find similarities) among words.
    These are word embeddings, and the representation is a vector.
  prefs: []
  type: TYPE_NORMAL
- en: There are infinite ways of generating vectors from words; for example, the most
    naive way to accomplish this is to generate as many dimensions (features of our
    vector, columns in a matrix representation) as our vocabulary, and then in each
    email (a row in the matrix representation), for each word that it contains, we
    can put a *1* (checkmark) in the column for that word in the vocabulary, resulting
    in vectors of the form [0, 0, 0, 0 ......, 1, 0, 0, 0,...., 1.....]. This representation
    is called one-hot encoding, but it is very inefficient as the number of features
    is the number of distinct words in the corpus, the length of the vocabulary, which
    is typically very high (~500k with our reduced vocabulary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we will take a look at more optimal ways to represent our words:
    word2vec and GloVe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**word2vec**: This algorithm was developed by Google in 2013 and is available
    pre-trained on the *Google News* corpus. It has a corpus of 3 billion words and
    a vocabulary of 3 million distinct words, each represented with 300 features.
    The intuition behind this algorithm is to calculate the probability of a given
    word by taking into account its context (surrounding words). The window size (how
    many words are being looked at the same time) is a parameter of the model and
    is a constant, which makes the model rely solely on the local context of each
    word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word2vec` combined with word co-occurrence (global statistics) to provide
    a more complete representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these representations, we can now compute operations in words in their
    new vector representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stronger` is to `strong` what `weaker` is to `weak`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This generates an output of `~1.9`, which is close.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`king`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The observant reader will have realized that word embeddings are similar to
    the techniques we saw in the previous recipe for dimensionality reduction, as
    those were learned representations as well. However, in this case, we are actually
    increasing the dimensionality to obtain new advantages (constant number of features
    and similar meaning representation).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are typically *learned* representations; that is, the representations
    are trained to minimize the distance among words that have a similar meaning or,
    in our case, are classified with the same label. In this recipe, we will use pre-trained
    representations for `word2vec` and GloVe, and in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    we will take a look at training.
  prefs: []
  type: TYPE_NORMAL
- en: PCA and t-SNE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in "the subsection", our current embeddings have either 300 features
    (`word2vec`) or 50 (GloVe). For proper visualizations, we need to apply dimensionality
    reduction techniques, as we saw in the previous recipe for CV.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this dataset, we can apply PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.30 – PCA for (a) word2vec and (b) GloVe embeddings](img/B16591_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.30 – PCA for (a) word2vec and (b) GloVe embeddings
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we can apply t-SNE as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31 – t-SNE for (a) word2vec and (b) GloVe embeddings](img/B16591_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.31 – t-SNE for (a) word2vec and (b) GloVe embeddings
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding plots, we can see how the ham and spam words in our embedding
    space are very close to each other, making it very difficult to separate the clusters.
    This is due to the fact that we are using pre-trained embeddings, from the news
    and Wikipedia datasets. These datasets and the corresponding embeddings are not
    suited to our task at hand. We will see how to train word embeddings to achieve
    better results in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'For PCA and t-SNE, we can choose three components instead of two, which will
    yield a 3D plot. For the code, visit the GitHub repository of the book: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the corpus of a text dataset, we need to understand the underlying
    connections among words in that corpus. One useful method to achieve this is with
    different visualizations of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we have learned how to discover patterns in our text datasets.
    We selected an imbalanced dataset, the *Enron Email* dataset, and we learned how
    to deal with binary classification datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We analyzed our dataset by taking a look at its internal structure and what
    the class imbalance looked like and checked the most common words in search of
    patterns and errors. We cleaned the dataset by removing punctuation marks, and
    we graphed the most frequent **bigrams** and **trigrams** and noticed several
    keywords that would help us classify our emails correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to generate some cool visualizations such as **word clouds**,
    and we understood why **word embeddings** are important and plotted them using
    the **dimensionality reduction techniques** we learned previously.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to know more about the Enron Email dataset and the Enron scandal,
    the following links will help:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enron Email** **dataset**: [http://www.cs.cmu.edu/~enron/](http://www.cs.cmu.edu/~enron/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enron** **scandal**: [https://en.wikipedia.org/wiki/Enron_scandal](https://en.wikipedia.org/wiki/Enron_scandal)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We had a brief overview of several important concepts I invite you to learn
    more about:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BOW**: [https://machinelearningmastery.com/gentle-introduction-bag-words-model/](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N-grams**: [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word** **clouds**: [https://amueller.github.io/word_cloud/](https://amueller.github.io/word_cloud/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, we barely scratched the surface of what word embeddings can offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**word2vec**: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GloVe**: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
