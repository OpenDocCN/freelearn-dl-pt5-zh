<html><head></head><body>
		<div id="_idContainer130">
			<h1 id="_idParaDest-226" class="chapter-number"><a id="_idTextAnchor248"/>14</h1>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor249"/>What to Do If the System Isn’t Working</h1>
			<p><a id="_idTextAnchor250"/>In this chapter, we will discuss how to improve systems. If the original model’s first round of training fails to produce a satisfactory performance or the real-world scenario that the system addresses undergoes changes, we need to modify something to enhance the system’s performance. In this chapter, we will discuss techniques such as adding new data and changing the structure of an application, while at the same time ensuring that new data doesn’t degrade the performance of the existing system. Clearly, this is a big topic, and there is a lot of room to explore how to improve the performance of <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) systems. It isn’t possible to cover all the possibilities here, but this chapter should give you a good perspective on the most important options and techniques that can improve <span class="No-Break">system performance.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Figuring out that a system <span class="No-Break">isn’t working</span></li>
				<li>Fixing <span class="No-Break">accuracy problems</span></li>
				<li>Moving on <span class="No-Break">to deployment</span></li>
				<li>Problems <span class="No-Break">after deployment</span></li>
			</ul>
			<p>The first step is to find out that a system isn’t working as well as desired. This chapter will include a number of examples of tools that can help with this. We will start by listing the software requirements needed to run <span class="No-Break">these examples.</span></p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor251"/>Technical requirements</h1>
			<p>We will be using the following data and software to run the examples in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Our usual development environment – that is, Python 3 and <span class="No-Break">Jupyter Notebook</span></li>
				<li>The <span class="No-Break">TREC dataset</span></li>
				<li>The Matplotlib and Seaborn packages, which we will use to display <span class="No-Break">graphical charts</span></li>
				<li>pandas and NumPy for numerical manipulation <span class="No-Break">of data</span></li>
				<li>The BERT NLU system, previously used in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> and <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a></li>
				<li>The Keras machine learning library, for working <span class="No-Break">with BERT</span></li>
				<li>NLTK, which we will use for generating <span class="No-Break">new data</span></li>
				<li>An OpenAI API key which we will use to access the <span class="No-Break">OpenAI tools</span></li>
			</ul>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor252"/><a id="_idTextAnchor253"/>Figuring out that a system isn’t working</h1>
			<p>Figuring out whether a<a id="_idIndexMarker1135"/> system isn’t working as well as it should be is important, both during initial development as well as during ongoing deployment. We’ll start by looking at poor performance during <span class="No-Break">initial development.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor254"/>Initial development</h2>
			<p>The primary techniques we will use <a id="_idIndexMarker1136"/>to determine that our system isn’t working as well as we'd like are the evaluation techniques we learned about in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>. We will apply those in this chapter. We will also use confusion matrices to detect specific classes that don’t work as well as the <span class="No-Break">other classes.</span></p>
			<p>It is always a good idea to look at the dataset at the outset and check the balance of categories because unbalanced data is a common source of problems. Unbalanced data does not necessarily mean that there will be accuracy problems, but it’s valuable to understand our class balance at the beginning. That way, we will be prepared to address accuracy issues caused by class imbalance as system <span class="No-Break">development progresses.</span></p>
			<h3>Checking category balance</h3>
			<p>For our data exploration in this<a id="_idIndexMarker1137"/> chapter, we will use the <strong class="bold">Text Retrieval Conference</strong> (<strong class="bold">TREC</strong>) dataset, which is<a id="_idIndexMarker1138"/> a commonly used multi-class classification dataset and can be downloaded from Hugging <span class="No-Break">Face (</span><a href="https://huggingface.co/datasets/trec"><span class="No-Break">https://huggingface.co/datasets/trec</span></a><span class="No-Break">).</span></p>
			<p class="callout-heading">Dataset citations</p>
			<p class="callout"><em class="italic">Learning Question Classifiers</em>, Li, Xin and Roth, Dan, <em class="italic">{COLING} 2002:</em> <em class="italic">The 19th International Conference on Computational Linguistics</em>, <span class="No-Break">2002, </span><a href="https://www.aclweb.org/anthology/C02-1150&#13;"><span class="No-Break">https://www.aclweb.org/anthology/C02-1150</span></a></p>
			<p class="callout"><em class="italic">Toward Semantics-Based Answer Pinpointing</em>, Hovy, Eduard and Gerber, Laurie and Hermjakob, Ulf and Lin, Chin-Yew and Ravichandran, Deepak, <em class="italic">Proceedings of the First International Conference on Human Language Technology Research</em>, <span class="No-Break">2001, </span><a href="https://www.aclweb.org/anthology/H01-1069&#13;"><span class="No-Break">https://www.aclweb.org/anthology/H01-1069</span></a></p>
			<p>The dataset consists of 5,452 training examples of questions that users might ask of a system and 500 test examples. The goal of the classification task is to identify the general topic of a question as the first step in answering it. The question topics are organized into two levels, consisting of six broad categories and 50 more specific subcategories that fall under the <span class="No-Break">broader topics.</span></p>
			<p>We will be working with the broad categories, which are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Abbreviation (</span><span class="No-Break"><strong class="source-inline">ABBR</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Description (</span><span class="No-Break"><strong class="source-inline">DESC</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Entity (</span><span class="No-Break"><strong class="source-inline">ENTY</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Human (</span><span class="No-Break"><strong class="source-inline">HUM</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Location (</span><span class="No-Break"><strong class="source-inline">LOC</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Number (</span><span class="No-Break"><strong class="source-inline">NUM</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>One important task at the beginning is to find out how many documents are in each class. We want to see whether all of the classes have enough texts for effective training and whether no classes are significantly more or less common than <span class="No-Break">the others.</span></p>
			<p>So far in this book, we have seen many ways to load datasets. One of the easiest ways to load a dataset is based on data being organized into folders, with separate folders for each class. Then, we can load the dataset with the <strong class="source-inline">tf.keras.utils.text_dataset_from_directory()</strong> function, which we used several times in <a id="_idIndexMarker1139"/>previous chapters, and see the class names. This is shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# find out the total number of text files in the dataset and what the classes are
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
training_ds = tf.keras.utils.text_dataset_from_directory(
    'trec_processed/training')
class_names = training_ds.class_names
print(class_names)
Found 5452 files belonging to 6 classes.
['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']</pre>
			<p>We can then count the number of files in each class and display them in a bar graph with this code, using the <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">seaborn</strong> <span class="No-Break">graphics libraries:</span></p>
			<pre class="source-code">
files_dict = {}
for class_name in class_names:
    files_count = training_ds.list_files(
        'trec_processed/training/' + class_name + '/*.txt')
    files_length = files_count.cardinality().numpy()
    category_count = {class_name:files_length}
    files_dict.update(category_count)
# Sort the categories, largest first
from collections import OrderedDict
sorted_files_dict = sorted(files_dict.items(),
    key=lambda t: t[1], reverse=True)
print(sorted_files_dict)
# Conversion to Pandas series
pd_files_dict = pd.Series(dict(sorted_files_dict))
# Setting figure, ax into variables
fig, ax = plt.subplots(figsize=(20,10))
# plot
all_plot = sns.barplot(x=pd_files_dict.index,
    y = pd_files_dict.values, ax=ax, palette = "Set2")
plt.xticks(rotation = 90)
plt.show()
[('ENTY', 1250), ('HUM', 1223), ('ABBR', 1162),
     ('LOC', 896), ('NUM', 835), ('DESC', 86)]</pre>
			<p>While this <a id="_idIndexMarker1140"/>code prints out the count of texts in each class as text output, it is also very helpful to see the totals as a bar graph. We can use the graphics libraries to create <span class="No-Break">this graph:</span></p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B19005_14_01.jpg" alt="Figure 14.1 – Coarse-grained class counts in the TREC data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Coarse-grained class counts in the TREC data</p>
			<p>As <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em> shows, the <strong class="source-inline">DESC</strong> class is much smaller than the others, and it is possible that there will be accuracy problems with this class. There are ways to address this situation, which is <a id="_idIndexMarker1141"/>one of the main topics of this chapter, but for now, we won’t make any changes until we see that this actually causes <span class="No-Break">a problem.</span></p>
			<h3>Doing initial evaluations</h3>
			<p>Once we have done<a id="_idIndexMarker1142"/> this initial exploration, we will want to try training one or more initial models for the data and evaluate them using some of the techniques we learned in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
			<p>For this exploration, we will use the BERT-based training process that was covered in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, so we won’t duplicate that here. However, there are a few changes in the model that we need to make because we are now working with a <em class="italic">categorical</em> classification problem (six classes), rather than a binary classification problem (two classes), and it is worth pointing these out. We can see the new model definition in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(),
        dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(
        tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder,
        trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(6, activation =
        tf.keras.activations.softmax,
        name='classifier')(net)
    return tf.keras.Model(text_input, net)</pre>
			<p>The two changes that are needed in the model definition for the categorical task are in the final layer, which<a id="_idIndexMarker1143"/> has six outputs, corresponding to the six classes, and a softmax activation function, as opposed to the sigmoid activation function that we used for <span class="No-Break">binary problems.</span></p>
			<p>The other changes that are needed for categorical data are changes in the loss function and the metrics, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
loss="sparse_categorical_crossentropy"
metrics = tf.metrics.CategoricalAccuracy()</pre>
			<p>Here, we will define the categorical loss and metrics functions. Other metrics are available, but we will just look at <span class="No-Break">accuracy here.</span></p>
			<p>After training the model, as we did in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, we can look at the final scores. If the model does not meet the overall performance expectations for the application using the metrics that have been chosen, you can try different hyperparameter settings, or you can try other models. This was the process we followed in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, where we compared the performance of three different models on the movie <span class="No-Break">review data.</span></p>
			<p>Keeping in mind that<a id="_idIndexMarker1144"/> larger models are likely to have better performance, you can try increasing the size of the models. There is a limit to this strategy – at some point, the larger models will become very slow and unwieldy. You might also see that the payoff from larger and larger models becomes smaller, and performance levels off. This probably means that increasing the size of the models will not solve <span class="No-Break">the problem.</span></p>
			<p>There are many different possibilities to look at different hyperparameter settings. This is, in general, a huge search space that can’t be fully explored, but there are some heuristics that you can use to find settings that could improve your results. Looking at the training history charts of loss and accuracy changes over epochs should give you a good idea of whether additional training epochs are likely to be helpful. Different batch sizes, learning rates, optimizers, and dropout layers can also <span class="No-Break">be explored.</span></p>
			<p>Another strategy to diagnose system performance is to look at the <span class="No-Break">data itself.</span></p>
			<p>One initial evaluation we can do is a more fine-grained check for weak classes, by looking at the probabilities of the classifications for a large number of items in the dataset. We will look at this in the <span class="No-Break">next section.</span></p>
			<h3>Checking for weak classes</h3>
			<p>Low probabilities for a class of items<a id="_idIndexMarker1145"/> are a sign that a system is not able to classify items with high confidence and has a good chance of making errors. To check for this, we can use the model to predict the classification of a subset of our data and look at the average scores, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
import seaborn as sns
scores = [[],[],[],[],[],[]]
for text_batch, label_batch in train_ds.take(100):
    for i in range(160):
        text_to_classify = [text_batch.numpy()[i]]
        prediction = classifier_model.predict(
            text_to_classify)
        classification = np.max(prediction)
        max_index = np.argmax(prediction)
        scores[max_index].append(classification)
averages = []
for i in range(len(scores)):
    print(len(scores[i]))
    averages.append(np.average(scores[i]))
print(averages)</pre>
			<p>This code goes through <a id="_idIndexMarker1146"/>a subset of the TREC training data, predicts each item’s class, saves the predicted class in the <strong class="source-inline">classification</strong> variable, and then adds it to the <strong class="source-inline">scores</strong> list for the <span class="No-Break">predicted class.</span></p>
			<p>The final step in the code is to iterate through the scores list and print the length and average score for each class. The results are shown in <span class="No-Break"><em class="italic">Table 14.1</em></span><span class="No-Break">:</span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Class</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Number </strong><span class="No-Break"><strong class="bold">of items</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Average score</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ABBR</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">792</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.9070532</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">DESC</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">39</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.8191106</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">HUM</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">794</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.8899161</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ENTY</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">767</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.9638871</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">LOC</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">584</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.9767452</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NUM</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">544</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.9651737</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 14.1 – The number of items and the average score for each class</p>
			<p>We can see from <em class="italic">Table 14.1</em> that the number of items and average probabilities of the class predictions vary quite a bit. As you will recall from the counts we did in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em>, we were already<a id="_idIndexMarker1147"/> concerned about the <strong class="source-inline">DESC</strong> class because it was so small relative to the other classes. We can investigate this a bit further by looking at the predicted classifications of the individual items in each class with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
def make_histogram(score_data,class_name):
    sns.histplot(score_data,bins = 100)
    plt.xlabel("probability score")
    plt.title(class_name)
    plt.show()
for i in range(len(scores)):
    make_histogram(scores[i],class_names[i])</pre>
			<p>Let’s look at the histograms for the <strong class="source-inline">DESC</strong> and <strong class="source-inline">LOC</strong> classes, which are at the extreme ends of the set of average scores. The <strong class="source-inline">LOC</strong> class is shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B19005_14_2.jpg" alt="Figure 14.2 – The distribution of probability scores for the LOC class"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – The distribution of probability scores for the LOC class</p>
			<p>We can see in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.2</em> that not <a id="_idIndexMarker1148"/>only is the average probability very high (which we saw in <em class="italic">Table 14.1</em> as well) but there also are very few probabilities under <strong class="bold">0.9</strong> in the <strong class="source-inline">LOC</strong> class. This class is likely to be very accurate in the <span class="No-Break">deployed application.</span></p>
			<p>There is a second, less obvious advantage to classes that show the pattern in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.2</em>. In a deployed interactive application, we don’t want a system to give users answers that it’s not very confident of. This is because they’re more likely to be wrong, which would mislead the users. For that reason, developers should define a <em class="italic">threshold</em> probability score, which an answer has to exceed before the system provides that answer to <span class="No-Break">the user.</span></p>
			<p>If the probability is lower than the threshold, the system should respond to the user that it doesn’t know the answer. The<a id="_idIndexMarker1149"/> value of the threshold has to be set by the developer, based on the trade-off between the risk of giving users wrong answers and the risk of annoying users by saying <em class="italic">I don’t know</em> too frequently. <em class="italic">In </em><span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.2</em> we can see that if we set the threshold to <strong class="bold">0.9</strong>, the system will not have to say <em class="italic">I don’t know</em> very often, which will improve user satisfaction with <span class="No-Break">the system.</span></p>
			<p>Let’s contrast <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.2</em> with a histogram for the <strong class="source-inline">DESC</strong> class, which we can see in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B19005_14_03.jpg" alt="Figure 14.3 – The distribution of probability scores for the DESC class"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – The distribution of probability scores for the DESC class</p>
			<p><span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.3</em> shows many probability scores less than <strong class="bold">0.9</strong>, and if we set the threshold for <em class="italic">don’t know</em> at <strong class="bold">0.9</strong>, a <em class="italic">don’t know</em> answer will be very frequent. As you will recall from <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em>, this class was also much smaller than the other classes, which probably accounts for these low scores. Clearly, the <strong class="source-inline">DESC</strong> class will be problematic <span class="No-Break">in deployment.</span></p>
			<p>A confusion matrix, such <a id="_idIndexMarker1150"/>as the one we reviewed in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, can also help detect underperforming classes. We can generate a confusion matrix for the TREC data with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
y_pred = classifier_model.predict(x_test)
y_pred = np.where(y_pred &gt; .5, 1,0)
print(y_pred)
print(y_test)
predicted_classes = []
for i in range(len(y_pred)):
    max_index = np.argmax(y_pred[i])
    predicted_classes.append(max_index)
# View the results as a confusion matrix
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,f1_score,classification_report
conf_matrix = confusion_matrix(y_test,predicted_classes,
    normalize=None)</pre>
			<p>This code generates the predicted classes from the test data (represented in the <strong class="source-inline">predicted_classes</strong> variable) and compares them to the true classes (represented in the <strong class="source-inline">y-test</strong> variable). We can use the scikit-learn <strong class="source-inline">confusion_matrix</strong> function to display the confusion matrix <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
# Displaying the confusion matrix
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 12})
disp = ConfusionMatrixDisplay(confusion_matrix =
    conf_matrix, display_labels = class_names)
print(class_names)
disp.plot(xticks_rotation=75,cmap=plt.cm.Blues)
plt.show()</pre>
			<p>We can see the resulting<a id="_idIndexMarker1151"/> confusion matrix in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.4</em>. The confusion matrix tells us how often each class was predicted to be each other class, <span class="No-Break">including itself:</span></p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B19005_14_04.jpg" alt="Figure 14.4 – The confusion matrix for the TREC test set"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – The confusion matrix for the TREC test set</p>
			<p>The correct <a id="_idIndexMarker1152"/>predictions can be seen on the main diagonal. For example, <strong class="source-inline">ABBR</strong> was correctly predicted as <strong class="source-inline">ABBR</strong> <em class="italic">137</em> times. We can also see the prediction errors for each class. The most frequent error was incorrectly classifying <strong class="source-inline">ENTY</strong> as <strong class="source-inline">ABBR</strong> <em class="italic">11</em> times. In this particular example, we don’t see a lot of evidence that specific classes get confused with each other, although there is a tendency for <strong class="source-inline">ENTY</strong> to be confused <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">ABBR</strong></span><span class="No-Break">.</span></p>
			<p>Finally, we can look at the classification report to see the <strong class="source-inline">precision</strong>, <strong class="source-inline">recall</strong>, and <strong class="source-inline">F1</strong> scores for each class, as well as the overall averages for the entire test set. The recall scores in the classification report for <strong class="source-inline">DESC</strong> and <strong class="source-inline">ENTY</strong> are somewhat lower than the other recall scores, which <a id="_idIndexMarker1153"/>reflects the fact that some of the items in those classes are incorrectly recognized <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">ABBR</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
print(classification_report(y_test, predicted_classes, target_names = class_names))
['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']
              precision    recall  f1-score   support
        ABBR       0.90      0.99      0.94       138
        DESC       1.00      0.78      0.88         9
        ENTY       0.97      0.80      0.88        94
         HUM       0.97      0.97      0.97        65
         LOC       0.96      0.98      0.97       113
         NUM       0.94      0.96      0.95        81
    accuracy                           0.94       500
   macro avg       0.96      0.91      0.93       500
weighted avg       0.94      0.94      0.94       500</pre>
			<p>It’s worth pointing out at this point that the decision of whether the system is <em class="italic">good enough</em> really depends on the application and the developer’s decision. In some applications, it’s better to give the user some result, even if it might be wrong, while in other applications, it’s important for every result to be correct, even if the system has to say <em class="italic">I don’t know</em> almost all the time. Going back to the ideas of precision and recall that we covered in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, another way of putting this is to say that in some applications, recall is more important, and in other cases, precision is <span class="No-Break">more important.</span></p>
			<p>If we want to improve the performance of the TREC application, the next step is to decide how to <a id="_idIndexMarker1154"/>address our performance concerns and improve <span class="No-Break">overall accuracy.</span></p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor255"/>Fixing accuracy problems</h1>
			<p>In this section, we will look at fixing performance problems through two strategies. The first one involves issues that can be addressed by changing data, and the second strategy involves issues that require<a id="_idIndexMarker1155"/> restructuring the application. Generally, changing the data is easier, and it is a better strategy if it is important to keep the structure of the application the same – that is, we don’t want to remove classes or introduce new classes. We’ll start by discussing changing the data and then discuss restructuring <span class="No-Break">the application.</span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor256"/>Changing data</h2>
			<p>Changing data can greatly<a id="_idIndexMarker1156"/> improve the performance of your system; however, you won’t always have this option. For example, you might not have control over the dataset if you work with a standard dataset that you intend to compare to other researchers’ work. You can’t change the data if you are in that situation because if you do, your system’s performance won’t be comparable to that of other researchers. If <a id="_idIndexMarker1157"/>your system’s performance isn’t satisfactory but you can’t change the data, the only options are to improve the algorithms by using a different model or adjusting <span class="No-Break">the hyperparameters.</span></p>
			<p>On the other hand, if you work on an application where you do have control over a dataset, changing data can be a very effective way to improve <span class="No-Break">your system.</span></p>
			<p>Many performance issues are the result of not having enough data, either overall, or in specific classes. Other performance issues can be due to annotation errors . We’ll start with a brief discussion of <span class="No-Break">annotation errors.</span></p>
			<h3>Annotation errors</h3>
			<p>It is possible that the poor<a id="_idIndexMarker1158"/> performance of systems in supervised learning applications is due to annotation errors. Another way of putting this is to say that the supervision of data was wrong, and the system was trained to do the wrong thing. Perhaps an annotator accidentally assigned some data to the wrong class. If the data is training data, data in the wrong class will make the model less accurate, or if the data is test data, the item would be scored incorrectly because the model <span class="No-Break">was wrong.</span></p>
			<p>Checking for occasional annotation errors by reviewing the annotation of every item in the dataset can be very time-consuming, and it is not likely to improve the system much. This is because if the <a id="_idIndexMarker1159"/>dataset is large enough, this kind of sporadic error is unlikely to have much of an impact on the quality of the overall system. However, if you suspect that annotation errors are causing problems, a simple check for low-confidence items can be helpful without requiring every annotation to be checked. This can be done by using a variation of the code we used in the <em class="italic">Checking for weak classes</em> section to check for weak classes. In that code, we predicted the class of each item in the dataset, kept track of its probabilities (scores), and averaged the probabilities of all the items in the class. To modify the code to look instead for individual items with low probabilities, you could record each item and its probability individually, and then look for low-probability items in the final list. You are encouraged to try this exercise <span class="No-Break">for yourself.</span></p>
			<p>On the other hand, it is also possible that data contains not only occasional mistakes but also systematic annotation errors. Systematic errors might be due to differences in the annotators’ understanding of the meanings of the classes, leading to the similar items being assigned to different classes by different annotators. Ideally, these kinds of errors can be avoided, or at least reduced, by preparing clear annotation guidelines for annotators before the annotation process begins, or even by giving them <span class="No-Break">training classes.</span></p>
			<p>Tools such as the <em class="italic">kappa</em> statistic, which was mentioned in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, can measure divergent annotations among annotators. If the kappa statistic shows that there is a lot of divergence across annotators, some of the data might need to be re-annotated using clarified guidelines. It can also happen that it is impossible to get annotators to agree because the decisions that the annotators have to make are inherently too subjective for people to agree on, no matter how much guidance they are given. This is a sign that the problem is not really suitable for NLU in the first place because there might not be a real correct classification for <span class="No-Break">this data.</span></p>
			<p>However, assuming that we do have a problem with objective classifications, in addition to addressing annotation errors, we can also improve system performance by creating a more balanced dataset. To do this, we will first look at adding and removing existing data <span class="No-Break">from classes.</span></p>
			<h3>Adding and removing existing data from classes</h3>
			<p>Unbalanced amounts of data in different classes are a common situation that can lead to poor model performance. The main reason that a dataset can be unbalanced is that this imbalance represents the actual situation in the application domain. For example, an application that is supposed to<a id="_idIndexMarker1160"/> detect online hate speech will most likely encounter many more examples of non-hate speech than actual hate speech, but it is nevertheless important to find instances of hate speech, even if they are rare. Another example of a naturally unbalanced dataset would be a banking application where we find many more utterances about checking account balances than utterances about changing account addresses. Changing the address on an account just doesn’t happen very often compared to <span class="No-Break">checking balances.</span></p>
			<p>There are several ways to make the sizes of the classes <span class="No-Break">more even.</span></p>
			<p>Two common approaches are to duplicate data in the smaller classes or remove data from the larger classes. Adding data is called <strong class="bold">oversampling</strong> and removing data is called <strong class="bold">undersampling</strong>. The obvious approach to<a id="_idIndexMarker1161"/> oversampling is to randomly copy some of the data<a id="_idIndexMarker1162"/> instances and add them to the training data. Similarly, you can undersample by randomly removing instances from the classes that are too large. There are also other more sophisticated approaches to undersampling and oversampling, and you can find many online discussions about these topics – here, for example: <a href="https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data">https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data</a>. However, we will not review these here because they can become <span class="No-Break">quite complex.</span></p>
			<p>Undersampling and oversampling<a id="_idIndexMarker1163"/> can be helpful, but you should understand that they have to be used thoughtfully. For example, in the TREC dataset, trying to undersample the five frequent classes so that they have no more instances than the <strong class="source-inline">DESC</strong> class would require throwing out hundreds of instances from the larger classes, along with the information that they contain. Similarly, oversampling a small class such as <strong class="source-inline">DESC</strong> so that it contains the same number of instances as the larger classes means that there will be many duplicate instances of the <strong class="source-inline">DESC</strong> texts. This could result in overfitting the examples in <strong class="source-inline">DESC</strong> and consequently make it hard for the model to generalize to new <span class="No-Break">test data.</span></p>
			<p>It is easy to see that while undersampling and oversampling can potentially be useful, they are not automatic solutions. They are probably most helpful when classes are not extremely different in size and where there are plenty of examples, even in the smallest classes. You should also keep in mind that the classes don’t have to be exactly balanced for a system to <span class="No-Break">perform well.</span></p>
			<p>Another approach to adding data is to create new data, which we will discuss in the <span class="No-Break">next section.</span></p>
			<h3>Generating new data</h3>
			<p>If your dataset has <a id="_idIndexMarker1164"/>underrepresented classes, or is too small overall, you can also add generated data to the entire dataset or just to the smaller classes. We will look at the following three ways to <span class="No-Break">do this:</span></p>
			<ul>
				<li>Generating new data <span class="No-Break">from rules</span></li>
				<li>Generating new data <span class="No-Break">from LLMs</span></li>
				<li>Using crowdworkers to get <span class="No-Break">new data</span></li>
			</ul>
			<h4>Generating new data from rules</h4>
			<p>One way to create<a id="_idIndexMarker1165"/> new data is to write rules to generate new examples of data, based on the data that you already have. The <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>), which we <a id="_idIndexMarker1166"/>used in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, can be useful for this. As an example, let’s suppose you’re working on a local business <a id="_idIndexMarker1167"/>search chatbot, and find that you need more data in the <strong class="source-inline">restaurant search</strong> class. You could write a <strong class="bold">context-free grammar</strong> (<strong class="bold">CFG</strong>), which we covered in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, to generate more data with the following code that uses the NLTK <span class="No-Break"><strong class="source-inline">parse</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
from nltk.parse.generate import generate
from nltk import CFG
grammar = CFG.fromstring("""
S -&gt; SNP VP
SNP -&gt; Pro
VP -&gt; V NP PP
Pro -&gt; 'I'
NP -&gt; Det Adj N
Det -&gt; 'a'
N -&gt; 'restaurant' | 'place'
V -&gt; 'am looking for' | 'would like to find'
PP -&gt; P Adv
P -&gt; 'near'| 'around'
Adv -&gt; 'here'
Adj -&gt; 'Japanese' | 'Chinese' | 'Middle Eastern' | 'Mexican'
for sentence in generate(grammar,n = 10):
    print(" ".join(sentence))</pre>
			<p>Note that the rules in an NLTK CFG can be any context-free rules; they don’t have to correspond to actual linguistic categories. For example, we could have called the last Adj_Cuisine instead. We might want to do this if we want to be able to generate sentences with other adjectives, such as <strong class="source-inline">good</strong> or <strong class="source-inline">low-priced</strong>. The rule names and the rules <a id="_idIndexMarker1168"/>themselves don’t matter to the NLTK CFG package; the only thing that matters is that the CFG is written in the syntax that the NLTK CFG package expects. The names and the rules can be any rules that you find convenient to generate <span class="No-Break">new examples.</span></p>
			<p>The last two lines in the preceding code will generate 10 examples of sentences from this grammar, with the <span class="No-Break">following result:</span></p>
			<pre class="source-code">
I am looking for a Japanese restaurant near here
I am looking for a Japanese restaurant around here
I am looking for a Japanese place near here
I am looking for a Japanese place around here
I am looking for a Chinese restaurant near here
I am looking for a Chinese restaurant around here
I am looking for a Chinese place near here
I am looking for a Chinese place around here
I am looking for a Middle Eastern restaurant near here
I am looking for a Middle Eastern restaurant around here</pre>
			<p>If you want to generate all of the possible sentences from these rules, you will leave out the <span class="No-Break">parameter, </span><span class="No-Break"><strong class="source-inline">n=10</strong></span><span class="No-Break">.</span></p>
			<p>This is a fast way to generate a lot of sentences, but as you can see, the sentences are quite repetitious. This is because the NLTK <strong class="source-inline">generate</strong> method will produce every possible sentence that the grammar covers. Adding a lot of repetitious sentences to your training set could skew the model to these kinds of sentences, which in turn might make it harder for <a id="_idIndexMarker1169"/>the model to recognize more varied restaurant search sentences. One approach to getting a wider variety of sentences from an NLTK CFG would be to write a broader grammar, generate all the sentences it covers, and then randomly select a subset of the generated sentences to add to the <span class="No-Break">training set.</span></p>
			<p>Using <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) to generate<a id="_idIndexMarker1170"/> new examples is another useful and easy option, which we will discuss in the <span class="No-Break">following section.</span></p>
			<h4>Generating new data from LLMs</h4>
			<p>Online LLMs such as ChatGPT are <a id="_idIndexMarker1171"/>another very good way to get more training data because you can simply ask them to generate the appropriate training data. For example, let’s say ChatGPT was given the <span class="No-Break">following prompt:</span></p>
			<p><em class="italic">generate 20 requests to find local restaurants of different cuisines and </em><span class="No-Break"><em class="italic">price ranges</em></span></p>
			<p>ChatGPT (<a href="https://chat.openai.com/chat">chat.openai.com/chat</a>) would produce the <span class="No-Break">following answer:</span></p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B19005_14_05.jpg" alt="Figure 14.5 – The ChatGPT-generated restaurant query data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – The ChatGPT-generated restaurant query data</p>
			<p>(For brevity, not all the results <span class="No-Break">are shown.)</span></p>
			<p>You can see that these sentences are much less repetitious than the ones from NLTK’s <strong class="source-inline">generate</strong> method. In<a id="_idIndexMarker1172"/> the initial ChatGPT prompt, you can also restrict the question style – for example, you could ask for generated questions in an informal style. This results in informal sentences such as <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B19005_14_06.jpg" alt="Figure 14.6 – ChatGPT-generated restaurant query data in an informal style"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – ChatGPT-generated restaurant query data in an informal style</p>
			<p>You can also control the variation among the responses by changing the <em class="italic">temperature</em> parameter, which is available in the ChatGPT API. Temperature settings vary between zero and two. A temperature setting of zero means that the responses will be less varied, and a higher setting means that they will be <span class="No-Break">more varied.</span></p>
			<p>Within ChatGPT, a low temperature means that the generation model chooses the next word for a response among the higher probability words, and a high temperature means that the model will select the next word from among the words with lower probabilities. The result with a higher temperature setting will include more varied responses, but some <a id="_idIndexMarker1173"/>of them might not make sense. <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.7</em> shows how to set the temperature in code directly from <span class="No-Break">the API.</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B19005_14_07.jpg" alt="Figure 14.7 – Setting the GPT temperature using the OpenAI API"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7 – Setting the GPT temperature using the OpenAI API</p>
			<p>The code in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.7</em> sets the value of the <strong class="source-inline">temperature</strong> parameter to <strong class="source-inline">1.5</strong>, which results in a fairly diverse set of responses. You can see these at the bottom of <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.7</em>. The code also sets the <strong class="source-inline">model</strong> parameter to use the <strong class="source-inline">gpt-3.5-turbo</strong> model and sets the message to send in the <strong class="source-inline">messages</strong> parameter. If you are interested in experimenting with other GPT API calls, you can find other API parameters in the OpenAI API documentation <span class="No-Break">at </span><a href="https://platform.openai.com/docs/api-reference"><span class="No-Break">https://platform.openai.com/docs/api-reference</span></a><span class="No-Break">.</span></p>
			<p>Note that you will need to set the <strong class="source-inline">openai.api_key</strong> variable at line 3 to your own OpenAI user key to run this code, since the OpenAI API is a <span class="No-Break">paid service.</span></p>
			<p>If you use an LLM to generate data, be sure to check the results and decide whether the responses represent the kind of examples that your users would really say and, hence, should be included in your training data. For example, some of the informal requests in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.6</em> might be more informal than many users would say to <span class="No-Break">a chatbot.</span></p>
			<p>A final way to add new data<a id="_idIndexMarker1174"/> to underrepresented classes is to hire crowdworkers to create <span class="No-Break">more data.</span></p>
			<h4>Using crowdworkers to get new data</h4>
			<p>Getting more data from<a id="_idIndexMarker1175"/> crowdworkers is time-consuming and possibly expensive, depending on how much data you need and how complicated it is. Nevertheless, it would be an option if you didn't get enough data using <span class="No-Break">other methods.</span></p>
			<p>Of course, all of these methods we have outlined in this section (using rules, using LLMs, and using crowdworkers) can be combined – not all of the new training data has to come from the <span class="No-Break">same place.</span></p>
			<p>Another approach similar to changing the data is to change the application itself, which we will discuss in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor257"/>Restructuring an application</h2>
			<p>In some cases, the best solution to classes that are not being predicted well is to restructure an application. As in the <a id="_idIndexMarker1176"/>case of changing the data, you won’t always have the option to do this if this is a standard dataset that the research community uses to compare work among different labs, as the application has to have the same structure as that used by other researchers for the results to <span class="No-Break">be comparable.</span></p>
			<p>If you do have control <a id="_idIndexMarker1177"/>over the application structure, you can add, remove, or combine classes that don’t perform well. This can greatly improve the overall application performance. Let’s start by looking at an artificial example of an application that needs restructuring, and the different ways that this restructuring might <span class="No-Break">be done.</span></p>
			<h3>Visualizing the need for class restructuring</h3>
			<p>Visualizing datasets can <a id="_idIndexMarker1178"/>often provide immediate insight into potential performance problems. We can visualize how similar classes in a dataset are to each other in a couple <span class="No-Break">of ways.</span></p>
			<p>First, confusion matrices such as the one in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.4</em> are a good source of information about which classes are similar to each other and consequently get confused for each other. We saw immediately from <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.4</em> that <strong class="source-inline">ENTY</strong> and <strong class="source-inline">DESC</strong> were quite often confused with <strong class="source-inline">ABBR</strong>. We might want to add data to those classes, as discussed in the previous section, or we could also consider restructuring the application, which we will <span class="No-Break">discuss next.</span></p>
			<p>A second visualization<a id="_idIndexMarker1179"/> technique is to use the topic modeling techniques that we saw in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, to see problems with the <span class="No-Break">application structure.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.8</em> shows how an artificially <a id="_idIndexMarker1180"/>constructed dataset of four classes might look if we clustered them based on the <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a> tools, <strong class="bold">Sentence Bert</strong> and <strong class="bold">BERTopic</strong>. We can<a id="_idIndexMarker1181"/> immediately see that there are some problems with the classes in <span class="No-Break">this dataset.</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B19005_14_08.jpg" alt="Figure 14.﻿8 – Unsupervised clustering of four classes with artificially generated data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8 – Unsupervised clustering of four classes with artificially generated data</p>
			<p>First of all, the instances of <strong class="bold">Class 1</strong>, represented by circles, seem to cluster into two different classes, one centered around the point (<em class="italic">0.5, 0.5</em>) and the other centered around the point (0<em class="italic">.5</em>, <em class="italic">1.75</em>). It seems unlikely that these clusters should both be grouped into the same class if they are actually that different. <strong class="bold">Class 1</strong> should probably be split, and the instances currently assigned to Class 1 should be assigned to at least two, and possibly three, <span class="No-Break">new classes.</span></p>
			<p><strong class="bold">Class 2</strong>, represented by squares, and <strong class="bold">Class 3</strong>, represented by triangles, seem problematic. They are <a id="_idIndexMarker1182"/>not completely mixed together, but they are not completely separate either. Some of the instances of both classes are likely to be misclassified because of their similarity to the other class. If you see classes such as <strong class="bold">Class 3</strong> and <strong class="bold">Class 4</strong>, with this kind of overlap, consider merging the classes if they appear to be similar in meaning (if they aren’t similar in meaning, consider adding more data to one or both classes). Finally, <strong class="bold">Class 4</strong>, represented by stars, is compact and doesn’t overlap with any other classes. It shouldn’t require <span class="No-Break">any adjustments.</span></p>
			<p>Let’s now take a look at three restructuring options – merging classes, dividing classes, and introducing <span class="No-Break">new classes.</span></p>
			<h3>Merging classes</h3>
			<p>Classes that are much smaller than the rest of the classes can be merged with other semantically similar classes, especially if they are frequently confused with those classes. This can be a good strategy because, in many real applications, there are classes that simply don’t occur very often, but unlike the hate speech example mentioned earlier, it isn’t always critical to be <a id="_idIndexMarker1183"/>able to tell the difference between the original classes. Of course, this is only possible with a multi-class problem – that is, a problem with more than two classes – since merging the classes in a binary (two-class) problem will put everything in one class and leave us with nothing <span class="No-Break">to classify.</span></p>
			<p>In some cases, merging classes can be accomplished by adding all the data from one class to the data of the other class, which is the simplest restructuring that we can do. A slightly more complex merger of classes can be done if the new structure is more complicated – for example, if it involves adding slots. For example, it is possible that classes such as <strong class="bold">Class 2</strong> and <strong class="bold">Class 3</strong> in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.8</em> are actually not different enough to be worth trying <span class="No-Break">to separate.</span></p>
			<p>As an example, suppose we work on a generic personal assistant application, with classes such as <strong class="source-inline">play music</strong>, <strong class="source-inline">find a restaurant</strong>, <strong class="source-inline">get weather forecast</strong>, <strong class="source-inline">find a bookstore</strong>, and <strong class="source-inline">find a bank</strong>. It might turn out that <strong class="source-inline">find a restaurant</strong> has much more data than <strong class="source-inline">find a bookstore</strong>, and as a result, <strong class="source-inline">find a bookstore</strong> is often confused with <strong class="source-inline">find a restaurant</strong>. In that case, it would be worth considering whether all the <strong class="source-inline">find a</strong> classes should be merged into one larger class. This class could be called <strong class="source-inline">local business search</strong>, with <strong class="source-inline">bookstore</strong>, <strong class="source-inline">restaurant</strong>, and <strong class="source-inline">bank</strong> being treated as slots, as discussed in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></p>
			<p>Another strategy is to<a id="_idIndexMarker1184"/> separate classes such as <strong class="bold">Class 1</strong> in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.5</em> into two different classes. The next section discusses <span class="No-Break">dividing classes.</span></p>
			<h3>Dividing classes</h3>
			<p>Sometimes it makes sense to divide a large class into separate smaller classes if there appear to be systematic differences between two or more groups within the class. Tools such as BERTopic can help suggest names for new classes if the new name isn’t obvious from looking at the instances in<a id="_idIndexMarker1185"/> each group. Unfortunately, dividing a class into new classes isn’t as easy as merging classes because the examples in the new classes will need to be annotated with their new names. Although re-annotation is more work, dividing and re-annotating classes is necessary if you have to divide a large class into more meaningful <span class="No-Break">new classes.</span></p>
			<h3>Introducing an "other" class</h3>
			<p>Introducing an <strong class="source-inline">other</strong> class is a variant of the strategy of merging classes. If there are several small classes that don’t really have enough training data to be reliably classified, it can sometimes be useful to group them together in an <strong class="source-inline">other</strong> class – that is, a class that contains items that don’t fit into any of the other classes. One type of application that this approach can be useful for is call routing in a telephone <span class="No-Break">self-service application.</span></p>
			<p>In these applications, there can sometimes be hundreds of destinations where a call can be routed. In nearly every application of this kind, there are some infrequent classes for which there is much less data than other classes. Sometimes, it is best to not try to identify these classes because trying to do so accurately will be difficult with the small amounts of data available. A better strategy would be to group them together into an <strong class="source-inline">other</strong> class. It still might be hard to identify items in the <strong class="source-inline">other</strong> category because the items it contains will not be very similar, but it will keep them from interfering with the overall application accuracy. How items in the <strong class="source-inline">other</strong> class are handled depends on the specific application’s goals, but <a id="_idIndexMarker1186"/>options include handling them manually (for example, with a human call center agent) or simply telling users that the system can’t handle <span class="No-Break">their question.</span></p>
			<p>After the accuracy issues that were identified during initial development have been identified and addressed, it is time to deploy <span class="No-Break">the system.</span></p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor258"/>Moving on to deployment</h1>
			<p>If we’ve fixed the performance issues we’ve discussed so far, we will have trained a model that meets our performance <a id="_idIndexMarker1187"/>expectations, and we can move on to deployment, when the system is installed and does the task that it was designed for. Like any software, a deployed NLU model can have problems with system and hardware issues, such as network issues, scalability, and general software problems. We won’t discuss these kinds of problems because they aren’t specific <span class="No-Break">to NLU.</span></p>
			<p>The next section will cover considerations to address NLU performance problems that occur <span class="No-Break">after deployment.</span></p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor259"/>Problems after deployment</h1>
			<p>After an NLU system is<a id="_idIndexMarker1188"/> developed and put into place in an application, it still requires monitoring. Once the system has reached an acceptable level of performance and has been deployed, it can be tempting to leave it alone and assume that it doesn’t need any more attention, but this is not the case. At the very least, the deployed system will receive a continuous stream of new data that can be challenging to the existing system if it is different from the training data in some way. On the other hand, if it is not different, it can be used as new training data. Clearly, it is better to detect performance problems from internal testing than to learn about them from negative <span class="No-Break">customer feedback.</span></p>
			<p>At a high level, we can think of new performance problems as either being due to a change in the system itself, or due to a change in the <span class="No-Break">deployment context.</span></p>
			<p>Changes in system performance due to system changes should be detected by testing before the new system is deployed. This kind of testing is very similar to the kind of testing that has to be done for any software deployment, so we won’t cover it in any detail. Degradation in performance can be detected by versioning the system and running an evaluation with a fixed set of data and metrics after every change. This is useful to both detect decreases in performance but also to document improvements <span class="No-Break">in performance.</span></p>
			<p>As with any machine-learning-based system, new data can cause problems with an NLU system because it is different in some significant way from the training data. These kinds of differences are frequently due to changes in the <span class="No-Break">deployment context.</span></p>
			<p>What do we mean by changes in the <strong class="bold">deployment context</strong>? The deployment context refers to everything about<a id="_idIndexMarker1189"/> the application, except for the NLU system itself. Specifically, it can include the users, their demographics, their geographical locations, the backend information that’s being provided, and even events in the world such as weather. Any of these can change the characteristics of texts that the application processes. These changes alter the correspondence between the training data and the new data being processed, which will lead to a decrease <span class="No-Break">in performance.</span></p>
			<p>Some changes in the deployment context can be predicted. For example, if a company introduces a new product, this will introduce new vocabulary that a customer support chatbot, voice assistant, or email router needs to recognize, since, after all, we expect customers to be<a id="_idIndexMarker1190"/> talking about it. It is a best practice to perform an evaluation on new data after changes like the introduction of a new product occurs, and decide whether the system should be retrained with <span class="No-Break">additional data.</span></p>
			<p>On the other hand, some changes can’t be predicted – for example, the COVID-19 pandemic introduced a lot of new vocabulary and concepts that medical or public health NLU applications needed to be trained on. Because some deployment context changes can’t be predicted, it is a good idea to periodically perform an evaluation using new data co<a id="_idTextAnchor260"/>ming in from <span class="No-Break">the deployment.</span></p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor261"/>Summary</h1>
			<p>In this chapter, you have learned about a number of important strategies to improve the performance of NLU applications. You first learned how to do an initial survey of the data and identify possible problems with the training data. Then, you learned how to find and diagnose problems with accuracy. We then described different strategies to improve performance – specifically, adding data and restructuring the application. The final topic we covered was a review of problems that can occur in deployed applications and how they can <span class="No-Break">be addressed.</span></p>
			<p>In the final chapter, we will provide an overview of the book and a look to the future. We will discuss where there is potential for improvement in the state of the art of NLU performance, as well as faster training, more challenging applications, and what we can expect from NLU technology as the new LLMs become more <span class="No-Break">widely used.</span></p>
		</div>
	</body></html>