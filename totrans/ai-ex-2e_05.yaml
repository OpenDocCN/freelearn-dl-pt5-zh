- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to Use Decision Trees to Enhance K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter addresses two critical issues. First, we will explore how to implement
    k-means clustering with dataset volumes that exceed the capacity of the given
    algorithm. Second, we will implement decision trees that verify the results of
    an ML algorithm that surpasses human analytic capacity. We will also explore the
    use of random forests.
  prefs: []
  type: TYPE_NORMAL
- en: When facing such difficult problems, choosing the right model for the task often
    proves to be the most difficult task in ML. When we are given an unfamiliar set
    of features to represent, it can be a somewhat puzzling prospect. Then we have
    to get our hands dirty and try different models. An efficient estimator requires
    good datasets, which might change the course of the project.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter builds on the k-means clustering (or KMC) program developed in
    *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*. The issue of
    large datasets is addressed. This exploration will lead us into the world of the
    law of large numbers (LLN), the central limit theorem (CLT), the Monte Carlo estimator,
    decision trees, and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Human intervention in a process such as the one described in this chapter is
    not only unnecessary, but impossible. Not only does machine intelligence surpass
    humans in many cases, but the complexity of a given problem itself often surpasses
    human ability, due to the complex and ever-changing nature of real-life systems.
    Thanks to machine intelligence, humans can deal with increasing amounts of data
    that would otherwise be impossible to manage.
  prefs: []
  type: TYPE_NORMAL
- en: With our toolkit, we will build a solution to analyze the results of an algorithm
    without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning with KMC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining if AI must or must not be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data volume issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the NP-hard characteristic of KMC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random sampling concerning LLN, CLT, and the Monte Carlo estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling a training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning with decision trees and random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining KMC to decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter begins with unsupervised learning with KMC. We will explore methods
    to avoid running large datasets through random sampling. The output of the KMC
    algorithm will provide the labels for the supervised decision tree algorithm.
    The decision tree will verify the results of the KMC process, a task no human
    could do with large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: All the Python programs and files in this chapter are available at [https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH05](https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH0).
  prefs: []
  type: TYPE_NORMAL
- en: There is also a Jupyter notebook named `COLAB_CH05.ipynb` that contains all
    of the Python programs in one run. You can upload it directly to Google Colaboratory
    ([https://colab.research.google.com/](https://colab.research.google.com/)) using
    your Google Account.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning with KMC with large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KMC takes unlabeled data and forms clusters of data points. The names (integers)
    of these clusters provide a basis to then run a supervised learning algorithm
    such as a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how to use KMC with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: When facing a project with large unlabeled datasets, the first step consists
    of evaluating if machine learning will be feasible or not. Trying to avoid AI
    in a book on AI may seem paradoxical. However, in AI, as in real life, you should
    use the right tools at the right time. If AI is not necessary to solve a problem,
    do not use it.
  prefs: []
  type: TYPE_NORMAL
- en: Use a **proof of concept** (**POC**) approach to see if a given AI project is
    possible or not. A POC costs much less than the project itself and helps to build
    a team that believes in the outcome. Or, the POC might show that it is too risky
    to go forward with an ML solution. Intractable problems exist. It's best to avoid
    spending months on something that will not work.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is exploring the data volume and the ML estimator model that
    will be used.
  prefs: []
  type: TYPE_NORMAL
- en: If the POC proves that a particular ML algorithm will solve the problem at hand,
    the next thing to do is to address data volume issues. The POC shows that the
    model works on a sample dataset. Now, the implementation process can begin.
  prefs: []
  type: TYPE_NORMAL
- en: Anybody who has run a machine learning algorithm with a large dataset on a laptop
    knows that it takes some time for a machine learning program to train and test
    these samples. A machine learning program or a deep learning convolutional neural
    network consumes a large amount of machine power. Even if you run an ANN using
    a **GPU** (short for **graphics processing unit**) hoping to get better performance
    than with CPUs, it still takes a lot of time for the training process to run through
    all the learning epochs. An epoch means that we have tried one set of weights,
    for example, to measure the accuracy of the result. If the accuracy is not sufficient,
    we run another epoch, trying other weights until the accuracy is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: If you go on and you want to train your program on datasets exceeding 1,000,000
    data points, for example, you will consume significant local machine power.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you need to use a KMC algorithm in a corporation with hundreds of millions
    to billions of records of data coming from multiple SQL Server instances, Oracle
    databases, and a big data source. For example, suppose that you are working for
    the phone operating activity of a leading phone company. You must apply a KMC
    program to the duration of phone calls in locations around the world over a year.
    That represents millions of records per day, adding up to billions of records
    in a year.
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning KMC training program running billions of records will consume
    too much CPU/GPU and take too much time even if it works. On top of that, a billion
    records might only represent an insufficient amount of features. Adding more features
    will dramatically increase the size of such a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The question now is to find out if KMC will even work with a dataset this size.
    A KMC problem is **NP-hard**. The **P** stands for **polynomial** and the **N**
    for **non-deterministic**.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to our volume problem requires some theoretical considerations.
    We need to identify the difficulty of the problems we are facing.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the difficulty of the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first need to understand what level of difficulty we are dealing with. One
    of the concepts that come in handy is NP-hard.
  prefs: []
  type: TYPE_NORMAL
- en: NP-hard – the meaning of P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The P in NP-hard means that the time to solve or verify the solution of a P
    problem is polynomial (poly=many, nomial=terms). For example, *x*³ is a polynomial.
    The N means that the problem is non-deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once *x* is known, then *x*³ will be computed. For *x* = 3,000,000,000 and
    only 3 elementary calculations, this adds up to:'
  prefs: []
  type: TYPE_NORMAL
- en: log *x*³ = 28.43
  prefs: []
  type: TYPE_NORMAL
- en: It will take 10^(28.43) calculations to compute this particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems scary, but it isn''t that scary for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: In the world of big data, the number can be subject to large-scale randomized
    sampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMC can be trained in mini-batches (subsets of the dataset) to speed up computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial time means that this time will be more or less proportional to the
    size of the input. Even if the time it takes to train the KMC algorithm remains
    a bit fuzzy, as long as the time it will take to verify the solution remains proportional
    thanks to the batch size of the input, the problem remains a polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: An exponential algorithm increases with the amount of data, not the number of
    calculations. An exponential function of this example would be *f*(*x*) = 3^x
    = 3^(3,000,000,000) calculations. Such functions can often be broken down into
    multiple classical algorithms. Functions of this type exist in the corporate world,
    but they are out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: NP-hard – the meaning of non-deterministic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Non-deterministic problems require a heuristic approach, which implies some
    form of heuristics, such as a trial and error approach. We try a set of weights,
    for example, evaluate the result, and then go on until we find a satisfactory
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of hard
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NP-hard can be transposed into an NP problem with some optimization. This means
    that NP-hard is as hard or harder than an NP problem.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can use batches to control the size of the input, the calculation
    time, and the size of the outputs. That way, we can bring an NP-hard problem down
    to an NP problem.
  prefs: []
  type: TYPE_NORMAL
- en: One way of creating batches to avoid running an algorithm on a dataset that
    will prove too large for it is to use random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing random sampling with mini-batches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A large portion of machine learning and deep learning contains random sampling
    in various forms. In this case, a training set of billions of elements will prove
    difficult, if not impossible, to implement without random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random sampling is used in many methods: Monte Carlo, stochastic gradient descent,
    random forests, and many algorithms. No matter what name the sampling takes, they
    share common concepts to various degrees, depending on the size of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling on large datasets can produce good results, but it requires
    relying on the LLN, which we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the LLN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In probability, the LLN states that when dealing with very large volumes of
    data, significant samples can be effective enough to represent the whole dataset.
    For example, we are all familiar with polls that use small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This principle, like all principles, has its merits and limits. But whatever
    the limitations, this law applies to everyday machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, sampling resembles polling. A smaller number of individuals
    represent a larger overall dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sampling mini-batches and averaging them can prove as efficient as calculating
    the whole dataset as long as a scientific method is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: Training with mini-batches or subsets of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an estimator in one form or another to measure the progression of the training
    session until a goal has been reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may be surprised to read "until a goal has been reached" and not "until
    the optimal solution has been reached."
  prefs: []
  type: TYPE_NORMAL
- en: The optimal solution may not represent the best solution. All the features and
    all the parameters are often not expressed. Finding a good solution will often
    be enough to classify or predict efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The LLN explains why random functions are widely used in machine learning and
    deep learning. Random samples provide efficient results if they respect the CLT.
  prefs: []
  type: TYPE_NORMAL
- en: The CLT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LLN applied to the example of the KMC project must provide a reasonable
    set of centroids using random sampling. If the centroids are correct, then the
    random sample is reliable.
  prefs: []
  type: TYPE_NORMAL
- en: A centroid is the geometrical center of a set of datasets, as explained in *Chapter
    4*, *Optimizing Your Solutions with K-Means Clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach can now be extended to the CLT, which states that when training
    a large dataset, a subset of mini-batch samples can be sufficient. The following
    two conditions define the main properties of the CLT:'
  prefs: []
  type: TYPE_NORMAL
- en: The variance between the data points of the subset (mini-batch) remains reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The normal distribution pattern with mini-batch variances remains close to the
    variance of the whole dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Monte Carlo estimator, for example, can provide a good basis to see if the
    samples respect the CLT.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Monte Carlo estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The name Monte Carlo comes from the casinos in Monte Carlo and gambling. Gambling
    represents an excellent memoryless random example. No matter what happens before
    a gambler plays, prior knowledge provides no insight. For example, the gambler
    plays 10 games, losing some and winning some, creating a distribution of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the distribution of *f*(*x*) is computed. Then random samples are
    extracted from a dataset, for example, *x*[1], *x*[2], *x*[3],..., *x*[n].
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) can be estimated through the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The estimator ![](img/B15438_05_002.png) represents the average of the result
    of the predictions of a KMC algorithm or any implemented model.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that a sample of a dataset can represent a full dataset, just as
    a group of people can represent a population when polling for elections, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that we can safely use random samples, just like in polling a population
    for elections, we can now process a full large dataset directly, or preferably
    with random samples.
  prefs: []
  type: TYPE_NORMAL
- en: Trying to train the full training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*, a KMC
    algorithm with a six-cluster configuration produced six centroids (geometric centers),
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/KMeansClusters.jpg](img/B15438_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Six centroids'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is now how to avoid costly machine resources to train this KMC dataset
    when dealing with large datasets. The solution is to take random samples from
    the dataset in the same way polling is done on a population for elections, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Training a random sample of the training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sampling/k-means_clustering_minibatch.py` program provides a way to verify
    the mini-batch solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program begins by loading the data in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading the dataset might create two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The dataset is too large to be loaded in the first place.** In this case,
    load the datasets batch by batch. Using this method, you can test the model on
    many batches to fine-tune your solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The dataset can be loaded, but the KMC algorithm chosen cannot absorb the
    volume of data.** A good choice for the size of the mini-batch will solve this
    problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a dataset has been loaded, the program will start the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mini-batch dataset called `dataset1` will be randomly created using Monte
    Carlo''s large data volume principle with a mini-batch size of 1,000\. Many variations
    of the Monte Carlo method apply to machine learning. For our example, it will
    be enough to use a random function to create the mini-batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the KMC algorithm runs on a standard basis, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot, displaying the result produced, resembles the full
    dataset trained by KMC in *Chapter 4*, *Optimizing Your Solutions with K-Means
    Clustering*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/MiniBatchOutput.jpg](img/B15438_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Output (KMC)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The centroids obtained are consistent, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output will vary slightly at each run since this is a stochastic process.
    In this section, we broke the dataset down into random samples to optimize the
    training process. Another way to perform random sampling is to shuffle the dataset
    before training it.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling as another way to perform random sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sampling/k-means_clustering_minibatch_shuffling.py` program provides another
    way to solve a random sampling approach.
  prefs: []
  type: TYPE_NORMAL
- en: KMC is an unsupervised training algorithm. As such, it trains *unlabeled* data.
    A single random computation does not consume a large amount of machine resources,
    but several random selections in a row can.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shuffling can reduce machine consumption costs. Proper shuffling of the data
    before starting training, just like shuffling cards before a poker game, will
    avoid repetitive and random mini-batch computations. In this model, the loading
    data phase and training phase do not change. However, instead of one or several
    random choices for `dataset1`, the mini-batch dataset, we shuffle the complete
    dataset once before starting the training. The following code shows how to shuffle
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we select the first 1,000 shuffled records for training, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result in the following screenshot corresponds to the one with the full
    dataset and the random mini-batch dataset sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/Shuffled_mini_batch_training.jpg](img/B15438_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Full and random mini-batch dataset sample'
  prefs: []
  type: TYPE_NORMAL
- en: The centroids produced can provide first-level results to confirm the model,
    as shown in the following output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The geometric centers or centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Using shuffling instead of random mini-batches has two advantages: limiting
    the number of mini-batch calculations and preventing training the same samples
    twice. If your shuffling algorithm works, you will only need to shuffle the dataset
    once. If not, you might have to go back and use random sampling, as explained
    in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling and shuffling helped to solve one part of the dataset volume
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, now we must explore the other aspect of the implementation of a large
    dataset ML algorithm: verifying the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Chaining supervised learning to verify unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section explores how to verify the output of an unsupervised KMC algorithm
    with a supervised algorithm: a decision tree.'
  prefs: []
  type: TYPE_NORMAL
- en: KMC takes an input with no labels and produces an output with labels. The unsupervised
    process makes sense out of the chaos of incoming data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example in this chapter focuses on two related features: location and distance.
    The clusters produced are location-distance subsets of data within the dataset.
    The input file contains two columns: distance and location. The output file contains
    three columns: distance, location, and a label (cluster number).'
  prefs: []
  type: TYPE_NORMAL
- en: The output file can thus be chained to a supervised learning algorithm, such
    as a decision tree. The decision tree will use the labeled data to produce a visual,
    white-box, machine thought process. Also, a decision tree can be trained to verify
    the results of the KMC algorithm. The process starts with preprocessing raw data.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing raw data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, it was shown that with large datasets, mini-batches will be necessary.
    Loading billions of records of data in memory is not an option. A random selection
    was applied in `sampling/k-means_clustering_minibatch.py` as part of the KMC algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since we are chaining our algorithms in a pipeline and since we are
    not training the model, we could take the random sampling function from `sampling/k-means_clustering_minibatch.py`
    and isolate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The code could be applied to datasets extracted in a preprocessing phase from
    packs of data retrieved from a big data environment, for example. The preprocessing
    phase would be repeated in cycles. We will now explore the pipeline that goes
    from raw data to the output of the chained ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline of scripts and ML algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ML **pipeline** will take raw data and perform dimension reduction or other
    preprocessing tasks that are not in the scope of this book. Preprocessing the
    data sometimes requires more than ML algorithms such as SQL scripts. Our exploration
    starts right after when ML algorithms such as KMC take over. However, a pipeline
    can run from raw data to ML using classical non-AI scripting as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline described in the following sections can be broken down into three
    major steps, preceded by classical preprocessing scripting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 0**: A standard process performs a random sampling of a **training dataset**
    with classical preprocessing scripts before running the KMC program. This aspect
    is out of the scope of the ML process and this book. By doing this, we avoid overloading
    the ML Python programs. The training data will first be processed by a KMC algorithm
    and sent to the decision tree program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 1**: A KMC multi-ML program, `kmc2dt_chaining.py`, reads the training
    dataset produced by step 0 using a **saved model** from *Chapter 4*, *Optimizing
    Your Solutions with K-Means Clustering*. The KMC program takes the unlabeled data,
    makes predictions, and produces a labeled output in a file called `ckmc.csv`.
    The output label is the cluster number of a line of the dataset containing a distance
    and location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: The decision tree program, `decision_tree.py`, reads the output
    of the KMC predictions, `ckmc.csv`. The decision tree algorithm trains its model
    and saves the trained model in a file called `dt.sav`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2.1**: The training phase is over. The pipeline now takes raw data retrieved
    by successive equal-sized datasets. This **batch** process will provide a fixed
    amount of data. Calculation time can be planned and mastered. This step is out
    of the scope of the ML process and this book.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2.2**: A random sampling script processes the batch and produces a prediction
    dataset for the **predictions**.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: `kmc2dt_chaining.py` will now run a KMC algorithm that is chained
    to a decision tree that will verify the results of the KMC. The KMC algorithm
    produces predictions. The decision tree makes predictions on those predictions.
    The decision tree will also provide a visual graph in a PNG for users and administrators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps 2.1 to 3 can run on a twenty-four seven basis in a continuous process.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that random forests are an interesting alternative to
    the decision tree component. It can replace the decision tree algorithm in `kmc2dt_chaining.py`.
    In the next section, we will explore random forests in this context with `random_forest.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – training and exporting data from an unsupervised ML algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kmc2dt_chaining.py` can be considered as the **chaining** of a KMC program
    to a decision tree program that will verify the results. Each program forms a link
    of the chain.'
  prefs: []
  type: TYPE_NORMAL
- en: From a decision tree project's perspective, `kmc2dt_chaining.py` can be considered
    as a **pipeline** taking unlabeled data and labeling it for the supervised decision
    tree program. A pipeline takes raw data and transforms it using more than one
    ML program.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase of the chained model, `kmc2dt_chaining.py` runs to
    provide datasets for the training of the decision tree. The parameter `adt=0`
    limits the process to the KMC function, which is the first link of the chain.
    The decision tree in this program will thus not be activated in this phase.
  prefs: []
  type: TYPE_NORMAL
- en: '`kmc2dt_chaining.py` will load the dataset, load the saved KMC mode, make predictions,
    and export the labeled result:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the dataset**: `data.csv`, the dataset file, is the same one used in
    *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*. The two features,
    `location` and `distance`, are loaded:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Load the KMC model**: The k-means cluster model, `kmc_model.sav`, was saved
    by `k-means_clustering_2.py` in *Chapter 4*, *Optimizing Your Solutions with K-Means
    Clustering*. It is now loaded using the `pickle` module to save it:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Make predictions**: No further training of the KMC model is required at this
    point. The model can run predictions on the mini-batches it receives. We can use
    an **incremental** process to verify the results on a large scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is not sufficiently scaled, other algorithms could be applied. In
    this case, the dataset does not require additional scaling. The KMC algorithm
    will make predictions on the sample and produce an output file for the decision
    tree.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For this example, the predictions will be generated line by line:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are stored in a NumPy array:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Export the labeled data**: The predictions are saved in a file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This output file is special; it is now labeled:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output file contains three columns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Column 1 = feature 1 = location; `80`, for example, on the first line
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Column 2 = feature 2 = distance; `53`, for example, on the first line
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Column 3 = label = cluster calculated; `5`, for example, on the first line
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In our chained ML algorithms, this output data will become the input data of
    the next ML algorithm, the decision tree.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 2 – training a decision tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In *Chapter 3*, *Machine Intelligence – Evaluation Functions and Numerical
    Convergence*, a decision tree was described and used to visualize a priority process.
    `Decision_Tree_Priority.py` produced the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant signe, texte  Description générée automatiquement](img/B15438_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Decision tree priorities'
  prefs: []
  type: TYPE_NORMAL
- en: The tree starts with a node with a high Gini value. The node is split into two,
    and each node below is a "leaf" in this case because Gini=0.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree algorithm implemented in this book uses Gini impurity.
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity represents the probability of a data point being incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree will start with the highest impurities. It will split the probability
    into two branches after having calculated a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: When a branch reaches a Gini impurity of 0, it reaches its **leaf**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's state that *k* is the probability of a data point being incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset *X* from *Chapter 3*, *Machine Intelligence – Evaluation Functions
    and Numerical Convergence*, contains six data points. Four data points are low,
    and two data points are high:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X* = {Low, Low, High, High, Low, Low}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation of Gini impurity calculates the probability of each feature occurring
    and multiplies the result by 1—the probability of each feature occurring on the
    remaining values—as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applied to the *X* dataset with four lows out of six and two highs out of six,
    the result will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*(*k*) = (4/6) * (1 – 4/6) + (2/6) * (1 – 2/6)'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*(*k*)=(0.66 * 0.33) + (0.33 * 0.66)'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*(*k*)=0.222 + 0.222=0.444'
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a data point will be incorrectly predicted is 0.444, as
    shown in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The decision train is built on **the gain of information** on the features that
    contain the highest Gini impurity value.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore the Python implementation of a decision tree to prepare
    it to be chained to the KMC program.
  prefs: []
  type: TYPE_NORMAL
- en: Training the decision tree
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train the decision tree, `decision_tree.py` will load the dataset, train
    the model, make predictions, and save the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the dataset**: Before loading the dataset, you will need to import the
    following modules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The versions of the modules will vary as the editors produce them and also depend
    on how often you update the versions and the code. For example, you might get
    a warning when you try to unpickle a KMC model from version 0.20.3 when using
    version 0.21.2\. As long as it works, it is fine for educational purposes. However,
    in production, an administrator should have a database with the list of packages
    used and their versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is loaded, labeled, and split into training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The data in this example will contain two features (location and distance)
    and a label (cluster number) provided by the output of the KMC algorithm. The
    following header shows how the dataset is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Training the model**: Once the datasets are ready, the decision tree classifier
    is created and the model is trained:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Making predictions**: Once the model is trained, predictions are made on
    the test dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The predictions use the features to predict a cluster number in the test dataset,
    as shown in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Measuring the results with metrics**: A key part of the process is to measure
    the results with metrics. If the accuracy approaches 1, then the KMC output chained
    to the decision tree algorithm is reliable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, the accuracy was 0.97\. The model can predict the cluster of
    a distance and location with a 0.97 probability, which proves its efficiency.
    The training of the chained ML solution is over, and a prediction cycle begins.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can generate a PNG file of the decision tree with `decision_tree.py`. Uncomment
    the last paragraph of the program, which contains the export function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that once you have implemented this function, you can activate or deactivate
    it with the `graph` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The following image produced for this example can help you understand the thought
    process of the whole chained solution (KMC and the decision tree).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Image output from the code example'
  prefs: []
  type: TYPE_NORMAL
- en: The image file, `dt_kmc.png`, is available on GitHub in `CH05`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – a continuous cycle of KMC chained to a decision tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training of the chained KMC algorithm chained to a decision tree algorithm
    is over.
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing phase using classical big data batch retrieval methods will
    continuously provide randomly sampled datasets with a script.
  prefs: []
  type: TYPE_NORMAL
- en: '`kmc2dt_chaining.py` can focus on running KMC predictions and passing them
    on to decision tree predictions for white-box checking. The continuous process
    imports a dataset, loads saved models, predicts, and measures the results at the
    decision tree level.'
  prefs: []
  type: TYPE_NORMAL
- en: The chained process can run on a twenty-four seven basis if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modules used are required for both the KMC and the decision trees implemented
    in the training programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading the KMC dataset and model**: The KMC dataset has been prepared in
    the preprocessing phase. The trained model has been previously saved. The dataset
    is loaded with pandas and the model is loaded with `pickle`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Predicting and saving**: The goal is to predict the batch dataset line by
    line and save the result in an array in a white-box approach that can be verified
    by the administrator of the system:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `ckmc.csv` file generated is the entry point of the next link of the chain:
    the decision tree.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The two lines containing the `print` instruction are commented for standard
    runs. However, you may wish to explore the outputs in detail if your code requires
    maintenance. That is why I recommend adding maintenance lines in the code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Loading the dataset for the decision tree**: The dataset for the decision
    tree is loaded in the same way as in `decision_tree.py`. A parameter activates
    the decision tree part of the code: `adt=1`. The white-box quality control approach
    can thus be activated or deactivated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The program loads the dataset, loads the model, and splits the data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although the dataset has been split, only the test data is used to verify the
    predictions of the decision tree and the quality of the KMC outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Predicting and measuring the results**: The decision tree predictions are
    made and the accuracy of the results is measured:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once again, in this example, as in `decision_tree.py`, the accuracy is 0.97.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Double-checking the accuracy of the predictions**: In the early days of a
    project or for maintenance purposes, double-checking is recommended. The function
    is activated or deactivated with a parameter named `doublecheck`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The prediction results are checked line by line against the original labels
    and measured:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program will print out the predictions line by line, stating if they are
    `True` or `False`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, the accuracy is 0.99, which is high. Only 9 out of 1,000 predictions
    were `False`. This result is not the same as the metrics function because it is
    a simple calculation that does not take mean errors or other factors into account.
    However, it shows that the KMC produced good results for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees provide a good approach for the KMC. However, random forests
    take machine learning to another level and provide an interesting alternative,
    if necessary, to the use of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests as an alternative to decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random forests open mind-blowing ML horizons. They are ensemble meta-algorithms.
    As an ensemble, they contain several decision trees. As meta-algorithms, they
    go beyond having one decision tree making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run an ensemble (several decision trees) as a meta-algorithm (several algorithms
    training and predicting the same data), the following module is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand how a random forest works as a meta-algorithm, let''s focus on
    three key parameters in the following classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`n_estimators=25`: This parameter states the number of **trees** in the **forest**.
    Each tree will run its prediction. The main method to reach the final prediction
    is obtained by averaging the predictions of each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each split of each tree, features are randomly permuted. The trees use different
    feature approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bootstrap=True`: When bootstrap is activated, a smaller sample is bootstrapped
    from the sample provided. Each tree thus bootstraps its own samples, adding more
    variety.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state=None`: When `random_state=None` is activated, the random function
    uses `np.random`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also use the other methods by consulting the scikit-learn documentation
    (see *Further reading* at the end of the chapter). My preference is to use `np.random`.
    Note that to split the training state, I use scikit-learn's default random generator
    example with `random_state=0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This shows the importance of these small changes in parameters. After many tests,
    this is what I preferred. But maybe, in other cases, other `random_state` values
    are preferable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Beyond these key concepts and parameters, `random_forest.py` can be built in
    a clear, straightforward way.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_forest.py` is built with the same structure as the KMC or decision
    tree program. It loads a dataset, prepares the features and target variables,
    splits the dataset into training and testing sub-datasets, predicts, and measures.
    `random_forest.py` also contains a custom double-check function that will display
    each prediction, its status (`True` or `False`), and provide an independent accuracy rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading the dataset**: `ckmc.csv` was generated by the preceding KMC program.
    This time, it will be read by `random_forest.py` instead of `decision_tree.py`.
    The dataset is loaded, and the features and target variables are identified. Note
    the `pp` variable, which will trigger the `print` function or not. This is useful
    for switching from production mode to maintenance mode with a single variable
    change. Change `pp=0` to `pp=1` if you wish to switch to maintenance mode. In
    this case `pp` is activated:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program prints the labeled data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program prints the target cluster numbers to predict:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Splitting the data, creating the classifier, and training the model**: The
    dataset is split into training and testing data. The random forest classifier
    is created with 25 estimators. Then the model is trained:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The program is ready to predict.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Predicting and measuring the accuracy of the trained model**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output results and metrics are satisfactory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A mean error approaching 0 is an efficient result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Double-checking**: A double-checking function is recommended in the early
    stages of an ML project and for maintenance purposes. The function is activated
    by the `doublecheck` parameter as in `kmc2dt_chaining.py`. Set `doublecheck=1`
    if you wish to activate the maintenance mode. It is in fact, the same template:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy of the random forest is efficient. The output of the measurement
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The absolute error is a simple arithmetic approach that does not take mean errors
    or other factors into account. The score will vary from one run to another because
    of the random nature of the algorithm. However, in this case, 6 errors out of
    1,000 predictions is a good result.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble meta-algorithms such as random forests can replace the decision tree
    in `kmc2dt_chaining.py` with just a few lines of code, as we just saw in this
    section, tremendously boosting the whole chained ML process.
  prefs: []
  type: TYPE_NORMAL
- en: Chained ML algorithms using ensemble meta-algorithms are extremely powerful
    and efficient. In this chapter, we used a chained ML solution to deal with large
    datasets and perform automatic quality control on machine intelligence predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it may seem paradoxical, try to avoid AI before jumping into a project
    that involves millions to billions of records of data (such as SQL, Oracle, and
    big data). Try simpler classical solutions like big data methods. If the AI project
    goes through, LLN will lead to random sampling over the datasets, thanks to CLT.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline of classical and ML processes will solve the volume problem, as well
    as the human analytic limit problem. The random sampling function does not need
    to run a mini-batch function included in the KMC program. Batches can be generated
    as a preprocessing phase using classical programs. These programs will produce
    random batches of equal size to the KMC NP-hard problem, transposing it into an
    NP problem.
  prefs: []
  type: TYPE_NORMAL
- en: KMC, an unsupervised training algorithm, will transform unlabeled data into
    a labeled data output containing a cluster number as a label.
  prefs: []
  type: TYPE_NORMAL
- en: In turn, a decision tree, chained to the KMC program, will train its model using
    the output of the KMC. The model will be saved just as the KMC model was saved.
    The random forests algorithm can replace the decision tree algorithm if it provides
    better results during the training phase of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In production mode, a chained ML program containing the KMC trained model and
    the decision tree trained model can make classification predictions on fixed random
    sampled batches. Real-time metrics will monitor the quality of the process. The
    chained program, being continuous, can run twenty-four seven, providing reliable
    real-time results without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter explores yet another ML challenge: the increasing amount of
    language translations generated by global business and social communication.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of k clusters is not that important. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mini-batches and batches contain the same amount of data. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means can run without mini-batches. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Must centroids be optimized for result acceptance? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not take long to optimize hyperparameters. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It sometimes takes weeks to train a large dataset. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees and random forests are unsupervised algorithms. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees: [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random forests: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
