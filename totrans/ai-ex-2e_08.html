<html><head></head><body>
  <div id="_idContainer122">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-143" class="chapterTitle">Solving the XOR Problem with a Feedforward Neural Network</h1>
    <p class="normal">In the course of a corporate project, there always comes the point when a problem that seems impossible to solve hits you. At that point, you try everything you've learned, but it doesn't work for what's asked of you. Your team or customer begins to look elsewhere. It's time to react.</p>
    <p class="normal">In this chapter, an impossible-to-solve business case regarding material optimization will be resolved successfully with a hand-made <a id="_idIndexMarker351"/>version of a <strong class="bold">feedforward neural network</strong> (<strong class="bold">FNN</strong>) with backpropagation.</p>
    <p class="normal">Feedforward networks are one of the key building blocks of deep learning. The battle around the XOR function perfectly illustrates how deep learning regained popularity in corporate environments. XOR is an exclusive OR function that we will explore later in this chapter. The XOR FNN illustrates one of the critical functions of neural networks: <strong class="bold">classification</strong>. Once information becomes classified into subsets, it opens the doors to <strong class="bold">prediction</strong> and many other functions of neural networks, such as representation learning.</p>
    <p class="normal">An XOR FNN will be built from scratch to demystify deep learning from the start. A vintage, start-from-scratch method will be applied, blowing the deep learning hype off the table.</p>
    <p class="normal">The following topics will be covered in this chapter:</p>
    <ul>
      <li class="list">Explaining the XOR problem</li>
      <li class="list">How to hand-build an FNN</li>
      <li class="list">Solving XOR with an FNN</li>
      <li class="list">Classification</li>
      <li class="list">Backpropagation</li>
      <li class="list">A cost function</li>
      <li class="list">Cost function optimization</li>
      <li class="list">Error loss</li>
      <li class="list">Convergence</li>
    </ul>
    <p class="normal">Before we begin building an FNN, we'll first introduce XOR and its limitations in the first artificial neural model.</p>
    <h1 id="_idParaDest-144" class="title">The original perceptron could not solve the XOR function</h1>
    <p class="normal">The original perceptron was designed in the 1950s and improved in the late 1970s. The original perceptron <a id="_idIndexMarker352"/>contained one neuron that could not solve the XOR function.</p>
    <p class="normal">An XOR function means that you have to choose an <a id="_idIndexMarker353"/>exclusive OR (XOR).</p>
    <p class="normal">This can be difficult to grasp, as we're not used to thinking about the way in which we use <em class="italics">or</em> in our everyday lives. In truth, we use <em class="italics">or</em> interchangeably as either inclusive or exclusive all of the time. Take this simple example:</p>
    <p class="normal">If a friend were to come and visit me, I may ask them, "Would you like tea or coffee?" This is basically the offer of tea XOR coffee; I would not expect my friend to ask for both tea and coffee! My friend will choose one or the other.</p>
    <p class="normal">I may follow up my question with, "Would you like milk or sugar?" In this case, I would not be surprised if my friend wanted both. This is an inclusive <em class="italics">or</em>.</p>
    <p class="normal">XOR, therefore, means "You can have one or the other, but not both."</p>
    <p class="normal">We will develop these concepts in the chapter through more examples.</p>
    <p class="normal">To solve this XOR function, we will build an FNN.</p>
    <p class="normal">Once the feedforward network for solving the XOR problem is built, it will be applied to an optimization example. The material optimizing example will choose the best combinations of dimensions among billions to minimize the use of corporate resources with the generalization of the XOR function.</p>
    <p class="normal">First, a solution to the XOR limitation of a perceptron must be clarified.</p>
    <h2 id="_idParaDest-145" class="title">XOR and linearly separable models</h2>
    <p class="normal">In the late 1960s, it was mathematically proven that a perceptron could <em class="italics">not</em> solve an XOR function. Fortunately, today, the <a id="_idIndexMarker354"/>perceptron and its neocognitron version form the core model for neural networking.</p>
    <p class="normal">You may be tempted to think, <em class="italics">so what?</em> However, the entire field of neural networks relies on solving problems such as this to classify patterns. Without pattern classification, images, sounds, and words mean nothing to a machine.</p>
    <h3 id="_idParaDest-146" class="title">Linearly separable models</h3>
    <p class="normal">The McCulloch-Pitts 1943 neuron (see <em class="italics">Chapter 2</em>, <em class="italics">Building a Reward Matrix – Designing Your Datasets</em>) led to Rosenblatt's 1957-59 perceptron <a id="_idIndexMarker355"/>and the 1960 Widrow-Hoff adaptive linear element (Adaline).</p>
    <p class="normal">These models are linear models based on an <em class="italics">f</em>(<em class="italics">x</em>, <em class="italics">w</em>) function that requires a line to separate results. A perceptron cannot achieve this goal and thus cannot classify many objects it faces.</p>
    <p class="normal">A standard linear function can separate values. <strong class="bold">Linear separability</strong> can be represented in the following graph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_08_01.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_01-1.png"/></figure>
    <p class="packt_figref">Figure 8.1: Linearly separable patterns</p>
    <p class="normal">Imagine that the line separating the preceding dots and the part under it represent a picture that needs to be represented by a machine learning or deep learning application. The dots above the line represent <em class="italics">clouds</em> in the sky; the dots below the line represent <em class="italics">trees</em> on a hill. The line represents the slope of that hill.</p>
    <p class="normal">To be linearly separable, a <a id="_idIndexMarker356"/>function must be able to separate the <em class="italics">clouds</em> from the <em class="italics">trees</em> to classify them. The prerequisite to classification is <strong class="bold">separability</strong> of some sort, linear or nonlinear.</p>
    <h3 id="_idParaDest-147" class="title">The XOR limit of a linear model, such as the original perceptron</h3>
    <p class="normal">A linear model cannot <a id="_idIndexMarker357"/>solve the XOR problem expressed as follows in a table:</p>
    <table id="table001-6" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <strong class="heading">Value of x1</strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">Value of x2</strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">Output</strong>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Lines 3 and 4 show an exclusive OR (XOR). Imagine that you are offering a child a piece of cake OR a piece of candy (1 or 1):</p>
    <ul>
      <li class="list"><strong class="bold">Case 1</strong>: The child answers: "I want candy or nothing at all!" (0 or 1). That's exclusive OR (XOR)!</li>
      <li class="list"><strong class="bold">Case 2</strong>: The child answers: "I want a cake or nothing at all!" (1 or 0). That's an exclusive OR (XOR) as well!</li>
    </ul>
    <p class="normal">The following graph shows the linear inseparability of the XOR function represented by one perceptron:</p>
    <figure class="mediaobject"><img src="../Images/B15438_08_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.2: Linearly inseparable patterns</p>
    <p class="normal">The values of the table represent the Cartesian coordinates in this graph. The circles with a cross at (1, 1) and (0, 0) cannot be separated from the circles at (1, 0) and (0, 1). That's a huge problem. It means that Frank Rosenblatt's <em class="italics">f</em>(<em class="italics">x</em>, <em class="italics">w</em>) perceptron cannot separate, and thus can not classify, these dots into <em class="italics">clouds</em> and <em class="italics">trees</em>. Thus, in many cases, the perceptron cannot identify values that require linear separability.</p>
    <p class="normal">Having invented the most powerful neural concept of the twentieth century—a neuron that can learn—Frank Rosenblatt had to bear with this limitation through the 1960s.</p>
    <p class="normal">As explained with the preceding cake-OR-candy example, the absence of an XOR function limits applications in <a id="_idIndexMarker358"/>which you must choose exclusively between two options. There are many "it's-either-that-or-nothing" situations in real-life applications. For a self-driving car, it could be either turn left or turn right, but don't swerve back and forth while making the decision!</p>
    <p class="normal">We will solve this limitation with a vintage solution, starting by building, and later implementing, an FNN.</p>
    <h1 id="_idParaDest-148" class="title">Building an FNN from scratch</h1>
    <p class="normal">Let's perform a mind experiment. Imagine we are in 1969. We have today's knowledge but nothing to prove it. We know <a id="_idIndexMarker359"/>that a perceptron cannot implement the exclusive OR function XOR.</p>
    <p class="normal">We have an advantage because we now know a solution exists. To start our experiment, we only have a pad, a pencil, a sharpener, and an eraser waiting for us. We're ready to solve the XOR problem from scratch on paper before programming it. We have to find a way to classify those dots with a neural network.</p>
    <h2 id="_idParaDest-149" class="title">Step 1 – defining an FNN</h2>
    <p class="normal">We have to be unconventional to solve this problem. We must forget the complicated words and <a id="_idIndexMarker360"/>theories of the twenty-first century.</p>
    <p class="normal">We can write a neural network layer in high-school format. A hidden layer will be:</p>
    <p class="center"><em class="italics">h</em><sub>1</sub> = <em class="italics">x</em> * <em class="italics">w</em></p>
    <p class="normal">OK. Now we have one layer. A layer is merely a function. This function can be expressed as:</p>
    <p class="center"><em class="italics">f</em>(<em class="italics">x</em>, <em class="italics">w</em>)</p>
    <p class="normal">In which <em class="italics">x</em> is the input value, and <em class="italics">w</em> is some value to multiply <em class="italics">x</em> by. Hidden means that the computation is not visible, just as <em class="italics">x</em> = 2 and <em class="italics">x</em> + 2 is the hidden layer that leads to 4.</p>
    <p class="normal">At this point, we have defined a neural network in three lines:</p>
    <ul>
      <li class="list">Input <em class="italics">x</em>.</li>
      <li class="list">Some function that changes its value, like 2 × 2 = 4, which transformed 2. That is a layer. And if the result is superior to 2, for example, then great! The output is 1, meaning yes or true. Since we don't see the computation, this is the <em class="italics">hidden</em> layer.</li>
      <li class="list">An output.</li>
    </ul>
    <p class="normal"><em class="italics">f</em>(<em class="italics">x</em>, <em class="italics">w</em>) is the building block of any <a id="_idIndexMarker361"/>neural network. "Feedforward" means that we will be going from layer 1 to layer 2, moving forward in a sequence.</p>
    <p class="normal">Now that we know that basically any neural network is built with values transformed by an operation to become an output of something, we need some logic to solve the XOR problem.</p>
    <h2 id="_idParaDest-150" class="title">Step 2 – an example of how two children can solve the XOR problem every day</h2>
    <p class="normal">An example follows of how two children can solve the XOR problem using a straightforward everyday <a id="_idIndexMarker362"/>example. I strongly recommend this method. I have taken very complex problems, broken them down into small parts to a child's level, and often solved them in a few minutes. Then, you get the sarcastic answer from others such as "Is that all you did?" But, the sarcasm vanishes when the solution works over and over again in high-level corporate projects.</p>
    <p class="normal">First, let's convert the XOR problem into a candy problem in a store. Two children go to the store and want to buy candy. However, they only have enough money to buy one pack of candy. They have to agree on a choice between two packs of different candy. Let's say pack one is chocolate and the other is chewing gum. Then, during the discussion between these two children, 1 means yes, 0 means no. Their budget limits the options of these two children:</p>
    <ul>
      <li class="list">Going to the store and not buying any chocolate <strong class="bold">or</strong> chewing gum = (no, no) = (0, 0). That's not an option for these children! So the answer is false.</li>
      <li class="list">Going to the store and buying both chocolate <strong class="bold">and</strong> chewing gum = (yes, yes) = (1, 1). That would be fantastic, but that's not possible. It's too expensive. So, the answer is, unfortunately, false.</li>
      <li class="list">Going to the store and either buying chocolate <strong class="bold">or</strong> chewing gum = (1, 0 or 0, 1) = (yes or no) or (no or yes). That's possible. So, the answer is true.</li>
    </ul>
    <p class="normal">Imagine the two children. The eldest one is reasonable. The younger one doesn't really know how to count yet and wants to buy both packs of candy.</p>
    <p class="normal">We express this on paper:</p>
    <ul>
      <li class="list"><em class="italics">x</em><sub>1</sub> (eldest child's decision, yes or no, 1 or 0) * <em class="italics">w</em><sub>1</sub> (what the elder child thinks). The elder child is thinking this, or:<p class="center"><em class="italics">x</em><sub>1</sub> * <em class="italics">w</em><sub>1</sub> or <em class="italics">h</em><sub>1</sub> = <em class="italics">x</em><sub>1</sub> * <em class="italics">w</em><sub>1</sub></p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The elder child weighs a decision like we all do every day, such as purchasing a car (<em class="italics">x</em> = 0 or 1) multiplied by the cost (<em class="italics">w</em><sub>1</sub>).</p>
      </li>
      <li class="list"><em class="italics">x</em><sub>2</sub> (the younger child's decision, yes or no, 1 or 0) * <em class="italics">w</em><sub>3</sub> (what the younger child thinks). The younger child is also thinking this, or:<p class="center"><em class="italics">x</em><sub>2</sub> * <em class="italics">w</em><sub>3</sub> or <em class="italics">h</em><sub>2</sub> = <em class="italics">x</em><sub>2</sub> * <em class="italics">w</em><sub>3</sub></p>
      </li>
    </ul>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="bold">Theory</strong>: <em class="italics">x</em><sub>1</sub> and <em class="italics">x</em><sub>2</sub> are the inputs. <em class="italics">h</em><sub>1</sub> and <em class="italics">h</em><sub>2</sub> are neurons (the result of a calculation). Since <em class="italics">h</em><sub>1</sub> and <em class="italics">h</em><sub>2</sub> contain calculations that <a id="_idIndexMarker363"/>are not visible during the process, they are hidden neurons. <em class="italics">h</em><sub>1</sub> and <em class="italics">h</em><sub>2</sub> thus form a hidden layer. <em class="italics">w</em><sub>1</sub> and <em class="italics">w</em><sub>3</sub> are weights that represent how we "weigh" a decision, stating that something is more important than something else.</p>
    </div>
    <p class="normal">Now imagine the two children talking to each other.</p>
    <p class="normal">Hold it a minute! This means that now, each child is communicating with the other:</p>
    <ul>
      <li class="list"><em class="italics">x</em><sub>1</sub> (the elder child) says <em class="italics">w</em><sub>2</sub> to the younger child. Thus, <em class="italics">w</em><sub>2</sub> = this is what I think and am telling you:<p class="center"><em class="italics">x</em><sub>1</sub> * <em class="italics">w</em><sub>2</sub></p>
      </li>
      <li class="list"><em class="italics">x</em><sub>2</sub> (the younger child) says, "please add my views to your decision," which is represented by <em class="italics">w</em><sub>4</sub><sub>:</sub>
        <p class="center"><em class="italics">x</em><sub>2</sub> * <em class="italics">w</em><sub>4</sub></p>
      </li>
    </ul>
    <p class="normal">We now have the first two equations expressed in high-school-level code. It's what one thinks plus what one says to the other, asking the other to take that into account:</p>
    <pre class="programlisting"><code class="hljs stylus">h1=(x1*w1)+(x2*w4) <span class="hljs-selector-id">#II</span><span class="hljs-selector-class">.A</span><span class="hljs-selector-class">.weight</span> of hidden neuron <span class="hljs-selector-tag">h1</span>
h2=(x2*w3)+(x1*w2) <span class="hljs-selector-id">#II</span><span class="hljs-selector-class">.B</span><span class="hljs-selector-class">.weight</span> of hidden neuron <span class="hljs-selector-tag">h2</span>
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">h1</code> sums up what is going on in one child's mind: personal opinion + the other child's opinion.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">h2</code> sums up what is going on in the other child's mind and conversation: personal opinion + the other child's opinion.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="bold">Theory</strong>: The calculation now contains two input values and one hidden layer. Since, in the next step, we are going to apply calculations to <code class="Code-In-Text--PACKT-">h1</code> and <code class="Code-In-Text--PACKT-">h2</code>, we are in a feedforward neural network. We are moving from the input to <a id="_idIndexMarker364"/>another layer, which will lead us to another layer, and so on. This process of going from one layer to another is the basis of deep learning. The more layers you have, the deeper the network is. The reason <code class="Code-In-Text--PACKT-">h1</code> and <code class="Code-In-Text--PACKT-">h2</code> form a hidden layer is that their output is just the input of another layer.</p>
    </div>
    <p class="normal">For this example, we don't need complicated numbers in an activation function such as logistic sigmoid, so we state whether the output values are less than 1 or not:</p>
    <p class="normal">if <em class="italics">h</em><sub>1</sub> + <em class="italics">h</em><sub>2</sub> &gt;= 1 then <em class="italics">y</em><sub>1</sub> = 1</p>
    <p class="normal">if <em class="italics">h</em><sub>1</sub> + <em class="italics">h</em><sub>2</sub> &lt; 1 then <em class="italics">y</em><sub>2</sub> = 0</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="bold">Theory</strong>: <em class="italics">y</em><sub>1</sub> and <em class="italics">y</em><sub>2</sub> form a second hidden layer. These variables can be scalars, vectors, or matrices. They are neurons.</p>
    </div>
    <p class="normal">Now, a problem comes up. Who is right? The elder child or the younger child?</p>
    <p class="normal">The only way seems to be to play around, with the weights <em class="italics">W</em> representing all the weights. Weights in a neural network work like weights in our everyday lives. We <em class="italics">weigh</em> decisions all the time. For example, there are two books to purchase, and we will "weigh" our decisions. If one is interesting and cheaper, it will weigh more or less in our decision, for example.</p>
    <p class="normal">The children in our case agree on purchasing at least something, so from now on, <em class="italics">w</em><sub>3</sub> = <em class="italics">w</em><sub>2</sub>, <em class="italics">w</em><sub>4</sub> = <em class="italics">w</em><sub>1</sub>. The younger and elder child will thus share some of the decision weights.</p>
    <p class="normal">Now, somebody has to be an influencer. Let's leave this hard task to the elder child. The elder child, being more reasonable, will continuously deliver the bad news. You have to subtract something from your choice, represented by a minus (–) sign.</p>
    <p class="normal">Each time they reach the point <em class="italics">h</em><sub style="font-style: italic;">i</sub>, the eldest child applies a critical negative view on purchasing packs of candy. It's –<em class="italics">w</em> of everything comes up to be sure not to go over the budget. The opinion of the elder child is biased, so let's call the variable a bias, <em class="italics">b</em><sub>1</sub>. Since the younger child's opinion is biased as well, let's call this view a bias too, <em class="italics">b</em><sub>2</sub>. Since the eldest child's view is always negative, –<em class="italics">b</em><sub>1</sub> will be applied to all of the eldest child's thoughts.</p>
    <p class="normal">When we apply this decision process to their view, we obtain:</p>
    <p class="center"><em class="italics">h</em><sub>1</sub> = <em class="italics">y</em><sub>1</sub> * –<em class="italics">b</em><sub>1</sub></p>
    <p class="center"><em class="italics">h</em><sub>2</sub> = <em class="italics">y</em><sub>2</sub> * <em class="italics">b</em><sub>2</sub></p>
    <p class="normal">Then, we just have to use the same result. If the result is &gt;=1, then the threshold has been reached. The threshold is calculated as shown in the following function:</p>
    <p class="center"><em class="italics">y</em> = <em class="italics">h</em><sub>1</sub> + <em class="italics">h</em><sub>2</sub></p>
    <p class="normal">We will first start effectively finding the weights, starting by setting the weights and biases to 0.5, as follows:</p>
    <p class="center"><em class="italics">w</em><sub>1</sub> = 0.2; <em class="italics">w</em><sub>2</sub> = 0.5; <em class="italics">b</em><sub>1</sub> = 0.5</p>
    <p class="center"><em class="italics">w</em><sub>3</sub> = <em class="italics">w</em><sub>2</sub>; <em class="italics">w</em><sub>4</sub> = <em class="italics">w</em><sub>1</sub>; <em class="italics">b</em><sub>2</sub> = <em class="italics">b</em><sub>1</sub></p>
    <p class="normal">It's not a full program yet, but its theory is done.</p>
    <p class="normal">Only the communication going on between the two children is making the difference; we will focus on <a id="_idIndexMarker365"/>only modifying <em class="italics">w</em><sub>2</sub> and <em class="italics">b</em><sub>1</sub> after a first try. It works on paper after a few tries.</p>
    <p class="normal">We now write the basic mathematical function, which is, in fact, the program itself on paper:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Solution to the XOR implementation with</span>
<span class="hljs-comment">#a feedforward neural network(FNN)</span>
<span class="hljs-comment">#I.Setting the first weights to start the process</span>
<span class="hljs-attribute">w1</span>=0.5;w2=0.5;b1=0.5
<span class="hljs-attribute">w3</span>=w2;w4=w1;b2=b1
<span class="hljs-comment">#II.hidden layer #1 and its output</span>
h1=(x1*w1)+(x2*w4) #II.A.weight of hidden neuron h1
h2=(x2*w3)+(x1*w2) #II.B.weight of hidden neuron h2
<span class="hljs-comment">#III.threshold I, hidden layer 2</span>
<span class="hljs-keyword">if</span>(h1&gt;=1): <span class="hljs-attribute">h1</span>=1
<span class="hljs-keyword">if</span>(h1&lt;1): <span class="hljs-attribute">h1</span>=0
<span class="hljs-keyword">if</span>(h2&gt;=1): <span class="hljs-attribute">h2</span>=1
<span class="hljs-keyword">if</span>(h2&lt;1): <span class="hljs-attribute">h2</span>=0
h1= h1 * -b1
h2= h2 * b2
<span class="hljs-comment">#IV.Threshold II and Final OUTPUT y</span>
<span class="hljs-attribute">y</span>=h1+h2
<span class="hljs-keyword">if</span>(y&gt;=1): <span class="hljs-attribute">y</span>=1
<span class="hljs-keyword">if</span>(y&lt;1): <span class="hljs-attribute">y</span>=0
<span class="hljs-comment">#V.Change the critical weights and try again until a solution is found</span>
<span class="hljs-attribute">w2</span>=w2+0.5
<span class="hljs-attribute">b1</span>=b1+0.5
</code></pre>
    <p class="normal">Let's go from the solution on paper to Python.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Why wasn't this deceivingly <a id="_idIndexMarker366"/>simple solution found in 1969? Because <em class="italics">it seems simple today but wasn't so at that time</em>, like all inventions found by our genius predecessors. Nothing is easy at all in artificial intelligence and mathematics.</p>
    </div>
    <p class="normal">In the next section, we'll stick with the solution proposed here, and implement it in Python.</p>
    <h2 id="_idParaDest-151" class="title">Implementing a vintage XOR solution in Python with an FNN and backpropagation</h2>
    <p class="normal">To stay <a id="_idIndexMarker367"/>in the spirit of a 1969 vintage solution, we will not use NumPy, TensorFlow, Keras, or any <a id="_idIndexMarker368"/>other high-level library. Writing a vintage FNN with backpropagation written in high-school mathematics is fun.</p>
    <p class="normal">If you break <a id="_idIndexMarker369"/>a problem <a id="_idIndexMarker370"/>down into very elementary parts, you understand it better and provide a solution to that specific problem. You don't need to use a huge truck to transport a loaf of bread.</p>
    <p class="normal">Furthermore, by <a id="_idIndexMarker371"/>thinking through the minds of children, we went against running 20,000 or more episodes in <a id="_idIndexMarker372"/>modern CPU-rich solutions to solve the XOR problem. The logic used proves that both inputs can have the same parameters as long as one bias is negative (the elder reasonable critical child) to make the system provide a reasonable answer.</p>
    <p class="normal">The basic Python solution quickly reaches a result in a few iterations, approximately 10 iterations (epochs or episodes), depending on how we think it through. An epoch can be related to a try. Imagine looking at somebody practicing basketball:</p>
    <ul>
      <li class="list">The person throws the ball toward the hoop but misses. That was an epoch (an episode can be used as well).</li>
      <li class="list">The person thinks about what happened and changes the way the ball will be thrown.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">This improvement is what makes it a learning epoch (or episode). It is not a simple memoryless try. Something is really happening to improve performance.</p>
      </li>
      <li class="list">The person <a id="_idIndexMarker373"/>throws the ball again (next epoch) and again (next epochs) until the overall <a id="_idIndexMarker374"/>performance has improved. This is how a neural network improves over epochs.</li>
    </ul>
    <p class="normal"><code class="Code-In-Text--PACKT-">FNN_XOR_vintage_tribute.py</code> contains (at the top of the code) a result matrix with four columns.</p>
    <p class="normal">Each <a id="_idIndexMarker375"/>element of the matrix represents the status (<code class="Code-In-Text--PACKT-">1</code> = correct, <code class="Code-In-Text--PACKT-">0</code> = false) of the four predicates <a id="_idIndexMarker376"/>to solve:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment">#FEEDFORWARD NEURAL NETWORK(FNN) WITH BACK PROPAGATION SOLUTION FOR XOR</span>
<span class="hljs-attr">result</span>=[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>] <span class="hljs-comment">#trained result</span>
<span class="hljs-attr">train</span>=<span class="hljs-number">4</span> <span class="hljs-comment">#dataset size to train</span>
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">train</code> variable is the number of predicates to solve: (0, 0), (1, 1), (1, 0), (0, 1). The variable of the predicate to solve is <code class="Code-In-Text--PACKT-">pred</code>.</p>
    <p class="normal">The core of the program is <a id="_idIndexMarker377"/>practically a copy of the sheet of paper we wrote, as in the following <a id="_idIndexMarker378"/>code:</p>
    <pre class="programlisting"><code class="hljs reasonml">#II hidden layer <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> its output
def hidden<span class="hljs-constructor">_layer_y(<span class="hljs-params">epoch</span>,<span class="hljs-params">x1</span>,<span class="hljs-params">x2</span>,<span class="hljs-params">w1</span>,<span class="hljs-params">w2</span>,<span class="hljs-params">w3</span>,<span class="hljs-params">w4</span>,<span class="hljs-params">b1</span>,<span class="hljs-params">b2</span>,<span class="hljs-params">pred</span>,<span class="hljs-params">result</span>)</span>:
    h1=(x1*w1)+(x2*w4) #<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">II</span>.</span><span class="hljs-module"><span class="hljs-identifier">A</span>.</span></span>weight <span class="hljs-keyword">of</span> hidden neuron h1
    h2=(x2*w3)+(x1*w2) #<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">II</span>.</span><span class="hljs-module"><span class="hljs-identifier">B</span>.</span></span>weight <span class="hljs-keyword">of</span> hidden neuron h2
#<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">III</span>.</span></span>threshold I,a hidden layer <span class="hljs-number">2</span> <span class="hljs-keyword">with</span> bias
    <span class="hljs-keyword">if</span>(h1&gt;=<span class="hljs-number">1</span>):h1=<span class="hljs-number">1</span>;
    <span class="hljs-keyword">if</span>(h1&lt;<span class="hljs-number">1</span>):h1=<span class="hljs-number">0</span>;
    <span class="hljs-keyword">if</span>(h2&gt;=<span class="hljs-number">1</span>):h2=<span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span>(h2&lt;<span class="hljs-number">1</span>):h2=<span class="hljs-number">0</span>
    h1= h1<span class="hljs-operator"> * </span>-b1
    h2= h2<span class="hljs-operator"> * </span>b2
    
#IV. threshold II <span class="hljs-keyword">and</span> OUTPUT y
    y=h1+h2
    <span class="hljs-keyword">if</span>(y&lt;<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> pred&gt;=<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> pred&lt;<span class="hljs-number">2</span>):
        result<span class="hljs-literal">[<span class="hljs-identifier">pred</span>]</span>=<span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span>(y&gt;=<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> pred&gt;=<span class="hljs-number">2</span> <span class="hljs-keyword">and</span> pred&lt;<span class="hljs-number">4</span>):
        result<span class="hljs-literal">[<span class="hljs-identifier">pred</span>]</span>=<span class="hljs-number">1</span>
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">pred</code> is an argument of the function from <code class="Code-In-Text--PACKT-">1</code> to <code class="Code-In-Text--PACKT-">4</code>. The four predicates are represented in the following table:</p>
    <table id="table002-4" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <strong class="heading">Predicate (pred)</strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">x<sub>1</sub></strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">x<sub>2</sub></strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">Expected result</strong>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">2</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">3</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">That is why <em class="italics">y</em> must be &lt;1 for predicates 0 and 1. Then, <em class="italics">y</em> must be &gt;=1 for predicates 2 and 3.</p>
    <p class="normal">Now, we <a id="_idIndexMarker379"/>have to call the following function limiting the <a id="_idIndexMarker380"/>training to 50 epochs, which are more than enough:</p>
    <pre class="programlisting"><code class="hljs xml">#I Forward and backpropagation
for epoch in range(50):
    if(epoch<span class="hljs-tag">&lt;<span class="hljs-name">1):</span>
        <span class="hljs-attr">w1</span>=<span class="hljs-string">0.5;w2</span>=<span class="hljs-string">0.5;b1</span>=<span class="hljs-string">0.5</span>
    <span class="hljs-attr">w3</span>=<span class="hljs-string">w2;w4</span>=<span class="hljs-string">w1;b2</span>=<span class="hljs-string">b1</span>
</span></code></pre>
    <p class="normal">At the first <a id="_idIndexMarker381"/>epoch, the weights and <a id="_idIndexMarker382"/>biases are all set to <code class="Code-In-Text--PACKT-">0.5</code>. No use thinking! Let the program do the job. As explained previously, the weight and bias of <code class="Code-In-Text--PACKT-">x2</code> are equal.</p>
    <p class="normal">Now, the <a id="_idIndexMarker383"/>hidden layers and <code class="Code-In-Text--PACKT-">y</code> calculation function are called four times, one for each predicate to train, as <a id="_idIndexMarker384"/>shown in the following code snippet:</p>
    <pre class="programlisting"><code class="hljs angelscript">#I.A forward propagation on epoch <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> IV.backpropagation starting epoch <span class="hljs-number">2</span>
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range (<span class="hljs-number">4</span>):
        <span class="hljs-keyword">if</span>(t==<span class="hljs-number">0</span>):x1 = <span class="hljs-number">1</span>;x2 = <span class="hljs-number">1</span>;pred=<span class="hljs-number">0</span>
        <span class="hljs-keyword">if</span>(t==<span class="hljs-number">1</span>):x1 = <span class="hljs-number">0</span>;x2 = <span class="hljs-number">0</span>;pred=<span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span>(t==<span class="hljs-number">2</span>):x1 = <span class="hljs-number">1</span>;x2 = <span class="hljs-number">0</span>;pred=<span class="hljs-number">2</span>
        <span class="hljs-keyword">if</span>(t==<span class="hljs-number">3</span>):x1 = <span class="hljs-number">0</span>;x2 = <span class="hljs-number">1</span>;pred=<span class="hljs-number">3</span>
        #forward propagation on epoch <span class="hljs-number">1</span>
        hidden_layer_y(epoch,x1,x2,w1,w2,w3,w4,b1,b2,pred,result)
</code></pre>
    <p class="normal">Now, the system must train. To do that, we need to measure the number of predictions, 1 to 4, that are correct at each iteration and decide how to change the weights/biases until we obtain proper results. We'll do that in the following section.</p>
    <h3 id="_idParaDest-152" class="title">A simplified version of a cost function and gradient descent</h3>
    <p class="normal">Slightly more complex gradient descent will be described in the next chapter. In this chapter, only a <a id="_idIndexMarker385"/>one-line equation will do the job. The only thing to bear in mind as an unconventional thinker is: <em class="italics">so what?</em> The <a id="_idIndexMarker386"/>concept of gradient descent is minimizing loss or errors between the present result and a goal to attain.</p>
    <p class="normal">First, a cost function is needed.</p>
    <p class="normal">There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We need to find out how many are correctly trained at each epoch.</p>
    <p class="normal">The cost function will measure the difference between the training goal (4) and the result of this epoch or training iteration (result).</p>
    <p class="normal">When 0 convergence is reached, it means the training has succeeded.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">result[0,0,0,0]</code> contains a <code class="Code-In-Text--PACKT-">0</code> for each value if none of the four predicates have been trained correctly. <code class="Code-In-Text--PACKT-">result[1,0,1,0]</code> means two out of the four predicates are correct. <code class="Code-In-Text--PACKT-">result[1,1,1,1]</code> means that all four predicates have been trained and that the training can stop. <code class="Code-In-Text--PACKT-">1</code>, in this case, means that the correct training result was obtained. It can be <code class="Code-In-Text--PACKT-">0</code> or <code class="Code-In-Text--PACKT-">1</code>. The <code class="Code-In-Text--PACKT-">result</code> array is the result counter.</p>
    <p class="normal">The cost function will express this training by having a value of <code class="Code-In-Text--PACKT-">4</code>, <code class="Code-In-Text--PACKT-">3</code>, <code class="Code-In-Text--PACKT-">2</code>, <code class="Code-In-Text--PACKT-">1</code>, or <code class="Code-In-Text--PACKT-">0</code> as the training goes down the slope to 0.</p>
    <p class="normal">Gradient descent measures the value of the descent to find the direction of the slope: up, down, or 0. Then, once you have that slope and the steepness of it, you can optimize the weights. A derivative is a way to know whether you are going up or down a slope.</p>
    <p class="normal">Each time we move up or down the slope, we check to see whether we are moving in the right direction. We will assume that we will go one step at a time. So if we change directions, we will change our pace by one step. That one step value is our <strong class="bold">learning rate</strong>. We will measure our progression at each step. However, if we feel comfortable with our results, we might walk 10 steps at a time and only check to see if we are on the right track every 10 steps. Our learning rate will thus have increased to 10 steps.</p>
    <p class="normal">In this case, we hijacked the concept and used it to set the learning rate to <code class="Code-In-Text--PACKT-">0.05</code> with a one-line function. Why not? It helped to solve gradient descent optimization in one line:</p>
    <pre class="programlisting"><code class="hljs elixir">    if(convergence&lt;0)<span class="hljs-symbol">:w2+</span>=training_step;b1=w2
</code></pre>
    <p class="normal">By applying the vintage children-buying-candy logic to the whole XOR problem, we found that only <code class="Code-In-Text--PACKT-">w2</code> needed to be optimized. That's why <code class="Code-In-Text--PACKT-">b1=w2</code>. That's because <code class="Code-In-Text--PACKT-">b1</code> is doing the tough job of saying something negative (<code class="Code-In-Text--PACKT-">-</code>) all the time, which completely changes the course of the resulting outputs.</p>
    <p class="normal">The rate is set at <code class="Code-In-Text--PACKT-">0.05</code>, and the program finishes training in 10 epochs:</p>
    <pre class="programlisting"><code class="hljs angelscript">epoch: <span class="hljs-number">10</span> optimization <span class="hljs-number">0</span> w1: <span class="hljs-number">0.5</span> w2: <span class="hljs-number">1.0</span> w3: <span class="hljs-number">1.0</span> w4: <span class="hljs-number">0.5</span> b1: <span class="hljs-number">-1.0</span> b2: <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">This is a logical <em class="italics">yes</em> or <em class="italics">no</em> problem. The way the network is built is pure logic. Nothing can stop us from using whatever training rates we wish. In fact, that's what gradient descent is about. There are many gradient descent methods. If you invent your own and it works for your solution, that is fine.</p>
    <p class="normal">This one-line <a id="_idIndexMarker387"/>code is enough, in this case, to see whether the slope is going down. As long as the slope is negative, the <a id="_idIndexMarker388"/><a id="_idIndexMarker389"/>function is going downhill to <em class="italics">cost</em> = 0:</p>
    <pre class="programlisting"><code class="hljs livecodeserver">    convergence=<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">result</span>)-train <span class="hljs-comment">#estimating the direction of the slope</span>
    <span class="hljs-keyword">if</span>(convergence&gt;=<span class="hljs-number">-0.00000001</span>): break
</code></pre>
    <p class="normal">The following diagram sums up the whole process:</p>
    <figure class="mediaobject"><img src="../Images/B15438_08_03.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_03.png"/></figure>
    <p class="packt_figref">Figure 8.3: A feedforward neural network model (FNN)</p>
    <p class="normal">We can see that all of the arrows of the layers go forward in this "feedforward" neural network. However, the arrow that stems from the <em class="italics">y</em> node and goes backward can seem confusing. This line represents a change in weights to train the model. This means that we go back to changing the weights and running the network for another epoch (or episode). The system is adjusting its weights epoch by epoch until the overall result is correct.</p>
    <p class="normal">Too simple? Well, it works, and that's all that counts in real-life development. If your code is bug-free and does the job, then that's what matters.</p>
    <p class="normal">Finding a simple development tool means nothing more than that. It's just another tool in the toolbox. We can get this XOR function to work on a neural network and generate income.</p>
    <div class="packt_tip">
      <p>Companies are not interested in how smart you are but how efficient (profitable) you can be.</p>
    </div>
    <p class="normal">A company's <a id="_idIndexMarker390"/>survival relies on multiple constraints: delivering on time, offering good prices, providing a product <a id="_idIndexMarker391"/>with a reasonable quality level, and many more factors besides.</p>
    <p class="normal">When we come up with a solution, it is useless to show how smart we can be writing tons of code. Our company or customers expect an efficient solution that will run well and is easy to maintain. In short, focus on efficiency. Once we have a good solution, we need to show that it works. In this case, we proved that linear separability was achieved.</p>
    <h3 id="_idParaDest-153" class="title">Linear separability was achieved</h3>
    <p class="normal">Bear in mind that the whole purpose of this feedforward network with backpropagation through a cost function was to transform a linear non-separable function into a linearly separable function to <a id="_idIndexMarker392"/>implement the classification of features presented to the system. In this case, the features had a <code class="Code-In-Text--PACKT-">0</code> or <code class="Code-In-Text--PACKT-">1</code> value.</p>
    <p class="normal">One of the core goals of a layer in a neural network is to make the input make sense, meaning to be able to separate one kind of information from another.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">h1</code> and <code class="Code-In-Text--PACKT-">h2</code> will produce the Cartesian coordinate linear separability training axis, as implemented in the following code:</p>
    <pre class="programlisting"><code class="hljs armasm">    h1= h1 * -<span class="hljs-keyword">b1
</span>    h2= h2 * <span class="hljs-keyword">b2
</span>    print(h1,h2)
</code></pre>
    <p class="normal">Running the program provides a view of the nonlinear input values once the hidden layers have trained them. The nonlinear values then become linear values in a linearly separable function:</p>
    <pre class="programlisting"><code class="hljs angelscript">linearly separability through cartesian training <span class="hljs-number">-1.0000000000000004</span> <span class="hljs-number">1.0000000000000004</span>
linearly separability through cartesian training <span class="hljs-number">-0.0</span> <span class="hljs-number">0.0</span>
linearly separability through cartesian training <span class="hljs-number">-0.0</span> <span class="hljs-number">1.0000000000000004</span>
linearly separability through cartesian training <span class="hljs-number">-0.0</span> <span class="hljs-number">1.0000000000000004</span>
epoch: <span class="hljs-number">10</span> optimization <span class="hljs-number">0</span> w1: <span class="hljs-number">0.5</span> w2: <span class="hljs-number">1.0</span> w3: <span class="hljs-number">1.0</span> w4: <span class="hljs-number">0.5</span> b1: <span class="hljs-number">-1.0</span> b2: <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">The intermediate result and goal are not a bunch of numbers on a screen to show that the program works. The result is a set of Cartesian values that can be represented in the following linearly separated graph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_08_04.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_04-1.png"/></figure>
    <p class="packt_figref">Figure 8.4: Linearly separable patterns</p>
    <p class="normal">We have now obtained a separation between the top values, representing the intermediate <a id="_idIndexMarker393"/>values of the (1, 0) and (0, 1) inputs, and the bottom values, representing the (1, 1) and (0, 0) inputs. The top values are separated from the bottom values by a clear line. We now have <em class="italics">clouds</em> on top and <em class="italics">trees</em> below the line that separates them.</p>
    <p class="normal">The layers of the neural network have transformed nonlinear values into linearly separable values, making classification possible through standard separation equations, such as the one in the following code:</p>
    <pre class="programlisting"><code class="hljs angelscript">#IV. threshold II <span class="hljs-keyword">and</span> OUTPUT y
    y=h1+h2 # logical separation
    <span class="hljs-keyword">if</span>(y&lt;<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> pred&gt;=<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> pred&lt;<span class="hljs-number">2</span>):
        result[pred]=<span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span>(y&gt;=<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> pred&gt;=<span class="hljs-number">2</span> <span class="hljs-keyword">and</span> pred&lt;<span class="hljs-number">4</span>):
        result[pred]=<span class="hljs-number">1</span>
</code></pre>
    <p class="normal">The ability of a neural network to make non-separable information separable and classifiable represents <a id="_idIndexMarker394"/>one of the core powers of deep learning. From this technique, many operations can be performed on data, such as subset optimization.</p>
    <p class="normal">In the next section, we'll look at a practical application for our FNN XOR solution.</p>
    <h1 id="_idParaDest-154" class="title">Applying the FNN XOR function to optimizing subsets of data</h1>
    <p class="normal">There are more than 7.5 billion people breathing air on this planet. In 2050, there might be 2.5 billion <a id="_idIndexMarker395"/>more of us. All of these people need to wear clothes and eat. Just those two activities involve classifying data into <a id="_idIndexMarker396"/>subsets for industrial purposes.</p>
    <p class="normal"><strong class="bold">Grouping</strong> is a core concept for any production. Production relating to producing clothes and food requires grouping to <a id="_idIndexMarker397"/>optimize production costs. Imagine not grouping and delivering one T-shirt at a time from one continent to another instead of grouping T-shirts in a container and grouping many containers (not just two on a ship). Let's focus on clothing, for example.</p>
    <p class="normal">A chain of stores needs to replenish the stock of clothing in each store as the customers purchase their products. In this case, the corporation has 10,000 stores. The brand produces jeans, for example. Their average product is a faded jean. This product sells a slow 50 units a month per store. That adds up to 10,000 stores × 50 units = 500,000 units or stock-keeping units (SKUs) per month. These units are sold in all sizes, grouped into average, small, and large. The sizes sold per month are random.</p>
    <p class="normal">The main factory for this product has about 2,500 employees producing those jeans at an output of about 25,000 jeans per day. The employees work in the following main fields: cutting, assembling, washing, lasering, packaging, and warehousing.</p>
    <p class="normal">The first difficulty arises with the purchase and use of fabric. The fabric for this brand is not cheap. Large amounts are necessary. Each pattern (the form of pieces of the pants to be assembled) needs to be cut by wasting as little fabric as possible.</p>
    <p class="normal">Imagine you have an empty box you want to fill up to optimize the volume. If you only put soccer balls in it, there will be a lot of space. If you slip tennis balls in the empty spaces, space will decrease. If, on top of that, you fill the remaining empty spaces with ping pong balls, you will have optimized the available space in the box.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Building optimized subsets can be applied to containers, warehouse flows and storage, truckload optimizing, and almost all human activities.</p>
    </div>
    <p class="normal">In the apparel business, if 1% to 10% of the fabric is wasted while manufacturing jeans, the company will <a id="_idIndexMarker398"/>survive the competition. At over 10%, there is a real problem to solve. Losing 20% of all the fabric consumed in <a id="_idIndexMarker399"/>manufacturing jeans can bring the company down and force it into bankruptcy.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The main rule is to combine larger pieces and smaller pieces to make optimized cutting patterns.</p>
    </div>
    <p class="normal">Optimization of space through larger and smaller objects can be applied to cutting the forms, which are the patterns of the jeans, for example. Once they are cut, they will be assembled at the sewing stations.</p>
    <p class="normal">The problem can be summed up as:</p>
    <ul>
      <li class="list">Creating subsets of the 500,000 SKUs to optimize the cutting process for the month to come in a given factory</li>
      <li class="list">Making sure that each subset contains smaller sizes and larger sizes to minimize the loss of fabric by choosing 6 sizes per day to build 25,000 unit subsets per day</li>
      <li class="list">Generating cut plans of an average of 3 to 6 sizes per subset per day for a production of 25,000 units per day</li>
    </ul>
    <p class="normal">In mathematical terms, this means trying to find subsets of sizes among 500,000 units for a given day.</p>
    <p class="normal">The task is to find 6 well-matched sizes among 500,000 units, as shown in the following combination formula:</p>
    <figure class="mediaobject"><img src="../Images/B15438_08_001.png" alt=""/></figure>
    <p class="normal">At this point, most people abandon the idea and find some easy way out of this, even if it means wasting fabric.</p>
    <p class="normal">The first reaction we all have is that this is more than the number of stars in the universe and all that hype. However, that's not the right way to look at it at all. The right way is to look exactly in the opposite direction.</p>
    <p class="normal">The key to this problem is to observe the particle at a microscopic level, at the <strong class="bold">bits of information</strong> level. Analyzing detailed data is necessary to obtain reliable results. This is a fundamental concept of machine learning and deep learning. Translated into our field, it means that to process an image, ML and DL process pixels. </p>
    <p class="normal">So, even if the pictures to process represent large quantities, it will come down to small units of information to analyze:</p>
    <table id="table003-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">yottabyte (YB)</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">10<sup>24</sup></p>
          </td>
          <td class="No-Table-Style">
            <p class="content">yobibyte (YiB)</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">2<sup>80</sup></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">It might be surprising to see these large numbers appear suddenly! However, when trying to combine thousands of elements, the combinations become exponential. When you extend this to the large population that major apparel brands have to deal with, it becomes rapidly exponential as well.</p>
    <p class="normal">Today, Google, Facebook, Amazon, and others have yottabytes of data to classify and make sense of. Using the term <strong class="bold">big data</strong> doesn't mean much. It's just a lot of data, and so what?</p>
    <p class="normal">You do not need to analyze the individual positions of each data point in a dataset but use the probability distribution.</p>
    <p class="normal">To understand that, let's go to a store to buy <a id="_idIndexMarker400"/>some jeans for a family. One of the parents wants a pair of jeans, and so does a teenager in <a id="_idIndexMarker401"/>that family. They both go and try to find their size in the pair of jeans they want. The parent finds 10 pairs of jeans in size <em class="italics">x</em>. All of the jeans are part of the production plan. The parent picks one at <em class="italics">random</em>, and the teenager does the same. Then they pay for them and take them home.</p>
    <p class="normal">Some systems work fine with random choices: random transportation (taking jeans from the store to home) of particles (jeans, other product units, pixels, or whatever is to be processed), making up that fluid (a dataset).</p>
    <p class="normal">Translated into our factory, this means that a stochastic (random) process can be introduced to solve the problem.</p>
    <p class="normal">All that was required is that small and large sizes were picked at random among the 500,000 units to produce. If 6 sizes from 1 to 6 were to be picked per day, the sizes could be classified as follows in a table:</p>
    <p class="center"><em class="italics">Smaller sizes</em> = <em class="italics">S</em> = {1, 2, 3}</p>
    <p class="center"><em class="italics">Larger sizes</em> = <em class="italics">L</em> = {4, 5, 6}</p>
    <p class="normal">Converting this into numerical subset names, <em class="italics">S</em> = 1 and <em class="italics">L</em> = 6. By selecting large and small sizes to produce at the same time, the fabric will be optimized, as shown in the following table:</p>
    <table id="table004" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <strong class="heading">Size of choice 1</strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">Size of choice 2</strong>
          </td>
          <td class="No-Table-Style">
            <strong class="heading">Output</strong>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">6</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">6</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">6</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">6</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">1</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">You will notice that the first two lines contain the same value. This will not optimize fabric consumption. If you put only large size 6 products together, there will be "holes" in the pattern. If you only put small size 1 products together, then they will fill up all of the space and leave no room for larger products. Fabric cutting is optimal when large and small sizes are present on the same roll of fabric.</p>
    <p class="normal">Doesn't this sound familiar? It looks exactly like our vintage FNN, with 1 instead of 0 and 6 instead of 1.</p>
    <p class="normal">All that has <a id="_idIndexMarker402"/>to be done is to stipulate that subset <em class="italics">S</em> = <em class="italics">value</em> 0, and subset <em class="italics">L</em> = <em class="italics">value</em> 1; and the previous code <a id="_idIndexMarker403"/>can be generalized.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">FFN_XOR_generalization.py</code> is the program that generalizes the previous code, as shown in the following snippet.</p>
    <p class="normal">If this works, then smaller and larger sizes will be chosen to send to the cut planning department, and the fabric will be optimized. Applying the randomness concept of Bellman's equation, a stochastic process is applied, choosing customer unit orders at random (each order is one size and a unit quantity of 1):</p>
    <pre class="programlisting"><code class="hljs routeros">    <span class="hljs-attribute">w1</span>=0.5;w2=1;b1=1
    <span class="hljs-attribute">w3</span>=w2;w4=w1;b2=b1
    <span class="hljs-attribute">s1</span>=random.randint(1,500000)#choice <span class="hljs-keyword">in</span> one <span class="hljs-builtin-name">set</span> s1
    <span class="hljs-attribute">s2</span>=random.randint(1,500000)#choice <span class="hljs-keyword">in</span> one <span class="hljs-builtin-name">set</span> s2
</code></pre>
    <p class="normal">The weights and bias are now constants obtained by the result of the XOR training FNN. The training is over; the FNN is now used to provide results. Bear in mind that the word <em class="italics">learning</em> in machine learning and deep learning doesn't mean you have to train systems forever. In stable environments, training is run only when the datasets change. At one point in a project, you are hopefully using deep <em class="italics">trained</em> systems and not simply exploring the training phase of a deep <em class="italics">learning</em> process. The goal is not to spend all corporate resources on learning but on using trained models.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Deep learning architecture must rapidly become deep trained models to produce a profit.</p>
    </div>
    <p class="normal">For this prototype validation, the size of a given order is random. <code class="Code-In-Text--PACKT-">0</code> means the order fits in the <em class="italics">S</em> subset; <code class="Code-In-Text--PACKT-">1</code> means the order fits in the <em class="italics">L</em> subset. The data generation function reflects the random nature of consumer behavior in the following six-size jeans consumption model:</p>
    <pre class="programlisting"><code class="hljs reasonml">    x1=random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)#property <span class="hljs-keyword">of</span> choice:size smaller=<span class="hljs-number">0</span>
    x2=random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)#property <span class="hljs-keyword">of</span> choice :size bigger=<span class="hljs-number">1</span>
    hidden<span class="hljs-constructor">_layer_y(<span class="hljs-params">x1</span>,<span class="hljs-params">x2</span>,<span class="hljs-params">w1</span>,<span class="hljs-params">w2</span>,<span class="hljs-params">w3</span>,<span class="hljs-params">w4</span>,<span class="hljs-params">b1</span>,<span class="hljs-params">b2</span>,<span class="hljs-params">result</span>)</span>
</code></pre>
    <p class="normal">Once two customer orders have been chosen at random in the correct size category, the FNN is activated and <a id="_idIndexMarker404"/>runs like the previous example. Only the <code class="Code-In-Text--PACKT-">result</code> array has been changed since we are <a id="_idIndexMarker405"/>using the same core program. Only a yes (<code class="Code-In-Text--PACKT-">1</code>) or no (<code class="Code-In-Text--PACKT-">0</code>) is expected, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs reasonml">#II hidden layer <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> its output
def hidden<span class="hljs-constructor">_layer_y(<span class="hljs-params">x1</span>,<span class="hljs-params">x2</span>,<span class="hljs-params">w1</span>,<span class="hljs-params">w2</span>,<span class="hljs-params">w3</span>,<span class="hljs-params">w4</span>,<span class="hljs-params">b1</span>,<span class="hljs-params">b2</span>,<span class="hljs-params">result</span>)</span>:
    h1=(x1*w1)+(x2*w4) #<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">II</span>.</span><span class="hljs-module"><span class="hljs-identifier">A</span>.</span></span>weight <span class="hljs-keyword">of</span> hidden neuron h1
    h2=(x2*w3)+(x1*w2) #<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">II</span>.</span><span class="hljs-module"><span class="hljs-identifier">B</span>.</span></span>weight <span class="hljs-keyword">of</span> hidden neuron h2
#<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">III</span>.</span></span>threshold I,a hidden layer <span class="hljs-number">2</span> <span class="hljs-keyword">with</span> bias
    <span class="hljs-keyword">if</span>(h1&gt;=<span class="hljs-number">1</span>):h1=<span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span>(h1&lt;<span class="hljs-number">1</span>):h1=<span class="hljs-number">0</span>
    <span class="hljs-keyword">if</span>(h2&gt;=<span class="hljs-number">1</span>):h2=<span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span>(h2&lt;<span class="hljs-number">1</span>):h2=<span class="hljs-number">0</span>
    h1= h1<span class="hljs-operator"> * </span>-b1
    h2= h2<span class="hljs-operator"> * </span>b2
#IV. threshold II <span class="hljs-keyword">and</span> OUTPUT y
    y=h1+h2
    <span class="hljs-keyword">if</span>(y&lt;<span class="hljs-number">1</span>):
        result<span class="hljs-literal">[<span class="hljs-number">0</span>]</span>=<span class="hljs-number">0</span>
    <span class="hljs-keyword">if</span>(y&gt;=<span class="hljs-number">1</span>):
        result<span class="hljs-literal">[<span class="hljs-number">0</span>]</span>=<span class="hljs-number">1</span>
</code></pre>
    <p class="normal">The number of subsets to produce needs to be calculated to determine the volume of positive results required.</p>
    <p class="normal">The choice is made of 6 sizes among 500,000 units. But, the request is to produce a daily production plan for the factory. The daily production target is 25,000. Also, each subset can be used about 20 times. There is always, on average, 20 times the same size in a given pair of jeans available.</p>
    <p class="normal">Six sizes are required to obtain good fabric optimization. This means that after three choices, the result represents one subset of potential optimized choices:</p>
    <p class="center"><em class="italics">R</em> = 120 × 3 subsets of two sizes = 360</p>
    <p class="normal">The magic number has been found. For every 3 choices, the goal of producing 6 sizes multiplied by a repetition of 20 will be reached.</p>
    <p class="normal">The production-per-day request is 25,000:</p>
    <p class="center">The number of subsets requested = 25000/3=8333. 333</p>
    <p class="normal">The system can run 8,333 products as long as necessary to produce the volume of subsets requested. In <a id="_idIndexMarker406"/>this case, the range is set to a sample of 1,000,000 products. It can be extended or reduced <a id="_idIndexMarker407"/>when needed. The system is filtering the correct subsets through the following function:</p>
    <pre class="programlisting"><code class="hljs maxima"><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000000</span>):
    ...(a <span class="hljs-built_in">block</span> of code <span class="hljs-built_in">is</span> here <span class="hljs-keyword">in</span> the <span class="hljs-built_in">program</span>)...
    <span class="hljs-keyword">if</span>(result[<span class="hljs-number">0</span>]&gt;<span class="hljs-number">0</span>):
        subsets+=<span class="hljs-number">1</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"Subset:"</span>,subsets,<span class="hljs-string">"size subset #"</span>,x1,<span class="hljs-string">" and "</span>,<span class="hljs-string">"size subset #"</span>,x2,<span class="hljs-string">"result:"</span>,result[<span class="hljs-number">0</span>],<span class="hljs-string">"order #"</span>,<span class="hljs-string">" and "</span>,s1,<span class="hljs-string">"order #"</span>,s2)
    <span class="hljs-keyword">if</span>(subsets&gt;=<span class="hljs-number">8333</span>):
        <span class="hljs-built_in">break</span>
</code></pre>
    <p class="normal">When the 8,333 subsets have been found respecting the smaller-larger size distribution, the system stops, as shown in the following output:</p>
    <pre class="programlisting"><code class="hljs angelscript">Subset: <span class="hljs-number">8330</span> size subset # <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> size subset # <span class="hljs-number">0</span> result: <span class="hljs-number">1</span> order # <span class="hljs-keyword">and</span> <span class="hljs-number">53154</span> order # <span class="hljs-number">14310</span>
Subset: <span class="hljs-number">8331</span> size subset # <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> size subset # <span class="hljs-number">0</span> result: <span class="hljs-number">1</span> order # <span class="hljs-keyword">and</span> <span class="hljs-number">473411</span> order # <span class="hljs-number">196256</span>
Subset: <span class="hljs-number">8332</span> size subset # <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> size subset # <span class="hljs-number">0</span> result: <span class="hljs-number">1</span> order # <span class="hljs-keyword">and</span> <span class="hljs-number">133112</span> order # <span class="hljs-number">34827</span>
Subset: <span class="hljs-number">8333</span> size subset # <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> size subset # <span class="hljs-number">1</span> result: <span class="hljs-number">1</span> order # <span class="hljs-keyword">and</span> <span class="hljs-number">470291</span> order # <span class="hljs-number">327392</span>
</code></pre>
    <p class="normal">This example proves the point. <em class="italics">Simple solutions can solve very complex problems.</em></p>
    <p class="normal">Two main functions, among some minor ones, must be added:</p>
    <ul>
      <li class="list">After each choice, the orders chosen must be removed from the 500,000-order dataset. When an order has been selected, processing it again will generate errors in the global results. This will preclude choosing the same order twice and reduce the number of choices to be made.</li>
      <li class="list">An optimization function to regroup the results for production purposes, for example. The idea is not to run through the records randomly, but to organize them by sets. This way, each set can be controlled independently.</li>
    </ul>
    <p class="normal">Application information:</p>
    <ul>
      <li class="list">The core calculation part of the application is fewer than 50 lines long.</li>
      <li class="list">With a few control <a id="_idIndexMarker408"/>functions and arrays, the program might reach 200 lines maximum. The goal of the control functions is to check and see whether the results reach the <a id="_idIndexMarker409"/>overall goal. For example, every 1,000 records, a local result could be checked to see whether it fits the overall goal.</li>
      <li class="list">This results in easy maintenance for a team.</li>
    </ul>
    <p class="normal">Optimizing the number of lines of code to create a powerful application can prove to be very efficient for many business problems.</p>
    <h1 id="_idParaDest-155" class="title">Summary</h1>
    <p class="normal">Building a small neural network from scratch provides a practical view of the elementary properties of a neuron. We saw that a neuron requires an input that can contain many variables. Then, weights are applied to the values with biases. An activation function then transforms the result and produces an output.</p>
    <p class="normal">Neural networks, even one- or two-layer networks, can provide real-life solutions in a corporate environment. A real-life business case was implemented using complex theory broken down into small functions. Then, these components were assembled to be as minimal and profitable as possible.</p>
    <p class="normal">It takes talent to break a problem down into elementary parts and find a simple, powerful solution. It requires more effort than just typing hundreds to thousands of lines of code to make things work. A well-thought through algorithm will always be more profitable, and software maintenance will prove more cost-effective.</p>
    <p class="normal">Customers expect quick-win solutions. Artificial intelligence provides a large variety of tools that satisfy that goal. When solving a problem for a customer, do not look for the best theory, but the simplest and fastest way to implement a profitable solution, no matter how unconventional it seems.</p>
    <p class="normal">In this case, an enhanced FNN perceptron solved a complex business problem. In the next chapter, we will explore a convolutional neural network (CNN). We will build a CNN with TensorFlow 2.x, layer by layer, to classify images.</p>
    <h1 id="_idParaDest-156" class="title">Questions</h1>
    <ol>
      <li class="list">Can the perceptron alone solve the XOR problem? (Yes | No)</li>
      <li class="list">Is the XOR function linearly non-separable? (Yes | No)</li>
      <li class="list">One of the main goals of layers in a neural network is classification. (Yes | No)</li>
      <li class="list">Is deep learning the only way to classify data? (Yes | No)</li>
      <li class="list">A cost function shows the increase in the cost of a neural network. (Yes | No)</li>
      <li class="list">Can simple arithmetic be enough to optimize a cost function? (Yes | No)</li>
      <li class="list">A feedforward network requires inputs, layers, and an output. (Yes | No)</li>
      <li class="list">A feedforward network always requires training with backpropagation. (Yes | No)</li>
      <li class="list">In real-life applications, solutions are only found by following existing theories. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-157" class="title">Further reading</h1>
    <ul>
      <li class="list">Linear separability: <a href="http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html"><span class="url">http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html</span></a></li>
    </ul>
  </div>
</body></html>