<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Convolutional Neural Networks
                </header>
            
            <article>
                
<p class="mce-root">This chapter introduces convolutional neural networks, starting with the convolution operation and moving forward to ensemble layers of convolutional operations, with the aim of learning about filters that operate over datasets. The pooling strategy is then introduced to show how such changes can improve the training and performance of a model. The chapter concludes by showing how to visualize the filters <span>learned</span>.</p>
<p>By the end of this chapter, you will be familiar with the motivation behind convolutional neural networks and will know how the convolution operation works in one and two dimensions. When you finish this chapter, you will know how to implement convolution in layers so as to learn filters through gradient descent. Finally, you will have a chance to use many tools that you learned previously, including dropout and batch normalization, but now you will know how to use pooling as an alternative to reduce the dimensionality of the problem and create levels of information abstraction. </p>
<p>This chapter is organized as follows:</p>
<ul>
<li>Introduction to convolutional neural networks</li>
<li>Convolution in <em>n</em>-dimensions</li>
<li>Convolutional layers</li>
<li>Pooling strategies</li>
<li>Visualization of filters</li>
</ul>
<h1 id="uuid-ea3fed3c-f130-4a77-9cce-5501df0070f9">Introduction to convolutional neural networks</h1>
<p>Previously, in <a href="03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml">Chapter 11</a>, <em>Deep and Wide Neural Networks</em>, we used a dataset that was very challenging for a general-purpose network. However, <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) will prove to be more effective, as you will see. CNNs have been around since the late 80s (LeCun, Y., et al. (1989)). They have transformed the world of computer vision and audio processing (Li, Y. D., et al. (2016)). If you have some kind of AI-based object recognition capability in your smartphone, chances are it is using some kind of CNN architecture; for example:</p>
<ul>
<li>The recognition of objects in images</li>
<li>The recognition of a digital fingerprint</li>
<li>The recognition of voice commands</li>
</ul>
<p>CNNs are interesting because they have solved some of the most challenging problems in computer vision, including beating a human being at an image recognition problem called ImageNet (Krizhevsky, A., et al. (2012)<em>)</em>. If you can think of the most complex object recognition tasks, CNNs should be your first choice for experimentation: they will never disappoint!</p>
<p>The key to the success of CNNs is their unique ability to <strong>encode spatial relationships</strong>. If we contrast two different datasets, one about student school records that includes current and past grades, attendance, online activity, and so on, and a second dataset about images of cats and dogs, if we aim to classify students or cats and dogs, the data is different. In one we have student features that have no spatial relationships.</p>
<p>For example, if grades are the first feature, attendance does not have to be next to it, so their positions can be interchanged and the classification performance should not be affected, right? However, with images of cats and dogs, features (pixels) of eyes have to be adjacent to a nose or an ear; when you change the spatial features and observe an ear in the middle of two eyes (strange), the performance of the classifier should be affected because there is usually no cat or dog that has an ear in between its eyes. This is the type of spatial relationship that CNNs are good at encoding. You can also think of audio or speech processing. You know that some sounds must come after others in certain words. If the dataset allows for spatial relationships, CNNs have the potential to perform well.</p>
<h1 id="uuid-2b9d4f72-5755-499a-be07-74dc15597084">Convolution in n-dimensions</h1>
<p class="mce-root"><span>The name of CNNs comes from their signature operation: </span><strong>convolution</strong><span>. This operation is a mathematical operation that is very common in the signal processing area. Let's go ahead and discuss the convolution operation.</span></p>
<h2 id="uuid-f8ae0f2b-e5e9-4c2e-b104-e6787dd01cee">1-dimension</h2>
<p>Let's start with the discrete-time convolution function in one dimension. Suppose that we have input data, <img class="fm-editor-equation" src="assets/42ad14d7-4801-41de-8544-14d064f7699d.png" style="width:3.33em;height:1.00em;"/>, and some weights, <img class="fm-editor-equation" src="assets/e36d108f-f5b2-4205-bfb8-f85808f76dcd.png" style="width:3.83em;height:1.00em;"/>, we can define the discrete-time convolution operation between the two as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e5873a07-cbe5-4f7c-ba38-1088178ab472.png" style="width:22.67em;height:3.83em;"/>.</p>
<p>In this equation, the convolution operation is denoted by a <strong>*</strong> symbol. Without complicating things too much, we can say that <img class="fm-editor-equation" src="assets/8f014c06-f6a8-402e-9f00-0e79616523e0.png" style="width:1.08em;height:0.83em;"/> is inverted, <img class="fm-editor-equation" src="assets/a2dbdcc5-92fb-4538-b80d-949e4edb4024.png" style="width:3.00em;height:1.17em;"/>, and then shifted, <img class="fm-editor-equation" src="assets/3cfe2848-09e8-4fb0-9069-e4c711729d34.png" style="width:3.42em;height:1.00em;"/>. The resulting vector is <img class="fm-editor-equation" src="assets/781cc345-61b4-4a24-936a-8aeba31cd79c.png" style="width:5.08em;height:1.00em;"/>, which can be interpreted as the <em>filtered</em> version of the input when the filter <img class="fm-editor-equation" src="assets/4dea30d8-7115-4bcc-8634-64707a585ecd.png" style="width:1.17em;height:0.92em;"/> is applied.</p>
<p>If we define the two vectors as follows, <img class="fm-editor-equation" src="assets/667d186e-3d70-423b-a31b-05a9832f428a.png" style="width:4.50em;height:1.08em;"/> and <img class="fm-editor-equation" src="assets/04ea475a-0074-4c06-b144-ed21e87c8651.png" style="width:6.08em;height:1.08em;"/>, then the convolution operation yields <img class="fm-editor-equation" src="assets/45c8af6e-5ee5-494c-8f9c-033d812a2478.png" style="width:9.42em;height:1.33em;"/>.</p>
<p class="mce-root"/>
<p><em>Figure 12.1</em> shows every single step involved in obtaining this result by inverting and shifting the filter and multiplying across the input data:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/54f721f1-4fd2-497f-ae21-6733dc208006.png" style="width:15.83em;height:24.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.1 - Example of a convolution operation involving two vectors</div>
<p>In NumPy, we can achieve this by using the <kbd>convolve()</kbd> method as follows:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1])<br/>print(h)</pre>
<p>This outputs the following:</p>
<pre><span>[-2, 1, 2, 1, -2]</span></pre>
<p>Now, if you think about it, the most "complete" information is when the filter fully overlaps with the input data, and that is for <img class="fm-editor-equation" src="assets/1c903d14-4aa7-4725-8ac5-12b0e536298d.png" style="width:2.92em;height:1.00em;"/>. In Python, you can get that by using the <kbd>'valid'</kbd> argument as follows:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1], 'valid')<br/>print(h)</pre>
<p>This simply gives the following:</p>
<pre>2</pre>
<p>Once again, this is only to maximize the <em>relevant</em> information because the convolution operation is more <em>uncertain</em> around the edges of the vector, that is, at the beginning and the end where the vectors do not fully overlap. Furthermore, for convenience, we could obtain an output vector of the same size as the input by using the <kbd>'same'</kbd> argument as follows:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1], 'same')<br/>print(h)</pre>
<p>This prints the following:</p>
<pre>[1 2 1]</pre>
<p>Here are some practical reasons for each of the three ways of using convolution:</p>
<ul>
<li>Use <kbd>'valid'</kbd> when you need all the <em>good </em>information without any of the noise caused by the partial overlaps of the filter.</li>
<li>Use <kbd>'same'</kbd> when you want to make it easier for the computations to work. This will make it easy in the sense that you will have the same dimensions in the input and the output.</li>
<li>Otherwise, use nothing to obtain the full analytical solution to the convolution operation for any purposes that you want.</li>
</ul>
<div class="packt_infobox">Convolution became very popular with the surge of microprocessors specialized in multiplying and adding numbers extremely quickly and with the development of the <strong>fast Fourier transform</strong> (<strong>FFT</strong>) algorithm. The FFT exploits the mathematical property that convolution in the discrete time domain is equivalent to multiplication in the Fourier domain and vice versa.</div>
<p>Now, let's move on to the next dimension.</p>
<h2 id="uuid-d718049a-5acc-429c-a66c-b937396e4c8b">2-dimensions</h2>
<p>A two-dimensional convolution is very similar to the one-dimensional convolution. However, rather than having a vector, we will have a matrix, and that's why images are directly applicable here. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<div>Let's say that we have two matrices: one represents some input data, and the other is a filter, like so:</div>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/747990d4-5b12-4a09-a25d-26fb3ac8571f.png" style="width:18.17em;height:3.83em;"/>.</p>
<p>We can calculate the two-dimensional discrete convolution by inverting (in both dimensions) and shifting (also in both dimensions) the filter. The equation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6e04a69a-26bc-4b1a-b54b-76a0d621c719.png" style="width:42.00em;height:3.92em;"/></p>
<p>This is very similar to the one-dimensional version. The following diagram illustrates the first two steps and the last one, to save space and avoid repetition:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e1f06f2e-de3f-47be-8e56-57bf9822d95d.png" style="width:22.42em;height:25.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.2 - Two-dimensional discrete convolution example</div>
<p>In Python, we can calculate the two-dimensional convolution using SciPy's <span><kbd>convolve2d</kbd> method, as follows:</span></p>
<pre>import numpy as np<br/>from scipy.signal import convolve2d<br/>x = np.array([[2,2,2],[2,3,2],[2,2,2]])<br/>w = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])<br/>h = convolve2d(x,w)<br/>print(h)</pre>
<p><span>This outputs the following</span>:</p>
<pre>[[-2 -4 -6 -4 -2]<br/> [-4  9  5  9 -4]<br/> [-6  5  8  5 -6]<br/> [-4  9  5  9 -4]<br/> [-2 -4 -6 -4 -2]]</pre>
<p>The results shown here are the full analytical result. However, similar to the one-dimensional implementation, if you only want results that fully overlap, you can invoke a <kbd>'valid'</kbd> result, or if you want a result of the same size as the input, you can invoke the <kbd>'same'</kbd> alternative as follows:</p>
<pre>import numpy as np<br/>from scipy.signal import convolve2d<br/>x = np.array([[2,2,2],[2,3,2],[2,2,2]])<br/>w = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])<br/>h = convolve2d(x,w,mode='valid')<br/>print(h)<br/>h = convolve2d(x,w,mode='same')<br/>print(h)</pre>
<p>This would yield the following:</p>
<pre>[[8]]<br/><br/>[[9 5 9]<br/> [5 8 5]<br/> [9 5 9]]</pre>
<p>Now, let's move on to n-dimensional convolutions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<h2 id="uuid-3934766c-2c9d-4c7a-ade6-29784a6384b5">n-dimensions</h2>
<p>Once you have understood convolution in one and two dimensions, you have understood the basic concept behind it. However, you might still need to perform convolutions in larger dimensions, for example, in multispectral datasets. For this, we can simply prepare NumPy arrays of any number of  dimensions and then use SciPy's <kbd>convolve()</kbd> functionality. Consider the following example:</p>
<pre>import numpy as np<br/>from scipy.signal import convolve<br/>x = np.array([[[1,1],[1,1]],[[2,2],[2,2]]])<br/>w = np.array([[[1,-1],[1,-1]],[[1,-1],[1,-1]]])<br/>h = convolve(x,w)<br/>print(h)</pre>
<p>Here, vectors <img class="fm-editor-equation" src="assets/a239161e-8542-499f-bb20-3ad3a3fd22ca.png" style="width:5.58em;height:1.08em;"/> are three-dimensional arrays, and can be convolved successfully, producing the following output:</p>
<pre>[[[ 1 0 -1]<br/>  [ 2 0 -2]<br/>  [ 1 0 -1]]<br/><br/> [[ 3 0 -3]<br/>  [ 6 0 -6]<br/>  [ 3 0 -3]]<br/><br/> [[ 2 0 -2]<br/>  [ 4 0 -4]<br/>  [ 2 0 -2]]]</pre>
<p>The only difficult part about n-dimensional convolutions could be visualizing them or imagining them in your mind. We humans can easily understand one, two, and three dimensions, but larger dimensional spaces are tricky to illustrate. But remember, if you understand how convolution works in one and two dimensions, you can trust that the math works and the algorithms work in any dimensions. </p>
<p>Next, let's look at how to <em>learn</em> such convolutional filters by defining Keras layers and adding them to a model.</p>
<h1 id="uuid-3ac18e84-2222-4188-9832-bdf607f889bb">Convolutional layers</h1>
<p>Convolution has a number of properties that are very interesting in the field of deep learning:</p>
<ul>
<li>It can successfully encode and decode spatial properties of the data.</li>
<li>It can be calculated relatively quickly with the latest developments.</li>
<li>It can be used to address several computer vision problems.</li>
<li>It can be combined with other types of layers for maximum performance.</li>
</ul>
<p>Keras has wrapper functions for TensorFlow that involve the most popular dimensions, that is, one, two, and three dimensions: <kbd>Conv1D</kbd>, <kbd>Conv2D</kbd>, and <kbd>Conv3D</kbd>. In this chapter, we will continue to focus on two-dimensional convolutions, but be sure that if you have understood the concept, you can easily go ahead and use the others.</p>
<h2 id="uuid-7f254de3-c1c1-42b6-8133-ae3f30e3b03e">Conv2D</h2>
<p>The two-dimensional convolution method has the following signature: <kbd>tensorflow.keras.layers.Conv2D</kbd>. The most common arguments used in a convolutional layer are the following:</p>
<ul>
<li><kbd>filters</kbd> refers to the number of filters to be learned in this particular layer and affects the dimension of the output of the layer.</li>
<li><kbd>kernel_size</kbd> refers to the size of the filters; for example, in the case of <em>Figure 12.2</em>, it would be size (3,3).</li>
<li><kbd>strides=(1, 1)</kbd> is new for us. Strides is defined as the size of the steps that are taken when the filters are sliding across the input. All the examples we have shown so far assume that we follow the original definition of convolution and take unit steps. However, in convolutional layers, you can take larger steps, which will lead to smaller outputs but also the loss of information.</li>
<li><kbd>padding='valid'</kbd> refers to the way of dealing with the information in the edges of the convolution result. Note that the options here are only <kbd>'valid'</kbd> or <kbd>'same'</kbd>, and that there is no way of obtaining the full analytical result. The meaning is the same as we have seen before in this chapter. </li>
<li><kbd>activation=None</kbd> gives the option to include an activation function in the layer if you need one; for example, <kbd>activation='relu'</kbd>.</li>
</ul>
<p>To exemplify this, consider a convolutional layer such as the one shown in the following diagram, where the first layer is convolutional (in 2D) with 64 filters of size 9x9 and a stride of 2, 2 (that is, two in each direction). We will explain the rest of the model in the following diagram as we proceed:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/0025a4ec-aba0-4f83-9a6c-ce8f45623b47.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.3 - Architecture of a convolutional neural network for CIFAR 10</div>
<p>The first convolutional layer in the diagram can be defined as follows:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>Conv2D</strong><br/>input_shape = (1, 32, 32, 3)<br/>x = tf.random.normal(input_shape)<br/>l = <strong>Conv2D(64, (9,9), strides=(2,2), activation='relu',</strong> <br/>           input_shape=input_shape)(l)<br/>print(l.shape)</pre>
<p>This essentially will create a convolutional layer with the given specifications. The print statement will effectively produce the following:</p>
<pre>(1, 12, 12, 64)</pre>
<p>If you do the math, each and every single filter out of the 64 will produce a 23x23 <kbd>'valid'</kbd> output, but since a (2,2) stride is being used, an 11.5x11.5 output should be obtained. However, since we cannot have fractions, TensorFlow will round up to 12x12. Therefore, we end up with the preceding shape as the output. </p>
<h2 id="uuid-de11df58-c426-42bb-b79c-df9b15d5dc49">The layer+activation combo</h2>
<p>As mentioned previously, the <kbd>Conv2D</kbd> class has the ability to include an activation function of your choice. This is much appreciated because it will save some lines of code for all who want to learn to code efficiently. However, we have to be careful not to forget to document somewhere the type of activation used. </p>
<p><em>Figure 12.3</em> shows the activation in a separate block. This is a good idea to keep track of what activations are used throughout. The most common activation function for a convolutional layer is a ReLU, or any of the activations of the ReLU family, for example, leaky ReLU and ELU. The next <em>new </em>element is a pooling layer. Let's talk about this.</p>
<h1 id="uuid-351dc64d-6af2-437d-9097-0b63a532e957">Pooling strategies</h1>
<p><span>You will usually find pooling a</span>ccompanying convolutional layers. Pooling is an idea that is intended to reduce the number of computations by reducing the dimensionality of the problem. We have a few pooling strategies available to us in Keras, but the most important and popular ones are the following two:</p>
<ul>
<li>AveragePooling2D</li>
<li>MaxPooling2D</li>
</ul>
<p>These also exist for other dimensions, such as 1D. However, in order to understand pooling, we can simply look at the example in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/d6c68d88-284b-460f-abb6-48b69dfe190c.png" style="width:20.08em;height:14.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.4 - Max pooling example in 2D</div>
<p>In the diagram, you can observe how max pooling would look at individual 2x2 squares moving two spaces at a time, which leads to a 2x2 result. The whole point of pooling is to <strong>find a smaller summary of the data</strong> in question. When it comes to neural networks, we often look at neurons that are <em>excited</em> the most, and so it makes sense to look at the maximum values as good representatives of larger portions of data. However, remember that you can also look at the average of the data (<kbd>AveragePooling2D</kbd>), which is also good in all senses. </p>
<div class="packt_infobox">There is a slight difference in time performance in favor of max pooling, but this is very small.</div>
<p>In Keras, we can implement pooling very easily. In the case of max pooling in 2D, for example, we can simply do the following:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>MaxPooling2D</strong><br/>x = tf.constant([[-2, -4, -6, -4],<br/>                 [-4, 9, 5, 9],<br/>                 [-6, 5, 8, 5],<br/>                 [-4, 9, 5, 9]])<br/>x = tf.reshape(x, [1, 4, 4, 1])<br/><strong>y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')</strong><br/>print(tf.reshape(y(x), [2, 2]))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This produces the same output as in <em>Figure 12.4</em>:</p>
<pre>tf.Tensor(<br/>[[9 9]<br/> [9 9]], shape=(2, 2), dtype=int32)</pre>
<p>We can also do the same for average pooling as follows:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>AveragePooling2D</strong><br/>x = tf.constant([[-2., -4., -6., -4],<br/>                 [-4., 9., 5., 9.],<br/>                 [-6., 5., 8., 5.],<br/>                 [-4., 9., 5., 9.]])<br/>x = tf.reshape(x, [1, 4, 4, 1])<br/><strong>y = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')</strong><br/>print(tf.reshape(y(x), [2, 2]))</pre>
<p>This gives the following output:</p>
<pre>tf.Tensor(<br/>[[-0.25 1. ]<br/> [ 1. 6.75]], shape=(2, 2), dtype=float32)</pre>
<p>Both pooling strategies work perfectly fine in terms of summarizing the data. You will be safe in choosing either one. </p>
<p>Now for the big reveal. We will put all of this together in a CNN next.</p>
<h1 id="uuid-351d2255-c218-4bb2-b8ad-9e1152672fb4">Convolutional neural network for CIFAR-10</h1>
<p>We have reached the point where we can actually implement a fully functional CNN after looking at the individual pieces: understanding the convolution operation, understanding pooling, and understanding how to implement convolutional layers and pooling. Now we will be implementing the CNN architecture shown in <em>Figure 12.3</em>.</p>
<h2 id="uuid-56038b34-d126-4de4-8613-e447a2d024ca">Implementation</h2>
<p>We will be implementing the network in <em>Figure 12.3</em> step by step, broken down into sub-sections.</p>
<h3 id="uuid-88bd59fa-1323-4038-a8ab-af82bdebf891">Loading data</h3>
<p>Let's load the CIFAR-10 dataset as follows:</p>
<pre>from tensorflow.keras.datasets import <strong>cifar10</strong><br/>from tensorflow.keras.utils import to_categorical<br/>import numpy as np<br/><br/># The data, split between train and test sets:<br/>(x_train, y_train), (x_test, y_test) = <strong>cifar10</strong>.load_data()<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/><br/>y_train = to_categorical(y_train, 10)<br/>y_test = to_categorical(y_test, 10)<br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>This should effectively load the dataset and print its shape, which is as follows:</p>
<pre>x_train shape: (50000, 32, 32, 3)<br/>x_test shape: (10000, 32, 32, 3)</pre>
<p>This is very straightforward, but we can go further and verify that the data is loaded correctly by loading and plotting the first image of every class in the <kbd>x_train</kbd> set as follows:</p>
<pre>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>(_, _), (_, labels) = cifar10.load_data()<br/>idx = <span>[3, 6, 25, 46, 58, 85, 93, 99, 108, 133]</span><br/><br/>clsmap = {0: 'airplane',<br/>          1: 'automobile', <br/>          2: 'bird', <br/>          3: 'cat', <br/>          4: 'deer',<br/>          5: 'dog',<br/>          6: 'frog',<br/>          7: 'horse',<br/>          8: 'ship',<br/>          9: 'truck'}<br/><br/>plt.figure(figsize=(10,4))<br/>for i, (img, y) in enumerate(zip(x_test[idx].reshape(10, 32, 32, 3), labels[idx])):<br/>  plt.subplot(2, 5, i+1)<br/>  plt.imshow(img, cmap='gray')<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.title(str(y[0]) + ": " + clsmap[y[0]])<br/>plt.show()</pre>
<p>This will produce the <span><span>output</span></span> shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/3031d060-a19c-492f-91ca-df7a029dffb0.png" style="width:44.92em;height:19.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.5 - Samples of CIFAR-10</div>
<p>Next, we will implement the layers of the network.</p>
<h3 id="uuid-e62cc70c-681f-4614-9fbf-66462e3af260">Compiling the model</h3>
<p>Again, recall the model in <em>Figure 12.3</em>, and how we can implement it as shown here. Everything you are about to see is something we have looked at in this and previous chapters:</p>
<pre># Importing the Keras libraries and packages<br/>from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten<br/>from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import RMSprop<br/><br/># dimensionality of input and latent encoded representations<br/>inpt_dim = (32, 32, 3)<br/><br/>inpt_img = Input(shape=inpt_dim)<br/><br/># Convolutional layer<br/>cl1 = Conv2D(64, (9, 9), strides=(2, 2), input_shape = inpt_dim, <br/>             activation = 'relu')(inpt_img)<br/><br/># Pooling and BatchNorm<br/>pl2 = MaxPooling2D(pool_size = (2, 2))(cl1)<br/>bnl3 = BatchNormalization()(pl2)</pre>
<p>We continue adding more convolutional layers like so:</p>
<pre># Add a second convolutional layer<br/>cl4 = Conv2D(128, (3, 3), strides=(1, 1), activation = 'relu')(bnl3)<br/>pl5 = MaxPooling2D(pool_size = (2, 2))(cl4)<br/>bnl6 = BatchNormalization()(pl5)<br/><br/># Flattening for compatibility<br/>fl7 = Flatten()(bnl6)<br/><br/># Dense layers + Dropout<br/>dol8 = Dropout(0.5)(fl7)<br/>dl9 = Dense(units = 256, activation = 'relu')(dol8)<br/>dol10 = Dropout(0.2)(dl9)<br/>dl11 = Dense(units = 64, activation = 'relu')(dol10)<br/>dol12 = Dropout(0.1)(dl11)<br/>output = Dense(units = 10, activation = 'sigmoid')(dol12)<br/><br/>classifier = Model(inpt_img, output)</pre>
<p>Then we can compile the model and print a summary as follows:</p>
<pre># Compiling the CNN with RMSprop optimizer<br/>opt = RMSprop(learning_rate=0.001)<br/><br/>classifier.compile(optimizer = opt, loss = 'binary_crossentropy', <br/>                   metrics = ['accuracy'])<br/><br/>print(classifier.summary())</pre>
<p>This will output a summary of the network that will look like this:</p>
<pre>Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape          Param # <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 32, 32, 3)]   0 <br/>_________________________________________________________________<br/>conv2d (Conv2D)              (None, 12, 12, 64)    15616 <br/>_________________________________________________________________<br/>max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)      0 <br/>_________________________________________________________________<br/>batch_normalization (BatchNo (None, 6, 6, 64)      256 <br/>_________________________________________________________________<br/>.<br/>.<br/>.<br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 64)            0 <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 10)            650 <br/>=================================================================<br/>Total params: <strong>238,666</strong><br/>Trainable params: 238,282<br/>Non-trainable params: 384</pre>
<p>One thing that must be very obvious to you at this point is the number of parameters of this network. If you recall from the previous chapter, you will be surprised that this network has nearly a quarter of a million parameters, while the wide or deep network had a few million parameters. Furthermore, you will see shortly that this relatively small network, while still <em>overparameterized</em>, is going to perform better than the networks in the previous chapter that had more parameters.</p>
<p>Next, let's train the network.</p>
<h3 id="uuid-8f716761-1800-43a5-bbe9-ace5af7a29ea">Training the CNN</h3>
<p>We can train the CNN using the <em>callbacks</em> that we studied in <a href="03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml">Chapter 11</a>, <em>Deep and Wide Neural Networks</em>, to stop the network early if there is no progress, and to reduce the learning rate to focus the efforts of the gradient descent algorithm if it reaches a <em>plateau</em>.</p>
<p>We will train it as follows:</p>
<pre># Fitting the CNN to the images<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=35, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>hist = classifier.fit(x_train, y_train, batch_size=100, epochs=1000, <br/>                   callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                   validation_data=(x_test, y_test))<br/><br/>classifier.save_weights("cnn.hdf5")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The results of this will vary from computer to computer. For example, it may take fewer or more epochs, or the gradient might take a different direction if the mini-batches (which are selected at random) contain several edge cases. However, for the most part, you should get a similar result to this:</p>
<pre>Epoch 1/1000<br/>500/500 [==============================] - 3s 5ms/step - loss: 0.2733 - accuracy: 0.3613 - val_loss: 0.2494 - val_accuracy: 0.4078 - lr: 0.0010<br/>Epoch 2/1000<br/>500/500 [==============================] - 2s 5ms/step - loss: 0.2263 - accuracy: 0.4814 - val_loss: 0.2703 - val_accuracy: 0.4037 - lr: 0.0010<br/>.<br/>.<br/>.<br/>Epoch 151/1000<br/>492/500 [============================&gt;.] - ETA: 0s - loss: 0.0866 - accuracy: 0.8278<br/>Epoch 00151: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8275 - val_loss: 0.1153 - val_accuracy: 0.7714 - lr: 7.8125e-06<br/>Epoch 152/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.8285 - val_loss: 0.1154 - val_accuracy: 0.7707 - lr: 3.9063e-06<br/>Epoch 153/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0861 - accuracy: 0.8305 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06<br/>Epoch 154/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.8306 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06<br/>Epoch 155/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8295 - val_loss: 0.1153 - val_accuracy: 0.7715 - lr: 3.9063e-06<br/>Epoch 156/1000<br/>496/500 [============================&gt;.] - ETA: 0s - loss: 0.0857 - accuracy: 0.8315Restoring model weights from the end of the best epoch.<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0857 - accuracy: 0.8315 - val_loss: 0.1153 - val_accuracy: 0.7713 - lr: 3.9063e-06<br/>Epoch 00156: early stopping</pre>
<p>At this point, when the training is finished, you can get an estimate of the accuracy of 83.15%. Be careful, this is not a <strong>balanced</strong> accuracy. For that, we will take a look at the <strong>Balanced Error Rate</strong> (<strong>BER</strong>) metric in the next section. But before we do that, we can look at the training curve to see how the loss was minimized.</p>
<p class="mce-root"/>
<p>The following code will produce what we want:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], color='#dc267f')<br/>plt.title('Model Loss Progress')<br/>plt.ylabel('Brinary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p>This gives the plot shown in <em>Figure 12.6</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/cbf15643-e4bd-451a-86c6-1d71011cb5c4.png" style="width:39.50em;height:24.50em;"/></div>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.6 - Loss minimization for a CNN on CIFAR-10</div>
<p>From this diagram, you can appreciate the bumps that the learning curve has, particularly visible on the training set curve, which are due to the reduction in the learning rate through the callback function, <kbd>ReduceLROnPlateau</kbd>. The training stops after the loss no longer improves on the test set, thanks to the <kbd>EarlyStopping</kbd> callback. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<h2 id="uuid-a714e0cc-30fc-42d5-b213-db25ea7235ff">Results</h2>
<p><span>Now, let's look at objective, numerical results:</span></p>
<pre>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import balanced_accuracy_score<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>(_, _), (_, labels) = cifar10.load_data()<br/><br/>y_ = labels<br/>y_hat = classifier.predict(x_test)<br/>y_pred = np.argmax(y_hat, axis=1)<br/><br/>print(classification_report(np.argmax(y_test, axis=1), <br/>                            np.argmax(y_hat, axis=1), <br/>                            labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))<br/>cm = confusion_matrix(np.argmax(y_test, axis=1), <br/>                      np.argmax(y_hat, axis=1), <br/>                      labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/>print(cm)<br/>ber = 1- balanced_accuracy_score(np.argmax(y_test, axis=1), <br/>                                 np.argmax(y_hat, axis=1))<br/>print('BER', ber)</pre>
<p>This will give us the following numerical results, which we can compare with the results from the previous chapter:</p>
<pre>  precision  recall  f1-score  support<br/><br/>0      0.80    0.82      0.81     1000<br/>1      0.89    0.86      0.87     1000<br/>2      0.73    0.66      0.69     1000<br/>3      0.57    0.63      0.60     1000<br/>4      0.74    0.74      0.74     1000<br/>5      0.67    0.66      0.66     1000<br/>6      0.84    0.82      0.83     1000<br/>7      0.82    0.81      0.81     1000<br/>8      0.86    0.88      0.87     1000<br/>9      0.81    0.85      0.83     1000<br/><br/>               accuracy  0.77     10000<br/><br/>[[821  12  36  18  12   8   4   4  51  34]<br/> [ 17 860   3   7   2   6   8   1  22  74]<br/> [ 61   2 656  67  72  53  43  24  11  11]<br/> [ 11   7  47 631  55 148  38  36  10  17]<br/> [ 21   2  48  63 736  28  31  54  12   5]<br/> [ 12   3  35 179  39 658  16  41   4  13]<br/> [  2   4  32  67  34  20 820   8   8   5]<br/> [ 12   3  18  41  42  52   5 809   3  15]<br/> [ 43  22  12  12   2   5   3   0 875  26]<br/> [ 29  51  10  19   2   3   5   9  26 846]]<br/><br/>BER 0.2288</pre>
<p>Accuracy for specific classes can be as high as 87%, while the lowest accuracy is 66%. This is much better than the previous models in the previous chapter. The BER is of 0.2288, which can all be interpreted as a balanced accuracy of 77.12%. This matches the accuracy reported in the test set during training, which indicates that the model was trained properly. For comparison purposes, the following diagram shows a visual representation of the confusion matrix:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"> <img src="assets/db92150e-3920-4e15-b975-40e7e6f287f7.png" style="width:29.08em;height:25.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.7 - Confusion matrix for a CNN trained over CIFAR-10</div>
<p>It might be a bit clearer from the visual confusion matrix that classes 3 and 5 can be confused between themselves more than other classes. Classes 3 and 5 correspond to cats and dogs, respectively.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>That's it. As you can see, this is a nice result already, but you could perform more experiments on your own. You can edit and add more convolutional layers to your model and make it better. If you are curious, there are other larger CNNs that have been very successful. Here are the two most famous ones:</p>
<ul>
<li>VGG-19: This contains 12 convolutional layers and 3 dense layers (Simonyan, K., et al. (2014)).</li>
<li>ResNet: This contains 110 convolutional layers and 1 dense layer (He, K., et al. (2016)). This particular configuration can achieve an error rate as low as 6.61% (±0.16%) on CIFAR-10.</li>
</ul>
<p>Let's discuss next how to visualize the filters <span>learned</span>.</p>
<h2 id="uuid-557bd542-2709-4a09-8a81-52d27b6af210">Visualization of filters</h2>
<p>This last piece in this chapter deals with the visualization of the learned filters. This may be useful to you if you want to do research on what the network is learning. It may help with the <em>explainability </em>of the network. However, note that the deeper the network is, the more complicated it gets to understand it.</p>
<p>The following code will help you visualize the filters of the first convolutional layer of the network:</p>
<pre>from sklearn.preprocessing import MinMaxScaler<br/><br/>cnnl1 = classifier.layers[1].name   # get the name of the first conv layer<br/>W = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters<br/>wshape = W.shape  #save the original shape<br/><br/># this part will scale to [0, 1] for visualization purposes<br/>scaler = MinMaxScaler()<br/>scaler.fit(W.reshape(-1,1))<br/>W = scaler.transform(W.reshape(-1,1))<br/>W = W.reshape(wshape)<br/><br/># since there are 64 filters, we will display them 8x8<br/>fig, axs = plt.subplots(8,8, figsize=(24,24))<br/>fig.subplots_adjust(hspace = .25, wspace=.001)<br/>axs = axs.ravel()<br/>for i in range(W.shape[-1]):<br/>  # we reshape to a 3D (RGB) image shape and display<br/>  h = np.reshape(W[:,:,:,i], (9,9,3))<br/>  axs[i].imshow(h)<br/>  axs[i].set_title('Filter ' + str(i))</pre>
<p class="mce-root"/>
<p>This code depends heavily on knowing which layer you want to visualize, the number of filters you want to visualize, and the size of the filters themselves. In this case, we want to visualize the first convolutional layer. It has 64 filters (displayed in an 8x8 grid), and each filter is 9x9x3 because the input is color images. <em>Figure 12.8</em> shows the resulting plot of the code shown previously:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/f37fe0c8-877d-4ae1-8d94-b468a47f6295.png" style="width:46.33em;height:47.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12.8 - Filters learned in the first convolutional layer</div>
<p>If you are an expert in image processing, you may recognize some of these patterns as they resemble Gabor filters (Jain, A. K., et al. (1991)). Some of these filters are designed to look for edges, textures, or specific shapes. The literature suggests that in convolutional networks, deeper layers usually encode highly complex information, while the first layers are used to detect features such as edges.</p>
<p>Feel free to go ahead and try to display another layer by making the necessary modifications.</p>
<h1 id="uuid-4c7dcb24-fcba-499d-bf73-6fab4e48d6e5">Summary</h1>
<p>This intermediate chapter showed how to create CNNs. You learned about the convolution operation, which is the fundamental concept behind them. You also learned how to create convolutional layers and aggregated pooling strategies. You designed a network to learn filters to recognize objects based on CIFAR-10 and learned how to display the learned filters.</p>
<p>At this point, you should feel confident explaining the motivation behind convolutional neural networks rooted in computer vision and signal processing. You should feel comfortable coding the convolution operation in one and two dimensions using NumPy, SciPy, and Keras/TensorFlow. Furthermore, you should feel confident implementing convolution operations in layers and learning filters through gradient descent techniques. If you are asked to show what the network has learned, you should feel prepared to implement a simple visualization method to display the filters learned.</p>
<p>CNNs are great at encoding highly correlated spatial information, such as images, audio, or text. However, there is an interesting type of network that is meant to encode information that is sequential in nature. <a href="a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml">Chapter 13</a>, <em>Recurrent Neural Networks</em>, will present the most fundamental concepts of recurrent networks, leading to long short-term memory models. We will explore multiple variants of sequential models with applications in image classification and natural language processing.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-d0e9aa9d-bec6-4548-b8cd-b76e2527e6f1">Questions and answers</h1>
<ol>
<li><strong>What data summarization strategy discussed in this chapter can reduce the dimensionality of a convolutional model?</strong></li>
</ol>
<p style="padding-left: 60px">Pooling.</p>
<ol start="2">
<li><strong>Does adding more convolutional layers make the network better? </strong></li>
</ol>
<p style="padding-left: 60px">Not always. It has been shown that more layers has a positive effect on networks, but there are certain occasions when there is no gain. You should determine the number of layers, filter sizes, and pooling experimentally.</p>
<ol start="3">
<li><strong>What other applications are there for CNNs?</strong></li>
</ol>
<p style="padding-left: 60px">Audio processing and classification; image denoising; image super-resolution; text summarization and other text-processing and classification tasks; the encryption of data.</p>
<h1 id="uuid-1a08f56c-8d54-4b50-a37f-627f094cd410">References</h1>
<ul>
<li>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). <em>Backpropagation applied to handwritten zip code recognition</em>. <em>Neural computation</em>, 1(4), 541-551.</li>
<li><span>Li, Y. D., Hao, Z. B., and Lei, H. (2016). <em>Survey of convolutional neural networks</em>. <em>Journal of Computer Applications</em>, 36(9), 2508-2515.</span></li>
<li>Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). <em>Imagenet classification with deep convolutional neural networks</em>. In <em>Advances in neural information processing systems</em> (pp. 1097-1105).</li>
<li>Simonyan, K., and Zisserman, A. (2014). <em>Very deep convolutional networks for large-scale image recognition</em>. arXiv preprint arXiv:1409.1556.</li>
<li>He, K., Zhang, X., Ren, S., and Sun, J. (2016). <em>Deep residual learning for image recognition</em>. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).</li>
<li>Jain, A. K., and Farrokhnia, F. (1991). <em>Unsupervised texture segmentation using Gabor filters</em>. <em>Pattern recognition</em>, 24(12), 1167-1186.</li>
</ul>


            </article>

            
        </section>
    </body></html>