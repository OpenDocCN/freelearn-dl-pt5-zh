- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders are a type of model that was built mainly to accomplish **representation
    learning**. Representation learning is a type of deep learning task that focuses
    on generating a compact and representative feature to represent any single data
    sample, be it image, text, audio, video, or multimodal data. After going through
    some form of representation learning, a model will be able to map inputs into
    more representable features, which can be used to differentiate itself from other
    sample inputs. The representation obtained will exist in a latent space where
    different input samples will co-exist together. These representations are also
    known as **embeddings**. The applications of autoencoders will be tied closely
    to representation learning applications, and some applications include generating
    predictive features for other subsequent supervised learning objectives, comparing
    and contrasting samples in the wild, and performing effective sample recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Note that autoencoders are not the only way to execute representation learning.
    The topic of representation learning will be discussed further in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*, and [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised* *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know that an autoencoder learns to generate distinctive representations
    or, in other words, embeddings. But what’s the architecture like? Let’s discover
    a standard form of the architecture and then discover a couple more useful advancements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the standard autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring autoencoder variations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Seaborn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NumPy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Keras`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PyTorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5).'
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the standard autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders are more of a concept than an actual neural network architecture.
    This is due to the fact that they can be based on different base neural network
    layers. When dealing with images, you build CNN autoencoders, and when dealing
    with text, you might want to build RNN autoencoders. When dealing with multimodal
    datasets with images, text, audio, numerical, and categorical data, well, you
    use a combination of different layers as a base. Autoencoders are mainly based
    on three components, called the **encoder**, the **bottleneck layers**, and the
    **decoder**. This is illustrated in *Figure 5**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The autoencoder concept](img/B18187_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The autoencoder concept
  prefs: []
  type: TYPE_NORMAL
- en: The encoder for a standard autoencoder typically takes in high-dimensional data
    and compresses it to an arbitrary scale smaller than the original data dimensions,
    which will result in what is known as a **bottleneck representation**, where it
    ties itself to the bottleneck, signifying a compact representation without any
    useless information. The bottleneck component then gets passed into the decoder,
    where it will expand the dimensionality using the exact opposite of the scale
    used by the encoder, resulting in output with the same dimensions as the input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Encoder and decoder structures are not exclusive to autoencoders but are also
    utilized in other architectures, such as transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The difference is the bottleneck component that holds the representative features.
    It is commonly compressed and in smaller dimensions, but sometimes, it can be
    made larger to hold more representative features for predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders, in general, are trained to reconstruct input data. The training
    process of the autoencoder model involves comparing the distance between the generated
    output data and the input data. After being optimized to generate the input data,
    when the model is capable of reconstructing the original input data completely,
    it can be said that the bottleneck has a more compact and summarized representation
    of the input data than the original input data itself. The compact representation
    can then be used subsequently to achieve other tasks, such as sample recognition,
    or can even be used generally to save space, by storing the smaller bottleneck
    feature instead of the original large input data. The encoder and decoder are
    not constrained to a single layer and can be defined with multiple layers. However,
    the standard autoencoder only has a single bottleneck feature. This is also known
    as **code**, or a **latent feature**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore the different variations of autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring autoencoder variations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For tabular data, the network structure can be pretty straightforward. It simply
    uses an MLP with multiple fully connected layers that gradually shrink the number
    of features for the encoder, and multiple fully connected layers that gradually
    increase the data outputs to the same dimension and size as the input for the
    decoder.
  prefs: []
  type: TYPE_NORMAL
- en: For time-series or sequential data, RNN-based autoencoders can be used. One
    of the most cited research projects about RNN-based autoencoders is a version
    where LSTM-based encoders and decoders are used. The research paper is called
    *Sequence to Sequence Learning with Neural Networks* by Ilya Sutskever, Oriol
    Vinyals, and Quoc V. Le ([https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)).
    Instead of stacking encoder LSTMs and decoder LSTMs, using the hidden state output
    sequence of each of the LSTM cells vertically, the decoder layer sequentially
    continues the sequential flow of the encoder LSTM and outputs the reconstructed
    input in reversed order. An additional decoder LSTM layer was also used to concurrently
    optimize, to predict future sequences. This structure is shown in *Figure 5**.2*.
    Note that it is also possible to adapt this to the video image modality using
    raw flattened image pixels as input. The architecture is also called **seq2seq**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – An LSTM-based autoencoder structure](img/B18187_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – An LSTM-based autoencoder structure
  prefs: []
  type: TYPE_NORMAL
- en: The hidden state outputs of the encoder LSTM can be considered to be the latent
    feature of the LSTM autoencoder. Transformers, a new architecture that can also
    deal with sequential data, which will be introduced later in this chapter, have
    some variations that can also be considered a kind of autoencoder – some transformers
    can be autoencoders, but not all transformers are autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: For image data, by using convolutional layers, we can scale down the features
    gradually with multiple convolutional and pooling layers, until the stage where
    a global pooling layer is applied and the data becomes a 1-dimensional feature.
    This represents the encoder of the autoencoder generating the bottleneck feature.
    This workflow is the same as the one we discussed in the previous section on CNN.
    However, for the decoder, to scale up the 1-dimensional pooled feature into 2-dimensional
    image-like data again, a special form of convolution is needed, called the **transpose
    convolution**.
  prefs: []
  type: TYPE_NORMAL
- en: The variations mentioned in this topic are all about using the standard autoencoder
    structures but implementing them using different neural network types. There are
    also two additional variations of the autoencoder, where one variation is based
    on data input manipulation, and another variation is based on an actual modification
    of the autoencoder structure to achieve the data generation goal.
  prefs: []
  type: TYPE_NORMAL
- en: For the variation used for data manipulation, the idea is to add noise to the
    input data during training and maintain the original input data without the added
    noise, to be used as the target to predictively reconstruct the data. This variation
    of the autoencoder is called a **denoising autoencoder** because its goal is primarily
    to *denoise* the data. Since the goal has shifted from compressing the data to
    denoising the data, the bottleneck features are not restricted to being small
    in size. The features that are utilized afterward are not constrained to just
    the single bottleneck feature but, instead, can be the features from multiple
    intermediate layers in the network, or just simply the denoised reconstructed
    output. This method takes advantage of the innate capability of neural networks
    to perform automated feature engineering. The most notable usage of a denoising
    autoencoder is in the first-place solution of a Kaggle competition based on tabular
    data, hosted by Porto Seguro, an insurance company, where multiple features from
    intermediate layers were fed into a separate MLP to predict whether a driver will
    file an insurance claim in the future ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)).
  prefs: []
  type: TYPE_NORMAL
- en: For the variation that uses a modification of the autoencoder structure, the
    idea is to produce two bottleneck feature vectors that represent a list of standard
    deviation values and mean values so that different bottleneck feature values can
    be sampled, based on the mean and standard deviation values. These sampled bottleneck
    feature values can then be passed into the decoder to generate new random data
    outputs. This variation of the autoencoder is called a **variational autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered a good overview of autoencoder variations, let’s dig
    further into CNN autoencoders and build a CNN autoencoder using deep learning
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by going through what a **transpose convolution** is. *Figure 5**.3*
    shows an example transpose convolution operation on a 2x2 sized input with a 2x2
    sized convolutional filter, with a stride of 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – A transposed convolutional filter operation](img/B18187_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – A transposed convolutional filter operation
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.3*, note that each of the 2x2 input data is marked with a number
    from **1** to **4**. These numbers are used to map the output results, presented
    as 3x3 outputs. The convolutional kernel applies each of its weights individually
    to every value in the input data in a sliding window manner, and the outputs from
    the four convolutional operations are presented in the bottom part of the figure.
    After the operation is done, each of the outputs will be elementwise added to
    form the final output and subjected to a bias. This example process depicts how
    a 2x2 input can be scaled up to a 3x3 data size without relying completely on
    padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a convolutional autoencoder model in `Pytorch` below and train
    it on the `Fashion MNIST` image dataset, an image dataset comprising fashion items
    such as shoes, bags, and clothes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the overall convolutional autoencoder structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code presented here is a convolutional autoencoder structure in `PyTorch`
    with an encoder and decoder variable placeholder. The encoder is responsible for
    taking in an image and reducing its dimensionality until it has a single dimension
    with a small representation footprint – the bottleneck feature. The decoder will
    then take in the bottleneck feature and produce a feature map of the same size
    as the original input image. The encoder and decoder will be defined in the next
    two steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The encoder will be designed to take in a grayscale image (one channel) of
    size 28x28\. This image dimension is the default size of the Fashion MNIST image
    dataset. The following logic shows the code to define the encoder, replacing the
    placeholder defined in *step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The defined encoder has two convolutional layers, each followed by the non-linear
    activation, `ReLU`, and a pooling layer. The filter sizes of the convolutional
    layers are `16` and `4` The second pooling layer is a global average pooling layer
    meant to reduce the 4x9x9 feature map to 4x1x1, where each channel will have only
    one value to represent itself. This means that the encoder will squeeze the dimensionality
    of the original 28x28 image, which adds up to around 784 pixels for only four
    features, which is a 99.4% compression rate!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The decoder will then take these four features per image and produce an output
    feature map of 28x28 again, reproducing the original image size. The entire model
    has no padding applied. The decoder will be defined as follows, replacing the
    placeholder decoder defined in *step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Three convolutional transpose layers are used here. Each convolutional layer
    is followed by a non-linear activation layer, where the first two layers used
    `ReLU` (being the standard non-linear activation) and the last layer used `sigmoid`.
    `sigmoid` is used here, as the `fashion MNIST` data is already normalized to have
    values between `0` and `1`. The convolutional transpose layer defined here adopts
    a similar number of filter configurations from the encoder, from `16` to `4` and
    finally, to `1` filter to produce only one channel grayscale image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have defined the convolutional autoencoder, let’s load up the fashion
    MNIST data from the `torchvision` library. This tutorial will use the `Catalyst`
    library for ease of training, so let’s take the `fashion MNIST` dataset loader
    and feeder class from `torchvision` and modify it for usage in the `Catalyst`
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The class from `torchvision` already has the necessary logic to download and
    load the `fashion MNIST` dataset. However, the data feeder method, `getitem`,
    is not in the expected format for image generation and, thus, requires this modification
    for this experiment to work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note that the `Pillow` library is used to load the image in *step 5*. This
    is so that we can easily use the tool from `torchvision` to perform different
    transformation steps, such as image augmentation. However, in this experiment,
    we will directly convert the `pillow` image into `Pytorch` tensors, using the
    transform logic that follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load the training and validation datasets of `fashion MNIST`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code downloads the dataset into the `fashion_mnist` folder if
    it doesn’t already exist. Additionally, the `loaders` variable is here so that
    it can be consumed by the `Catalyst` library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since the optimization goal is to reduce the different of reproduced pixel
    values compared to the target pixel values, we will use the mean squared error
    as the reconstruction loss here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It’s important to note that while reconstruction loss is a common objective
    in unsupervised representation learning, there may be other metrics or objectives
    used, depending on the specific algorithm or approach. For example, in `Catalyst`
    so that we can train our model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define a generally usable function that can make it easy to perform
    multiple training and validation experiments through code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These are just basic training and evaluation boiler code without any tricks
    applied. We will explore in depth the tricks to train supervised models in [*Chapter
    8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised* *Deep Learning*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are ready to train and evaluate the CNN autoencoder model through the
    following logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The best performing `cnn_autoencoder` weights based on the validation loss will
    be automatically loaded after training on 20 epochs is complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After training the preceding model with the provided training code, on the
    Fashion MNIST dataset with 1x28x28 image dimensions, you should get something
    similar to the example input/output pairs shown in *Figure 5**.4* through the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the
    bottom represents the original image, and the top represents the reproduced image](img/B18187_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the bottom
    represents the original image, and the top represents the reproduced image
  prefs: []
  type: TYPE_NORMAL
- en: With the results, it’s clear that the bottleneck features are capable of reproducing
    the entire picture somewhat completely, and they can be treated as more representative
    and compact features for use instead of the original input data. Increasing the
    number of representation values of the bottleneck feature from 4 to something
    like 10 features should also increase the quality of the reproducible image. Feel
    free to try it out and experiment with the parameters!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders are considered a fundamental method to achieve representation learning
    across data modalities. Consider the architecture as a shell that you can fit
    in a variety of other neural network components, allowing you to ingest data of
    different modalities or benefit from more advanced neural network components.
  prefs: []
  type: TYPE_NORMAL
- en: However, do note that they are not the only method to learn representative features.
    There are many more applications for autoencoders that primarily revolve around
    different training objectives using the same architecture. Two of these adaptations
    that were briefly introduced in this chapter are denoising autoencoders and variational
    autoencoders, which will be introduced properly in [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised Deep Learning*. Now, let’s shift gears again to discover
    the model family of transformers!
  prefs: []
  type: TYPE_NORMAL
