["```py\n>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```", "```py\ndef sum_squared_error(y,  t):\n    return 0.5 * np.sum((y-t)**2)\n```", "```py\n>>> # Assume that \"2\" is correct\n>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n>>>\n>>>  #  Example 1: \"2\" is the most probable (0.6)\n>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n>>> sum_squared_error(np.array(y), np.array(t))\n0.097500000000000031\n>>>\n>>>  #  Example 2: \"7\" is the most probable (0.6)\n>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n>>> sum_squared_error(np.array(y), np.array(t))\n0.59750000000000003\n```", "```py\ndef cross_entropy_error(y, t):\n    delta = 1e-7\n    return -np.sum(t * np.log(y + delta))\n```", "```py\n>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n>>> cross_entropy_error(np.array(y), np.array(t))\n0.51082545709933802\n>>>\n>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n>>> cross_entropy_error(np.array(y), np.array(t))\n2.3025840929945458\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nimport numpy as np\nfrom dataset.mnist import load_mnist\n(x_train,  t_train),  (x_test,  t_test)  =  /\n    load_mnist(normalize=True, one_hot_label=True)\nprint(x_train.shape) # (60000, 784)\nprint(t_train.shape) # (60000, 10)\n```", "```py\ntrain_size = x_train.shape[0]\nbatch_size = 10\nbatch_mask = np.random.choice(train_size, batch_size) \nx_batch = x_train[batch_mask]\nt_batch = t_train[batch_mask]\n```", "```py\n>>> np.random.choice(60000, 10)\narray([ 8013, 14666, 58210, 23832, 52091, 10153, 8107, 19410, 27260,\n21411])\n```", "```py\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n```", "```py\ndef cross_entropy_error(y, t): \n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n```", "```py\n# Bad implementation sample\ndef numerical_diff(f,  x): \n    h = 10e-50\n    return (f(x+h) - f(x)) / h\n```", "```py\n>>> np.float32(1e-50)\n0.0\n```", "```py\ndef numerical_diff(f,  x): \n    h = 1e-4 # 0.0001\n    return (f(x+h) - f(x-h)) / (2*h)\n```", "```py\ndef function_1(x):\n    return 0.01*x**2 + 0.1*x\n```", "```py\nimport numpy as np\nimport  matplotlib.pylab  as  plt\nx = np.arange(0.0, 20.0, 0.1) # The array x containing 0 to 20 in increments of 0.1\ny = function_1(x)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.plot(x, y)\nplt.show()\n```", "```py\n>>> numerical_diff(function_1, 5)\n0.1999999999990898\n>>> numerical_diff(function_1, 10)\n0.2999999999986347\n```", "```py\ndef function_2(x):\n    return x[0]**2 + x[1]**2\n    # or return np.sum(x**2)\n```", "```py\n>>> def function_tmp1(x0):\n...\treturn x0*x0 + 4.0**2.0\n...\n>>> numerical_diff(function_tmp1, 3.0)\n6.00000000000378\n```", "```py\n>>> def function_tmp2(x1):\n...\treturn 3.0**2.0 + x1*x1\n...\n>>> numerical_diff(function_tmp2, 4.0)\n7.999999999999119\n```", "```py\ndef numerical_gradient(f,   x): \n    h = 1e-4 # 0.0001\n    grad = np.zeros_like(x) # Generate an array with the same shape as x\n    for idx in range(x.size):\n        tmp_val = x[idx]\n        # Calculate f(x+h)\n        x[idx] = tmp_val + h\n        fxh1 = f(x)\n        # Calculate f(x-h)\n        x[idx] = tmp_val - h\n        fxh2 = f(x)\n        grad[idx] = (fxh1 - fxh2) / (2*h)\n        x[idx] = tmp_val # Restore the original value\n    return grad\n```", "```py\n>>> numerical_gradient(function_2, np.array([3.0, 4.0]))\narray([ 6., 8.])\n>>> numerical_gradient(function_2, np.array([0.0, 2.0]))\narray([ 0., 4.])\n>>> numerical_gradient(function_2, np.array([3.0, 0.0]))\narray([  6.,    0.])\n```", "```py\ndef gradient_descent(f,  init_x,  lr=0.01,  step_num=100): \n    x = init_x\n    for i in range(step_num):\n        grad = numerical_gradient(f, x)\n        x -= lr * grad\n    return x\n```", "```py\n>>> def function_2(x):\n...    return x[0]**2 + x[1]**2\n...\n>>> init_x = np.array([-3.0, 4.0])\n>>> gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)\narray([ -6.11110793e-10, 8.14814391e-10])\n```", "```py\n# When the learning rate is too large: lr=10.0\n>>> init_x = np.array([-3.0, 4.0])\n>>>  gradient_descent(function_2,  init_x=init_x,  lr=10.0,  step_num=100)\narray([ -2.58983747e+13, -1.29524862e+12])\n# When the learning rate is too small: lr=1e-10\n>>> init_x = np.array([-3.0, 4.0])\n>>> gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)\narray([-2.99999994, 3.99999992])\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nimport numpy as np\nfrom common.functions import softmax, cross_entropy_error\nfrom common.gradient import numerical_gradient\nclass simpleNet:\n    def __ init __ (self):\n        self.W = np.random.randn(2,3) # Initialize with a Gaussian distribution\n    def predict(self, x):\n        return np.dot(x, self.W)\n    def loss(self, x, t):\n        z = self.predict(x) \n        y = softmax(z)\n        loss = cross_entropy_error(y, t)\n        return  loss\n```", "```py\n>>> net = simpleNet()\n>>> print(net.W) # Weight parameters\n[[ 0.47355232 0.9977393 0.84668094]\n[ 0.85557411 0.03563661 0.69422093]]\n>>>\n>>> x = np.array([0.6, 0.9])\n>>> p = net.predict(x)\n>>> print(p)\n[ 1.05414809 0.63071653 1.1328074]\n>>> np.argmax(p) # Index for the maximum value\n2\n>>>\n>>> t = np.array([0, 0, 1]) # Correct label\n>>> net.loss(x, t)\n0.92806853663411326\n```", "```py\n>>> def f(W):\n...    return net.loss(x, t)\n...\n>>> dW = numerical_gradient(f, net.W)\n>>> print(dW)\n[[ 0.21924763 0.14356247 -0.36281009]\n [ 0.32887144 0.2153437 -0.54421514]]\n```", "```py\n>>> f = lambda w: net.loss(x, t)\n>>> dW = numerical_gradient(f, net.W)\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nfrom common.functions import *\nfrom common.gradient import numerical_gradient\nclass TwoLayerNet:\n    def __ init __ (self, input_size, hidden_size, output_size,\n                          weight_init_std=0.01):\n        # Initialize weights\n        self.params = {}\n        self.params['W1'] = weight_init_std * /\n                            np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size) \n        self.params['W2'] = weight_init_std * /\n                            np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n    def predict(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 =  np.dot(z1, W2) + b2\n        y = softmax(a2)\n        return y\n    # x: input data, t: label data\n    def loss(self, x, t):\n        y = self.predict(x)\n        return cross_entropy_error(y, t)\n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n    # x: input data, t: teacher data\n    def numerical_gradient(self, x, t):\n        loss_W = lambda  W:  self.loss(x,  t)\n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1']) \n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2']) \n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        return grads\n```", "```py\nnet = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\nnet.params['W1'].shape # (784, 100)\nnet.params['b1'].shape # (100,)\nnet.params['W2'].shape # (100, 10)\nnet.params['b2'].shape # (10,)\n```", "```py\nx = np.random.rand(100, 784) # Dummy input data (for 100 images)\ny = net.predict(x)\n```", "```py\nx = p.random.rand(100, 784) # Dummy input data (for 100 images)\nt = np.random.rand(100, 10) # Dummy correct label (for 100 images)\ngrads = net.numerical_gradient(x, t) # Calculate gradients\ngrads['W1'].shape # (784, 100)\ngrads['b1'].shape # (100,)\ngrads['W2'].shape # (100, 10)\ngrads['b2'].shape # (10,)\n```", "```py\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom two_layer_net import TwoLayerNet\n(x_train, t_train), (x_test, t_test) = \\\n    load_mnist(normalize=True, one_hot_label=True)\ntrain_loss_list = []\n# Hyper-parameters\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\nfor i in range(iters_num):\n    # Obtain a mini-batch\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    # Calculate a gradient\n    grad = network.numerical_gradient(x_batch, t_batch)\n    # grad = network.gradient(x_batch, t_batch) # fast version!\n    # Update the parameters\n    for key in ('W1', 'b1', 'W2', 'b2'): \n        network.params[key] -= learning_rate * grad[key]\n    # Record learning progress\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n```", "```py\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom two_layer_net import TwoLayerNet\n(x_train, t_train), (x_test, t_test) = \\\n    load_mnist(normalize=True, one_hot_label=True)\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n# Number of iterations per epoch\niter_per_epoch = max(train_size / batch_size, 1)\n# Hyper-parameters\niters_num = 10000\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, \noutput_size=10)\nfor i in range(iters_num):\n    # Obtain a mini-batch\n    batch_mask = np.random.choice(train_size, batch_size) \n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    # Calculate a gradient\n    grad = network.numerical_gradient(x_batch, t_batch) \n    # grad = network.gradient(x_batch, t_batch) # Quick version!\n    # Update the parameters\n    for key in ('W1', 'b1', 'W2', 'b2'): \n        network.params[key] -= learning_rate * grad[key]\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    # Calculate recognition accuracy for each epoch\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\ntest_acc_list.append(test_acc)\n        print(\"train acc, test acc | \" + str(train_acc) + \" , \" + str(test_acc))\n```"]