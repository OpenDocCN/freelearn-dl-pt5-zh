<html><head></head><body>
		<div id="_idContainer004">
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Preface</h1>
			<p><strong class="bold">Reinforcement</strong> <strong class="bold">Learning</strong> (<strong class="bold">RL</strong>) is a field of artificial intelligence used for creating self-learning autonomous agents. This book takes a pragmatic approach to RL and uses practical examples inspired by real-world business and industry problems to teach you about RL techniques.</p>
			<p>Starting with an overview of RL elements, you'll get to grips with Markov chains and Markov decision processes, which comprise the mathematical foundations of modeling an RL problem. You'll then cover Monte Carlo methods and <strong class="bold">temporal</strong> <strong class="bold">difference</strong> (<strong class="bold">TD</strong>) learning methods that are used for solving RL problems. Next, you'll learn about deep Q-learning, policy gradient algorithms, actor-critic methods, model-based methods, and multi-agent reinforcement learning. As you advance, you'll delve into many novel algorithms with advanced implementations using modern Python libraries. You'll also find out how to implement RL to solve real-world challenges faced in areas such as autonomous systems, supply chain management, games, finance, smart cities, and cybersecurity. Finally, you'll gain a clear understanding of which method to use and when, how to avoid common pitfalls, and how to overcome challenges faced in implementing RL.</p>
			<p>By the end of this reinforcement learning book, you'll have mastered how to train and deploy your own RL agents for solving RL problems.</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Who this book is for</h1>
			<p>This book is for expert machine learning practitioners and deep learning researchers looking to implement advanced RL concepts in real-world projects. This book will also appeal to RL experts who want to tackle complex sequential decision-making problems through self-learning agents. Working knowledge of Python programming and machine learning along with prior experience RL is required.</p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>What this book covers</h1>
			<p><a href="B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Reinforcement Learning</em>, provides an introduction to RL, gives motivating examples and success stories, and looks at RL applications in industry. It then gives fundamental definitions to refresh your mind on RL concepts and concludes with a section on software and hardware setup.</p>
			<p><a href="B14160_02_Final_SK_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Multi-Armed Bandits</em>, covers a rather simpler RL setting, that is, bandit problems without context, which have tremendous applications in industry as an alternative to the traditional A/B testing. The chapter also serves as an introduction to a very fundamental RL concept: exploration versus exploitation. We also solve a prototype online advertising case with four different methods.</p>
			<p><a href="B14160_03_Final_SK_ePub.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Contextual Bandits</em>, takes the discussion on MABs to an advanced level by adding context to the decision-making process and involving deep neural networks in decision making. We adapt a real dataset from the US Census to an online advertising problem. We conclude the chapter with a section on the applications of bandit problems in industry and business.</p>
			<p><a href="B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Making of the Markov Decision Process</em>, builds the mathematical theory with which we model RL problems. We start with Markov chains, where we describe types of states, ergodicity, transitionary, and steady-state behavior. Then we go into Markov reward and decision processes. Along the way, we introduce return, discount, policy, and value functions, and Bellman optimality, which are key concepts in RL theory that will be frequently referred to in later chapters. We conclude the chapter with a discussion on partially observed Markov decision processes. Throughout the chapter, we use a grid world example to illustrate the concepts.</p>
			<p><a href="B14160_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Solving the Reinforcement Learning Problem</em>, introduces DP methods, which are fundamental to understanding how to solve an MDP. Key concepts such as policy evaluation, policy iteration, and value iteration are introduced and illustrated. Throughout the chapter, we solve an example inventory replenishment problem. We conclude the chapter with a discussion on the issues with using DP in real-world examples.</p>
			<p><a href="B14160_06_Final_SK_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">Deep Q-Learning at Scale</em>, provides an introduction to deep RL and covers deep Q-learning end to end. We start with a discussion on why deep RL is needed, then introduce RLlib, a popular and scalable RL library. After introducing the case studies we will work with (one simple, one medium-difficulty, and one video game example), we will build deep Q-learning methods from fitted Q-iteration to DQN to Rainbow. Then we will go into more advanced topics on distributed DQN (APEX), continuous DQN, and a discussion on important hyperparameters to tune. For classical DQN, you will implement it in TensorFlow. For Rainbow, we will use RLlib.</p>
			<p><a href="B14160_07_Final_SK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 7</em></a>, <em class="italic">Policy-Based Methods</em>, introduces the second important class of RL methods: policy-based methods. You will first learn how they are different and why they are needed. We then go into the details of several state-of-the-art policy gradient and trust region methods. We conclude the chapter with Actor-Critic algorithms. We mostly rely on RLlib implementations of these algorithms and focus on how and when to use them rather than lengthy implementation details.</p>
			<p><a href="B14160_08_Final_SK_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 8</em></a>, <em class="italic">Model-Based Methods</em>, shows what assumptions model-based methods make and what advantages they have over other methods. We also discuss the model behind the famous AlphaGo Zero. We conclude the chapter with an exercise that uses a model-based algorithm. The chapter includes a mix of using manual and RLlib implementations.</p>
			<p><a href="B14160_09_Final_SK_ePub.xhtml#_idTextAnchor200"><em class="italic">Chapter 9</em></a>, <em class="italic">Multi-Agent Reinforcement Learning</em>, gives you a framework to model multi-agent RL problems and introduces MADDPG to solve such problems. The chapter uses an RLlib MADDPG implementation.</p>
			<p><a href="B14160_10_Final_SK_ePub.xhtml#_idTextAnchor220"><em class="italic">Chapter 10</em></a>, <em class="italic">Machine Teaching</em>, discusses the machine teaching approach to break down complex problems into smaller pieces and make them solvable. This approach is necessary for many real-life problems and you will learn practical tips and tricks for how to design an RL model and go beyond algorithm selection in solving RL problems. </p>
			<p><a href="B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239"><em class="italic">Chapter 11</em></a>, <em class="italic">Generalization and Domain Randomization</em>, covers why partial observability and the sim2real gap are a problem, and how to overcome those by using LSTM-like models and domain randomization.</p>
			<p><a href="B14160_12_Final_SK_ePub.xhtml#_idTextAnchor260"><em class="italic">Chapter 12</em></a>, <em class="italic">Meta-Reinforcement Learning</em>, introduces approaches that allow us to use a single model for multiple tasks. As sample efficiency is a major problem in RL, this chapter exposes you to a very important future direction in RL.</p>
			<p><a href="B14160_13_Final_SK_ePub.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Other Advanced Topics</em>, introduces cutting-edge RL research. Many approaches discussed so far have certain assumptions and limitations. The topics discussed in this chapter address these limitations and give ideas about how to overcome them. At the end of this chapter, you will learn which approaches to look into when you hit the limitations of the algorithms we covered in earlier chapters.</p>
			<p><a href="B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306"><em class="italic">Chapter 14</em></a>, <em class="italic">Autonomous Systems</em>, covers the potential of RL for creating real-life autonomous systems. We cover success stories and sample problems for autonomous robots and self-driving cars. </p>
			<p><a href="B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329"><em class="italic">Chapter 15</em></a>, <em class="italic">Supply Chain Management</em>, gives you hands-on experience in inventory planning and bin packing problems. We model them as an RL problem and solve sample cases.</p>
			<p><a href="B14160_16_Final_SK_ePub.xhtml#_idTextAnchor348"><em class="italic">Chapter 16</em></a>, <em class="italic">Marketing, Personalization, and Finance</em>, covers RL applications in marketing, advertising, recommendation systems, and finance. This chapter gives you a broad understanding of how RL can be utilized in business, and what the opportunities and limitations are. In this chapter, we also go into examples of contextual multi-armed bandit problems.</p>
			<p><a href="B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365"><em class="italic">Chapter 17</em></a>, <em class="italic">Smart City and Cybersecurity</em>, covers sample problems from the area of smart cities and cybersecurity, such as traffic control, service provision regulation, and intrusion detection. We also discuss how multi-agent approaches can be used in these applications.</p>
			<p><a href="B14160_18_Final_SK_ePub.xhtml#_idTextAnchor388"><em class="italic">Chapter 18</em></a>, <em class="italic">Challenges and Future Directions in Reinforcement Learning</em>, goes into the details of what these challenges are and what state-of-the-art research suggests to overcome them. This chapter teaches you how to assess the feasibility of the RL approach for a given problem.</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>To get the most out of this book</h1>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</strong></p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Download the example code files</h1>
			<p>You can download the example code files for this book from your account at <a href="http://www.packt.com">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
			<p>You can download the code files by following these steps:</p>
			<ol>
				<li>Log in or register at <a href="http://www.packt.com">www.packt.com</a>.</li>
				<li>Select the <strong class="bold">Support</strong> tab.</li>
				<li>Click on <strong class="bold">Code Downloads</strong>.</li>
				<li>Enter the name of the book in the <strong class="bold">Search</strong> box and follow the onscreen instructions.</li>
			</ol>
			<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
			<ul>
				<li>WinRAR/7-Zip for Windows</li>
				<li>Zipeg/iZip/UnRarX for Mac</li>
				<li>7-Zip/PeaZip for Linux</li>
			</ul>
			<p>The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python">https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781838644147_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781838644147_ColorImages.pdf</a>.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "Install NVIDIA Modprobe, for example, for Ubuntu, using <strong class="source-inline">sudo apt-get install nvidia-modprobe</strong>."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">ug = UserGenerator()</p>
			<p class="source-code">visualize_bandits(ug)</p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code"><strong class="bold">./run_local.sh [Game] [Agent] [Num. actors] </strong></p>
			<p class="source-code"><strong class="bold">./run_local.sh atari r2d2 4</strong></p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit <a href="http://packt.com">packt.com</a>.</p>
		</div>
	</body></html>