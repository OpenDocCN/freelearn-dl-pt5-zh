["```py\n!pip install -q -U \"tensorflow-text==2.8.*\"\n!pip install -q tf-models-official==2.7.0\n!pip install numpy==1.21\nimport os\nimport shutil\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\nimport matplotlib.pyplot as plt #for plotting results\ntf.get_logger().setLevel('ERROR')\n```", "```py\nbatch_size = 32\nimport matplotlib.pyplot as plt\ntf.get_logger().setLevel('ERROR')\nAUTOTUNE = tf.data.AUTOTUNE\nraw_ds = tf.keras.utils.text_dataset_from_directory(\n    './movie_reviews',\nclass_names = raw_ds.class_names\nprint(class_names)\n```", "```py\nfrom sklearn.model_selection import train_test_split\ndef partition_dataset_tf(dataset, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=1000):\n    assert (train_split + test_split + val_split) == 1\n    if shuffle:\n        # Specify seed maintain the same split distribution between runs for reproducibilty\n        dataset = dataset.shuffle(shuffle_size, seed=42)\n    train_size = int(train_split * ds_size)\n    val_size = int(val_split * ds_size)\n    train_ds = dataset.take(train_size)\n    val_ds = dataset.skip(train_size).take(val_size)\n    test_ds = dataset.skip(train_size).skip(val_size)\n    return train_ds, val_ds, test_ds\ntrain_ds,val_ds,test_ds = partition_dataset_tf(\n    raw_ds,len(raw_ds))\n```", "```py\nbert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\nmap_name_to_handle = {\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n}\nmap_model_to_preprocess = {\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[\n    bert_model_name]\nbert_preprocess_model = hub.KerasLayer(\n    tfhub_handle_preprocess)\n```", "```py\ndef build_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(),\n        dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(\n        tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder,\n        trainable = True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None,\n        name='classifier')(net)\n    return tf.keras.Model(text_input, net)\n# plot the model's structure as a check\ntf.keras.utils.plot_model(classifier_model)\n```", "```py\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()\n```", "```py\nepochs = 15\nsteps_per_epoch = tf.data.experimental.cardinality(\n    train_ds).numpy()\nprint(steps_per_epoch)\nnum_train_steps = steps_per_epoch * epochs\n# a linear warmup phase over the first 10%\nnum_warmup_steps = int(0.1*num_train_steps)\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(\n        init_lr=init_lr, num_train_steps = num_train_steps,\n        num_warmup_steps=num_warmup_steps,\n        optimizer_type='adamw')\n```", "```py\nclassifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)\nclassifier_model.summary()\n```", "```py\nModel: model\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to\n==================================================================================================\n text (InputLayer)              [(None,)]            0           []\n preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']\n                                e, 128),\n                                 'input_type_ids':\n                                (None, 128),\n                                 'input_word_ids':\n                                (None, 128)}\n```", "```py\nprint(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(x=train_ds,\n                               validation_data=val_ds,\n                               verbose = 2,\n                               epochs=epochs)\nTraining model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\nEpoch 1/15\n50/50 - 189s - loss: 0.7015 - binary_accuracy: 0.5429 - val_loss: 0.6651 - val_binary_accuracy: 0.5365 - 189s/epoch - 4s/step\n```", "```py\nimport matplotlib.pyplot as plt\n!matplotlib inline\nhistory_dict = history.history\nprint(history_dict.keys())\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = range(1, len(acc) + 1)\n```", "```py\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\nplt.subplot(2, 1, 1)\n# r is for \"solid red line\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\ndict_keys(['loss', 'binary_accuracy', 'val_loss',\n    'val_binary_accuracy'])\n```", "```py\nloss, accuracy = classifier_model.evaluate(test_ds)\nprint(f'Loss: {loss}')\nprint(f'Binary Accuracy: {accuracy}')\n1/7 [===>..........................] - ETA: 9s - loss: 0.0239 - binary_accuracy: 0.9688\n2/7 [=======>......................] - ETA: 5s - loss: 0.0189 - binary_accuracy: 0.9844\n3/7 [===========>..................] - ETA: 4s - loss: 0.0163 - binary_accuracy: 0.9896\n4/7 [================>.............] - ETA: 3s - loss: 0.0140 - binary_accuracy: 0.9922\n5/7 [====================>.........] - ETA: 2s - loss: 0.0135 - binary_accuracy: 0.9937\n6/7 [========================>.....] - ETA: 1s - loss: 0.0134 - binary_accuracy: 0.9948\n7/7 [==============================] - ETA: 0s - loss: 0.0127 - binary_accuracy: 0.9955\n7/7 [==============================] - 8s 1s/step - loss: 0.0127 - binary_accuracy: 0.9955\nLoss: 0.012707981280982494\nAccuracy: 0.9955357313156128\n```", "```py\ndataset_name = 'movie_reviews'\nsaved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\nclassifier_model.save(saved_model_path, include_optimizer=False)\nreloaded_model = tf.saved_model.load(saved_model_path)\n]\n```", "```py\n    ! pip install --upgrade openai\n    ```", "```py\n!pip install --upgrade openai\n```", "```py\n    api_key =<your API key>\n    ```", "```py\n    openai.api_key = api_key\n    ```", "```py\n    {\"prompt\":\"this film is extraordinarily horrendous and i'm not going to waste any more words on it . \",\"completion\":\" negative\"}\n    ```", "```py\n    {\"prompt\":\"9 : its pathetic attempt at \\\" improving \\\" on a shakespeare classic . 8 : its just another piece of teen fluff . 7 : kids in high school are not that witty . … \",\"completion\":\" negative\"}\n    ```", "```py\n    {\"prompt\":\"claire danes , giovanni ribisi , and omar epps make a likable trio of protagonists , …\",\"completion\":\" negative\"}\n    ```", "```py\n!openai tools fine_tunes.prepare_data -f ./movies.csv -q\n```", "```py\nfile_name = \"./movies_prepared.jsonl\"\nupload_response = openai.File.create(\n  file=open(file_name, \"rb\"),\n  purpose='fine-tune'\n)\nfile_id = upload_response.id\n```", "```py\nopenai.FineTune.create(training_file=file_id, model=\"ada\")\nfine_tuned_model = fine_tune_response.fine_tuned_model\n```", "```py\nanswer = openai.Completion.create(\n  model = fine_tuned_model,\n    engine = \"ada\",\n  prompt = \" I don't like this movie \",\n  max_tokens = 10, # Change amount of tokens for longer completion\n  temperature = 0\n)\nanswer['choices'][0]['text']\n```"]