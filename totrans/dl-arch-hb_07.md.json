["```py\n    import json\n    import os\n    import numpy as np\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from catalyst import dl, utils\n    from catalyst.contrib.datasets import MNIST\n    from sklearn import datasets\n    from sklearn.metrics import log_loss\n    from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from torch import nn as nn from torch import optim from torch.utils.data import DataLoader, TensorDataset torch.manual_seed(0)\n    ```", "```py\n    class MLP(nn.Module):\n      def __init__(self, input_layer_size, output_layer_size, layer_configuration, activation_type='relu'):\n         super(MLP, self).__init__()\n         self.fully_connected_layers = nn.ModuleDict()\n         self.activation_type = activation_type\n         hidden_layer_number = 0\n         for hidden_layer_idx in range(len(layer_configuration)):\n         if hidden_layer_idx == 0:\n            self.fully_connected_layers[\n               str(hidden_layer_number)\n            ] = nn.Linear(\n               input_layer_size,\n               layer_configuration[hidden_layer_idx]\n            )\n            hidden_layer_number += 1\n         if hidden_layer_idx == len(layer_configuration) - 1:\n            self.fully_connected_layers[\n               str(hidden_layer_number)\n            ] = nn.Linear(\n               layer_configuration[hidden_layer_idx],\n               output_layer_size\n            )\n         else:\n            self.fully_connected_layers[\n               str(hidden_layer_number)\n            ] = nn.Linear(\n               layer_configuration[hidden_layer_idx],\n               layer_configuration[hidden_layer_idx+1]\n            )\n            hidden_layer_number += 1\n      def forward(self, x):\n         for fc_key in self.fully_connected_layers:\n            x = self.fully_connected_layers[fc_key](x)\n            if fc_key != str(len(self.fully_connected_layers) -1):\n               x = F.relu(x)\n         return x\n    ```", "```py\n    def train_and_evaluate_mlp(\n      trial_number, layer_configuration, epochs,\n      input_layer_size, output_layer_size ,\n      load_on_stage_start=False, best_or_last='last',\n      verbose=False\n    ):\n      criterion = nn.CrossEntropyLoss()\n      runner = dl.SupervisedRunner(\n         input_key=\"features\", output_key=\"logits\",\n         target_key=\"targets\", loss_key=\"loss\"\n      )\n      model = MLP(\n         input_layer_size=input_layer_size,\n         layer_configuration=layer_configuration,\n         output_layer_size=output_layer_size,\n      )\n      optimizer = optim.Adam(model.parameters(), lr=0.02)\n      checkpoint_logdir = \"logs/trial_{}\".format(\n         trial_number\n      )\n      runner.train(\n         model=model, criterion=criterion,\n         optimizer=optimizer, loaders=loaders,\n         num_epochs=epochs,\n         callbacks=[\n            dl.CheckpointCallback(\n               logdir=checkpoint_logdir,\n               loader_key=\"valid\",\n               metric_key=\"loss\",\n               mode=\"all\",\n               load_on_stage_start=\"last_full\" if load_on_stage_start else None,\n            )\n         ], logdir=\"./logs\", valid_loader=\"valid\",\n         valid_metric=\"loss\", minimize_valid_metric=True,\n         verbose=verbose\n      )\n      with open(os.path.join(checkpoint_logdir, '_metrics.json'), 'r') as f:\n         metrics = json.load(f)\n         if best_or_last == 'last':\n            valid_loss = metrics['last']['_score_']\n         else:\n            valid_loss = metrics['best']['valid']['loss']\n      return valid_loss\n    ```", "```py\n    def get_random_configurations(\n      number_of_configurations, rng\n    ):\n      layer_configurations = []\n      for _ in range(number_of_configurations):\n         layer_configuration = []\n         number_of_hidden_layers = rng.randint(\n            low=1, high=6\n         )\n         for _ in range(number_of_hidden_layers):\n            layer_configuration.append(\n               rng.randint(low=2, high=100)\n            )\n         layer_configurations.append(layer_configuration)\n      layer_configurations = np.array(\n         layer_configurations\n      )\n      return layer_configurations\n    ```", "```py\n    iris = datasets.load_iris()\n    iris_input_dataset = iris['data']\n    target = torch.from_numpy(iris['target'])\n    scaler = MinMaxScaler()\n    scaler.fit(iris_input_dataset)\n    iris_input_dataset = torch.from_numpy(\n      scaler.transform(iris_input_dataset)\n    ).float()\n    (\n      X_train, X_test, y_train, y_test\n    ) = train_test_split(\n      iris_input_dataset, target, test_size=0.33,\n      random_state=42\n    )\n    training_dataset = TensorDataset(X_train, y_train)\n    validation_dataset =  TensorDataset(X_test, y_test)\n    train_loader = DataLoader(\n      training_dataset, batch_size=10, num_workers=1\n    )\n    valid_loader = DataLoader(\n      validation_dataset, batch_size=10, num_workers=1\n    )\n    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n    ```", "```py\n    rng = np.random.RandomState(1234)\n    number_of_configurations = 20\n    layer_configurations = get_random_configurations(\n      number_of_configurations, rng\n    )\n    successive_halving_epochs = [5, 5, 5]\n    ```", "```py\n    for succesive_idx, successive_halving_epoch in enumerate(successive_halving_epochs):\n      valid_losses = []\n      for idx, layer_configuration in enumerate(layer_configurations):\n         trial_number = trial_numbers[idx]\n         valid_loss = train_and_evaluate_mlp(\n            trial_number, layer_configuration,\n            epochs=successive_halving_epoch,\n            load_on_stage_start=False if succesive_idx==0 else True\n         )\n         valid_losses.append(valid_loss)\n         if succesive_idx != len(successive_halving_epochs) - 1:\n            succesive_halved_configurations = np.argsort(\n               valid_losses\n            )[:int(len(valid_losses)/2)]\n            layer_configurations = layer_configurations[\n               succesive_halved_configurations\n            ]\n            trial_numbers = trial_numbers[\n               succesive_halved_configurations\n            ]\n    ```", "```py\n    best_loss_idx = np.argmin(valid_losses)\n    best_layer_configuration = layer_configurations[best_loss_idx]\n    best_loss_trial_number = trial_numbers[best_loss_idx]\n    ```", "```py\n    import math\n    ```", "```py\n    resource_per_conf = 30  # R\n    N = 3\n    ```", "```py\n    s_max = int(math.log(resource_per_conf, N))\n    bracket_resource = (s_max + 1) * resource_per_conf  bracket_best_valid_losses = []\n    bracket_best_layer_configuration = []\n    for s in range(s_max, -1, -1):\n      number_of_configurations = int(\n         (bracket_resource / resource_per_conf) *\n         (N**s / (s+1))\n      )\n      r = resource_per_conf * N**-s\n      layer_configurations = get_random_configurations(\n         number_of_configurations, rng\n      )\n      trial_numbers = np.array(\n         range(len(layer_configurations))\n      )\n      valid_losses = []\n      for i in range(s+1):\n         number_of_configurations_i = int(\n            number_of_configurations * N**-i\n         )\n         r_i = int(r * N**i)\n         valid_losses = []\n         for idx, layer_configuration in enumerate(layer_configurations):\n            trial_number = '{}_{}'.format(\n               s, trial_numbers[idx]\n            )\n            valid_loss = train_and_evaluate_mlp(\n               trial_number, layer_configuration,\n               epochs=r_i, load_on_stage_start=False if\n               s==s_max else True\n            )\n            valid_losses.append(valid_loss)\n            if succesive_idx != len(successive_halving_epochs) - 1:\n               succesive_halved_configurations = np.argsort(\n                valid_losses\n               )[:int(number_of_configurations_i/N)]\n               layer_configurations = layer_configurations[\n                  succesive_halved_configurations\n               ]\n               trial_numbers = trial_numbers[\n                succesive_halved_configurations\n               ]\n               best_loss_idx = np.argmin(valid_losses)\n               best_layer_configuration = layer_configurations[best_loss_idx]\n               best_loss_trial_number = trial_numbers[\n                  best_loss_idx\n               ]\n               bracket_best_valid_losses.append(\n                  valid_losses[best_loss_idx]\n               )\n               bracket_best_layer_configuration.append(\n                  best_layer_configuration\n               )\n    ```", "```py\n    from sklearn.gaussian_process import GaussianProcessRegressor\n    ```", "```py\n    def get_bayesian_optimization_input_features(\n      layer_configurations\n    ):\n      bo_input_features = []\n      for layer_configuration in layer_configurations:\n         bo_input_feature = layer_configuration + [0] * (6 - len(layer_configuration))\n         bo_input_features.append(bo_input_feature)\n      return bo_input_features\n    ```", "```py\n    number_of_configurations = [100, 2000]\n    epochs_per_conf = 15\n    topk_models = 5\n    ```", "```py\n    Trial_numbers = np.array(\n      range(len(layer_configurations))\n    )\n    trial_number = 0\n    model = None\n    best_valid_losses_per_iteration = []\n    best_configurations_per_iteration = []\n    overall_bo_input_features = []\n    overall_bo_valid_losses = []\n    for number_of_configuration in number_of_configurations:\n      valid_losses = []\n      layer_configurations = get_random_configurations(\n         number_of_configuration, rng\n      )\n      if model:\n         bo_input_features = get_bayesian_optimization_input_features(layer_configurations)\n         predicted_valid_losses = model.predict(\n            bo_input_features\n         )\n         top_k_idx = np.argsort(\n            predicted_valid_losses\n         )[:topk_models]\n         layer_configurations = layer_configurations[\n            top_k_idx\n         ]\n      for idx, layer_configuration in enumerate(layer_configurations):\n         trial_identifier = 'bo_{}'.format(trial_number)\n         valid_loss = train_and_evaluate_mlp(\n            trial_number, layer_configuration,\n            epochs=epochs_per_conf,\n            load_on_stage_start=False, best_or_last='best'\n         )\n         valid_losses.append(valid_loss)\n         trial_number += 1\n      best_loss_idx = np.argmin(valid_losses)\n      best_valid_losses_per_iteration.append(\n         valid_losses[best_loss_idx]\n      )\n      best_configurations_per_iteration.append(\n         layer_configurations[best_loss_idx]\n      )\n      bo_input_features = get_bayesian_optimization_input_features(layer_configurations)\n      overall_bo_input_features.extend(bo_input_features)\n      overall_bo_valid_losses.extend(valid_losses)\n      model = GaussianProcessRegressor()\n      model.fit(\n         overall_bo_input_features,\n         overall_bo_valid_losses\n      )\n    ```"]