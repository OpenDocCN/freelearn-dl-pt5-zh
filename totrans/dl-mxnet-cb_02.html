<html><head></head><body>
<div id="_idContainer044">
<h1 class="c apter-number" id="_idParaDest-28"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-29"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.2.1">Working with MXNet and Visualizing Datasets – Gluon and DataLoader</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we learned how to set up MXNet. </span><span class="koboSpan" id="kobo.3.2">We also verified how MXNet could leverage our hardware to provide maximum performance. </span><span class="koboSpan" id="kobo.3.3">Before applying </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">deep learning</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">DL</span></strong><span class="koboSpan" id="kobo.7.1">) to solve specific problems, we need to understand how to load, manage, and visualize the datasets we will be working with. </span><span class="koboSpan" id="kobo.7.2">In this chapter, we will start using MXNet to analyze some toy datasets in the domains of numerical regression, data classification, image classification, and text classification. </span><span class="koboSpan" id="kobo.7.3">To manage those tasks efficiently, we will see new MXNet libraries and functions such as Gluon (an API for DL) </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">and DataLoader.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Understanding regression datasets – loading, managing, and visualizing the </span><em class="italic"><span class="koboSpan" id="kobo.12.1">House </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.13.1">Sales</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.14.1"> dataset</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Understanding classification datasets – loading, managing, and visualizing the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Iris dataset</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Understanding image datasets – loading, managing, and visualizing the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Fashion-MNIST dataset</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Understanding text datasets – loading, managing, and visualizing the Enron </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">Email dataset</span></span></li>
</ul>
<h1 id="_idParaDest-30"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.22.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.23.1">Preface</span></em><span class="koboSpan" id="kobo.24.1">, no other requirements apply to </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch02"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch02</span></span></a></p>
<p><span class="koboSpan" id="kobo.29.1">Furthermore, you can directly access each recipe from Google Colab; for example, for the first recipe of this chapter, </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">visit </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch02/2_1_Toy_Dataset_for_Regression_Load_Manage_and_Visualize_House_Sales_Dataset.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch02/2_1_Toy_Dataset_for_Regression_Load_Manage_and_Visualize_House_Sales_Dataset.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.32.1">.</span></span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.33.1">Understanding regression datasets – loading, managing, and visualizing the House Sales dataset</span></h1>
<p><span class="koboSpan" id="kobo.34.1">The</span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.35.1"> training process of </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">machine learning</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.38.1">ML</span></strong><span class="koboSpan" id="kobo.39.1">) models can be divided into three </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">main sub-groups:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.41.1">Supervised learning (SL)</span></strong><span class="koboSpan" id="kobo.42.1">: The</span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.43.1"> expected outputs are known for at least </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">some data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Unsupervised learning (UL)</span></strong><span class="koboSpan" id="kobo.46.1">: The</span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.47.1"> expected outputs are not known but the data has some features that could help with understanding its </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">internal distribution</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Reinforcement learning (RL)</span></strong><span class="koboSpan" id="kobo.50.1">: An</span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.51.1"> agent explores the environment and makes decisions based on the inputs acquired from </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">the environment</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.53.1">There is also an approach that falls in between the first two sub-groups</span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.54.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">weakly SL</span></strong><span class="koboSpan" id="kobo.56.1">, where there are not enough known outputs to follow an SL approach for one of the </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">following reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.58.1">The outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">are inaccurate</span></span></li>
<li><span class="koboSpan" id="kobo.60.1">Only some of the output features are </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">known (incomplete)</span></span></li>
<li><span class="koboSpan" id="kobo.62.1">They are not exactly the expected outputs but are connected/related to the task we intend to </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">achieve (inexact)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.64.1">With SL, one of the most common problem types is </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">regression</span></strong><span class="koboSpan" id="kobo.66.1">. </span><span class="koboSpan" id="kobo.66.2">In regression problems, we want to estimate numerical outputs given a variable number of input features. </span><span class="koboSpan" id="kobo.66.3">In this recipe, we will analyze a toy regression dataset from Kaggle: </span><em class="italic"><span class="koboSpan" id="kobo.67.1">House Sales in King </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.68.1">County, USA</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">The House Sales dataset</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.71.1"> presents </span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.72.1">the problem of estimating the price of a house (in $) given the </span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.73.1">following</span><a id="_idIndexMarker056"/> <span class="No-Break"><span class="koboSpan" id="kobo.74.1">19 features:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.75.1">Date</span></strong><span class="koboSpan" id="kobo.76.1"> of the </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">home sale</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.78.1">Number</span></strong> <span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.79.1">of</span></strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">bedrooms</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">Number</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.82.1">of</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.83.1">bathrooms</span></strong><span class="koboSpan" id="kobo.84.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.85.1">0.5</span></strong><span class="koboSpan" id="kobo.86.1"> accounts for a room with a toilet but </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">no shower</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">Sqft_living</span></strong><span class="koboSpan" id="kobo.89.1">: Square feet of the apartment’s interior </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">living space</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.91.1">Sqft_lot</span></strong><span class="koboSpan" id="kobo.92.1">: Square feet of the </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">land space</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.94.1">Number</span></strong> <span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.95.1">of</span></strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.96.1">floors</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">Waterfront</span></strong><span class="koboSpan" id="kobo.98.1"> view </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">or not</span></span></li>
<li><span class="koboSpan" id="kobo.100.1">An index from 0 to 4 of how good the view of the </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">property is</span></span></li>
<li><span class="koboSpan" id="kobo.102.1">An index from 1 to 5 on the condition of </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">the apartment</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">Grade</span></strong><span class="koboSpan" id="kobo.105.1">: An index from 1 to 13, with 1 being the worst and 13 </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">the best</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">Sqft_above</span></strong><span class="koboSpan" id="kobo.108.1">: Square feet of the interior housing space that is above </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">ground level</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">Sqft_basement</span></strong><span class="koboSpan" id="kobo.111.1">: Square feet of the interior housing space that is below </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">ground level</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">Yr_built</span></strong><span class="koboSpan" id="kobo.114.1">: The year the house was </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">initially built</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">Yr_renovated</span></strong><span class="koboSpan" id="kobo.117.1">: The year of the house’s </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">last renovation</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">Zipcode</span></strong><span class="koboSpan" id="kobo.120.1"> area the house </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">is in</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.122.1">Latitude (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">Lat</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.125.1">Longitude (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.126.1">Long</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">)</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">Sqft_living15</span></strong><span class="koboSpan" id="kobo.129.1">: Square feet of interior housing living space for the nearest </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">15 neighbors</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">Sqft_lot15</span></strong><span class="koboSpan" id="kobo.132.1">: Square feet of the land lots of the nearest </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">15 neighbors</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.134.1">These</span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.135.1"> data features are provided for </span><em class="italic"><span class="koboSpan" id="kobo.136.1">21,613</span></em><span class="koboSpan" id="kobo.137.1"> houses along with the price (value to </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">be estimated).</span></span></p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.139.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.140.1">The</span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.141.1"> following dataset is provided under the </span><em class="italic"><span class="koboSpan" id="kobo.142.1">CC0 Public Domain</span></em><span class="koboSpan" id="kobo.143.1"> license and can be downloaded </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">from </span></span><a href="https://www.kaggle.com/harlfoxem/housesalesprediction"><span class="No-Break"><span class="koboSpan" id="kobo.145.1">https://www.kaggle.com/harlfoxem/housesalesprediction</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.146.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">To read the data, we are going to use a very well-known library to manage data, </span><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">pandas</span></strong><span class="koboSpan" id="kobo.149.1">, and we will use the most common data structure for the library, </span><strong class="bold"><span class="koboSpan" id="kobo.150.1">DataFrames</span></strong><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">Moreover, in order to plot the data and several visualizations we will compute, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">matplotlib</span></strong><span class="koboSpan" id="kobo.153.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">pyplot</span></strong><span class="koboSpan" id="kobo.155.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">seaborn</span></strong><span class="koboSpan" id="kobo.157.1"> libraries. </span><span class="koboSpan" id="kobo.157.2">Therefore, we must run the </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.159.1">
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns</span></pre> <p><span class="koboSpan" id="kobo.160.1">If you do not have these libraries installed, they can be easily installed with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">terminal commands:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.162.1">
!pip3 install matplotlib==3.7.1
!pip3 install pandas==1.5.3
 !pip3 install seaborn==0.12.2</span></pre> <p><span class="koboSpan" id="kobo.163.1">Therefore, to load the data, we can simply retrieve the file containing the dataset (available in the GitHub repository for the book) and </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">process it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.165.1">
# Retrieve Dataset (House Sales Prices) from GitHub repository for Deep Learning with MXNet Cookbook by Packt
!wget </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/raw/main/ch02/kc_house_data.zip</span></strong><span class="koboSpan" id="kobo.167.1">
# Uncompress kc_house_data.csv file
!unzip /content/kc_house_data.zip
house_df = pd.read_csv("kc_house_data.csv")</span></pre> <p><span class="koboSpan" id="kobo.168.1">This is all we </span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.169.1">need to start working with our </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">regression dataset.</span></span></p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.171.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.172.1">In this section, we </span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.173.1">will run an </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">exploratory data analysis</span></strong><span class="koboSpan" id="kobo.175.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.176.1">EDA</span></strong><span class="koboSpan" id="kobo.177.1">) that will help us understand which features are important (and which are not) to predict the price of </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">a house:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.179.1">Data structure</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.180.1">Correlation study</span></span></li>
<li><span class="koboSpan" id="kobo.181.1">Living square feet analysis, square feet above ground level analysis, and neighbors’ living square </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">feet analysis</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.183.1">Grade analysis</span></span></li>
<li><span class="koboSpan" id="kobo.184.1">Rooms (bedrooms and </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">bathrooms) analysis</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.186.1">Views analysis</span></span></li>
<li><span class="koboSpan" id="kobo.187.1">Year-built and </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">year-renovated analysis</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.189.1">Location analysis</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.190.1">Data structure</span></h3>
<p><span class="koboSpan" id="kobo.191.1">Let’s analyze what </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.192.1">our data looks like. </span><span class="koboSpan" id="kobo.192.2">For this, we will use common operations on </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">pandas</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.194.1"> DataFrames:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.195.1">
house_df.info()</span></pre> <p><span class="koboSpan" id="kobo.196.1">From the output, we can draw the </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">following conclusions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.198.1">The data is complete (all columns have 21,613 values, </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">as expected).</span></span></li>
<li><span class="koboSpan" id="kobo.200.1">There are no </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">NULL</span></strong><span class="koboSpan" id="kobo.202.1"> values (the data </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">is clean!).</span></span></li>
<li><span class="koboSpan" id="kobo.204.1">Apart from the features described previously, there is a feature called </span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">id</span></strong><span class="koboSpan" id="kobo.206.1">. </span><span class="koboSpan" id="kobo.206.2">This feature is not needed as the index already allows us to uniquely identify </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">each property.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.208.1">In order to</span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.209.1"> grasp what the values look like, let’s display the first </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">five properties:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.211.1">
house_df.head()</span></pre> <p><span class="koboSpan" id="kobo.212.1">So far, we have had a look at the features. </span><span class="koboSpan" id="kobo.212.2">Now, let’s take a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">price distribution:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.214.1">
house_df.hist(column = "price", bins = 24)
 plt.show()</span></pre> <p><span class="koboSpan" id="kobo.215.1">These commands will display a histogram of prices that shows how many houses in the dataset have a certain price (column selected in the previous commands). </span><span class="koboSpan" id="kobo.215.2">Histograms work in ranges (also known as </span><em class="italic"><span class="koboSpan" id="kobo.216.1">buckets</span></em><span class="koboSpan" id="kobo.217.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.218.1">bins</span></em><span class="koboSpan" id="kobo.219.1">); in our case, we have chosen 24. </span><span class="koboSpan" id="kobo.219.2">As the maximum price is $8M, when applying 24 ranges, we have 3 ranges per million dollars, specifically (all values in millions of $): [0 – 0.33), [0.33 - 0.66), [0.66 - 1), ... </span><span class="koboSpan" id="kobo.219.3">until [7.66 - </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">8].</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">The following is </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<span class="koboSpan" id="kobo.223.1"><img alt="Figure 2.1 – Price distribution" src="image/B16591_02_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.224.1">Figure 2.1 – Price distribution</span></p>
<h3><span class="koboSpan" id="kobo.225.1">Correlation study</span></h3>
<p><span class="koboSpan" id="kobo.226.1">Here, we </span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.227.1">will analyze how each feature correlates with each other and, most importantly, how each feature correlates with </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">the price.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">First, as previously discussed, we are going to remove the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.230.1">id</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.231.1"> feature:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.232.1">
house_df = house_df.drop(["id"], axis=1)</span></pre> <p><span class="koboSpan" id="kobo.233.1">We can now compute the pairwise </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">correlation diagram:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.235.1">
house_corr = house_df.corr()</span></pre> <p><span class="koboSpan" id="kobo.236.1">To easily visualize the calculated correlations, we will plot </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">a heatmap:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.238.1">
plt.figure(figsize=(20, 10))
colormap = sns.color_palette("rocket_r", as_cmap=True)
sns.heatmap(house_corr, annot=True, cmap=colormap)
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.239.1">These code statements yield the </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<span class="koboSpan" id="kobo.241.1"><img alt="Figure 2.2 – House features correlation matrix" src="image/B16591_02_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.242.1">Figure 2.2 – House features correlation matrix</span></p>
<p><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.243.1">Note in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.244.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.245.1">.2</span></em><span class="koboSpan" id="kobo.246.1"> that the darker the cell is, the larger the </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">correlation value.</span></span></p>
<p><span class="koboSpan" id="kobo.248.1">To emphasize the first row (which is the most important as it shows the relationship between price and the input features), we</span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.249.1"> will run the </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.251.1">
house_corr["price"].drop(["price"]).sort_values(ascending = False).plot.bar(figsize=(5,5))
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.252.1">And we have the </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<span class="koboSpan" id="kobo.254.1"><img alt="Figure 2.3 – House features: price correlation" src="image/B16591_02_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.255.1">Figure 2.3 – House features: price correlation</span></p>
<p><span class="koboSpan" id="kobo.256.1">The following conclusions can be drawn from </span><em class="italic"><span class="koboSpan" id="kobo.257.1">Figures 2.2</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.258.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.259.1">2.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">Living square feet</span></strong><span class="koboSpan" id="kobo.262.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">grade</span></strong><span class="koboSpan" id="kobo.264.1"> are the most highly correlated features with price (0.7 and </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">0.67 respectively)</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">Above square feet</span></strong><span class="koboSpan" id="kobo.267.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">neighbors' living square feet</span></strong><span class="koboSpan" id="kobo.269.1"> are very correlated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">living square feet</span></strong><span class="koboSpan" id="kobo.271.1"> (0.88 and 0.76, respectively, which points to a degree </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">of redundancy)</span></span></li>
<li><span class="koboSpan" id="kobo.273.1">The </span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.274.1">number of each type of room has the following </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">correlation coefficients:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.276.1">Number of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.277.1">bathrooms</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">: 0.53</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.279.1">Number of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.280.1">bedrooms</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">: 0.31</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.282.1">Number of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.283.1">floors</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">: 0.26</span></span></li></ul></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">View</span></strong><span class="koboSpan" id="kobo.286.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">waterfront</span></strong><span class="koboSpan" id="kobo.288.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">renovation year</span></strong><span class="koboSpan" id="kobo.290.1"> have some correlation with </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">price</span></strong><span class="koboSpan" id="kobo.292.1"> (0.4, 0.27, </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">and 0.13)</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">Location</span></strong><span class="koboSpan" id="kobo.295.1"> is correlated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">price</span></strong><span class="koboSpan" id="kobo.297.1"> as well, with </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">latitude</span></strong><span class="koboSpan" id="kobo.299.1"> being the most important location </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">feature (0.31)</span></span></li>
<li><span class="koboSpan" id="kobo.301.1">The rest of the features seem not to make a large contribution to the price of </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">the property</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.303.1">Therefore, from an initial analysis, the most correlated features with </span><em class="italic"><span class="koboSpan" id="kobo.304.1">price</span></em><span class="koboSpan" id="kobo.305.1"> are, by order of importance: </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">living square feet</span></strong><span class="koboSpan" id="kobo.307.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">grade</span></strong><span class="koboSpan" id="kobo.309.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">number of bathrooms</span></strong><span class="koboSpan" id="kobo.311.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">view</span></strong><span class="koboSpan" id="kobo.313.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">latitude</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">In the next sections, we will confirm these </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">initial conclusions.</span></span></p>
<h3><span class="koboSpan" id="kobo.319.1">Square feet analysis</span></h3>
<p><span class="koboSpan" id="kobo.320.1">From the</span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.321.1"> correlation diagram, we identified a strong correlation between </span><em class="italic"><span class="koboSpan" id="kobo.322.1">living square feet</span></em><span class="koboSpan" id="kobo.323.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.324.1">price</span></em><span class="koboSpan" id="kobo.325.1"> (as expected), and a potential redundancy with </span><em class="italic"><span class="koboSpan" id="kobo.326.1">above square feet</span></em><span class="koboSpan" id="kobo.327.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.328.1">neighbors’ living square feet</span></em><span class="koboSpan" id="kobo.329.1">. </span><span class="koboSpan" id="kobo.329.2">To analyze this in more detail, let’s plot each variable </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">versus price:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<span class="koboSpan" id="kobo.331.1"><img alt="Figure 2.4 – Price compared with several features: a) living square feet, b) above square feet, c) neighbors’ living square feet" src="image/B16591_02_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.332.1">Figure 2.4 – Price compared with several features: a) living square feet, b) above square feet, c) neighbors’ living square feet</span></p>
<p><span class="koboSpan" id="kobo.333.1">As expected, the </span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.334.1">plots are very similar, which indicates a high correlation (and redundancy) among these variables. </span><span class="koboSpan" id="kobo.334.2">Furthermore, we can observe the largest density of data points occurs for prices less than $3M and less than 5,000 square feet. </span><span class="koboSpan" id="kobo.334.3">As most of our data lies in these areas, we can consider houses outside these ranges as outliers and </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">remove them.</span></span></p>
<h3><span class="koboSpan" id="kobo.336.1">Grade analysis</span></h3>
<p><span class="koboSpan" id="kobo.337.1">Similarly, we </span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.338.1">can compare the </span><em class="italic"><span class="koboSpan" id="kobo.339.1">grade</span></em><span class="koboSpan" id="kobo.340.1"> feature against </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">the price:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<span class="koboSpan" id="kobo.342.1"><img alt="Figure 2.5 – House grade versus price" src="image/B16591_02_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.343.1">Figure 2.5 – House grade versus price</span></p>
<p><span class="koboSpan" id="kobo.344.1">There is a</span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.345.1"> clear direct correlation between the grade and the price; the higher the grade, the higher the price. </span><span class="koboSpan" id="kobo.345.2">It is also noteworthy that the highest values of grade are much </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">less frequent.</span></span></p>
<h3><span class="koboSpan" id="kobo.347.1">Rooms analysis</span></h3>
<p><span class="koboSpan" id="kobo.348.1">Let’s display </span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.349.1">in more detail the relationship between the price and the number of </span><em class="italic"><span class="koboSpan" id="kobo.350.1">floors</span></em><span class="koboSpan" id="kobo.351.1">, </span><em class="italic"><span class="koboSpan" id="kobo.352.1">bedrooms</span></em><span class="koboSpan" id="kobo.353.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.355.1">bathrooms</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.357.1"><img alt="Figure 2.6 – Price compared with several features: a) number of floors, b) number of bedrooms, c) number of bathrooms" src="image/B16591_02_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.358.1">Figure 2.6 – Price compared with several features: a) number of floors, b) number of bedrooms, c) number of bathrooms</span></p>
<p><span class="koboSpan" id="kobo.359.1">From the figure, you </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.360.1">can observe </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.362.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.363.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.364.1">.6 (a)</span></em><span class="koboSpan" id="kobo.365.1">, we can see for a small number of floors (1-3), there is a direct correlation between the price of the house and this number. </span><span class="koboSpan" id="kobo.365.2">However, from the fourth floor, this correlation disappears, indicating a lack of data on this segment (houses with four or more floors are much </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">less common).</span></span></li>
<li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.367.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.368.1">.6 (b)</span></em><span class="koboSpan" id="kobo.369.1">, the comparison with the number of bedrooms, is a similar scenario to the previous chart comparing the number of floors. </span><span class="koboSpan" id="kobo.369.2">We can see how for a small number of bedrooms, there is a direct correlation between the price of the house and this number. </span><span class="koboSpan" id="kobo.369.3">However, from four bedrooms up, this correlation disappears, and other features need to be taken </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">into account.</span></span></li>
</ul>
<p class="callout- eading"><span class="koboSpan" id="kobo.371.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.372.1">When looking carefully at the data, you will realize that in the row with index </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">15870</span></strong><span class="koboSpan" id="kobo.374.1">, there is an outlier; it is a house with 33 bedrooms. </span><span class="koboSpan" id="kobo.374.2">I do not know if this is the actual number of bedrooms of the house (I expect not!), but to properly analyze the dataset, this house, an outlier, was removed from it. </span><span class="koboSpan" id="kobo.374.3">See the code </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">for details.</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.376.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.378.1">.6 (c)</span></em><span class="koboSpan" id="kobo.379.1">, we can see there is a direct correlation between the number of bathrooms; nevertheless, there</span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.380.1"> is also some uncertainty (the chart grows wider as we increase the number </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">of bathrooms).</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.382.1">Views analysis</span></h3>
<p><span class="koboSpan" id="kobo.383.1">In this section, we </span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.384.1">will take a look in more detail at how </span><em class="italic"><span class="koboSpan" id="kobo.385.1">view</span></em><span class="koboSpan" id="kobo.386.1"> quality and a </span><em class="italic"><span class="koboSpan" id="kobo.387.1">waterfront view</span></em><span class="koboSpan" id="kobo.388.1"> (whether the house has this view or not) have a connection with </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">the price:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<span class="koboSpan" id="kobo.390.1"><img alt="Figure 2.7 – View quality (a) and waterfront view (b) versus price" src="image/B16591_02_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.391.1">Figure 2.7 – View quality (a) and waterfront view (b) versus price</span></p>
<p><span class="koboSpan" id="kobo.392.1">From these plots individually, it is a little bit more difficult to draw conclusions. </span><span class="koboSpan" id="kobo.392.2">Other variables seem to be needed to see a clear connection between the view quality and the price, and similarly with the </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">waterfront view.</span></span></p>
<h3><span class="koboSpan" id="kobo.394.1">Year-built and year-renovated analysis</span></h3>
<p><span class="koboSpan" id="kobo.395.1">The</span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.396.1"> following plots show how the features of which year a house was built and if and when a house was renovated are correlated </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">with price:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.398.1"><img alt="Figure 2.8 – Price compared with construction year (a) and renovation (b)" src="image/B16591_02_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.399.1">Figure 2.8 – Price compared with construction year (a) and renovation (b)</span></p>
<p><span class="koboSpan" id="kobo.400.1">From the figure, you can observe </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.402.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.403.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.404.1">.8 (a)</span></em><span class="koboSpan" id="kobo.405.1">, we can see a slight linear increase in the price, suggesting that the more recently the house was built, the more expensive </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">it is.</span></span></li>
<li><span class="koboSpan" id="kobo.407.1">For </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.408.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.409.1">.8 (b)</span></em><span class="koboSpan" id="kobo.410.1">, instead of analyzing the year, we split the dataset into two categories – houses that had been renovated and those that had not – and we plotted these two categories against price. </span><span class="koboSpan" id="kobo.410.2">Regardless, it is a little bit more difficult to draw conclusions. </span><span class="koboSpan" id="kobo.410.3">Other variables seem to be needed to see a clear connection between the renovation year and </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">the price.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.412.1">Location analysis</span></h3>
<p><span class="koboSpan" id="kobo.413.1">In this section, we </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.414.1">will take a look in more detail at how latitude and longitude are connected </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">to price:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.416.1"><img alt="Figure 2.9 – Location versus price" src="image/B16591_02_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.417.1">Figure 2.9 – Location versus price</span></p>
<p><span class="koboSpan" id="kobo.418.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.419.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.420.1">.9</span></em><span class="koboSpan" id="kobo.421.1">, we</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.422.1"> can conclude that location plays an important role in the price of a house. </span><span class="koboSpan" id="kobo.422.2">Very clearly, the northern area of King County is more valued than the southern area. </span><span class="koboSpan" id="kobo.422.3">And there is a particular central region where houses are significantly more expensive than other </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">nearby regions.</span></span></p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.424.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.425.1">Regression problems are one of the most common problems where SL approaches can be applied. </span><span class="koboSpan" id="kobo.425.2">By studying in depth a classic regression dataset, </span><em class="italic"><span class="koboSpan" id="kobo.426.1">King County House Price Prediction</span></em><span class="koboSpan" id="kobo.427.1">, we can discover the most important connections between the input features (square feet, grade, and number of bathrooms) and the output feature (price). </span><span class="koboSpan" id="kobo.427.2">This analysis will help us build a model to predict the price in the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">next chapter.</span></span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.429.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.430.1">In this section, we focused on an individual analysis of each feature against price. </span><span class="koboSpan" id="kobo.430.2">However, some features are better understood when combined with others or preprocessed. </span><span class="koboSpan" id="kobo.430.3">We did a simple exploration of this topic, by combining the houses that have been renovated in one category and comparing this with the category of non-renovated houses. </span><span class="koboSpan" id="kobo.430.4">Furthermore, for the location analysis, we used a 2D map to plot the latitude and longitude to </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">discover patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.432.1">However, there are plenty of relationships and analyses to be done, and I suggest you explore the dataset by yourself, create your own hypothese or hunches, and analyze the data to discover </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">new insights.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Furthermore, there are many other regression datasets to play with; a small suggestion can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">here: </span></span><a href="https://www.kaggle.com/rtatman/datasets-for-regression-analysis"><span class="No-Break"><span class="koboSpan" id="kobo.436.1">https://www.kaggle.com/rtatman/datasets-for-regression-analysis</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.437.1">.</span></span></p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.438.1">Understanding classification datasets – loading, managing, and visualizing the Iris dataset</span></h1>
<p><span class="koboSpan" id="kobo.439.1">In the previous recipe, we studied </span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.440.1">one of the most common problem types in SL: regression. </span><span class="koboSpan" id="kobo.440.2">In this recipe, we will take a closer look at another of these problem </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">types: </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.442.1">classification</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">In classification problems, we </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.445.1">want to estimate a categorial output, a class, from a set of given classes, using a variable number of input features. </span><span class="koboSpan" id="kobo.445.2">In this recipe, we will analyze a toy classification dataset from Kaggle: the Iris dataset, one of the most renowned </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">classification datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.447.1">The Iris dataset </span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.448.1">presents the problem of estimating the </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">iris</span></strong><span class="koboSpan" id="kobo.450.1"> class of the flower of plants, from three classes (iris setosa, iris versicolor, and iris virginica) with the help of the following </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">four features:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.452.1">Sepal length (</span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.454.1">Sepal width (</span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.456.1">Petal length (</span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.458.1">Petal width (</span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">in cm)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.460.1">These data features are provided for 150 flowers, with 50 instances for each of the 3 classes (making it a </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">balanced dataset).</span></span></p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.462.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.463.1">This</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.464.1"> dataset is provided under the </span><em class="italic"><span class="koboSpan" id="kobo.465.1">CC0 Public Domain</span></em><span class="koboSpan" id="kobo.466.1"> license and can be downloaded </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">from </span></span><a href="https://www.kaggle.com/uciml/iris"><span class="No-Break"><span class="koboSpan" id="kobo.468.1">https://www.kaggle.com/uciml/iris</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.469.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.470.1">To read, manage, and visualize the data, we are going to follow a similar approach to the toy regression dataset in the previous recipe. </span><span class="koboSpan" id="kobo.470.2">We will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">pandas</span></strong><span class="koboSpan" id="kobo.472.1"> to manage the data, and we will use the most common data structure for</span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.473.1"> the library, DataFrames. </span><span class="koboSpan" id="kobo.473.2">Moreover, in order to plot the data and several visualizations we will compute, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">matplotlib</span></strong><span class="koboSpan" id="kobo.475.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">pyplot</span></strong><span class="koboSpan" id="kobo.477.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">seaborn</span></strong><span class="koboSpan" id="kobo.479.1"> libraries. </span><span class="koboSpan" id="kobo.479.2">Therefore, we must run the </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.481.1">
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns</span></pre> <p><span class="koboSpan" id="kobo.482.1">To load the data, we will introduce a new library that is very useful for managing datasets, called </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.484.1">. </span><span class="koboSpan" id="kobo.484.2">This library comes pre-installed with a set of datasets, including the </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">Iris dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.486.1">
from sklearn import datasets</span></pre> <p><span class="koboSpan" id="kobo.487.1">If you do not have the previously mentioned libraries installed, they can be easily installed with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">terminal commands:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.489.1">
!pip3 install matplotlib
!pip3 install pandas
!pip3 install seaborn
!pip3 install scikit-learn</span></pre> <p><span class="koboSpan" id="kobo.490.1">Therefore, to load the data, we can simply read the dataset by making use of </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">scikit-learn</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.492.1">library functions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.493.1">
iris = datasets.load_iris()
iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
 iris_df.insert(0, "class", iris.target)</span></pre> <p><span class="koboSpan" id="kobo.494.1">This is all we need to start working with our </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">classification dataset.</span></span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.496.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.497.1">In this section, we</span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.498.1"> will run an EDA that will help us understand which features are important (and which are not) to predict the iris class of a flower by completing </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">the following:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.500.1">Data structure</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.501.1">Correlation study</span></span></li>
<li><span class="koboSpan" id="kobo.502.1">One-versus-one comparison (</span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">pair plots)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.504.1">Violin plot</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.505.1">Data structure</span></h3>
<p><span class="koboSpan" id="kobo.506.1">Let’s analyze </span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.507.1">what our data looks like. </span><span class="koboSpan" id="kobo.507.2">For this, we will use common operations on </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">pandas</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.509.1"> DataFrames:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.510.1">
iris_df.info()</span></pre> <p><span class="koboSpan" id="kobo.511.1">From the output, we can draw the </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">following conclusions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.513.1">The data is complete (all columns have 150 values, </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">as expected)</span></span></li>
<li><span class="koboSpan" id="kobo.515.1">There are no </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">NULL</span></strong><span class="koboSpan" id="kobo.517.1"> values (the data </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">is clean!)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.519.1">In order to grasp what the values look like, let’s display the first </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">five properties:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.521.1">
iris_df.head()</span></pre> <p><span class="koboSpan" id="kobo.522.1">So far, we have looked at what the features look like. </span><span class="koboSpan" id="kobo.522.2">Now, let’s take a look at what the iris class distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">looks like:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.524.1">
iris.target_names</span></pre> <p><span class="koboSpan" id="kobo.525.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.527.1">
array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')</span></pre> <p><span class="koboSpan" id="kobo.528.1">If we want to confirm that there are 50 instances per class, we can run </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.530.1">
iris_df.groupby("class").size()</span></pre> <p><span class="koboSpan" id="kobo.531.1">This</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.532.1"> yields the </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.534.1">
Class
0 50
1 50
2 50
dtype: int64</span></pre> <p><span class="koboSpan" id="kobo.535.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">0</span></strong><span class="koboSpan" id="kobo.537.1"> corresponds to </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">setosa</span></strong><span class="koboSpan" id="kobo.539.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">1</span></strong><span class="koboSpan" id="kobo.541.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">versicolor</span></strong><span class="koboSpan" id="kobo.543.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">2</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.545.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.546.1">virginica</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.548.1">Correlation study</span></h3>
<p><span class="koboSpan" id="kobo.549.1">Here, we will </span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.550.1">analyze how each feature correlates with each other and, most importantly, how each feature correlates with the </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">iris class.</span></span></p>
<p><span class="koboSpan" id="kobo.552.1">We can compute a pairwise </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">correlation diagram:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.554.1">
iris_corr = iris_df.corr()</span></pre> <p><span class="koboSpan" id="kobo.555.1">To easily visualize the calculated correlations, we will plot </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">a heatmap:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.557.1">
plt.figure(figsize=(10, 10))
colormap = sns.color_palette("rocket_r", as_cmap=True)
sns.heatmap(iris_corr, annot=True, cmap=colormap)
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.558.1">These code statements yield the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<span class="koboSpan" id="kobo.560.1"><img alt="Figure 2.10 – Flower features correlation matrix" src="image/B16591_02_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.561.1">Figure 2.10 – Flower features correlation matrix</span></p>
<p><span class="koboSpan" id="kobo.562.1">Let’s</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.563.1"> note in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.564.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.565.1">.10</span></em><span class="koboSpan" id="kobo.566.1"> that the darker the cell is, the larger the </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">correlation value.</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">To emphasize the first row (most important as it shows the relationship between the iris class and the input features), we will run the </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.570.1">
iris_corr["class"].drop(["class"]).sort_values(
    ascending = False).plot.bar(figsize=(5,5))
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.571.1">And we have the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<span class="koboSpan" id="kobo.573.1"><img alt="Figure 2.11 – Flower features: iris class correlation" src="image/B16591_02_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.574.1">Figure 2.11 – Flower features: iris class correlation</span></p>
<p><span class="koboSpan" id="kobo.575.1">The following</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.576.1"> conclusions can be drawn from </span><em class="italic"><span class="koboSpan" id="kobo.577.1">Figures 2.10</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.578.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.579.1">2.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.581.1">Petal measurements (length and width) are highly correlated; analyzing and training both might not yield any </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">additional information.</span></span></li>
<li><span class="koboSpan" id="kobo.583.1">Petal measurements are the most highly correlated features with the </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">iris class.</span></span></li>
<li><span class="koboSpan" id="kobo.585.1">Sepal length and width are highly correlated as well but in opposite ways (sepal length is positively correlated while sepal width is </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">negatively correlated).</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.587.1">One-versus-one comparison (pair plots)</span></h3>
<p><span class="koboSpan" id="kobo.588.1">In classification problems, hue/brightness can be used to indicate the class of a plot. </span><span class="koboSpan" id="kobo.588.2">Moreover, as in this </span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.589.1">dataset we are able to work with a limited set of features (four), a pair plot diagram will be very useful to compare all features in a single plot. </span><span class="koboSpan" id="kobo.589.2">The code to plot this diagram is </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">shown here:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.591.1">
g = sns.pairplot(iris_df, hue="class", height=2, palette="rocket_r")
handles = g._legend_data.values()
labels = list(iris.target_names)
 g._legend.remove()
g.fig.legend(handles=handles, labels=labels, loc='upper left', ncol=3)</span></pre> <p><span class="koboSpan" id="kobo.592.1">And here’s the </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">displayed diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<span class="koboSpan" id="kobo.594.1"><img alt="Figure 2.12 – Flower features pair plot" src="image/B16591_02_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.595.1">Figure 2.12 – Flower features pair plot</span></p>
<p><span class="koboSpan" id="kobo.596.1">From this set of plots, we </span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.597.1">can draw the </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">following conclusions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.599.1">Setosa iris is easily differentiated using any of </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">the features</span></span></li>
<li><span class="koboSpan" id="kobo.601.1">Sepal features overlap among the different </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">iris classes</span></span></li>
<li><span class="koboSpan" id="kobo.603.1">Petal features have a direct relationship with the iris class; that is, the smallest numbers </span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.604.1">point to </span><strong class="bold"><span class="koboSpan" id="kobo.605.1">setosa</span></strong><span class="koboSpan" id="kobo.606.1">, medium numbers point to </span><strong class="bold"><span class="koboSpan" id="kobo.607.1">versicolor</span></strong><span class="koboSpan" id="kobo.608.1">, and the largest numbers point </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">to </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.610.1">virginica</span></strong></span></li>
<li><span class="koboSpan" id="kobo.611.1">There is a region on the boundaries</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.612.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.613.1">versicolor</span></strong><span class="koboSpan" id="kobo.614.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.615.1">virginica</span></strong><span class="koboSpan" id="kobo.616.1"> where </span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.617.1">both groups overlap, for a petal length larger than ~5 cm and a petal width larger than ~</span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">1.5 cm</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.619.1">Violin plot</span></h3>
<p><span class="koboSpan" id="kobo.620.1">Another </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.621.1">plot that might help with understanding the relationships between features and the iris class is a violin plot. </span><span class="koboSpan" id="kobo.621.2">The code to generate this plot is </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.623.1">
fig, axs = plt.subplots(2, 1)
 sns.violinplot(x="class", y="petal length (cm)", data=iris_df, size=5, palette='rocket_r', ax = axs[0])
 sns.violinplot(x="class", y="petal width (cm)", data=iris_df, size=5, palette='rocket_r', ax = axs[1])</span></pre> <p><span class="koboSpan" id="kobo.624.1">And here’s the </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">displayed diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.626.1"><img alt="Figure 2.13 – Flower features violin plot" src="image/B16591_02_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.627.1">Figure 2.13 – Flower features violin plot</span></p>
<p><span class="koboSpan" id="kobo.628.1">In these plots, the conclusions we found are even clearer, with the setosa (</span><strong class="source-inline"><span class="koboSpan" id="kobo.629.1">0</span></strong><span class="koboSpan" id="kobo.630.1">) iris class clearly separable and where there is substantial overlap for versicolor (</span><strong class="source-inline"><span class="koboSpan" id="kobo.631.1">1</span></strong><span class="koboSpan" id="kobo.632.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">virginica (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.636.1">The violin plot also provides an indication of the distribution of </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.637.1">the values in our case (starting with 0 to match the indexes of the classes in </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">the code):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.639.1">Setosa</span></strong><span class="koboSpan" id="kobo.640.1">: Values are more likely to be found around the mean (~1.5 cm for petal length and ~0.25 cm for </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">petal width).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">Versicolor</span></strong><span class="koboSpan" id="kobo.643.1">: Normal distribution with mean values ~4.25 cm and ~1.3 cm, and standard distribution values ~0.5 cm and ~0.2 cm (for petal length and petal </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">width respectively).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.645.1">Virginica</span></strong><span class="koboSpan" id="kobo.646.1">: Uniform distribution between [~5.1, ~5.9] cm and [~1.8, ~2.3] cm (for petal length and petal </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">width respectively).</span></span></li>
</ul>
<h2 id="_idParaDest-39"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.648.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.649.1">Classification problems are one of the most common problems where </span><strong class="bold"><span class="koboSpan" id="kobo.650.1">supervised ML</span></strong><span class="koboSpan" id="kobo.651.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.652.1">SML</span></strong><span class="koboSpan" id="kobo.653.1">) approaches can be applied. </span><span class="koboSpan" id="kobo.653.2">By studying a classic classification dataset, Iris Class in depth we can discover the connections between the input features (petal length and petal width) and the output feature (iris class). </span><span class="koboSpan" id="kobo.653.3">This analysis will help us build a model to predict the class in the </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">next chapter.</span></span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.655.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.656.1">In this section, we focused on an individual analysis of each feature against the iris class. </span><span class="koboSpan" id="kobo.656.2">This was similar in principle to the analysis done for regression datasets, with the added information about the hue/brightness for each plot. </span><span class="koboSpan" id="kobo.656.3">A similar suggestion to the reader follows, to continue and deepen the analysis by themselves to discover </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">new insights.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">We mentioned the Iris dataset is one of the classical classification datasets; nonetheless, it dates back to 1936! </span><span class="koboSpan" id="kobo.658.2">Original </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">reference: </span></span><a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x"><span class="No-Break"><span class="koboSpan" id="kobo.660.1">https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.661.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.662.1">Furthermore, as we will explore in the next chapter, classification problems can be seen as a special case of regression problems. </span><span class="koboSpan" id="kobo.662.2">In the regression recipe, we studied the prices of houses, and presented them in a way that would allow a buyer to determine which are affordable, by taking the price and comparing it with a threshold, which could be, for example, our budget limit. </span><span class="koboSpan" id="kobo.662.3">Therefore, we can use that threshold to classify houses that are below it as affordable and above it as not affordable. </span><span class="koboSpan" id="kobo.662.4">We will explore this connection in depth in the </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">next chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.664.1">Furthermore, there are many other classification datasets to play with; a small selection can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">here: </span></span><a href="https://www.kaggle.com/search?q=classification+tags%3Aclassification"><span class="No-Break"><span class="koboSpan" id="kobo.666.1">https://www.kaggle.com/search?q=classification+tags%3Aclassification</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.667.1">.</span></span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.668.1">Understanding image datasets – loading, managing, and visualizing the Fashion-MNIST dataset</span></h1>
<p><span class="koboSpan" id="kobo.669.1">One of the fields that </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.670.1">has grown considerably in DL in the last years has been </span><strong class="bold"><span class="koboSpan" id="kobo.671.1">computer vision</span></strong><span class="koboSpan" id="kobo.672.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.673.1">CV</span></strong><span class="koboSpan" id="kobo.674.1">). </span><span class="koboSpan" id="kobo.674.2">Since the AlexNet revolution in 2012, CV has expanded from lab research to surpassing human performance in real-world datasets (known as “in </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">the wild”).</span></span></p>
<p><span class="koboSpan" id="kobo.676.1">In this recipe, we will </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.677.1">explore the simplest CV task: </span><strong class="bold"><span class="koboSpan" id="kobo.678.1">image classification</span></strong><span class="koboSpan" id="kobo.679.1">. </span><span class="koboSpan" id="kobo.679.2">Given </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.680.1">a set of images, our task is to correctly classify that image among a given set of </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">labels (classes).</span></span></p>
<p><span class="koboSpan" id="kobo.682.1">One of the most classic image classification datasets</span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.683.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.684.1">MNIST </span></strong><span class="koboSpan" id="kobo.685.1">(which stands for the </span><strong class="bold"><span class="koboSpan" id="kobo.686.1">Modified National Institute of Standards and Technology</span></strong><span class="koboSpan" id="kobo.687.1">) database. </span><span class="koboSpan" id="kobo.687.2">Similarly sized, but more suited for current CV analysis, is the </span><em class="italic"><span class="koboSpan" id="kobo.688.1">Fashion-MNIST dataset</span></em><span class="koboSpan" id="kobo.689.1">. </span><span class="koboSpan" id="kobo.689.2">This </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.690.1">dataset is a multi-label image classification dataset, with a training set of 60k examples and a test set of 10k examples, with each example belonging to 1 of these 10 categories (starting with 0 to match the indexes of the classes in </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">the code):</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.692.1">T-shirt/top</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.693.1">Trouser</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.694.1">Pullover</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.695.1">Dress</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.696.1">Coat</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.697.1">Sandal</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.698.1">Shirt</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.699.1">Sneaker</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.700.1">Bag</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.701.1">Ankle boot</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.702.1">Each image is grayscale with 28x28 pixel dimensions. </span><span class="koboSpan" id="kobo.702.2">This can be seen as each data point having 784 features. </span><span class="koboSpan" id="kobo.702.3">The dataset is composed of 6k images per class in the training set and 1k images per class in the test set (</span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">balanced dataset).</span></span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.704.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.705.1">This </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.706.1">dataset is provided under the </span><em class="italic"><span class="koboSpan" id="kobo.707.1">MIT</span></em><span class="koboSpan" id="kobo.708.1"> license and can be downloaded from the following </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">URL: </span></span><a href="https://github.com/zalandoresearch/fashion-mnist"><span class="No-Break"><span class="koboSpan" id="kobo.710.1">https://github.com/zalandoresearch/fashion-mnist</span></span></a></p>
<p><span class="koboSpan" id="kobo.711.1">This dataset is directly available from MXNet Gluon, and therefore we will use this library to access it. </span><span class="koboSpan" id="kobo.711.2">Moreover, as this dataset is significantly larger than the others we have explored so far, to handle the data efficiently, we will use the Gluon </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">DataLoader functionality:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.713.1">
from mxnet import gluon
training_data_raw = gluon.data.vision.FashionMNIST(train=True)
 test_data_raw = gluon.data.vision.FashionMNIST(train=False)</span></pre> <p class="callout- eading"><span class="koboSpan" id="kobo.714.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.715.1">Gluon is installed with MXNet; no further steps </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">are required.</span></span></p>
<p><span class="koboSpan" id="kobo.717.1">This is all we need to start working with the </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">Fashion-MNIST dataset.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.719.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.720.1">Sometimes, data needs to be modified (transformed) for some operations. </span><span class="koboSpan" id="kobo.720.2">This can be done by defining a </span><strong class="source-inline"><span class="koboSpan" id="kobo.721.1">transform</span></strong><span class="koboSpan" id="kobo.722.1"> function and passing it as a </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">parameter (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.724.1">transform=&lt;function_name&gt;</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">).</span></span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.726.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.727.1">In this section, we </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.728.1">will run an EDA that will help us understand which features are important (and which are not) to predict the category of a garment, with the help of the </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.730.1">Identifying the </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">data structure</span></span></li>
<li><span class="koboSpan" id="kobo.732.1">Describing examples </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">per class</span></span></li>
<li><span class="koboSpan" id="kobo.734.1">Understanding dimensionality </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">reduction techniques</span></span></li>
<li><span class="koboSpan" id="kobo.736.1">Visualizing </span><strong class="bold"><span class="koboSpan" id="kobo.737.1">Principal Component </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.738.1">Analysis</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.739.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.740.1">PCA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.742.1">Visualizing </span><strong class="bold"><span class="koboSpan" id="kobo.743.1">t-distributed Stochastic Neighbor </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.744.1">Embedding</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.745.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.746.1">t-SNE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.748.1">Visualizing </span><strong class="bold"><span class="koboSpan" id="kobo.749.1">Uniform Manifold Approximation and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.750.1">Projection</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.751.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.752.1">UMAP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.754.1">Visualizing </span><strong class="bold"><span class="koboSpan" id="kobo.755.1">Python Minimum-Distortion </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.756.1">Embedding</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.757.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.758.1">PyMDE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">)</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.760.1">Identifying the data structure</span></h3>
<p><span class="koboSpan" id="kobo.761.1">In order to optimize </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.762.1">memory usage to work with large-scale datasets, instead of loading the full dataset in memory, datasets are usually accessed</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.763.1"> through </span><strong class="bold"><span class="koboSpan" id="kobo.764.1">batches</span></strong><span class="koboSpan" id="kobo.765.1">, which are smaller packets </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.767.1">Gluon has its own way of generating batches, while also</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.768.1"> applying </span><strong class="bold"><span class="koboSpan" id="kobo.769.1">shuffling</span></strong><span class="koboSpan" id="kobo.770.1">, to increase robustness during training: on each epoch, batches are shown in a random manner to the network. </span><span class="koboSpan" id="kobo.770.2">For testing, as we want repeatable results, this is avoided. </span><span class="koboSpan" id="kobo.770.3">We </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.771.1">select a </span><strong class="bold"><span class="koboSpan" id="kobo.772.1">batch size</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.773.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.774.1">128</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.776.1">
batch_size = 128
training_data_aux = gluon.data.DataLoader(
    training_data_raw, batch_size= batch_size, shuffle=True)
 test_data_aux = gluon.data.DataLoader(
    test_data_raw, batch_size= batch_size, shuffle=False)</span></pre> <p class="callout- eading"><span class="koboSpan" id="kobo.777.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.778.1">DataLoader does not return a data structure but an iterator. </span><span class="koboSpan" id="kobo.778.2">Therefore, to access the data we need to iterate upon it, we use constructs such as </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.779.1">for</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.780.1"> loops.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">Let’s verify the</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.782.1"> data structure is </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">as expected:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.784.1">
training_data_size = 0
for X_batch, y_batch in training_data_aux:
    if not training_data_size:
        print("X_batch has shape {}, and y_batch has shape {}"        .format(X_batch.shape, y_batch.shape))
    training_data_size += X_batch.shape[0]
 print("Training Dataset Samples: {}".format(training_data_size))
 test_data_size = 0
for X_batch, y_batch in test_data_aux:
    test_data_size += X_batch.shape[0]
print("Test Dataset Samples: {}".format(test_data_size))</span></pre> <p><span class="koboSpan" id="kobo.785.1">We obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">expected output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.787.1">
X_batch has shape (128, 28, 28, 1), and y_batch has shape (128,)
 Training Dataset Samples: 60000
Test Dataset Samples: 10000</span></pre> <p class="callout- eading"><span class="koboSpan" id="kobo.788.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.789.1">Gluon loads grayscale images as images with one channel, and the dimension for each batch is (batch size, height, width, number of channels); in our example, (128, 28, </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">28, 1).</span></span></p>
<h3><span class="koboSpan" id="kobo.791.1">Describing examples per class</span></h3>
<p><span class="koboSpan" id="kobo.792.1">The </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.793.1">Fashion-MNIST dataset is a balanced dataset with 6k examples </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">per class:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.795.1"><img alt="Figure 2.14 – Fashion-MNIST dataset labels" src="image/B16591_02_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.796.1">Figure 2.14 – Fashion-MNIST dataset labels</span></p>
<p><span class="koboSpan" id="kobo.797.1">Let’s take a look at what each class looks like. </span><span class="koboSpan" id="kobo.797.2">In order to do this, we can plot 10 examples </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">per class:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.799.1"><img alt="Figure 2.15 – Fashion-MNIST dataset" src="image/B16591_02_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.800.1">Figure 2.15 – Fashion-MNIST dataset</span></p>
<p><span class="koboSpan" id="kobo.801.1">As we can </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.802.1">see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.803.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.804.1">.15</span></em><span class="koboSpan" id="kobo.805.1">, all instances can be differentiated fairly well by a human, except for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.806.1">T-shirt</span></strong><span class="koboSpan" id="kobo.807.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.808.1">top</span></strong><span class="koboSpan" id="kobo.809.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">Pullover</span></strong><span class="koboSpan" id="kobo.811.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">Coat</span></strong><span class="koboSpan" id="kobo.813.1">, and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">Shirt</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.815.1"> classes.</span></span></p>
<h3><span class="koboSpan" id="kobo.816.1">Understanding dimensionality reduction techniques</span></h3>
<p><span class="koboSpan" id="kobo.817.1">Apart </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.818.1">from the large number of data points that our dataset contains, the number of features in the image (that is, the number of pixels per image) is very high. </span><span class="koboSpan" id="kobo.818.2">In our toy dataset, each image has 784 features, which can be seen as 1 point in 784-dimensional space. </span><span class="koboSpan" id="kobo.818.3">In this space, it is extremely difficult to analyze relationships among features (for example, correlation, as we explored in previous datasets). </span><span class="koboSpan" id="kobo.818.4">Furthermore, it is not rare to work with higher-quality images, with resolutions over 1 MP (more than 1 million features). </span><span class="koboSpan" id="kobo.818.5">For a 4k image, the number of features is ~</span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">8 million.</span></span></p>
<p><span class="koboSpan" id="kobo.820.1">Therefore, in this</span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.821.1"> subsection and the next ones (on </span><em class="italic"><span class="koboSpan" id="kobo.822.1">PCA</span></em><span class="koboSpan" id="kobo.823.1">, </span><em class="italic"><span class="koboSpan" id="kobo.824.1">t-SNE</span></em><span class="koboSpan" id="kobo.825.1">, </span><em class="italic"><span class="koboSpan" id="kobo.826.1">UMAP</span></em><span class="koboSpan" id="kobo.827.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.828.1">PyMDE</span></em><span class="koboSpan" id="kobo.829.1">), we will work with techniques known</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.830.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.831.1">dimensionality reduction</span></strong><span class="koboSpan" id="kobo.832.1"> techniques. </span><span class="koboSpan" id="kobo.832.2">The idea behind these techniques is to be able to visualize high-dimensional features easily, typically in 2D or 3D, which are the kinds of visualizations humans are used to working with. </span><span class="koboSpan" id="kobo.832.3">These embeddings have two or three components that can be plotted in 2D or 3D. </span><span class="koboSpan" id="kobo.832.4">These representations are dataset-dependent; they are </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.833.1">learned</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.834.1"> representations.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">Each technique described has a different way of achieving this result. </span><span class="koboSpan" id="kobo.835.2">In this book, we will not deepen our knowledge of how each technique works, but the interested reader can find more information in the </span><em class="italic"><span class="koboSpan" id="kobo.836.1">There’s </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.837.1">more...</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.838.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.839.1">Please also note that although each technique is different, all of them require a vector as an input (feature vector). </span><span class="koboSpan" id="kobo.839.2">This means that there is some spatial information that is lost. </span><span class="koboSpan" id="kobo.839.3">In our example, from 28x28 images, we will input 784 </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">feature vectors.</span></span></p>
<h3><span class="koboSpan" id="kobo.841.1">Visualizing PCA</span></h3>
<p><span class="koboSpan" id="kobo.842.1">As </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.843.1">expected, we </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.844.1">can see some large clusters (</span><strong class="bold"><span class="koboSpan" id="kobo.845.1">Sneaker</span></strong><span class="koboSpan" id="kobo.846.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.847.1">Ankle boot</span></strong><span class="koboSpan" id="kobo.848.1">), and others are mostly overlapping (</span><strong class="bold"><span class="koboSpan" id="kobo.849.1">T-shirt</span></strong><span class="koboSpan" id="kobo.850.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.851.1">Pullover</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.852.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.853.1">Coat</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.855.1"><img alt="Figure 2.16 – Fashion-MNIST 2D PCA" src="image/B16591_02_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.856.1">Figure 2.16 – Fashion-MNIST 2D PCA</span></p>
<h3><span class="koboSpan" id="kobo.857.1">Visualizing t-SNE</span></h3>
<p><span class="koboSpan" id="kobo.858.1">Another technique</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.859.1"> for dimensionality reduction is t-SNE. </span><span class="koboSpan" id="kobo.859.2">This </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.860.1">technique is based on the idea of computing a probability distribution that represents similarities among neighbors. </span><span class="koboSpan" id="kobo.860.2">A recommended preliminary step is to compute PCA for 50 features and then pass these 50 feature vectors to the t-SNE algorithm. </span><span class="koboSpan" id="kobo.860.3">This is what we did to generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.862.1"><img alt="Figure 2.17 – Fashion-MNIST 2D t-SNE" src="image/B16591_02_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.863.1">Figure 2.17 – Fashion-MNIST 2D t-SNE</span></p>
<p><span class="koboSpan" id="kobo.864.1">In this </span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.865.1">plot, we</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.866.1"> can see in a clearer way how easily distinguishable objects are clustered in isolation (</span><strong class="bold"><span class="koboSpan" id="kobo.867.1">Trouser</span></strong><span class="koboSpan" id="kobo.868.1">, on the lower right, and </span><strong class="bold"><span class="koboSpan" id="kobo.869.1">Bag</span></strong><span class="koboSpan" id="kobo.870.1">, on the </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">upper left).</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.872.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.873.1">For PCA and t-SNE, we can choose three components instead of two, which will yield a 3D plot. </span><span class="koboSpan" id="kobo.873.2">For the code, visit the GitHub repository of the </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">book:</span></span><span class="No-Break"> </span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook"><span class="No-Break"><span class="koboSpan" id="kobo.875.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.876.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.877.1">Visualizing UMAP</span></h3>
<p><span class="koboSpan" id="kobo.878.1">Another </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.879.1">method for dimensionality reduction</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.880.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.881.1">UMAP</span></strong><span class="koboSpan" id="kobo.882.1">. </span><span class="koboSpan" id="kobo.882.2">UMAP </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.883.1">allows us to play with different parameters, such as the </span><em class="italic"><span class="koboSpan" id="kobo.884.1">number of neighbors</span></em><span class="koboSpan" id="kobo.885.1">, which helps the visualization of how to balance a local versus global structure. </span><span class="koboSpan" id="kobo.885.2">For an example with five neighbors, this is </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">the visualization:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.887.1"><img alt="Figure 2.18 – Fashion-MNIST UMA" src="image/B16591_02_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.888.1">Figure 2.18 – Fashion-MNIST UMA</span></p>
<p><span class="koboSpan" id="kobo.889.1">In this visualization, we can observe similar trends as in previous plots; that is, </span><strong class="bold"><span class="koboSpan" id="kobo.890.1">Bag</span></strong><span class="koboSpan" id="kobo.891.1"> is clustered in the upper-center region, and </span><strong class="bold"><span class="koboSpan" id="kobo.892.1">Trouser</span></strong><span class="koboSpan" id="kobo.893.1"> in the lower-center region. </span><span class="koboSpan" id="kobo.893.2">However, in this visualization, we can also note that there is a cluster on the left that contains data for </span><strong class="bold"><span class="koboSpan" id="kobo.894.1">Ankle boot</span></strong><span class="koboSpan" id="kobo.895.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.896.1">Sneaker</span></strong><span class="koboSpan" id="kobo.897.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.898.1">Sandal</span></strong><span class="koboSpan" id="kobo.899.1">, and another important cluster on the right for </span><strong class="bold"><span class="koboSpan" id="kobo.900.1">Shirt</span></strong><span class="koboSpan" id="kobo.901.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.902.1">Coat</span></strong><span class="koboSpan" id="kobo.903.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.904.1">Dress</span></strong><span class="koboSpan" id="kobo.905.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.906.1">T-shirt/top</span></strong><span class="koboSpan" id="kobo.907.1">, and we can see how these clusters overlap with </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">each other.</span></span></p>
<p><span class="koboSpan" id="kobo.909.1">To install UMAP, please run </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">this command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.911.1">
!pip3 install umap-learn</span></pre> <h3><span class="koboSpan" id="kobo.912.1">Visualizing PyMDE</span></h3>
<p><span class="koboSpan" id="kobo.913.1">Another </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.914.1">popular technique that provides insightful </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.915.1">visualizations is </span><strong class="bold"><span class="koboSpan" id="kobo.916.1">PyMDE</span></strong><span class="koboSpan" id="kobo.917.1">. </span><span class="koboSpan" id="kobo.917.2">PyMDE allows two main approaches, to preserve neighbors (local structure of the data is preserved) and to preserve distances. </span><span class="koboSpan" id="kobo.917.3">This preserves relationship attributes such as pairwise distances in the data. </span><span class="koboSpan" id="kobo.917.4">The approach to preserve neighbors is similar to the plots we </span><span class="No-Break"><span class="koboSpan" id="kobo.918.1">are seeing:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.919.1"><img alt="Figure 2.19 – Fashion-MNIST PyMDE" src="image/B16591_02_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.920.1">Figure 2.19 – Fashion-MNIST PyMDE</span></p>
<p><span class="koboSpan" id="kobo.921.1">As we can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.923.1">.19</span></em><span class="koboSpan" id="kobo.924.1">, very similar conclusions to UMAP can be drawn from a </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">PyMDE visualization.</span></span></p>
<p><span class="koboSpan" id="kobo.926.1">To install UMAP, please run the </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.928.1">
!pip3 install pymde</span></pre> <h2 id="_idParaDest-44"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.929.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.930.1">To understand </span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.931.1">an image dataset, we need to understand the underlying connections among images in that dataset. </span><span class="koboSpan" id="kobo.931.2">One useful method to achieve this is with </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">different visualizations.</span></span></p>
<p><span class="koboSpan" id="kobo.933.1">In this recipe, we have learned how to discover patterns in our image datasets. </span><span class="koboSpan" id="kobo.933.2">We selected a well-researched dataset, Fashion-MNIST, and learned one of the most important approaches for working with large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">datasets: </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.935.1">batching</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.937.1">We analyzed our dataset by taking a look at its internal structure and what the actual images looked like and tried to foresee where potential classification algorithms could have issues (such as similarities between coats and shirts and ankle boots </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">and sneakers).</span></span></p>
<p><span class="koboSpan" id="kobo.939.1">Every pixel is a dimension/feature of each image, and, therefore, to work with them, we learned about some dimensionality reduction techniques: PCA, t-SNE, UMAP, and PyMDE. </span><span class="koboSpan" id="kobo.939.2">With these visualizations, we were able to verify and extend our knowledge of </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">the dataset.</span></span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.941.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.942.1">There are many resources for MNIST and Fashion-MNIST, as these are well-researched datasets. </span><span class="koboSpan" id="kobo.942.2">I personally recommend </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.944.1">MNIST </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.945.1">database: </span></strong></span><a href="https://en.wikipedia.org/wiki/MNIST_database"><span class="No-Break"><span class="koboSpan" id="kobo.946.1">https://en.wikipedia.org/wiki/MNIST_database</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.947.1">zalandoresearch/fashion-mnist: </span></strong></span><a href="https://github.com/zalandoresearch/fashion-mnist"><span class="No-Break"><span class="koboSpan" id="kobo.948.1">https://github.com/zalandoresearch/fashion-mnist</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.949.1">We introduced some dimensionality reduction techniques, but we did not deepen our knowledge of them. </span><span class="koboSpan" id="kobo.949.2">If you want to understand better how each of these techniques works, I suggest the </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">following resources:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.951.1">PCA (from </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.952.1">Caltech)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">: </span></span><a href="http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/PrincipalComponentAnalysis.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.954.1">http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/PrincipalComponentAnalysis.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.955.1">t-SNE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">: </span></span><a href="https://lvdmaaten.github.io/tsne/"><span class="No-Break"><span class="koboSpan" id="kobo.957.1">https://lvdmaaten.github.io/tsne/</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.958.1">UMAP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">: </span></span><a href="https://umap-learn.readthedocs.io/"><span class="No-Break"><span class="koboSpan" id="kobo.960.1">https://umap-learn.readthedocs.io/</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.961.1">PyMDE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">: </span></span><a href="https://pymde.org/"><span class="No-Break"><span class="koboSpan" id="kobo.963.1">https://pymde.org/</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.964.1">In the code, you</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.965.1"> can find how to obtain the visualizations included. </span><span class="koboSpan" id="kobo.965.2">Furthermore, for PCA and t-SNE, as the number of components is a variable, 3D plots for both of them </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">are included.</span></span></p>
<p><span class="koboSpan" id="kobo.967.1">Finally, for readers interested in learning more about DL and its history, I recommend the following </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">link: </span></span><a href="https://www.skynettoday.com/overviews/neural-net-history"><span class="No-Break"><span class="koboSpan" id="kobo.969.1">https://www.skynettoday.com/overviews/neural-net-history</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.970.1">.</span></span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.971.1">Understanding text datasets – loading, managing, and visualizing the Enron Email dataset</span></h1>
<p><span class="koboSpan" id="kobo.972.1">Another field that has grown considerably in DL in </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.973.1">recent years is </span><strong class="bold"><span class="koboSpan" id="kobo.974.1">natural language processing</span></strong><span class="koboSpan" id="kobo.975.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.976.1">NLP</span></strong><span class="koboSpan" id="kobo.977.1">). </span><span class="koboSpan" id="kobo.977.2">Similarly to CV, this field aims to surpass human performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">real-world datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.979.1">In this recipe, we will explore </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.980.1">one of the simplest NLP tasks: </span><strong class="bold"><span class="koboSpan" id="kobo.981.1">text classification</span></strong><span class="koboSpan" id="kobo.982.1">. </span><span class="koboSpan" id="kobo.982.2">Given a set of sentences and paragraphs, our task is to correctly classify that text among a given set of </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">labels (classes).</span></span></p>
<p><span class="koboSpan" id="kobo.984.1">One of the</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.985.1"> most classic text classification tasks is to distinguish whether received email is spam or not (ham). </span><span class="koboSpan" id="kobo.985.2">These datasets are binary text classification datasets (only two labels to assign, </span><strong class="source-inline"><span class="koboSpan" id="kobo.986.1">0</span></strong><span class="koboSpan" id="kobo.987.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.988.1">1</span></strong><span class="koboSpan" id="kobo.989.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">ham</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.991.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.992.1">spam</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.994.1">In our</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.995.1"> specific scenario, we will use a real-world email dataset. </span><span class="koboSpan" id="kobo.995.2">This set of emails was made public during the investigation of the Enron scandal in the early 2000s by the US Government. </span><span class="koboSpan" id="kobo.995.3">This dataset was first published in 2004 and is composed of emails from ~150 users, mostly senior management at Enron. </span><span class="koboSpan" id="kobo.995.4">Only a subset (known as </span><strong class="source-inline"><span class="koboSpan" id="kobo.996.1">enron1</span></strong><span class="koboSpan" id="kobo.997.1">) is used in </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">this section.</span></span></p>
<p><span class="koboSpan" id="kobo.999.1">It contains 5,171 emails, with no training/test split (labels are provided for all examples). </span><span class="koboSpan" id="kobo.999.2">Being a real-world dataset, emails vary heavily with respect to subjects, content length, word count, and word length, and out of the box, the dataset only contains </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">two features:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1001.1">Label/class</span></strong><span class="koboSpan" id="kobo.1002.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1003.1">0</span></strong><span class="koboSpan" id="kobo.1004.1"> corresponds to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1005.1">Ham</span></strong><span class="koboSpan" id="kobo.1006.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1007.1">1</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1008.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1009.1">Spam</span></strong></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1010.1">Text</span></strong><span class="koboSpan" id="kobo.1011.1">: Includes the subject and the body of </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">the email</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1013.1">The dataset is composed of 3,672 examples of ham email (~70%) and 1,499 examples of spam email (~30%); it is a highly </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">imbalanced dataset.</span></span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.1015.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.1016.1">This dataset is</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.1017.1"> provided under the </span><em class="italic"><span class="koboSpan" id="kobo.1018.1">CC0 Public Domain</span></em><span class="koboSpan" id="kobo.1019.1"> license and can be downloaded </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">from </span></span><a href="https://www.kaggle.com/venky73/spam-mails-dataset"><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">https://www.kaggle.com/venky73/spam-mails-dataset</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1023.1">To read the data, we are going to follow a similar approach as seen in the recipe for regression tasks. </span><span class="koboSpan" id="kobo.1023.2">We are going to load the data from a CSV file, and we are going to work with the data using very well-known Python libraries: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1024.1">pandas</span></strong><span class="koboSpan" id="kobo.1025.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">pyplot</span></strong><span class="koboSpan" id="kobo.1027.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1028.1">seaborn</span></strong><span class="koboSpan" id="kobo.1029.1">. </span><span class="koboSpan" id="kobo.1029.2">Therefore, we must run the </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1031.1">
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns</span></pre> <p><span class="koboSpan" id="kobo.1032.1">Therefore, to load the data, we can simply read the file containing it (the file can be found in the book’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1033.1">GitHub repository):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1034.1">
emails_df = pd.read_csv("spam_ham_dataset.csv")</span></pre> <p><span class="koboSpan" id="kobo.1035.1">This is all we need to start working with our spam </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">email dataset.</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.1037.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.1038.1">In this section, we will run an EDA that will help us understand which features are important (and which are not) to predict whether an email is spam or not, the following are not worded as steps. </span><span class="koboSpan" id="kobo.1038.2">please either reword this to a more suitable lead-in or reword the following as steps. </span><span class="koboSpan" id="kobo.1038.3">if the latter, please change the circular bullets </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">to numbering:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">Data structure</span></span></li>
<li><span class="koboSpan" id="kobo.1041.1">Examples </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">per class</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">Content analysis</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1044.1">Data cleaning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">N-grams</span></span></li>
<li><span class="koboSpan" id="kobo.1046.1">Word processing (tokenizing, stop words, stemming, </span><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">and lemmatization)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">Word clouds</span></span></li>
<li><span class="koboSpan" id="kobo.1049.1">Word </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.1050.1">embeddings (word2vec and </span><strong class="bold"><span class="koboSpan" id="kobo.1051.1">Global Vectors for Word </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1052.1">Representation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1053.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1054.1">GloVe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">))</span></span></li>
<li><span class="koboSpan" id="kobo.1056.1">PCA </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">and t-SNE</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.1058.1">Data structure</span></h3>
<p><span class="koboSpan" id="kobo.1059.1">The </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.1060.1">first step we will carry out will be to reformat the dataset for </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">our purposes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1062.1">
# Removing Unnecessary column
emails_df.drop("Unnamed: 0", axis=1, inplace=True)
 # Changing column names
emails_df.columns = ["label", "text", "class"]</span></pre> <p><span class="koboSpan" id="kobo.1063.1">After these modifications, the shape of our email DataFrame is </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1065.1">
(5171, 3)</span></pre> <h3><span class="koboSpan" id="kobo.1066.1">Examples per class</span></h3>
<p><span class="koboSpan" id="kobo.1067.1">We will </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.1068.1">now take a look at each </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">class distribution:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1070.1">
Label
ham 3672
spam 1499
dtype: int64</span></pre> <p><span class="koboSpan" id="kobo.1071.1">The following is </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.1073.1"><img alt="Figure 2.20 – Spam emails dataset" src="image/B16591_02_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1074.1">Figure 2.20 – Spam emails dataset</span></p>
<p><span class="koboSpan" id="kobo.1075.1">As we can see, the</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.1076.1"> dataset is </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">highly imbalanced.</span></span></p>
<h3><span class="koboSpan" id="kobo.1078.1">Content analysis</span></h3>
<p><span class="koboSpan" id="kobo.1079.1">In this </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.1080.1">section, we are going to analyze the emails’ length and </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">their distribution:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.1082.1"><img alt="Figure 2.21 – Emails’ length" src="image/B16591_02_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1083.1">Figure 2.21 – Emails’ length</span></p>
<p><span class="koboSpan" id="kobo.1084.1">There is</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.1085.1"> a large outlier set corresponding to emails with more than 5,000 characters. </span><span class="koboSpan" id="kobo.1085.2">Let’s zoom in to the area where most of the emails lie and graph the length and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">word count:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.1087.1"><img alt="Figure 2.22 – Emails’ length (detailed)" src="image/B16591_02_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1088.1">Figure 2.22 – Emails’ length (detailed)</span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.1089.1"><img alt="Figure 2.23 – Emails’ word count" src="image/B16591_02_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1090.1">Figure 2.23 – Emails’ word count</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1091.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1092.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1093.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1094.1">.23</span></em><span class="koboSpan" id="kobo.1095.1">, we have defined a word without any semantic or dictionary approach, simply by specifying that each space-separated entity constitutes a word. </span><span class="koboSpan" id="kobo.1095.2">This approach has disadvantages that we will analyze further in this recipe and in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1096.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1098.1">By looking at </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.1099.1">this graph, we can conclude that in terms of emails’ length and word count, there is no significant difference between spam and legitimate emails. </span><span class="koboSpan" id="kobo.1099.2">We will need to understand more about the words, their meaning, and their relationships to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">our analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.1101.1">Therefore, let’s start by looking at which words are most frequent in </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.1103.1"><img alt="Figure 2.24 – Most frequent words" src="image/B16591_02_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1104.1">Figure 2.24 – Most frequent words</span></p>
<p><span class="koboSpan" id="kobo.1105.1">The</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.1106.1"> first and most important conclusion after seeing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1107.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1108.1">.24</span></em><span class="koboSpan" id="kobo.1109.1"> is that our initial space-separation approach to differentiate words was not enough when dealing with real-world datasets. </span><span class="koboSpan" id="kobo.1109.2">Punctuation errors and typos are very common, and furthermore, as expected, very common words such as “the” and “to” yield no real benefit in understanding the differences between spam and </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">legitimate emails.</span></span></p>
<h3><span class="koboSpan" id="kobo.1111.1">Data cleaning</span></h3>
<p><span class="koboSpan" id="kobo.1112.1">Let’s get rid</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.1113.1"> of some common issues when working with real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">text datasets:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">Punctuation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">Trailing characters</span></span></li>
<li><span class="koboSpan" id="kobo.1117.1">Clarifications” (text between </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">square brackets)</span></span></li>
<li><span class="koboSpan" id="kobo.1119.1">Words containing numbers </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">and links</span></span></li>
<li><span class="koboSpan" id="kobo.1121.1">The word </span><em class="italic"><span class="koboSpan" id="kobo.1122.1">subject</span></em><span class="koboSpan" id="kobo.1123.1"> (specific to our email </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">dataset structure)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1125.1">After </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.1126.1">processing </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.1127.1">our corpus (text data specific to our problem) through our cleaning function, the results are more similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">our expectations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.1129.1"><img alt="Figure 2.25 – Most frequent words (clean)" src="image/B16591_02_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1130.1">Figure 2.25 – Most frequent words (clean)</span></p>
<p><span class="koboSpan" id="kobo.1131.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1132.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1133.1">.25</span></em><span class="koboSpan" id="kobo.1134.1">, we </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.1135.1">can see how the new word corpus we are analyzing contains real words. </span><span class="koboSpan" id="kobo.1135.2">However, it is very clear that the most frequent words don't help to distinguish between spam and legitimate emails; words such as “the” and “to” are too common in the English language to be used properly for </span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">this classification.</span></span></p>
<h3><span class="koboSpan" id="kobo.1137.1">N-grams</span></h3>
<p><span class="koboSpan" id="kobo.1138.1">N-grams</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.1139.1"> from a corpus in NLP are a set of </span><em class="italic"><span class="koboSpan" id="kobo.1140.1">N</span></em><span class="koboSpan" id="kobo.1141.1"> co-occurring words in the corpus. </span><span class="koboSpan" id="kobo.1141.2">Typically in NLP, the most common N-grams are </span><em class="italic"><span class="koboSpan" id="kobo.1142.1">unigrams</span></em><span class="koboSpan" id="kobo.1143.1"> (one word), </span><em class="italic"><span class="koboSpan" id="kobo.1144.1">bigrams</span></em><span class="koboSpan" id="kobo.1145.1"> (two words), and </span><em class="italic"><span class="koboSpan" id="kobo.1146.1">trigrams</span></em><span class="koboSpan" id="kobo.1147.1"> (three words). </span><span class="koboSpan" id="kobo.1147.2">Plotting the most frequent N-grams helps us understand relationships among words and classes (spam or not). </span><span class="koboSpan" id="kobo.1147.3">A unigram is simply the most frequent word graph, as plotted in the previous section in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1148.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1149.1">.25</span></em><span class="koboSpan" id="kobo.1150.1">. </span><span class="koboSpan" id="kobo.1150.2">For bigrams (per class), see </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.1152.1"><img alt="Figure 2.26 – Most frequent bigrams in ham (a) and spam (b)" src="image/B16591_02_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1153.1">Figure 2.26 – Most frequent bigrams in ham (a) and spam (b)</span></p>
<p><span class="koboSpan" id="kobo.1154.1">For trigrams (per class), see </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.1156.1"><img alt="Figure 2.27 – Most frequent trigrams in ham (a) and spam (b)" src="image/B16591_02_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1157.1">Figure 2.27 – Most frequent trigrams in ham (a) and spam (b)</span></p>
<p><span class="koboSpan" id="kobo.1158.1">In these graphs, we </span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.1159.1">can start to grasp the underlying differences between the </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">two classes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1161.1">If there is a mention of </span><em class="italic"><span class="koboSpan" id="kobo.1162.1">Enron Corp</span></em><span class="koboSpan" id="kobo.1163.1">, it is very likely to be a </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">legitimate email</span></span></li>
<li><span class="koboSpan" id="kobo.1165.1">If there is a polite call to action (“please let me know”), it is very likely to be a </span><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">legitimate email</span></span></li>
<li><span class="koboSpan" id="kobo.1167.1">If there is a link, it is very likely to </span><span class="No-Break"><span class="koboSpan" id="kobo.1168.1">be spam</span></span></li>
<li><span class="koboSpan" id="kobo.1169.1">If there are typos (</span><em class="italic"><span class="koboSpan" id="kobo.1170.1">hou</span></em><span class="koboSpan" id="kobo.1171.1"> instead of </span><em class="italic"><span class="koboSpan" id="kobo.1172.1">how</span></em><span class="koboSpan" id="kobo.1173.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1174.1">ect</span></em><span class="koboSpan" id="kobo.1175.1"> instead of </span><em class="italic"><span class="koboSpan" id="kobo.1176.1">etc</span></em><span class="koboSpan" id="kobo.1177.1">, and so on), it is very likely to be a </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">legitimate email</span></span></li>
<li><span class="koboSpan" id="kobo.1179.1">If </span><em class="italic"><span class="koboSpan" id="kobo.1180.1">pills</span></em><span class="koboSpan" id="kobo.1181.1"> are mentioned, it is very likely to be spam (plus a bonus </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">for repetitiveness)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1183.1">We have also discovered that there are some nuances related to how the emails have been coded: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1184.1">nbsp</span></strong><span class="koboSpan" id="kobo.1185.1"> (for non-breaking space). </span><span class="koboSpan" id="kobo.1185.2">It is very likely that the email parser found some kind of unstructured spaces in the text and has redacted them with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1186.1">nbsp</span></strong><span class="koboSpan" id="kobo.1187.1"> keyword. </span><span class="koboSpan" id="kobo.1187.2">Coincidentally, these types of parsing nuances are much more common in spam emails than in legitimate ones, which will help us in </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">our analysis.</span></span></p>
<h3><span class="koboSpan" id="kobo.1189.1">Word processing</span></h3>
<p><span class="koboSpan" id="kobo.1190.1">Processing words in text is typically composed of </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">four steps:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">Tokenizing</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1193.1">Stop-words filtering</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">Stemming</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1195.1">Lemmatization</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1196.1">These steps</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.1197.1"> have their own individual complexity, and therefore we will use libraries available to run these steps, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.1198.1">Natural Language Toolkit</span></strong><span class="koboSpan" id="kobo.1199.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1200.1">NLTK</span></strong><span class="koboSpan" id="kobo.1201.1">). </span><span class="koboSpan" id="kobo.1201.2">To </span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.1202.1">install it, run the </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1204.1">
!pip3 install nltk</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.1205.1">Tokenizing</span></strong><span class="koboSpan" id="kobo.1206.1"> is the </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.1207.1">step that processes a text and returns a list of </span><strong class="bold"><span class="koboSpan" id="kobo.1208.1">tokens</span></strong><span class="koboSpan" id="kobo.1209.1">. </span><span class="koboSpan" id="kobo.1209.2">Each </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.1210.1">word is a token, but if there are also punctuation marks, these would become separate tokens. </span><span class="koboSpan" id="kobo.1210.2">Nevertheless, for our corpus, these have been removed in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">previous step.</span></span></p>
<p><span class="koboSpan" id="kobo.1212.1">Please note</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.1213.1"> that in this step, we have moved from a list of sentences and paragraphs per email, the corpus, to what is known as </span><strong class="bold"><span class="koboSpan" id="kobo.1214.1">bag of words</span></strong><span class="koboSpan" id="kobo.1215.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.1216.1">BOW</span></strong><span class="koboSpan" id="kobo.1217.1">, which is directly connected to the vocabulary used in </span><span class="No-Break"><span class="koboSpan" id="kobo.1218.1">the corpus.</span></span></p>
<p><span class="koboSpan" id="kobo.1219.1">After we have each word as an entity, we can remove those common words we had already identified such as “the” or “to." </span><span class="koboSpan" id="kobo.1219.2">These are known as stop words, and NLTK contains a set of these stop words for several languages. </span><span class="koboSpan" id="kobo.1219.3">We will use this available set to filter </span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">our corpus.</span></span></p>
<p><span class="koboSpan" id="kobo.1221.1">Stemming is the process of reducing derived (and inflected, if we want to be formal) words to their root, known as </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">the stem.</span></span></p>
<p><span class="koboSpan" id="kobo.1223.1">Lemmatization is the process of grouping together several different forms that can be analyzed as a single item, identified by the word’s lemma, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1224.1">dictionary form:</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.1225.1"><img alt="Figure 2.28 – Stemming and lemmatization" src="image/B16591_02_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1226.1">Figure 2.28 – Stemming and lemmatization</span></p>
<p><span class="koboSpan" id="kobo.1227.1">After processing</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.1228.1"> our BOW through these steps, we have reduced the number of words we are working with to ~10% of </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">the corpus:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1230.1">
Raw Corpus (Ham): 3133632
Processed Corpus (Ham): 317496 (~10%)
Raw Corpus (Spam): 1712737
Processed Corpus (Spam): 177780 (~10%)</span></pre> <h3><span class="koboSpan" id="kobo.1231.1">Word clouds</span></h3>
<p><span class="koboSpan" id="kobo.1232.1">With </span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.1233.1">our postprocessed BOW, we can generate one of the most impactful and popular visualizations of a text corpus, a </span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">word cloud:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.1235.1"><img alt="Figure 2.29 – Word clouds for (a) ham and (b) spam" src="image/B16591_02_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1236.1">Figure 2.29 – Word clouds for (a) ham and (b) spam</span></p>
<p><span class="koboSpan" id="kobo.1237.1">In these visualizations we can clearly see how </span><em class="italic"><span class="koboSpan" id="kobo.1238.1">Enron</span></em><span class="koboSpan" id="kobo.1239.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1240.1">please</span></em><span class="koboSpan" id="kobo.1241.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1242.1">let know</span></em><span class="koboSpan" id="kobo.1243.1"> are relevant for legitimate emails, whereas </span><em class="italic"><span class="koboSpan" id="kobo.1244.1">new</span></em><span class="koboSpan" id="kobo.1245.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1246.1">nbsp</span></em><span class="koboSpan" id="kobo.1247.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1248.1">compani</span></em><span class="koboSpan" id="kobo.1249.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1250.1">market</span></em><span class="koboSpan" id="kobo.1251.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1252.1">product</span></em><span class="koboSpan" id="kobo.1253.1"> are typically connected to </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">spam emails.</span></span></p>
<h3><span class="koboSpan" id="kobo.1255.1">Word embeddings</span></h3>
<p><span class="koboSpan" id="kobo.1256.1">So far, we</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.1257.1"> have taken a look at how individual words behave (frequency and length) and their connections to other words in terms of the most frequent N-grams (bigrams and trigrams). </span><span class="koboSpan" id="kobo.1257.2">However, we have not connected the meaning of the words among them. </span><span class="koboSpan" id="kobo.1257.3">For example, one would expect that </span><em class="italic"><span class="koboSpan" id="kobo.1258.1">Enron</span></em><span class="koboSpan" id="kobo.1259.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1260.1">corp</span></em><span class="koboSpan" id="kobo.1261.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1262.1">company</span></em><span class="koboSpan" id="kobo.1263.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1264.1">compani</span></em><span class="koboSpan" id="kobo.1265.1">, its stemmed counterpart) are close from a semantic point of view. </span><span class="koboSpan" id="kobo.1265.2">Therefore, we would like to have a representation where words with a similar meaning would have a similar representation. </span><span class="koboSpan" id="kobo.1265.3">Furthermore, if that representation had a constant number of dimensions, we could easily make comparisons (find similarities) among words. </span><span class="koboSpan" id="kobo.1265.4">These are word embeddings, and the representation is </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">a vector.</span></span></p>
<p><span class="koboSpan" id="kobo.1267.1">There are infinite ways of generating vectors from words; for example, the most naive way to accomplish this is to generate as many dimensions (features of our vector, columns in a matrix representation) as our vocabulary, and then in each email (a row in the matrix representation), for each word that it contains, we can put a </span><em class="italic"><span class="koboSpan" id="kobo.1268.1">1</span></em><span class="koboSpan" id="kobo.1269.1"> (checkmark) in the column for that word in the vocabulary, resulting in vectors of the form [0, 0, 0, 0 ......, 1, 0, 0, 0,...., 1.....]. </span><span class="koboSpan" id="kobo.1269.2">This representation is called one-hot encoding, but it is very inefficient as the number of features is the number of distinct words in the corpus, the length of the vocabulary, which is typically very high (~500k with our </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">reduced vocabulary).</span></span></p>
<p><span class="koboSpan" id="kobo.1271.1">Therefore, we will take a look at more optimal ways to represent our words: word2vec </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">and GloVe:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1273.1">word2vec</span></strong><span class="koboSpan" id="kobo.1274.1">: This </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.1275.1">algorithm was developed by Google in 2013 and is available pre-trained on the </span><em class="italic"><span class="koboSpan" id="kobo.1276.1">Google News</span></em><span class="koboSpan" id="kobo.1277.1"> corpus. </span><span class="koboSpan" id="kobo.1277.2">It has a corpus of 3 billion words and a vocabulary of 3 million distinct words, each represented with 300 features. </span><span class="koboSpan" id="kobo.1277.3">The intuition behind this algorithm is to calculate the probability of a given word by taking into account its context (surrounding words). </span><span class="koboSpan" id="kobo.1277.4">The window size (how many words are being looked at the same time) is a parameter of the model and is a constant, which makes the model rely solely on the local context of </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">each word.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1279.1">GloVe</span></strong><span class="koboSpan" id="kobo.1280.1">: This </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.1281.1">algorithm was developed at Stanford University in 2014 and is available pre-trained in several datasets. </span><span class="koboSpan" id="kobo.1281.2">We will use Wikipedia articles (as of 2014) and a news dataset corpus called </span><em class="italic"><span class="koboSpan" id="kobo.1282.1">Gigaword</span></em><span class="koboSpan" id="kobo.1283.1">. </span><span class="koboSpan" id="kobo.1283.2">This dataset has a corpus of 6 billion words and a vocabulary of 400k distinct words, each represented with 50, 100, 200, or 300 features. </span><span class="koboSpan" id="kobo.1283.3">We will use the 50-features model. </span><span class="koboSpan" id="kobo.1283.4">This algorithm merges the locality of the approach from Google in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1284.1">word2vec</span></strong><span class="koboSpan" id="kobo.1285.1"> combined with word co-occurrence (global statistics) to provide a more </span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">complete representation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1287.1">With these </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.1288.1">representations, we can now compute operations in words in their new </span><span class="No-Break"><span class="koboSpan" id="kobo.1289.1">vector representations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1290.1">Example 1</span></strong><span class="koboSpan" id="kobo.1291.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1292.1">stronger</span></strong><span class="koboSpan" id="kobo.1293.1"> is to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1294.1">strong</span></strong><span class="koboSpan" id="kobo.1295.1"> what </span><strong class="source-inline"><span class="koboSpan" id="kobo.1296.1">weaker</span></strong><span class="koboSpan" id="kobo.1297.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.1298.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1299.1">weak</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1301.1">
math_weaker = w2v["stronger"] - w2v["strong"] + w2v["weak"]
np.linalg.norm(math_weaker - w2v["weaker"])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1302.1">This generates an output of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1303.1">~1.9</span></strong><span class="koboSpan" id="kobo.1304.1">, which </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">is close.</span></span></p></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1306.1">Example 2</span></strong><span class="koboSpan" id="kobo.1307.1">: Most similar words </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1309.1">king</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1311.1">
[('kings', 0.7138046026229858), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864822864532471), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.543294370174408), ('throne', 0.5422104597091675)]</span></pre></li> </ul>
<p><span class="koboSpan" id="kobo.1312.1">The </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.1313.1">observant reader will have realized that word embeddings are similar to the techniques we saw in the previous recipe for dimensionality reduction, as those were learned representations as well. </span><span class="koboSpan" id="kobo.1313.2">However, in this case, we are actually increasing the dimensionality to obtain new advantages (constant number of features and similar </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">meaning representation).</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1315.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1316.1">Word embeddings are typically </span><em class="italic"><span class="koboSpan" id="kobo.1317.1">learned</span></em><span class="koboSpan" id="kobo.1318.1"> representations; that is, the representations are trained to minimize the distance among words that have a similar meaning or, in our case, are classified with the same label. </span><span class="koboSpan" id="kobo.1318.2">In this recipe, we will use pre-trained representations for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1319.1">word2vec</span></strong><span class="koboSpan" id="kobo.1320.1"> and GloVe, and in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1321.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.1322.1">, we will take a look </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">at training.</span></span></p>
<h3><span class="koboSpan" id="kobo.1324.1">PCA and t-SNE</span></h3>
<p><span class="koboSpan" id="kobo.1325.1">As discussed in</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.1326.1"> "the subsection", our current embeddings have either 300 features (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1327.1">word2vec</span></strong><span class="koboSpan" id="kobo.1328.1">) or 50 (GloVe). </span><span class="koboSpan" id="kobo.1328.2">For proper visualizations, we need to apply dimensionality reduction techniques, as we saw in the previous recipe </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">for CV.</span></span></p>
<p><span class="koboSpan" id="kobo.1330.1">For this dataset, we can </span><span class="No-Break"><span class="koboSpan" id="kobo.1331.1">apply PCA:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.1332.1"><img alt="Figure 2.30 – PCA for (a) word2vec and (b) GloVe embeddings" src="image/B16591_02_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1333.1">Figure 2.30 – PCA for (a) word2vec and (b) GloVe embeddings</span></p>
<p><span class="koboSpan" id="kobo.1334.1">Moreover, we </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.1335.1">can apply t-SNE </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">as well:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.1337.1"><img alt="Figure 2.31 – t-SNE for (a) word2vec and (b) GloVe embeddings" src="image/B16591_02_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1338.1">Figure 2.31 – t-SNE for (a) word2vec and (b) GloVe embeddings</span></p>
<p><span class="koboSpan" id="kobo.1339.1">From the</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.1340.1"> preceding plots, we can see how the ham and spam words in our embedding space are very close to each other, making it very difficult to separate the clusters. </span><span class="koboSpan" id="kobo.1340.2">This is due to the fact that we are using pre-trained embeddings, from the news and Wikipedia datasets. </span><span class="koboSpan" id="kobo.1340.3">These datasets and the corresponding embeddings are not suited to our task at hand. </span><span class="koboSpan" id="kobo.1340.4">We will see how to train word embeddings to achieve better results in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1341.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1342.1">.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1343.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1344.1">For PCA and t-SNE, we can choose three components instead of two, which will yield a 3D plot. </span><span class="koboSpan" id="kobo.1344.2">For the code, visit the GitHub repository of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1345.1">book: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook"><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.1348.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1349.1">To </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.1350.1">understand </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.1351.1">the corpus of a text dataset, we need to understand the underlying connections among words in that corpus. </span><span class="koboSpan" id="kobo.1351.2">One useful method to achieve this is with different visualizations of </span><span class="No-Break"><span class="koboSpan" id="kobo.1352.1">the corpus.</span></span></p>
<p><span class="koboSpan" id="kobo.1353.1">In this recipe, we have learned how to discover patterns in our text datasets. </span><span class="koboSpan" id="kobo.1353.2">We selected an imbalanced dataset, the </span><em class="italic"><span class="koboSpan" id="kobo.1354.1">Enron Email</span></em><span class="koboSpan" id="kobo.1355.1"> dataset, and we learned how to deal with </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.1356.1">binary </span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1">classification datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1358.1">We analyzed </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.1359.1">our dataset by taking a look at its internal structure and what the class imbalance looked like and checked the most common words in search of patterns and errors. </span><span class="koboSpan" id="kobo.1359.2">We cleaned the dataset by removing punctuation marks, and we graphed the most </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.1360.1">frequent </span><strong class="bold"><span class="koboSpan" id="kobo.1361.1">bigrams</span></strong><span class="koboSpan" id="kobo.1362.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1363.1">trigrams</span></strong><span class="koboSpan" id="kobo.1364.1"> and </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.1365.1">noticed several keywords that would help us classify our </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">emails correctly.</span></span></p>
<p><span class="koboSpan" id="kobo.1367.1">We learned how to generate some cool visualizations such as </span><strong class="bold"><span class="koboSpan" id="kobo.1368.1">word clouds</span></strong><span class="koboSpan" id="kobo.1369.1">, and we understood why </span><strong class="bold"><span class="koboSpan" id="kobo.1370.1">word embeddings</span></strong><span class="koboSpan" id="kobo.1371.1"> are important and plotted them using the </span><strong class="bold"><span class="koboSpan" id="kobo.1372.1">dimensionality reduction techniques</span></strong><span class="koboSpan" id="kobo.1373.1"> we </span><span class="No-Break"><span class="koboSpan" id="kobo.1374.1">learned previously.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.1375.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.1376.1">If you want to know more about the Enron Email dataset and the Enron scandal, the following links </span><span class="No-Break"><span class="koboSpan" id="kobo.1377.1">will help:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1378.1">Enron Email </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1379.1">dataset</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1380.1">: </span></span><a href="http://www.cs.cmu.edu/~enron/"><span class="No-Break"><span class="koboSpan" id="kobo.1381.1">http://www.cs.cmu.edu/~enron/</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1382.1">Enron </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1383.1">scandal</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">: </span></span><a href="https://en.wikipedia.org/wiki/Enron_scandal"><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">https://en.wikipedia.org/wiki/Enron_scandal</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1386.1">We had a brief overview of several important concepts I invite you to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.1387.1">more about:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1388.1">BOW</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1389.1">: </span></span><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/"><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">https://machinelearningmastery.com/gentle-introduction-bag-words-model/</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1391.1">N-grams</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">: </span></span><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1393.1">https://web.stanford.edu/~jurafsky/slp3/3.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1394.1">Word </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1395.1">clouds</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">: </span></span><a href="https://amueller.github.io/word_cloud/"><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">https://amueller.github.io/word_cloud/</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1398.1">Furthermore, we barely scratched the surface of what word embeddings </span><span class="No-Break"><span class="koboSpan" id="kobo.1399.1">can offer:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1400.1">word2vec</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1401.1">: </span></span><a href="https://code.google.com/archive/p/word2vec/"><span class="No-Break"><span class="koboSpan" id="kobo.1402.1">https://code.google.com/archive/p/word2vec/</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1403.1">GloVe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">: </span></span><a href="https://nlp.stanford.edu/projects/glove/"><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">https://nlp.stanford.edu/projects/glove/</span></span></a></li>
</ul>
</div>
</body></html>