["```py\nWeights:\n[[-1.2347414  -1.771029   -0.45138445]\n [ 0.57938355 -1.856082   -1.9768796 ]\n [-0.20801921  0.2444218  -0.03716067]\n [-0.48774993 -0.02261727  0.57461417]]\n <NDArray 4x3 @cpu(0)>\n Bias:\n [1.4661262  0.6862904  0.35496104]\n <NDArray 3 @cpu(0)>\n```", "```py\n0\n```", "```py\nAccuracy  : 0.5933333333333334\nPrecision : 0.4765151515151515\nRecall    : 0.5933333333333334\nF1-score  : 0.49722222222222223\n```", "```py\ndef create_classification_network(num_outputs = 3):\n    # MLP with Gluon\n    net = mx.gluon.nn.Sequential()\n    net.add(mx.gluon.nn.Dense(10, activation=\"relu\"))\n    net.add(mx.gluon.nn.Dense(10, activation=\"relu\"))\n    net.add(mx.gluon.nn.Dense(num_outputs))\n    # Note that the latest layer does not have an activation\n    # function whereas Softmax was expected.\n    # This is due to an optimization during training:\n    # the loss function includes the softmax computation. return net\n```", "```py\nParameters in forward computation graph, duplicate included\n   Total params: 193\n   Trainable params: 193\n   Non-trainable params: 0\nShared params in forward computation graph: 0\nUnique parameters in model: 193\n```", "```py\nExpected Output: 0\nModel Output: [[9.999949e-01 4.748235e-06 4.116847e-07]]\n Class Output: 0\nAccuracy    : 1.0\n```"]