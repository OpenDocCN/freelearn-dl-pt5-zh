["```py\n    pip install \"jina[demo]\"\n    ```", "```py\n    jina hello chatbot\n    ```", "```py\n└── chatbot                    \n    ├── app.py\n    ├── my_executors.py         \n    ├── static/         \n```", "```py\nfrom my_executors import MyTransformer, MyIndexer\n```", "```py\nfrom jina import Flow\nflow = (\n    Flow(cors=True)\n    .add(uses=MyTransformer)\n    .add(uses=MyIndexer)\n    )\n```", "```py\nfrom jina import Flow\nflow = (\n    Flow(cors=True)\n    .add(name='MyTransformer', uses=MyTransformer)\n    .add(name='MyIndexer', uses=MyIndexer) \n    .plot('chatbot_flow.svg')\n    )\n```", "```py\ntargets = {\n        'covid-csv': {\n            'url': url_of_your_data,\n            'filename': file_name_to_be_fetched,\n        }\n    }\n```", "```py\nwith f:\n  f.index(\n    DocumentArray.from_csv(\n      targets['covid-csv']['filename'], \n        field_resolver={'question': 'text'}\n    ),\n    show_progress=True,)\n  url_html_path = 'file://' + os.path.abspath(\n    os.path.join(os.path.dirname(\n       os.path.realpath(__file__)),'static/index.xhtml'\n    )\n  )\n  try:\n    webbrowser.open(url_html_path, new=2)\n  except:\n    pass\n  finally:\n    default_logger.info(\n      f'You should see a demo page opened in your \n      browser,'f'if not, you may open {url_html_path} \n      manually'\n    )\n  if not args.unblock_query_flow:\n    f.block()\n```", "```py\n    class MyTransformer(Executor):\n      \"\"\"Transformer executor class \"\"\"\n      def __init__(\n        self,\n        pretrained_model_name_or_path: str = \n        'sentence-transformers/paraphrase-mpnet-base-v2',    \n        pooling_strategy: str = 'mean',\n        layer_index: int = -1,\n        *args,\n        **kwargs,\n      ):\n      super().__init__(*args, **kwargs)\n      self.pretrained_model_name_or_path = \n        pretrained_model_name_or_path\n      self.pooling_strategy = pooling_strategy\n      self.layer_index = layer_index\n      self.tokenizer = AutoTokenizer.from_pretrained(\n        self.pretrained_model_name_or_path\n      )\n      self.model = AutoModel.from_pretrained(\n        pretrained_model_name_or_path, \n          output_hidden_states=True\n      )\n      self.model.to(torch.device('cpu'))\n    ```", "```py\n  @requests\n  def encode(self, docs: 'DocumentArray', *args, **kwargs):\n    with torch.inference_mode():\n      if not self.tokenizer.pad_token: \n        self.tokenizer.add_special_tokens({'pad_token':\n           '[PAD]'}) \n        self.model.resize_token_embeddings(len(\n          self.tokenizer.vocab))\n      input_tokens = self.tokenizer(\n                  docs[:, 'content'],\n                  padding='longest',\n                  truncation=True,\n                  return_tensors='pt',\n      )\n      input_tokens = {\n        k: v.to(torch.device('cpu')) for k, \n          v in input_tokens.items()\n              }\n      outputs = self.model(**input_tokens)\n      hidden_states = outputs.hidden_states\n      docs.embeddings = self._compute_embedding(\n        hidden_states, input_tokens)\n```", "```py\nclass MyIndexer(Executor):\n  \"\"\"Simple indexer class \"\"\"\n  def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    self.table_name = 'qabot_docs'\n    self._docs = DocumentArray(\n      storage='sqlite',\n      config={\n        'connection': os.path.join(\n         self.workspace, 'indexer.db'),\n        'table_name': self.table_name,\n      },\n    )\n  @requests(on='/index')\n  def index(self, docs: 'DocumentArray', **kwargs):\n    self._docs.extend(docs)\n  @requests(on='/search')\n  def search(self, docs: 'DocumentArray', **kwargs):\n    \"\"\"Append best matches to each document in docs\n    :param docs: documents that are searched\n    :param parameters: dictionary of pairs \n      (parameter,value)\n    :param kwargs: other keyword arguments\n    \"\"\"\n    docs.match(\n      self._docs,\n      metric='cosine',\n      normalization=(1, 0),\n      limit=1,\n    )\n```", "```py\n     jina hello fashion\n    ```", "```py\n└── fashion\n    ├── app.py\n    ├── my_executors.py\n    ├── helper.py\n    ├── demo.xhtml\n```", "```py\nfrom my_executors import MyEncoder, MyIndexer\n```", "```py\nfrom jina import Flow\nflow = (\n    Flow()\n    .add(name='MyEncoder', uses=MyEncoder, replicas=2)\n    .add(name='MyIndexer', uses=MyIndexer)\n    .plot('fashion_image_flow.svg')\n    )\n```", "```py\n    targets = {\n            'index-labels': {\n                'url': args.index_labels_url,\n                'filename': os.path.join(args.workdir, \n                'index-labels'),\n            },\n            'query-labels': {\n                'url': args.query_labels_url,\n                'filename': os.path.join(args.workdir, \n                'query-labels'),\n            },\n            'index': {\n                'url': args.index_data_url,\n                'filename': os.path.join(args.workdir,   \n              'index-original'),\n         },\n            'query': {\n                'url': args.query_data_url,\n                'filename': os.path.join(args.workdir, \n                 'query-original'),},\n        }\n    ```", "```py\n    with f:\n      f.index(index_generator(num_docs=targets['index']\n        ['data'].shape[0], target=targets), \n        show_progress=True,\n        )\n      groundtruths = get_groundtruths(targets)\n      evaluate_print_callback = partial(print_result, \n        groundtruths)\n      evaluate_print_callback.__name__ = \n        'evaluate_print_callback'\n      f.post(\n        '/search,\n        query_generator(num_docs=args.num_query, \n          target=targets),\n        shuffle=True,\n        on_done= evaluate_print_callback,\n        parameters={'top_k': args.top_k},\n        show_progress=True,\n        )\n      #write result to html\n      write_html(os.path.join(args.workdir, 'demo.xhtml'))\n    ```", "```py\n    class MyEncoder(Executor):\n      \"\"\"\n      Encode data using SVD decomposition\n      \"\"\"\n      def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        np.random.seed(1337)\n        # generate a random orthogonal matrix\n        H = np.random.rand(784, 64)\n        u, s, vh = np.linalg.svd(H, full_matrices=False)\n        self.oth_mat = u @ vh\n      @requests\n      def encode(self, docs: 'DocumentArray', **kwargs):\n        \"\"\"Encode the data using an SVD decomposition\n        :param docs: input documents to update with an \n          embedding\n        :param kwargs: other keyword arguments\n        \"\"\"\n        # reduce dimension to 50 by random orthogonal \n        # projection\n        content = np.stack(docs.get_attributes('content'))\n        content = content[:, :, :, 0].reshape(-1, 784)\n        embeds = (content / 255) @ self.oth_mat\n        for doc, embed, cont in zip(docs, embeds, \n          content):\n          doc.embedding = embed\n          doc.content = cont\n          doc.convert_image_tensor_to_uri()\n          doc.pop('tensor')\n    ```", "```py\n    class MyIndexer(Executor):\n      \"\"\"\n      Executor with basic exact search using cosine \n      distance\n      \"\"\"\n      def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if os.path.exists(self.workspace + '/indexer'):\n          self._docs = DocumentArray.load(self.workspace + \n          '/indexer')\n        else:\n          self._docs = DocumentArray()  \n      @requests(on='/index')\n      def index(self, docs: 'DocumentArray', **kwargs):\n        \"\"\"Extend self._docs\n        :param docs: DocumentArray containing Documents\n        :param kwargs: other keyword arguments\n        \"\"\"\n        self._docs.extend(docs)\n      @requests(on=['/search', '/eval'])\n      def search(self, docs: 'DocumentArray',\n        parameters: Dict, **kwargs):\n        \"\"\"Append best matches to each document in docs\n        :param docs: documents that are searched\n        :param parameters: dictionary of pairs \n          (parameter,value)\n        :param kwargs: other keyword arguments\n        \"\"\"\n        docs.match(\n          self._docs,\n          metric='cosine',\n          normalization=(1, 0),\n          limit=int(parameters['top_k']),\n        )\n      def close(self):\n        \"\"\"\n        Stores the DocumentArray to disk\n        \"\"\"\n        self._docs.save(self.workspace + '/indexer')\n    ```", "```py\ndef index_generator(num_docs: int, target: dict):\n  \"\"\"\n  Generate the index data.\n  :param num_docs: Number of documents to be indexed.\n  :param target: Dictionary which stores the data \n    paths\n  :yields: index data\n  \"\"\"\n  for internal_doc_id in range(num_docs):\n    # x_blackwhite.shape is (28,28)\n    x_blackwhite=\n      255-target['index']['data'][internal_doc_id]\n    # x_color.shape is (28,28,3)\n    x_color = np.stack((x_blackwhite,) * 3, axis=-1)\n    d = Document(content=x_color)\n    d.tags['id'] = internal_doc_id\n    yield d\n```", "```py\ndef query_generator(num_docs: int, target: dict):\n  \"\"\"\n  Generate the query data.\n  :param num_docs: Number of documents to be queried\n  :param target: Dictionary which stores the data paths\n  :yields: query data\n  \"\"\"\n  for _ in range(num_docs):\n    num_data = len(target['query-labels']['data'])\n    idx = random.randint(0, num_data - 1)\n    # x_blackwhite.shape is (28,28)\n    x_blackwhite = 255 - target['query']['data'][idx]\n    # x_color.shape is (28,28,3)\n    x_color = np.stack((x_blackwhite,) * 3, axis=-1)\n    d = Document(\n        content=x_color,\n         tags={\n         'id': -1,\n         'query_label': float(target['query-labels'] \n          ['data'][idx][0]),\n         },\n    )\n    yield d\n```", "```py\n    jina hello multimodal\n    ```", "```py\n└── multimodal                    \n    ├── app.py\n    ├── my_executors.py\n    ├── flow_index.yml\n    ├── flow_search.yml\n    ├── static/        \n```", "```py\n    targets = {\n            'people-img: {\n                'url': url_of_the_data,\n                'filename': file_name_to_be_fetched,\n            }\n        }\n    ```", "```py\n# Indexing Flow\nf = Flow.load_config('flow-index.yml')\nwith f, open(f'{args.workdir}/people-img/meta.csv', newline='') as fp:\n  f.index(inputs=DocumentArray.from_csv(fp), \n    request_size=10, show_progress=True)\n  f.post(on='/dump', target_executor='textIndexer')\n  f.post(on='/dump', target_executor='imageIndexer')\n  f.post(on='/dump', \n    target_executor='keyValueIndexer')\n```", "```py\n    # Search Flow\n    f = Flow.load_config('flow-search.yml')\n    # switch to HTTP gateway\n    f.protocol = 'http'\n    f.port_expose = args.port_expose\n    url_html_path = 'file://' + os.path.abspath(\n                os.path.join(cur_dir, \n                'static/index.xhtml'))\n    with f:\n      try:\n             webbrowser.open(url_html_path, new=2)\n      except:\n        pass  # intentional pass\n      finally:\n             default_logger.info(\n        f'You should see a demo page opened in your \n          browser,'f'if not, you may open {url_html_path} \n          manually'\n                )\n      if not args.unblock_query_flow:\n        f.block()\n    ```", "```py\nclass Segmenter(Executor):\n    @requests\n    def segment(self, docs: DocumentArray, **kwargs):\n        for doc in docs:\n            text = doc.tags['caption']\n            uri={os.environ[\"HW_WORKDIR\"]}/\n              people-img/{doc.tags[\"image\"]}'\n            chunk_text = Document(text=text, \n              mime_type='text/plain')\n            chunk_uri = Document(uri=uri, \n              mime_type='image/jpeg')\n            doc.chunks = [chunk_text, chunk_uri]\n            doc.uri = uri\n            doc.convert_uri_to_datauri()\n```", "```py\nclass TextCrafter(Executor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    @requests()\n    def filter(self, docs: DocumentArray, **kwargs):\n        filtered_docs = DocumentArray(\n            d for d in docs.traverse_flat(['c']) if \n              d.mime_type == 'text/plain'\n        )\n        return filtered_docs\n```", "```py\nclass ImageCrafter(Executor):\n    @requests(on=['/index', '/search'])\n    def craft(self, docs: DocumentArray, **kwargs):\n        filtered_docs = DocumentArray(\n            d for d in docs.traverse_flat(['c']) if \n              d.mime_type == 'image/jpeg'\n        )\n        target_size = 224\n        for doc in filtered_docs:\n            doc.convert_uri_to_image_blob()\n           doc.set_image_blob_shape(shape=(target_size, \n             target_size))\n            doc.set_image_blob_channel_axis(-1, 0)\n        return filtered_docs\n```", "```py\nclass TextEncoder(Executor):\n  \"\"\"Transformer executor class\"\"\"\n  def __init__(\n        self,\n        pretrained_model_name_or_path: str=\n        'sentence-transformers/paraphrase-mpnet-base-v2',\n        pooling_strategy: str = 'mean',\n        layer_index: int = -1,\n        *args,\n        **kwargs,\n  ):\n        super().__init__(*args, **kwargs)\n        self.pretrained_model_name_or_path = \n          pretrained_model_name_or_path\n        self.pooling_strategy = pooling_strategy\n        self.layer_index = layer_index\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.pretrained_model_name_or_path\n        )\n        self.model = AutoModel.from_pretrained(\n            self.pretrained_model_name_or_path, \n            output_hidden_states=True\n        )\n        self.model.to(torch.device('cpu'))\n```", "```py\ndef _compute_embedding(self, hidden_states: 'torch.Tensor', input_tokens:   Dict):\n  fill_vals = {'cls': 0.0,'mean': 0.0,'max': -np.inf,'min': \n    np.inf}\n      fill_val = torch.tensor(\n        fill_vals[self.pooling_strategy], \n          device=torch.device('cpu')\n      )\n  layer = hidden_states[self.layer_index]\n      attn_mask = \n        input_tokens['attention_mask']\n        .unsqueeze(-1).expand_as(layer)\n      layer = torch.where(attn_mask.bool(), layer,\n        fill_val)\n      embeddings = layer.sum(dim=1) / attn_mask.sum(dim=1)\n      return embeddings.cpu().numpy()\n```", "```py\n@requests\ndef encode(self, docs: 'DocumentArray', **kwargs):\n  with torch.inference_mode():\n        if not self.tokenizer.pad_token:\n              self.tokenizer.add_special_tokens({\n                'pad_token': '[PAD]'})\n      self.model.resize_token_embeddings(len(\n        self. tokenizer.vocab))\n    input_tokens = self.tokenizer(\n      docs.get_attributes('content'),\n      padding='longest',\n      truncation=True,\n      return_tensors='pt',\n            )\n            input_tokens = {\n      k: v.to(torch.device('cpu')) for k, v in \n        input_tokens.items()\n            }\n            outputs = self.model(**input_tokens)\n            hidden_states = outputs.hidden_states\n            docs.embeddings = self._compute_embedding(\n              hidden_states, input_tokens)\n```", "```py\nclass ImageEncoder(Executor):\n  def __init__(\n        self,\n    model_name: str = 'mobilenet_v2',\n    pool_strategy: str = 'mean',\n    channel_axis: int = -1, *args, **kwargs,\n  ):\n    super().__init__(*args, **kwargs)\n    self.channel_axis = channel_axis\n    self.model_name = model_name\n    self.pool_strategy = pool_strategy\n    self.pool_fn = getattr(np, self.pool_strategy)\n        model = getattr(models, \n          self.model_name)(pretrained=True)\n    self.model = model.features.eval()\n    self.model.to(torch.device('cpu'))    \n```", "```py\ndef _get_features(self, content):\n  return self.model(content)\ndef _get_pooling(self, feature_map: 'np.ndarray') -> 'np.ndarray':\n  if feature_map.ndim == 2 or self.pool_strategy is None:\n    return feature_map\n  return self.pool_fn(feature_map, axis=(2, 3))\n@requests\ndef encode(self, docs: DocumentArray, **kwargs):\n  with torch.inference_mode():\n    _input = torch.from_numpy(docs.blobs.astype('float32'))\n            _features = self._get_features(_input).detach()\n            _features = _features.numpy()\n            _features = self._get_pooling(_features)\n            docs.embeddings = _features\n```", "```py\nclass DocVectorIndexer(Executor):\n  def __init__(self, index_file_name: str, **kwargs):\n        super().__init__(**kwargs)\n    self._index_file_name = index_file_name\n    if os.path.exists(self.workspace + \n      f'/{index_file_name}'):\n      self._docs = DocumentArray.load(\n        self.workspace + f'/{index_file_name}')\n    else:\n      self._docs = DocumentArray()\n  @requests(on='/index')\n  def index(self, docs: 'DocumentArray', **kwargs):\n    self._docs.extend(docs)\n  @requests(on='/search')\n  def search(self, docs: 'DocumentArray', parameters: Dict, \n    **kwargs):\n    docs.match(\n      self._docs,\n      metric='cosine',\n              normalization=(1, 0),\n              limit=int(parameters['top_k']),\n    ) \n  @requests(on='/dump')\n  def dump(self, **kwargs):\n    self._docs.save(self.workspace + \n      f'/{self._index_file_name}')\n  def close(self):\n    \"\"\"\n    Stores the DocumentArray to disk\n    \"\"\"\n    self.dump()\n    super().close()\n```", "```py\nclass KeyValueIndexer(Executor):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    if os.path.exists(self.workspace + '/kv-idx'):\n      self._docs = DocumentArray.load(self.workspace + \n            '/kv-idx')\n    else:\n      self._docs = DocumentArray()\n  @requests(on='/index')\n  def index(self, docs: DocumentArray, **kwargs):\n    self._docs.extend(docs)\n  @requests(on='/search')\n  def query(self, docs: DocumentArray, **kwargs):\n    for doc in docs:\n              for match in doc.matches:\n        extracted_doc = self._docs[match.parent_id]\n        extracted_doc.scores = match.scores\n        new_matches.append(extracted_doc)\n      doc.matches = new_matches\n  @requests(on='/dump')\n  def dump(self, **kwargs):\n    self._docs.save(self.workspace + \n      f'/{self._index_file_name}')\n\n  def close(self):\n    \"\"\"\n    Stores the DocumentArray to disk\n    \"\"\"\n    self.dump()\n    super().close()\n```", "```py\n    class WeightedRanker(Executor):\n      @requests(on='/search')\n      def rank(\n        self, docs_matrix: List['DocumentArray'], \n        parameters: Dict, **kwargs\n      ) -> 'DocumentArray':\n        \"\"\"\n        :param docs_matrix: list of :class:`DocumentArray` \n          on multiple     requests to get bubbled up \n          matches.\n        :param parameters: the parameters passed into the \n          ranker, in     this case stores \n            :param kwargs: not used (kept to maintain \n              interface)\n        \"\"\"\n        result_da = DocumentArray()  \n        for d_mod1, d_mod2 in zip(*docs_matrix):\n                  final_matches = {}  # type: Dict[str, \n                    Document]\n    ```", "```py\n      for m in d_mod1.matches:\n        relevance_score = m.scores['cosine'].value * \n          d_mod1.weight\n        m.scores['relevance'].value = relevance_score\n        final_matches[m.parent_id] = Document(\n          m, copy=True)\n      for m in d_mod2.matches:\n        if m.parent_id in final_matches:\n          final_matches[m.parent_id].scores[\n            'relevance'\n          ].value = final_matches[m.parent_id].scores['relevance']\n          .value + (\n            m.scores['cosine'].value * d_mod2.weight\n          )\n        else:\n          m.scores['relevance'].value = (\n            m.scores['cosine'].value * d_mod2.weight\n          )\n              final_matches[m.parent_id] = Document(m, \n                copy=True)\n      da = DocumentArray(list(final_matches.values()))\n      da.sorted(da, key=lambda ma: \n        ma.scores['relevance'].value, reverse=True)\n      d = Document(matches=da[: int(parameters['top_k'])])\n      result_da.append(d)\n    return result_da\n    ```", "```py\n    jtype: Flow\n    version: '1'\n    executors:\n      - name: segment\n        uses:\n          jtype: Segmenter\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n    ```", "```py\n      - name: craftText\n        uses:\n          jtype: TextCrafter\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: encodeText\n        uses:\n          jtype: TextEncoder\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: textIndexer\n        uses:\n          jtype: DocVectorIndexer\n          with:\n            index_file_name: \"text.json\"\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n    ```", "```py\n      - name: craftImage\n        uses:\n          jtype: ImageCrafter\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n        needs: segment\n      - name: encodeImage\n        uses:\n          jtype: ImageEncoder\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: imageIndexer\n        uses:\n          jtype: DocVectorIndexer\n          with:\n            index_file_name: \"image.json\"\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n    ```", "```py\n      - name: keyValueIndexer\n        uses:\n          jtype: KeyValueIndexer\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n        needs: segment\n      - name: joinAll\n        needs: [textIndexer, imageIndexer, \n          keyValueIndexer]\n    ```", "```py\n    jtype: Flow\n    version: '1'\n    with:\n      cors: True\n    executors:\n      - name: craftText\n        uses:\n          jtype: TextCrafter\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: encodeText\n        uses:\n          jtype: TextEncoder\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: textIndexer\n        uses:\n          jtype: DocVectorIndexer\n          with:\n            index_file_name: \"text.json\"\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n    ```", "```py\n      - name: craftImage\n        uses:\n          jtype: ImageCrafter\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n        needs: gateway\n      - name: encodeImage\n        uses:\n          jtype: ImageEncoder\n          metas:\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n      - name: imageIndexer\n        uses:\n          jtype: DocVectorIndexer\n          with:\n            index_file_name: \"image.json\"\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n    ```", "```py\n      - name: weightedRanker\n        uses:\n          jtype: WeightedRanker\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n        needs: [ textIndexer, imageIndexer ]\n      - name: keyvalueIndexer\n        uses:\n          jtype: KeyValueIndexer\n          metas:\n            workspace: ${{ ENV.HW_WORKDIR }}\n            py_modules:\n              - ${{ ENV.PY_MODULE }}\n        needs: weightedRanker\n    ```"]