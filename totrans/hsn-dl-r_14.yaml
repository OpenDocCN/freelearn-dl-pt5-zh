- en: Deep Q-Learning for Maze Solving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use R to implement reinforcement learning
    techniques within a maze environment. In particular, we will create an agent to
    solve a maze by training the agent to perform actions and learn from failed attempts.
    We will learn how to define the maze environment and configure the agent to travel
    through it. We will also be adding neural networks to Q-learning. This provides
    us with an alternative way of getting the value for all the state-action pairs.
    We are going to iterate over our model numerous times to create the policy to
    get through the maze.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an environment for reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an agent to perform actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep Q-learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving performance with policy functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files used in this chapter at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  prefs: []
  type: TYPE_NORMAL
- en: Creating an environment for reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will define an environment for reinforcement learning. We
    could think of this as a typical maze where an agent needs to navigate the two-dimensional
    grid space to get to the end. However, in this case, we are going to use more
    of a physics-based maze. We will represent this using the mountain car problem.
    An agent is in a valley and needs to get to the top; however, it cannot simply
    go up the hill. It has to use momentum to get to the top. In order to do this,
    we need two functions. One function will start or reset the agent to a random
    point on the surface. The other function will describe where the agent is on the
    surface after a step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following code to define the `reset` function to provide a
    place for the agent to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can see that with this function, the first thing that happens is that the `position` variable
    is defined by taking one random value from a uniform distribution between `-0.6`
    and `-0.4`. This is the point on the surface where the agent will be placed. Next,
    the variable velocity is set to `0`, since our agent is not moving yet. The `reset`
    functions act merely to place the agent at a starting point. The `position` variable
    and the `velocity` variable are now added to a 1 x 2 matrix and this `matrix`
    variable is the starting spot and starting speed for our agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function takes a value for every action and calculates the next step
    that the agent will take. To code this function, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this function, the first part defines the position and velocity. In this
    case, this is taken from the `self` object, which we will cover next. The `self`
    variable contains details about the agent. Here, the `position` and `velocity`
    variables are taken from `self` and represent where the agent currently is on
    the surface and the current velocity. Then, the `action` argument is used to calculate
    the velocity. The next line constrains `velocity` between `-0.7` and `0.7`. After
    this, we calculate the next position by adding the velocity to the current position.
    Then, there is one more constraint line. If `position` goes past `-1.2`, then
    the agent is out of bounds and gets reset to the `-1.2` position with no velocity.
    Finally, a check is carried out to see whether the agent has reached its goal.
    If the state is greater than `0.5`, then the agent wins; otherwise, the agent
    keeps moving and attempts to reach the goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we finish with the two coding blocks, we will see that we have two functions
    defined in our **Environment** pane. Your **Environment** pane will appear as
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8674eb48-130b-404b-bdb6-9f72c15a330d.png)'
  prefs: []
  type: TYPE_IMG
- en: The combination of these two functions defines the shape of the surface and
    the location of the agent on the surface, as well as the placement of the target
    spot on that surface. The `reset` function is the initial placement of the agent
    and the `step` function defines the step the agent takes at every iteration. With
    these two functions, we have a way of defining the shape and boundaries of our
    environment and a mechanism for placing and moving our agent within this environment.
    Next, let's define our agent.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an agent to perform actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will define our agent for deep Q-learning. We have already
    seen how the preceding environment functions define how the agent moves. Here,
    we define the agent itself. In the previous chapter, we used Q-learning and were
    able to apply the Bellman equation to the new state that was the result of a given
    action. In this chapter, we will augment that portion of Q-learning with a neural
    network, which is what takes standard Q-learning and makes it deep Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to add this neural network model to the process, we need to define
    a class. This is something that is often done in object-oriented programming;
    however, it is done less often in a programming language such as R. To accomplish
    this, we will use the `R6` package for class creation. We will break up the creation
    of this `R6` class into numerous parts to make it easier to understand. A class
    provides instructions for instantiating and operating on a data object. In this
    case, our class will use declared variables to instantiate the data object and
    a series of functions, which are referred to as methods within a class context,
    to operate on the data object. In the following steps, we will just look at the
    individual parts of our class one by one to make it easier to understand the parts
    of the class that we are creating. However, running parts of the code will result
    in errors. After walking through all the parts, we will wrap everything in a function
    to create our class and this final, longer `R6` code that includes everything
    that you will run. To get started, we will set up the initial values using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When creating a class for this purpose using R, we first set two options. First,
    we set `portable` to `FALSE`, which means other classes cannot inherit methods
    or functions from this class. However, it also means that we can use the `self`
    keyword. Second, we set `lock_objects` to `FALSE`, since we will need to modify
    objects within this class.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define our initial values. We use `self` here, which is a special keyword
    that refers to the object created. Remember that a class is not an object—it is
    a constructor for creating an object. Here, we will create an instance of our
    class and this object will be the agent. The agent will initialize with the following
    values. The state size and action size will be passed in as arguments when creating
    the environment. The next memory is an empty deque. A **deque** is a special object
    type, which is double-ended so that values can be added to and removed from both
    sides. We will use this to store the steps the agent takes while trying to reach
    the goal. Gamma is the discount rate. Epsilon is the exploration rate. As we know,
    with deep learning, the goal is to balance exploration and exploitation, so we
    begin with an aggressive exploration rate. However, we then define an epsilon
    decay, which is how much the rate will be reduced, and an epsilon minimum, so
    the rate never reaches `0`. Lastly, the learning rate is just the constant value
    used when adjusting weights and the model takes the result of running our neural
    network model, which we will get to next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will give the class the power to act on variables by adding a function.
    In particular, we will add the `build_model` function to run the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The model takes the current state as input and the output will be one of the
    actions available when we predict the model. However, this function just returns
    the model because we will pass a different state argument to the model, depending
    on what part of the deep Q-learning path we are on when it is called. The model
    is called in two different scenarios, which we will cover shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we include a function for memory. The memory portion of this class will
    be a function to store the state, action, reward, and next state details as the
    agent attempts to solve the maze. We store those values in the agent''s memory
    by adding them to the deque using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use the `pushback` function to add a given value to the first position in
    the deque and move all the existing elements back by one. We do this for state,
    action, reward, and next state, and the flag that shows whether or not the puzzle
    is complete. This sequence is stored in the agent's memory, so it can exploit
    what it already knows by accessing this sequence in memory rather than continuing
    to explore when the exploration-versus-exploitation formula selects the exploitation
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will add some code to select the next action. To perform this task,
    we will use a check on the decaying epsilon value. Depending on whether or not
    the decaying epsilon is greater than a randomly selected value from a uniform
    distribution, one of two actions will take place. We set up the function for deciding
    on the next action by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As noted previously, there are two possible outcomes from this action function.
    First, if the randomly selected value from the uniform distribution is less than
    or equal to epsilon, then a value will be selected between the full range of active
    movements. Otherwise, the current state is used to predict the next action using
    the model we defined earlier, which results in weighted probabilities that any
    of these actions are correct. The action with the highest probability is selected.
    This is a balance between two different forms of exploration in seeking the correct
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having covered exploration steps previously, we will now write our `replay()`
    function, which will exploit what the agent already knows and has stored in memory.
    We code this exploitation function using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break apart the function for leveraging what the agent already knows
    to help solve the puzzle. The first thing we will do is select a random sequence
    from the memory deque. We will then place each element from our sample into a
    given part of a sequence for the agent: state, action, target, next state, and
    the `done` flag to indicate whether the maze is solved. Next, we will add some
    code to change the way that our model predicts. We start by defining the target
    using the resulting state from the sequence, leveraging what we have already learned
    from attempting this sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we predict on the state to get all the possible values the model would
    predict. We then insert the calculated value into that vector. When we run the
    model once more, we help train the model based on experience. This is also the
    step where epsilon is updated, which will result in more exploitation and less
    exploration during future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The very last step is to add a method or function for saving and loading our
    model. We add the means to save and load our model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With these methods, we are now able to save the model that we defined earlier,
    as well as load the trained models.
  prefs: []
  type: TYPE_NORMAL
- en: We will now need to take everything that we have covered and place it all in
    a overarching function that will take all the declared variables and functions
    and use them to create an `R6` class. To create our `R6` class, we will take all
    the code that we have just written and put it all together.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the complete code, we will have an `R6` class in our environment.
    It will have a class of `Environment`. If you click on it, you can see all the
    attributes of the class. You will notice that there are many attributes associated
    with creating a class that we did not specifically define; however, take a look
    at the following screenshot. We can see that the class is not portable, we can
    see the public fields where we will assign values, and we can see all the functions
    we defined, which are called methods when included as part of a class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d74bda2-7d86-4252-b88d-ae67c317616c.png)'
  prefs: []
  type: TYPE_IMG
- en: With this step, we have completely created an `R6` class to act as our agent.
    We have provided it with various means to take an action based on the current
    state and an element of randomness, and we have also provided a way for our agent
    to explore the surface of this maze to find the target location. We have also
    provided a means for the agent to recall what it has already learned from past
    experience and use that to inform future decisions. Altogether, we have a complete
    reinforcement learning agent that learns through trial and error and, importantly,
    learns from past mistakes and from continually taking actions at random.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deep Q-learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have defined the environment and our agent, which will make
    running our model quite straightforward. Remember that to get set up for reinforcement
    learning using R, we used a technique from object-oriented programming, which
    is not used very often in a programming language such as R. We created a class
    that describes an object, but is itself not an object. To create an object from
    a class, we must instantiate it. We set our initial values and instantiate an
    object using our `DQNAgent` class by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this block of code, we will see an agent object in our environment.
    The agent has a class of `Environment`; however, if we click on it, we will see
    something similar to the following screenshot, which contains some differences
    compared with our class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ef1b1d8-1d0d-4d94-ae68-dd27e6addb57.png)'
  prefs: []
  type: TYPE_IMG
- en: After running this line, we will now have an object that has inherited all the
    attributes defined in the class. We pass in a state size of `2` as an argument
    because, for this environment, the state is two-dimensional. The two dimensions
    are position and velocity. We see the value that we passed is reflected alongside
    the `state_size` field. We pass in an action size of `20` as an argument because
    for this game, we will allow the agent to use up to `20` units of force to propel
    forward or backward. We can see this value as well. We also can see all the methods;
    however, they are no longer nested under various methods—they are now all just
    inherited by the `agent` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our environment, we use the `makeEnvironment` function from the `reinforcelearn`
    package, which allows for custom environment creation. We use the following code
    to pass the `step` and `reset` functions as arguments to create the custom environment
    for the agent to navigate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding line of code, you will see an `env` object in your
    **Environment** pane. Note that this object also has a class of `Environment`.
    When we click on this object, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59c8dba6-55e5-4355-a3ce-16d19a18bb5d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding line of code used the functions that we created earlier to define
    an environment. We now have an instance of the environment, which includes a means
    of initializing a game, while the `step` function defines the range of possible
    motions that the agent can make every turn. Note that this is also an `R6` class,
    just like our agent class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we include two additional initial values. We establish the remaining
    initial values to complete our model setup by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first value of `FALSE` for `done` denotes that the objective is not yet
    complete. The batch size of `32` is the size of the exploration attempt or series
    of moves the agent will make before beginning to leverage what is already known
    before the next series of moves.
  prefs: []
  type: TYPE_NORMAL
- en: This is the complete model setup for deep Q-learning. We have an instance of
    our agent, which is an object created with the characteristics we established
    in the class earlier. We also have an environment defined with the parameters
    we set up when we created our `step` and `reset` functions. Lastly, we defined
    some initial values and now, everything is complete. The next step is just to put
    the agent in motion, which we will do next.
  prefs: []
  type: TYPE_NORMAL
- en: Running the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last step in reinforcement learning is to run the experiment. To do this,
    we need to drop the agent into the environment and then allow the agent to take
    steps until it reaches the goal. The agent is constrained by a limited number
    of possible moves and the environment also places another constraint—in our case,
    by setting boundaries. We set up a `for` loop that iterates through rounds of
    the agent attempting a legal move and then sees whether the maze has been successfully
    accomplished. The loop stops when the agent reaches the goal. To begin our experiment
    with our defined agent and environment, we write the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code runs the experiment that sets our agent in motion. The agent
    is governed by the values and functions in the class that we defined and is furthered
    by the environment that we created. As we can see, quite a few steps take place
    when we run our experiment. We will review each step here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the first line in the preceding code, we will see a starting
    state for our agent. If you view the `state` object, it will look something like
    this, where the position value is between `-0.4` and `-0.6` and the velocity is
    `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9bbec86f-5db7-48e7-be24-23f5415d686c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After running the remaining code block, we will see something like the following
    printed to the console, which shows the state at every tenth round:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d7b2c0e8-6db0-43a5-8f9e-4b16da70a73e.png)'
  prefs: []
  type: TYPE_IMG
- en: When we run this code, the first thing that happens is that the environment
    is reset and the agent is placed on the surface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, the loop is initiated. Every round in the loop has the following sequence
    of activities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, use the `act` function in the `agent` class to take an action. Remember,
    this function defines the allowable moves for the agent.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we pass the action that the agent takes through to the `step` function
    to get the results.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The output is the next state, which is where the agent lands after the action,
    as well as the reward based on whether the action led to a positive result, and
    finally the `done` flag, which indicates whether the target has been reached successfully.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: These three elements are output from the function as a `list` object.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The next steps are to assign them to their own objects. For `reward` and `done`,
    we just extract them from the list and assign them to an integer and logical data
    type, respectively. For the next state, it is a little more difficult. We first
    use `unlist` to extract the two values and then we place them in a 2 x 1 matrix.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After all of the elements in an agent's move are moved to their own objects,
    the reward is calculated. In our case, there are no intermediate accomplishments
    that would lead to a reward short of reaching the target, so `reward` and `done`
    operate in a similar way. Here, we see that if the `done` flag is set to `TRUE`,
    then `reward` is set to `0`, as defined in the `step` function, when `reward`
    is `TRUE`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, all the values that were output from the `step` function are added to
    the `memory` deque object. The `memorize` function takes each value and pushes
    it to the first element in the deque while pushing existing values back.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After this, the `state` object is assigned the value of the next state. This
    is because the next state is now the new current state as the agent takes a new
    step.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There is then a check to see whether the agent has reached the end of the maze.
    If so, the loop breaks and the epsilon value is printed to see how much was done
    through exploration and how much through exploitation. For all the other rounds,
    there is a secondary check that prints the current state and velocity for every
    tenth move.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The other conditional is the trigger for the `replay` function. After reaching
    the threshold, the agent pulls values from the memory deque and the process continues
    from there.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the entire process for running an experiment for reinforcement learning.
    With this process, we now have a method of reinforcement learning that is more
    robust than just using Q-learning. While using Q-learning is a good solution when
    the environment is limited and known, deep Q-learning is required when the environment
    scales up or changes dynamically. By iterating over a defined agent taking actions
    in a defined environment, we can see how well the defined agent can solve the
    problem presented in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance with policy functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have successfully coded an agent to use a neural network deep learning architecture
    to solve a problem. Let's now look at a few ways that we could improve our model.
    Unlike other machine learning, we cannot evaluate to a performance metric as usual,
    where we try to minimize some chosen error rate. Success in reinforcement learning
    is slightly more subjective. You may want an agent to complete a task as quickly
    as possible, to acquire as many points as possible, or to make the fewest mistakes
    possible. In addition, depending on the task, we may be able to alter the agent
    itself to see how it impacts results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at three possible methods for improving performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action size**:At times, this will be an option and, at times, it will not.
    If you are trying to solve a problem where the agent rules and environment rules
    are set externally, such as trying to optimize performance in a game such as chess,
    then this will not be an option. However, you can imagine a problem such as setting
    up a self-driving car and in this case, you could change the agent, if it would
    work better in this environment. With our experiment, try changing the action
    size value from `20` to `10` and also to `40` to see what happens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: We can also adjust the batch size to see how it impacts performance.
    Remember that when the move count for the agent reaches the threshold for a batch,
    the agent then selects values from memory to begin to leverage what is already
    known. By raising or lowering this threshold, we provide a policy for the agent
    that, more or less, exploration should be conducted before using what is already
    known. Change the batch size to `16`, `64`, and `128` to see which option results
    in the agent completing the challenge the quickest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network**:The last part of the agent policy that we will discuss modifying
    is the neural network. In many ways, this is the brain for the agent. By making
    changes, we can allow our agent to make choices that will lead to a more optimized
    performance. Within the `AgentDQN` class, add some layers to the neural network
    and then run the experiment again to see what happens. Then, make some changes
    to the number of units in each layer and run those experiments to see what happens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these changes, we could also make changes to the starting epsilon
    value, how quickly the epsilon decays, and the learning rate for the neural network
    model. All of these types of changes will impact the policy functions for the
    agent. When we change a value that alters the output of the act or replay function,
    then we modify the policy that the agent uses to solve the problem. We can make
    a policy for the agent to explore a wider or narrower number of actions if possible,
    or to use more or less time exploring the environment versus exploiting current
    knowledge, as well as adjust how quickly the agent learns from every move, how
    many times an agent may try a similar action to see whether it is always incorrect,
    and how drastically the agent tries to adjust after trying actions that lead towards
    failure.
  prefs: []
  type: TYPE_NORMAL
- en: As with any type of machine learning, there are a number of parameters that
    can be tuned to optimize performance in reinforcement learning. Unlike other problems,
    there may not be a standard metric to help tune these, and deciding on the values
    that will work best may be more subjective and rely more on trial and error experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we wrote code to conduct reinforcement learning using deep
    Q-learning. We noted that while Q-learning is a simpler approach, it requires
    a limited and known environment. Applying deep Q-learning allows us to solve problems
    at a larger scale. We also defined our agent, which required creating a class.
    The class defined our agent and we instantiated an object with the attributes
    defined in our class to solve the reinforcement learning challenge. We then created
    a custom environment using functions that defined boundaries, as well as the range
    of moves the agent could take and the target or objective. Deep Q-learning involves
    adding a neural network to select actions, rather than relying on the Q matrix,
    as in Q-learning. We then added a neural network to our agent class.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we put it all together by placing our agent object in our custom environment
    and letting it take various actions until it solved the problem. We further discussed
    some choices we could make to improve the agent's performance. With this framework,
    you are ready to apply reinforcement learning to any number of environments using
    any number of possible agents. The process will largely stay consistent; the changes
    will be in how the agent is programmed to act and learn and what the rules are
    in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: This completes *Hands-On Deep Learning with R*. Throughout this book, you have
    learned a wide variety of deep learning methods. In addition, we applied these
    methods to a diverse set of tasks. This book was written with a bias toward action.
    The goal of this book was to provide concise code that addresses practical projects.
    Using what you have learned in this book, I hope that I have prepared you well
    to begin solving real-world challenges using deep learning.
  prefs: []
  type: TYPE_NORMAL
