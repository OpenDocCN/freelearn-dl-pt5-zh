<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Magenta in the Browser with Magenta.js</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll talk about Magenta.js, a JavaScript implementation of Magenta that has gained in popularity for its ease of use, since it runs in the browser and can be shared as a web page. We'll introduce TensorFlow.js, the technology upon which Magenta.js is built, and show which models are available in Magenta.js, including how to convert our previously trained models. Then, we'll create small web applications using GANSynth and MusicVAE, for sampling audio and sequences respectively. Finally, we'll see how Magenta.js can interact with other applications, using the Web MIDI API and Node.js.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li>Introducing Magenta.js and TensorFlow.js</li>
<li>Creating a Magenta.js web application</li>
<li>Making Magenta.js interact with other apps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li><strong>Command line</strong> or <strong>Bash</strong> to launch Magenta from the terminal</li>
<li><strong>Python</strong> and <strong>Magenta</strong> to convert trained models for Magenta.js</li>
<li><strong>TensorFlow.js</strong> and <strong>Magenta.js</strong> to create music generation apps in the browser</li>
<li><strong>JavaScript</strong>, <strong>HTML</strong>, and <strong>CSS</strong> to write Magenta.js web applications</li>
<li>A <strong>recent browser</strong> (Chrome, Firefox, Edge, Safari) for up-to-date web APIs</li>
<li><strong>Node.js</strong> and <strong>npm</strong> to install Magenta.js and its dependencies server side</li>
<li><strong>FluidSynth</strong> to listen to generated MIDI from the browser</li>
</ul>
<p class="mce-root"/>
<p>In Magenta.js, we'll make the use of the <strong>Music RNN</strong> and <strong>MusicVAE</strong> models for MIDI sequence generation and <strong>GANSynth</strong> for audio generation. We'll cover their usage in depth, but if you feel like you need more information, the Magenta.js Music README in the Magenta.js source code (<a href="https://github.com/tensorflow/magenta-js/tree/master/music">github.com/tensorflow/magenta-js/tree/master/music</a>) is a good place to start. You can also take a look at the Magenta.js code, which is well documented. We also provide additional content in the last section, <em>Further reading</em>.</p>
<p>The code for this chapter is in the book's GitHub code repository in the <span><kbd>Chapter08</kbd> </span>folder, located at <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter08">github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter08</a>. The examples and code snippets used assume you are located in the chapter folder. For this chapter, you should <span><span>run </span></span><kbd>cd Chapter08</kbd>, before you start.</p>
<p>Check out the following video to see the Code in Action:<br/>
Placeholder link</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Magenta.js and TensorFlow.js</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we've covered Magenta in Python, its usage, and its inner workings. We'll now be looking at Google's Magenta.js, a smaller implementation of Magenta in JavaScript. Magenta and Magenta.js both have their advantages and disadvantages; let's compare them to see which one we should use, depending on the use case.</p>
<p>A Magenta.js application is easy to use and deploy since it executes in the browser. <strong>Developing and deploying a web application is easy</strong>: all you need is an HTML file and a web server, and your application is available for the whole world to see and use. This is a major advantage of making a browser-based application, since not only does it enable us to create our own music generation application easily, but it also makes it easy to use it collaboratively. See the <em>Further reading</em> section at the end of the chapter for great examples of popular Magenta.js web applications.</p>
<p>This is the power of a web browser: everyone has one, and a web page requires no installation to run. The downside of a Magenta.js web application is that it also runs in a browser: it isn't the best place to handle quality, real-time audio, and making your application interact with traditional music production tools, such as <strong>digital audio workstations</strong> (<strong>DAWs</strong>), is harder.</p>
<p class="mce-root"/>
<p>We'll be looking at the specifics of working in the browser as we go along. First, we'll be looking at Tone.js in the <em>Introducing Tone.js for sound synthesis in the browser</em> section, which describes the usage of the Web Audio API. Then, we'll be looking at the Web Workers API in the <em>Using the Web Workers API to offload computations from the UI thread</em> section, to make real-time audio easier. Finally, we'll be looking at making Magenta.js interact with other music applications, in the <em>Making Magenta.js interact with other apps</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing TensorFlow.js for machine learning in the browser</h1>
                </header>
            
            <article>
                
<p>First, let's introduce TensorFlow.js (<a href="https://www.tensorflow.org/js">www.tensorflow.org/js</a>), the project upon which Magenta.js is built. As its name suggests, TensorFlow.js is a JavaScript implementation of TensorFlow, making it possible to <strong>use and train</strong> <strong>models in the browser</strong>. Importing and running pre-trained models from TensorFlow SavedModel or Keras is possible.</p>
<p>Using TensorFlow.js is easy. You can use a <kbd>script</kbd> tag, as shown in the following code block:</p>
<pre><span>&lt;</span><span>html</span><span>&gt;</span><br/><span>&lt;</span><span>head</span><span>&gt;</span><br/>  <span>&lt;</span><span>script</span> <span>src</span><span>="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"</span><span>&gt;&lt;/</span><span>script</span><span>&gt;</span><br/>  <span>&lt;</span><span>script</span><span>&gt;</span><br/>    <span>const </span><span>model </span>= tf.sequential();<br/>    <span>model</span>.<span>add</span>(tf.layers.dense({<span>units</span>: <span>1</span>, <span>inputShape</span>: [<span>1</span>]}));<br/>    <span>model</span>.<span>compile</span>({<span>loss</span>: <span>'meanSquaredError'</span>, <span>optimizer</span>: <span>'sgd'</span>});<br/>  <span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;/</span><span>head</span><span>&gt;</span><br/><span>&lt;</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>html</span><span>&gt;</span></pre>
<p>Alternatively, you can use the <kbd>npm</kbd> or <kbd>yarn</kbd> command to run the following code block:</p>
<pre><span class="pl-k">import</span> <span class="pl-c1">*</span> <span class="pl-k">as</span> <span class="pl-smi">tf</span> <span class="pl-k">from</span> <span class="pl-s"><span class="pl-pds">'</span>@tensorflow/tfjs<span class="pl-pds">'</span></span>;
<span class="pl-k">const</span> <span class="pl-c1">model</span> <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">sequential</span>();
<span class="pl-smi">model</span>.<span class="pl-c1">add</span>(<span class="pl-smi">tf</span>.<span class="pl-c1">layers</span>.<span class="pl-en">dense</span>({units<span class="pl-k">:</span> <span class="pl-c1">1</span>, inputShape<span class="pl-k">:</span> [<span class="pl-c1">1</span>]}));
<span class="pl-smi">model</span>.<span class="pl-c1">compile</span>({loss<span class="pl-k">:</span> <span class="pl-s"><span class="pl-pds">'</span>meanSquaredError<span class="pl-pds">'</span></span>, optimizer<span class="pl-k">:</span> <span class="pl-s"><span class="pl-pds">'</span>sgd<span class="pl-pds">'</span></span>});</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Notice in both code snippets the usage of the <kbd>tf</kbd> variable, which is imported with the script (we'll be seeing more <kbd>tf</kbd> usage in this chapter's examples). We won't be looking at TensorFlow.js specifically, but we are going to use it in our Magenta.js code.</p>
<p>Another nice thing about TensorFlow.js is that it uses WebGL (<a href="https://www.khronos.org/registry/webgl/specs/latest/">www.khronos.org/registry/webgl/specs/latest/</a>) for its computation, meaning it is <strong>graphics processing unit</strong> (<strong>GPU</strong>) accelerated (if you have a GPU), without having to install CUDA libraries. The mathematical operations are implemented in WebGL shaders and the tensors are encoded in WebGL textures, which is a very clever use of WebGL. We don't have to do anything for GPU acceleration since the TensorFlow.js backend will handle it for us. When using the Node.js server side, the TensorFlow C API is used for hardware acceleration, meaning that the usage of CUDA libraries is possible.</p>
<p>Using WebGL has a few caveats, though, most notably that the computations might block the UI thread in some cases and that the memory used by a tensor allocation must be reclaimed (disposed) after usage. Regarding computation threading, we'll be looking at this in more depth when we look at web workers. Regarding memory management, we'll be showing proper usage in the code as we go along. See the <em>Further reading</em> section for more information on these issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Magenta.js for music generation in the browser</h1>
                </header>
            
            <article>
                
<p>Now that we understand what Tensorflow.js is, let's talk about Magenta.js. First, we need to understand what Magenta.js can and cannot do. For now, models cannot be trained in Magenta.js (with the exception of the partial training in MidiMe), but the models we've trained in the previous chapter can be converted and imported easily. Another limitation of Magenta.js is that not all models are present, but the most important ones are. While writing Magenta.js code, we'll see that most of the concepts we've already covered are there, sometimes with a different syntax.</p>
<p>Here is an overview of some of the pre-trained models present in Magenta.js:</p>
<ul>
<li><strong>Onsets and Frames</strong> for piano transcription, converting raw audio to MIDI</li>
<li><strong>Music RNN</strong> (<strong>long short-term memory</strong> (<strong>LSTM</strong>)-based networks) for monophonic and polyphonic MIDI generation, including the Melody RNN, Drums RNN, Improv RNN and Performance RNN models</li>
<li><strong>MusicVAE</strong> for single or trio sampling and interpolation, also including GrooVAE</li>
<li><strong>Piano Genie</strong> that maps an 8-key input to a full 88-key piano</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We've already talked about these models in the previous chapters. We can find the pre-trained checkpoints list either in the Magenta.js source code, in the <kbd>music/checkpoints/checkpoints.json</kbd> file, or in the hosted version, at <a href="https://goo.gl/magenta/js-checkpoints">goo.gl/magenta/js-checkpoints</a>. Most of the checkpoints (or bundles) we've used are present in Magenta.js, plus some new additions such as longer 4-bar MusicVAE and GrooVAE models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting trained models for Magenta.js</h1>
                </header>
            
            <article>
                
<p>Using pre-trained models is great, but we can also import our own trained models, such as the ones we trained in the previous chapter—<a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>. We are doing that by using the <kbd>checkpoint_converted.py</kbd> script that dumps the weights from a Magenta checkpoint to a format Magenta.js can use.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_01.html</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>Let's convert a simple RNN model for Magenta.js, by following these steps:</p>
<ol>
<li>First, we'll need the <kbd>checkpoint_converter.py</kbd> script from Magenta.js. The easiest way is to download the script directly from the source code on GitHub, as follows:</li>
</ol>
<pre style="padding-left: 60px">curl -o "<strong>checkpoint_converter.py</strong>" "https://raw.githubusercontent.com/tensorflow/magenta-js/master/scripts/checkpoint_converter.py"</pre>
<p style="padding-left: 60px">This should create the <kbd>checkpoint_converter.py</kbd> file locally.</p>
<ol start="2">
<li>Now, we'll need the TensorFlow.js Python packaging on which the <kbd>checkpoint_converter.py</kbd> script depends. Run the following code:</li>
</ol>
<pre style="padding-left: 60px"># While in your Magenta conda environment<br/>pip install tensorflowjs</pre>
<ol start="3">
<li>We can now run the conversion script using, for example, one of our previously trained DrumsRNN models (replacing <kbd>PATH_TO_TRAINING_DIR</kbd> with a proper value), as follows:</li>
</ol>
<pre style="padding-left: 60px">python checkpoint_converter.py "PATH_TO_TRAINING_DIR/drums_rnn_dance_drums/logdir/run1_small/train/model.ckpt-20000" "checkpoints/drums_rnn_dance_small"</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">This will create the <kbd>checkpoints/drums_rnn_dance_small</kbd> directory with a JSON metadata file and the checkpoint binary files that will get loaded by TensorFlow.js.</p>
<div class="packt_infobox">Remember that when referencing checkpoints in TensorFlow, you need to provide the prefix—for example, <kbd>model.ckpt-20000</kbd>, but not followed by <kbd>.data</kbd>, <kbd>.index</kbd>, or <kbd>.meta</kbd>.</div>
<ol start="4">
<li>Then, we need to create a JSON configuration file that describes the model configuration. Open the <kbd>checkpoints/drums_rnn_dance_small/config.json</kbd> <span>file </span>and enter this content:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  <span>"type"</span>: <span>"MusicRNN"</span>,<br/>  <span>"dataConverter"</span>: {<br/>    <span>"type"</span>: <span>"DrumsConverter"</span>,<br/>    <span>"args"</span>: {}<br/>  }<br/>}</pre>
<p style="padding-left: 60px">This is a minimal example for the DrumsRNN model, without any further configuration. Note that the <kbd>args</kbd> key for the <kbd>dataConverter</kbd> key is necessary, even if no arguments are provided. The <kbd>type</kbd> of <kbd>dataConverter</kbd> is one of the subclasses of <kbd>DataConverter</kbd>, located in the <kbd>data.ts</kbd> file in <kbd>music/src/core</kbd> in the Magenta.js source code. Other possible data converters could be <kbd>MelodyConverter</kbd>, <kbd>TrioConverter</kbd>, or <kbd>GrooveConverter</kbd>.</p>
<p style="padding-left: 60px">Other models and converters will require more configuration. The easiest way to find the proper configuration for a specific model is to find a similar Magenta pre-trained model and use similar values. To do that, follow the <em>Downloading pre-trained models locally</em> section, and find the information you want in the downloaded <kbd>config.json</kbd> file.</p>
<ol start="5">
<li>Our custom model is now converted to a format TensorFlow.js understands. Let's create a small web page that imports and initializes that model to test it, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>&lt;</span><span>html </span><span>lang</span><span>="en"</span><span>&gt;</span><br/><span>&lt;</span><span>body</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span> </span><span>src</span><span>="https://cdn.jsdelivr.net/npm/@magenta/music@1.12.0/dist/magentamusic.js"&gt;</span><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span>&gt;</span><br/>  <span>// Initialize a locally trained DrumsRNN model from the local directory<br/></span><span>  // at: checkpoints/drums_rnn_dance_small<br/></span><span>  </span><span>async function </span><span>startLocalModel</span>() {<br/>    <span>const </span><span>musicRnn </span>= <strong><span>new </span>mm.MusicRNN(<span>"http://0.0.0.0:8000/" </span>+</strong><br/><strong>        <span>"checkpoints/drums_rnn_dance_small"</span>)</strong>;<br/>    <span>await </span><span>musicRnn</span>.<span>initialize</span>();<br/>  }<br/><br/>  <span>// Calls the initialization of the local model<br/></span><span>  </span><span>try </span>{<br/>    <span>Promise</span>.<span>all</span>([<span>startLocalModel</span>()]);<br/>  } <span>catch </span>(error) {<br/>    <span>console</span>.<span>error</span>(error);<br/>  }<br/><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;/</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>html</span><span>&gt;</span></pre>
<p style="padding-left: 60px">Don't worry too much about the content of the HTML page, since it will be thoroughly explained in the following sections. The important part here is that the MusicRNN constructor (<kbd>mm.MusicRNN("URL")</kbd>) is loading our converted DrumsRNN checkpoint in the MusicRNN model.</p>
<p style="padding-left: 60px">You might have noticed that the URL of the checkpoint is local, at <kbd>http://0.0.0.0:8000</kbd>. This is because most browsers implement <strong>Cross-Origin Resource Sharing</strong> (<strong>CORS</strong>) restrictions, one of them being that a local file can only fetch resources starting with a <strong>Uniform Resource Identifier</strong> (<strong>URI</strong>) scheme of <kbd>http</kbd> or <kbd>https</kbd>.</p>
<ol start="6">
<li>The easiest way of circumventing that is to start a web server locally, like this:</li>
</ol>
<pre style="padding-left: 60px"><strong>python -m http.server</strong></pre>
<p style="padding-left: 60px">This will start a web server serving the current folder at <kbd>http://0.0.0.0:8000</kbd>, meaning the HTML file from the previous snippet will be served at <kbd>http://0.0.0.0:8000/example.html</kbd>, and our checkpoint at <kbd>http://0.0.0.0:8000/checkpoints/drums_rnn_dance_small</kbd>.</p>
<ol start="7">
<li>Open the HTML file and check the console. You should see the following:</li>
</ol>
<pre style="padding-left: 60px"><strong><span class="message-body-wrapper"><span class="message-flex-body"><span class="message-body devtools-monospace"><span class="objectBox objectBox-string">* Tone.js v13.8.25 * <br/></span></span></span></span><span class="message-body-wrapper"><span class="message-flex-body"><span class="message-body devtools-monospace"><span class="objectBox objectBox-string">MusicRNN </span> <span class="objectBox objectBox-string">Initialized model in 0.695s</span></span></span></span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This means that our model was successfully initialized.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading pre-trained models locally</h1>
                </header>
            
            <article>
                
<p>Downloading pre-trained models locally is useful if we want to serve them ourselves or if we want to check the <kbd>config.json</kbd> content:</p>
<ol>
<li>First, we'll need the <kbd>checkpoint_converter.py</kbd> script from Magenta.js. The easiest way is to download the script directly from the source code on GitHub, as follows:</li>
</ol>
<pre style="padding-left: 60px">curl -o "<strong>checkpoint_downloader.py</strong>" "https://raw.githubusercontent.com/tensorflow/magenta-js/master/scripts/checkpoint_downloader.py"</pre>
<p style="padding-left: 60px">This should create the <kbd>checkpoint_converter.py</kbd> file locally.</p>
<ol start="2">
<li>We can then call the script by entering the following code:</li>
</ol>
<pre style="padding-left: 60px">python checkpoint_downloader.py "https://storage.googleapis.com/magentadata/js/checkpoints/music_vae/mel_16bar_small_q2" "checkpoints/music_vae_mel_16bar_small_q2"</pre>
<p>This will download the <kbd>mel_16bar_small_q2</kbd> <span>MusicVAE pre-trained model </span>in the <kbd>checkpoints</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Tone.js for sound synthesis in the browser</h1>
                </header>
            
            <article>
                
<p>In this chapter, you'll hear generated audio in the browser, which means that audio synthesis, analogous to when we used FluidSynth in the previous chapters to listen to MIDI files, is happening in the browser, using the Web Audio API.</p>
<p>The <strong>Web Audio API</strong> (<a href="https://www.w3.org/TR/webaudio/">www.w3.org/TR/webaudio/</a>) provides fairly low-level concepts to handle sound sources, transformations, and routing, through the usage of audio nodes. First, we have a sound source that provides an array of sound intensities (see <a href="c5602f6c-c094-42f2-936f-98746cf04a49.xhtml">Chapter 1</a>, <em>Introduction on Magenta and Generative Art</em>, for a refresher on that), which could be a sound file (a sample) or an oscillator. Then, the sound source node can be connected to a transformation node such as a gain (to change the volume). Finally, the result needs to be connected to a destination (an output) for the sound to be heard in the speakers.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The specification is quite mature, listed as <em>W3C Candidate Recommendation, September 18, 2018</em>, so some implementation details might change, but it can be considered stable. In terms of support, all the major browsers support the Web Audio API, which is great. See the <em>Further reading</em> section for more information.</p>
<p>We won't be using the Web Audio API directly. Rather, we'll be using <strong>Tone.js</strong> (<a href="https://tonejs.github.io/">tonejs.github.io</a>), which is a JavaScript library built on top of the Web Audio API, providing higher-level functionalities. Another advantage of using Tone.js is that it can be resilient to change, in the event of the underlying Web Audio API changing.</p>
<p>Since the Web Audio API implementation changes from browser to browser, the quality of the audio might vary. For example, layering multiple audio samples from GANSynth resulted in audio clipping in Firefox but worked correctly in Chrome. Remember that for professional-grade audio quality, audio synthesis in the browser might not be the best choice.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Magenta.js web application</h1>
                </header>
            
            <article>
                
<p>Now that we have introduced the concepts surrounding Magenta.js, we'll be creating a web application using Magenta.js. Let's create a web application where we generate a trio of instruments (the drum kit, the bass, and the lead) using MusicVAE, where we can change the lead instrument for a GANSynth-generated instrument.</p>
<p>We'll be building this application step by step. First, we'll make an app that generates instruments, using GANSynth. Then, we'll create an app in which we can sample a trio sequence. Finally, we'll merge the two apps together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating instruments in the browser using GANSynth</h1>
                </header>
            
            <article>
                
<p>For the first part of our example, we'll use GANSynth to sample single instrument notes, which are short audio clips of 4 seconds. We'll be able to layer multiple audio clips, for interesting effects.</p>
<p>First, we'll create the HTML page and import the required scripts. Then, we'll write the GANSynth sampling code and explain each step in detail. We'll finish the example by listening to the generated audio.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the page structure</h1>
                </header>
            
            <article>
                
<p>We'll be keeping the page structure and style at a minimum, to focus on the Magenta.js code.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_02.html</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>First, let's create the page structure and import the required scripts, as follows:</p>
<pre><span>&lt;</span><span>html </span><span>lang</span><span>="en"</span><span>&gt;</span><br/><span>&lt;</span><span>body</span><span>&gt;</span><br/><span>&lt;</span><span>div</span><span>&gt;</span><br/>  <span>&lt;</span><span>button </span><span>disabled <strong>id</strong></span><strong><span>="button-sample-gansynth-note"</span></strong><span>&gt;<br/>    </span>Sample GANSynth note<br/>  <span>&lt;/</span><span>button</span><span>&gt;</span><br/>  <span>&lt;</span><span>div </span><strong><span>id</span><span>="container-plots"</span></strong><span>&gt;&lt;/</span><span>div</span><span>&gt;</span><br/><span>&lt;/</span><span>div</span><span>&gt;</span><br/><span>&lt;</span><span>script</span> <strong><span>src</span></strong><span><strong>="https://cdn.jsdelivr.net/npm/@magenta/music@1.12.0/dist/magentamusic.min.js"</strong>&gt;</span><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span>&gt;</span><br/>  <span>// GANSynth code</span><br/><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;/</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>html</span><span>&gt;</span></pre>
<p>The page structure contains only a button that will call the GANSynth generation and a container in which we'll draw the generated spectrogram.</p>
<p>There are two ways of using Magenta.js in the browser, as follows:</p>
<ol>
<li>We can import the whole Magenta.js music distribution in <kbd>dist/magentamusic.min.js</kbd>. In the Magenta documentation, this is referred to as the <strong>ES5 bundle</strong> method. This will include Magenta.js (bound on <kbd>mm</kbd>) and all its dependencies, including TensorFlow.js (bound on <kbd>mm.tf</kbd>) and Tone.js (bound on <kbd>mm.Player.tone</kbd>).</li>
<li>We can import only the Magenta.js elements that we need, under the <kbd>es6</kbd> folder. In the Magenta documentation, this is referred to as the <strong>ES6 bundle</strong> method. For example, if we only need the GANSynth model, we will need to import Tone.js (bound on <kbd>Tone</kbd>), Tensorflow.js (bound on <kbd>tf</kbd>), Magenta.js core (bound on <kbd>core</kbd>), and Magenta.js GANSynth (bound on <kbd>gansynth</kbd>).</li>
</ol>
<p class="mce-root"/>
<p>We won't talk about the differences between the ES5 and the ES6 bundles here. Just remember that the easiest way to go is to use the ES5 bundle method, importing one big file with everything. If you want more control over what is sent to the client (for performance reasons, for example), you'll want to use the ES6 bundle method. Remember that the module bindings are not the same between both methods, so you'll have to adapt your code if you change the imports.</p>
<p>Here are the ES6 bundle imports for the GANSynth model only:</p>
<pre>&lt;script src="https://cdn.jsdelivr.net/npm/tone@13.8.25/build/Tone.min.js"&gt;&lt;/script&gt;<br/>&lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.4.0/dist/tf.min.js"&gt;&lt;/script&gt;<br/>&lt;script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/core.js"&gt;&lt;/script&gt;<br/>&lt;script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/gansynth.js"&gt;&lt;/script&gt;</pre>
<p>This imports only the GANSynth model, which can be instantiated using <kbd>new gansynth.GANSynth(...)</kbd>. When using ES6 modules, we need to import each script individually. For our example, these are Tone.js, TensorFlow.js, Magenta.js core, and GANSynth.</p>
<p>We'll stick with ES5 bundles for our example, but feel free to use ES6 bundles if you feel like it. We'll be showing where the code differs between each approach in our examples.</p>
<div class="packt_infobox">You can find the ES6 code for this example in the <kbd>chapter_08_example_02_es6.html</kbd> file, in the source code of this chapter.</div>
<p>Now, let's write the GANSynth code (in the <kbd>GANSynth code</kbd> comment), and explain each step.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling audio using GANSynth</h1>
                </header>
            
            <article>
                
<p>Now that we have properly imported Magenta.js, we can write the GANSynth audio generation code by following these steps:</p>
<ol>
<li>First, we'll initialize the DOM elements and initialize GANSynth, like this:</li>
</ol>
<pre style="padding-left: 60px"><span>// Get DOM elements<br/></span><span>const </span><span>buttonSampleGanSynthNote </span>= <span>document<br/></span><span>    </span>.<span>getElementById</span>(<span>"button-sample-gansynth-note"</span>);<br/><span>const </span><span>containerPlots </span>= <span>document<br/>    </span>.<span>getElementById</span>(<span>"container-plots"</span>);<br/><br/><span>// Starts the GANSynth model and initializes it. When finished, enables<br/></span><span>// the button to start the sampling<br/></span><span>async function </span><span>startGanSynth</span>() {<br/>  <span>const </span><span>ganSynth </span>= <strong><span>new </span>mm.GANSynth(<span>"https://storage.googleapis.com/" </span>+</strong><br/><strong>      <span>"magentadata/js/checkpoints/gansynth/acoustic_only"</span>)</strong>;<br/>  <span>await </span><strong><span>ganSynth</span>.<span>initialize</span>()</strong>;<br/>  <span>window</span>.<span>ganSynth </span>= <span>gansynth</span>;<br/>  <span>buttonSampleGanSynthNote</span>.<span>disabled </span>= <span>false</span>;<br/>}</pre>
<p style="padding-left: 60px">Here, we instantiate GANSynth using <kbd>mm.GANSynth(...)</kbd>. Remember, the Magenta.js context is under the <kbd>mm</kbd> <span>variable </span>when imported using an ES5 module. The URL for the checkpoint is the same that we've used in the previous chapter—<a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>. Refer to that chapter if you want more information. We also make the reference to <kbd>ganSynth</kbd> globally so that we can call it later easily.</p>
<p style="padding-left: 60px">Using the Magenta.js ES6 bundle, we would have the following code:</p>
<pre style="padding-left: 60px"><span>const </span><span>ganSynth </span>= <strong><span>new </span><span>gansynth</span>.GANSynth</strong>(<span>"https://storage.googleapis.com/" </span>+<br/>    <span>"magentadata/js/checkpoints/gansynth/acoustic_only"</span>);</pre>
<p style="padding-left: 60px">For the ES6 bundle, the module variable is <kbd>gansynth.GANSynth</kbd> instead of <kbd>mm.GANSynth</kbd>.</p>
<p class="mce-root"/>
<ol start="2">
<li>Now, let's write an asynchronous function that will insert the generated spectrogram in the web page using a <kbd>canvas</kbd>, like this:</li>
</ol>
<pre style="padding-left: 60px"><span>// Plots the spectrogram of the given channel<br/></span><span>// see music/demos/gansynth.ts:28 in magenta.js source code<br/></span><span>async function </span><span>plotSpectra</span>(spectra, channel) {<br/>  <span>const </span><span>spectraPlot </span>= mm.tf.<strong>tidy</strong>(() =&gt; {<br/>    <span>// Slice a single example.<br/></span><span>    </span><span>let </span><span>spectraPlot </span>= mm.tf.<span>slice</span>(spectra, [<span>0</span>, <span>0</span>, <span>0</span>, channel], [<span>1</span>, -<span>1</span>, -<span>1</span>, <span>1</span>])<br/>        .reshape([<span>128</span>, <span>1024</span>]);<br/>    <span>// Scale to [0, 1].<br/></span><span>    </span><span>spectraPlot </span>= mm.tf.<span>sub</span>(<span>spectraPlot</span>, mm.tf.<span>min</span>(<span>spectraPlot</span>));<br/>    <span>spectraPlot </span>= mm.tf.div(<span>spectraPlot</span>, mm.tf.<span>max</span>(<span>spectraPlot</span>));<br/>    <span>return </span><span>spectraPlot</span>;<br/>  });<br/>  <span>// Plot on canvas.<br/></span><span>  </span><span>const </span><span>canvas </span>= <span>document</span>.<span>createElement</span>(<span>"canvas"</span>);<br/>  <span>containerPlots</span>.<span>appendChild</span>(<span>canvas</span>);<br/>  <span>await </span>mm.tf.browser.toPixels(<span>spectraPlot</span>, <span>canvas</span>);<br/>  <strong><span>spectraPlot</span>.dispose();</strong><br/>}</pre>
<p style="padding-left: 60px">This method creates a spectrogram plot and inserts it in a <kbd>canvas</kbd> element in the <kbd>containerPlots</kbd> elements we've previously declared. It will keep adding spectrograms for each generation.</p>
<p style="padding-left: 60px">You might have noticed the usage of <kbd>tf.tidy</kbd> and <kbd>dispose</kbd> in the example. Using those methods is necessary to avoid memory leaks in the TensorFlow.js code. This is because TensorFlow.js uses WebGL to make its computations, and <strong>WebGL resources need to be explicitly reclaimed</strong> after use. Any <kbd>tf.Tensor</kbd> needs to be disposed of after use by using <kbd>dispose</kbd>. The <kbd>tf.tidy</kbd> method can be used to dispose of all the tensors that are not returned by a function after executing it.</p>
<p style="padding-left: 60px">You might wonder what the <kbd>async</kbd> and <kbd>await</kbd> keywords are, in the previous JavaScript code. Those two keywords mark the usage of <strong>asynchronous methods</strong>. When calling a method that is marked with <kbd>async</kbd>, meaning it is asynchronous, the caller needs to mark the calls with <kbd>await</kbd>, meaning that it will wait (block) until a value is returned. The <kbd>await</kbd> keyword can be used only in <kbd>async</kbd> methods. In our example, the <kbd>mm.tf.browser.toPixels</kbd> method is marked with <kbd>async</kbd>, so we need to wait for its return using <kbd>await</kbd>. Calling an <kbd>async</kbd> method without using <kbd>await</kbd> can be done using the <kbd>Promise</kbd> syntax—<kbd>Promise.all([myAsyncMethod()])</kbd>.</p>
<p class="mce-root"/>
<div class="packt_tip">Promises were introduced in JavaScript to fix a recurring problem when writing asynchronous code: the <strong>callback hell</strong>. The callback hell is a problem that arises when multiple linked calls are all asynchronous, resulting in nested callbacks (from hell).<br/>
Promises are great because they provide a clean mechanism to handle complex chains of asynchronous calls, as well as proper error handling. However, they are a bit verbose, which is why the <kbd>async</kbd> and <kbd>await</kbd> keywords were introduced as syntactic sugar, to alleviate some of the common use cases around using promises.</div>
<ol start="3">
<li>Then, we write an asynchronous function that samples a note from GANSynth, plays it, and plots it using our previous method, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Samples a single note of 4 seconds from GANSynth and plays it repeatedly<br/></span><span>async function </span><span>sampleGanNote</span>() {<br/>  <span>const </span><span>lengthInSeconds </span>= <span>4.0</span>;<br/>  <span>const </span><span>sampleRate </span>= <span>16000</span>;<br/>  <span>const </span><span>length </span>= <span>lengthInSeconds </span>* <span>sampleRate</span>;<br/><br/>  <span>// The sampling returns a spectrogram, convert that to audio in<br/></span><span>  // a tone.js buffer<br/></span><span>  </span><span>const </span><span>specgrams </span>= <span>await </span><strong><span>ganSynth</span>.randomSample(<span>60</span>)</strong>;<br/>  <span>const </span><span>audio </span>= <span>await </span><strong><span>ganSynth</span>.specgramsToAudio(<span>specgrams</span>)</strong>;<br/>  <span>const </span><span>audioBuffer </span>= mm.Player.<span>tone</span>.<span>context</span>.<span>createBuffer</span>(<br/>      <span>1</span>, <span>length</span>, <span>sampleRate</span>);<br/>  <span>audioBuffer</span>.<span>copyToChannel</span>(<span>audio</span>, <span>0</span>, <span>0</span>);<br/><br/>  <span>// Play the sample audio using tone.js and loop it<br/></span><span>  </span><span>const </span><span>playerOptions </span>= {<strong><span>"url"</span>: <span>audioBuffer</span></strong>, <span>"loop"</span>: <span>true</span>, <span>"volume"</span>: -<span>25</span>};<br/>  <span>const </span><span>player </span>= <strong><span>new </span>mm.Player.<span>tone</span>.Player(<span>playerOptions</span>).toMaster()</strong>;<br/>  <span>player</span>.<span>start</span>();<br/><br/>  <span>// Plots the resulting spectrograms<br/></span><span>  </span><span>await </span><span>plotSpectra</span>(<span>specgrams</span>, <span>0</span>);<br/>  <span>await </span><span>plotSpectra</span>(<span>specgrams</span>, <span>1</span>);<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">We first sample from GANSynth using the <kbd>randomSample</kbd> method, using a base pitch of <kbd>60</kbd>, which is C4, as an argument. This tells the model to sample a value corresponding to that pitch. Then, the returned spectrogram is converted to audio using <kbd>specgramsToAudio</kbd>. Finally, we use a Tone.js audio buffer to play the sample, by instantiating a new player using the audio buffer. Since we instantiate a new player for each sample, each new audio sample will get layered on top of the others.</p>
<p style="padding-left: 60px">The code to instantiate the player, <kbd>mm.Player.tone.Player</kbd>, is a bit convoluted since we first need to find the reference to Tone.js that was already instantiated by the Magenta.js object using <kbd>mm.Player.tone</kbd> (here, the <kbd>Player</kbd> reference is a Magenta.js class).</p>
<p style="padding-left: 60px">Using ES6 bundles is more straightforward, as can be seen here:</p>
<pre style="padding-left: 60px"><span>const </span><span>player </span>= <strong><span>new </span>Tone.Player</strong>(<span>playerOptions</span>).toMaster();</pre>
<p style="padding-left: 60px">Since the Magenta.js ES6 bundle doesn't include Tone.js, it is initialized on its own and can be referenced directly, using the <kbd>Tone</kbd> variable.</p>
<ol start="4">
<li>Finally, let's wrap up our example by binding the button to an action and initializing GANSynth, like this:</li>
</ol>
<pre style="padding-left: 60px"><span>// Add on click handler to call the GANSynth sampling<br/></span><span>buttonSampleGanSynthNote</span>.<span>addEventListener</span>(<span>"click"</span>, () =&gt; {<br/>  <strong><span>sampleGanNote</span>()</strong>;<br/>});<br/><br/><span>// Calls the initialization of GANSynth<br/></span><span>try </span>{<br/>  <span>Promise</span>.<span>all</span>([<strong><span>startGanSynth</span>()</strong>]);<br/>} <span>catch </span>(error) {<br/>  <span>console</span>.<span>error</span>(error);<br/>}</pre>
<p>First, we bind our button the <kbd>sampleGanNote</kbd> method, then we initialize GANSynth, using the <kbd>startGanSynth</kbd> method.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the web application</h1>
                </header>
            
            <article>
                
<p>Now that we have our web application ready, we can test our code. Let's open the HTML page we've created using a browser. We should see a page similar to the one shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4912ffc-37a4-4d40-afe7-9d5f9b1acd48.png"/></p>
<p>In the previous figure, we've already generated some GANSynth samples. Each generation plots two spectrograms and keeps the previous ones on the page. On the right side of the preceding screenshot, in the console debugger, you can see Tone.js and GANSynth initializing. When that is completed, the <strong><span class="packt_screen">Sample GANSynth note</span></strong> button will get enabled.</p>
<p>Go ahead and generate sounds: you'll get pretty interesting effects when layering many of them. Congratulations—you've completed your first Magenta.js web application!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a trio using MusicVAE</h1>
                </header>
            
            <article>
                
<p>We'll now be using the MusicVAE model in Magenta.js to generate some sequences and play them directly in the browser, using Tone.js. The checkpoint we'll be using is a <kbd>trio</kbd> model, meaning we'll be generating three sequences at the same time: the drum kit, the bass, and the lead.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_03.html</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>Since the code is similar to the last section, we won't be going through all the content, but we'll explain the major differences:</p>
<ol>
<li>First, let's define the page structure and script imports, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>&lt;</span><span>html </span><span>lang</span><span>="en"</span><span>&gt;</span><br/><span>&lt;</span><span>body</span><span>&gt;</span><br/><span>&lt;</span><span>div</span><span>&gt;</span><br/>  <span>&lt;</span><span>button </span><span>disabled </span><strong><span>id</span><span>="button-sample-musicae-trio"</span></strong><span>&gt;</span><br/>    Sample MusicVAE trio<br/> <span>&lt;/</span><span>button</span><span>&gt;</span><br/>  <span>&lt;</span><span>canvas </span><strong><span>id</span><span>="canvas-musicvae-plot"</span></strong><span>&gt;&lt;/</span><span>canvas</span><span>&gt;</span><br/><span>&lt;/</span><span>div</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span> </span><strong><span>src</span></strong><span><strong>="https://cdn.jsdelivr.net/npm/@magenta/music@1.12.0/dist/magentamusic.min.js"</strong>&gt;</span><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span>&gt;</span><br/>  <span>// MusicVAE code</span><br/><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;/</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>html</span><span>&gt;</span></pre>
<p style="padding-left: 60px">The page has the same structure as the previous section. We'll be filling in the code in the <kbd>MusicVAE code</kbd> comment.</p>
<ol start="2">
<li>Then, let's initialize the MusicVAE model, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Get DOM elements<br/></span><span>const </span><span>buttonSampleMusicVaeTrio </span>= <span>document<br/></span><span>    </span>.<span>getElementById</span>(<span>"button-sample-musicae-trio"</span>);<br/><span>const </span><span>canvasMusicVaePlot </span>= <span>document<br/>    </span>.<span>getElementById</span>(<span>"canvas-musicvae-plot"</span>);<br/><br/><span>// Starts the MusicVAE model and initializes it. When finished, enables<br/></span><span>// the button to start the sampling<br/></span><span>async function </span><span>startMusicVae</span>() {<br/>  <span>const </span><span>musicvae </span>= <strong><span>new </span>mm.MusicVAE(<span>"https://storage.googleapis.com/" </span>+</strong><br/><strong>      <span>"magentadata/js/checkpoints/music_vae/trio_4bar"</span>)</strong>;<br/>  <span>await </span><strong><span>musicvae</span>.<span>initialize</span>()</strong>;<br/>  <span>window</span>.<span>musicvae </span>= <span>musicvae</span>;<br/>  <span>buttonSampleMusicVaeTrio</span>.<span>disabled </span>= <span>false</span>;<br/>}</pre>
<p style="padding-left: 60px">The URL for the checkpoint is the same as the one we used in the previous chapter—<a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with MusicVAE</em>. Refer to this chapter if you want more information on that checkpoint.</p>
<ol start="3">
<li>We now create a new Tone.js player to play the three generated sequences, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Declares a new player that have 3 synths for the drum kit (only the<br/></span><span>// bass drum), the bass and the lead.<br/></span><span>class </span>Player <span>extends </span>mm.BasePlayer {<br/><br/>  <strong><span>bassDrumSynth </span>= <span>new </span>mm.Player.<span>tone</span>.MembraneSynth().toMaster();</strong><br/><br/>  <strong><span>bassSynth </span>= <span>new </span>mm.Player.<span>tone</span>.Synth({</strong><br/><strong>    <span>volume</span>: <span>5</span>,</strong><br/><strong>    <span>oscillator</span>: {<span>type</span>: <span>"triangle"</span>}</strong><br/><strong>  }).toMaster();</strong><br/><br/>  <strong><span>leadSynth </span>= <span>new </span>mm.Player.<span>tone</span>.PolySynth(<span>5</span>).toMaster();</strong><br/><br/>  <span>// Plays the note at the proper time using tone.js<br/></span><span>  </span><span>playNote</span>(time, note) {<br/>    <span>let </span><span>frequency</span>, <span>duration</span>, <span>synth</span>;<br/>    <span>if </span>(note.isDrum) {<br/>      <span>if </span>(note.<span>pitch </span>=== <span>35 </span>|| note.<span>pitch </span>=== <span>36</span>) {<br/>        <span>// If this is a bass drum, we use the kick pitch for<br/></span><span>        // an eight note and the bass drum synth<br/></span><span>        </span><span>frequency </span>= <span>"C2"</span>;<br/>        <span>duration </span>= <span>"8n"</span>;<br/>        <strong><span>synth </span>= <span>this</span>.<span>bassDrumSynth</span></strong>;<br/>      }<br/>    } <span>else </span>{<br/>      <span>// If this is a bass note or lead note, we convert the<br/></span><span>      // frequency and the duration for tone.js and fetch<br/></span><span>      // the proper synth<br/></span><span>      </span><span>frequency </span>= <span>new </span>mm.Player.<span>tone</span>.Frequency(note.<span>pitch</span>, <span>"midi"</span>);<br/>      <span>duration </span>= note.<span>endTime </span>- note.<span>startTime</span>;<br/>      if (note.program &gt;= 32 &amp;&amp; note.program &lt;= 39) {<br/>        <strong>synth = this.bassSynth;</strong><br/>      } else {<br/>        <strong>synth = this.leadSynth;</strong><br/>      }<br/>    }<br/>    <span>if </span>(<span>synth</span>) {<br/>      <strong><span>synth</span>.triggerAttackRelease(<span>frequency</span>, <span>duration</span>, time, <span>1</span>);</strong><br/>    }<br/>  }<br/>}</pre>
<p style="padding-left: 60px">This code extends the <kbd>mm.BasePlayer</kbd> class in Magenta.js, which is useful because we only need to implement the <kbd>playNote</kbd> method to play the sequences. First, we define three synths: <kbd>bassDrumSynth</kbd>, <kbd>bassSynth</kbd>, and <kbd>leadSynth</kbd>, described here:</p>
<ul>
<li style="padding-left: 60px">The <strong>bass drum synth</strong> only plays the bass drum, which is represented by the <kbd>note.isDrum</kbd> property and MIDI notes 35 or 36 and always plays a <kbd>C2</kbd> frequency for an 8-note length <kbd>8n</kbd>, using the <kbd>MembraneSynth</kbd> from Tone.js. Remember: in the MIDI specification for the percussion channel, the instruments (Bass Drum, Snare, etc.) are defined by the note's pitch—for example, pitch 35 is Acoustic Bass Drum.</li>
<li style="padding-left: 60px">The <strong>bass synth</strong> only plays the programs from 32 to 39 inclusive, using the <kbd>Synth</kbd> from Tone.js with a triangle waveshape. Remember: from the MIDI specification, the program specifies which instrument should be played. For example, program 1 is Acoustic Grand Piano, and program 33 is Acoustic Bass.</li>
<li style="padding-left: 60px">The <strong>lead synth</strong> plays the other programs, using the <kbd>PolySynth</kbd> from Tone.js with five voices.</li>
</ul>
<p style="padding-left: 60px">One thing to notice for the bass and lead synths is that we first need to convert the MIDI note to a Tone.js frequency, using the <kbd>Frequency</kbd> class.</p>
<p style="padding-left: 60px">Another important thing to talk about is the <strong>note envelope</strong>, used on a synth in Tone.js with the <kbd>triggerAttackRelease</kbd> method. An envelope acts as a filter that will let the note be heard for a certain amount of time. You can think of a synthesizer as <em>always</em> <em>playing</em>, and the envelope—when closed—does not let the sound through. When opened, the envelope lets the sound be heard, using a certain <strong>slope</strong>, meaning the sound can come slowly (or fast), and end slowly (or fast). This is called, respectively, <strong>the attack</strong> and <strong>the release</strong> of the envelope. Each time we call the trigger method, the synth will be heard for the given duration, using a certain slope.</p>
<p class="mce-root"/>
<div class="packt_tip">You might have already heard the term <strong>Attack Decay Sustain Release</strong> (<strong>ADSR</strong>) when talking about envelopes. In Tone.js, we are using a simplified version of this, using only the <strong>Attack</strong> and the <strong>Release</strong> of the envelope. With an ADSR envelope, we have more control over the resulting shape. For the sake of our example, we'll stick with the simplified version.</div>
<ol start="4">
<li>Let's now sample the MusicVAE model, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Samples a trio of drum kit, bass and lead from MusicVAE and<br/></span><span>// plays it repeatedly at 120 QPM<br/></span><span>async function </span><span>sampleMusicVaeTrio</span>() {<br/>  <span>const </span><span>samples </span>= <span>await </span><strong><span>musicvae</span>.sample(<span>1</span>)</strong>;<br/>  <span>const </span><span>sample </span>= <span>samples</span>[<span>0</span>];<br/>  <span>new </span><strong>mm.PianoRollCanvasVisualizer</strong>(<span>sample</span>, <span>canvasMusicVaePlot</span>,<br/>      {<span>"pixelsPerTimeStep"</span>: <span>50</span>});<br/><br/>  const player = new Player();<br/>  mm.Player.tone.Transport.loop = true;<br/>  mm.Player.tone.Transport.loopStart = 0;<br/>  mm.Player.tone.Transport.loopEnd = 8;<br/>  <strong>player.start(sample, 120);</strong><br/>}</pre>
<p style="padding-left: 60px">First, we sample the MusicVAE model using the <kbd>sample</kbd> method and an argument of 1, which is the number of required samples. We then plot the resulting note sequence, using an <kbd>mm.PianoRollCanvasVisualizer</kbd> in the previously declared <kbd>canvas</kbd>. Finally, we start the player with the sample at 120 QPM and loop the 8-second sequence, using the Tone.js <kbd>Transport</kbd> class. Remember that the MusicVAE models have fixed length, meaning that by using the 4-bar trio model, we generate 8-second samples at 120 QPM.</p>
<p class="mce-root"/>
<ol start="5">
<li>Finally, let's wrap up our example by binding the button to an action and initializing the MusicVAE model, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Add on click handler to call the MusicVAE sampling<br/></span><span>buttonSampleMusicVaeTrio</span>.<span>addEventListener</span>(<span>"click"</span>, (event) =&gt; {<br/>  <strong><span>sampleMusicVaeTrio</span>();</strong><br/>  event.<span>target</span>.<span>disabled </span>= <span>true</span>;<br/>});<br/><br/><span>// Calls the initialization of MusicVAE<br/></span><span>try </span>{<br/>  <span>Promise</span>.<span>all</span>([<strong><span>startMusicVae</span>()</strong>]);<br/>} <span>catch </span>(error) {<br/>  <span>console</span>.<span>error</span>(error);<br/>}</pre>
<p style="padding-left: 60px">First, we bind our button the <kbd>sampleMusicVaeTrio</kbd> method, then we initialize the MusicVAE model using the <kbd>startMusicVae</kbd> method. You can see here that we are using the <kbd>Promise.all</kbd> call that we previously introduced to launch our asynchronous code.</p>
<ol start="6">
<li>Now that we have our web application ready, we can test our code. Let's open the HTML page we've created using a browser. We should see a page similar to the one shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/23e2e0b7-42e4-4f8c-86ed-4efcad9bbbf5.png"/></p>
<p>By pressing the <strong><span class="packt_screen">Sample MusicVAE trio</span></strong> button, the MusicVAE should sample a sequence, plot it, and play it using the synths we've defined. The generated plot is rather basic, since it doesn't differentiate the three instruments and has no time or pitch marker, but it can be customized using the <kbd>PianoRollCanvasVisualizer</kbd> class.</p>
<p>To generate a new sequence, reload the page to start again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a SoundFont for more realistic-sounding instruments</h1>
                </header>
            
            <article>
                
<p>When listening to the generated sound, you might notice that the music sounds a bit <em>basic</em> or <em>simple</em>. That is because we've used the default synths in Tone.js, which have the advantage of being easy to use, with the downside of not sounding as good as more complex synths. Remember that the Tone.js synth can be customized to sound better.</p>
<p>Instead of using a synthesizer, we can also use a SoundFont. SoundFonts are recorded notes of various instruments, and we've been using them in FluidSynth since the beginning of this book. In Magenta.js, we can use the <kbd>SoundFontPlayer</kbd> for that purpose, instead of using the <kbd>Player</kbd> instance, as shown in the following code block:</p>
<pre><span>const player = new </span>mm.SoundFontPlayer("https://storage.googleapis.com/" +<br/>    "magentadata/js/soundfonts/<span>salamander"</span>));<br/><span>player</span>.<span>start</span>(sequence, 120)</pre>
<p>The list of the SoundFonts hosted by the Magenta team can be found in the Magenta.js music README (<a href="https://github.com/tensorflow/magenta-js/tree/master/music">github.com/tensorflow/magenta-js/tree/master/music</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing generated instruments in a trio</h1>
                </header>
            
            <article>
                
<p>Now that we have MusicVAE generating a three-instrument sequence and GANSynth generating audio, let's make the two work together.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_04.html</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>Since the code is similar to the last section, we won't be going through all the content, but we'll explain the major differences:</p>
<ol>
<li>First, let's define the page structure and script imports, like this:</li>
</ol>
<pre style="padding-left: 60px"><span>&lt;</span><span>html </span><span>lang</span><span>="en"</span><span>&gt;</span><br/><span>&lt;</span><span>body</span><span>&gt;</span><br/><span>&lt;div&gt;<br/>  &lt;button disabled <strong>id="button-sample-musicae-trio"</strong>&gt;<br/>    Sample MusicVAE trio<br/>  &lt;/button&gt;<br/>  &lt;button disabled <strong>id="button-sample-gansynth-note"</strong>&gt;<br/>    Sample GANSynth note for the lead synth<br/>  &lt;/button&gt;<br/>  &lt;canvas <strong>id="canvas-musicvae-plot"</strong>&gt;&lt;/canvas&gt;<br/>  &lt;div <strong>id="container-plots"</strong>&gt;&lt;/div&gt;<br/>&lt;/div&gt;</span><br/><span>&lt;</span><span>script</span><span> </span><strong><span>src</span></strong><span><strong>="https://cdn.jsdelivr.net/npm/@magenta/music@1.12.0/dist/magentamusic.min.js"</strong>&gt;</span><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;</span><span>script</span><span>&gt;</span><br/>  <span>// MusicVAE + GANSynth code</span><br/><span>&lt;/</span><span>script</span><span>&gt;</span><br/><span>&lt;/</span><span>body</span><span>&gt;</span><br/><span>&lt;/</span><span>html</span><span>&gt;</span></pre>
<p style="padding-left: 60px">The page has the same structure as the previous section. We'll be filling in the code in the <kbd>MusicVAE + GANSynth code</kbd> comment.</p>
<ol start="2">
<li>Then, let's initialize both the MusicVAE model and the GANSynth model, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Get DOM elements<br/></span><span>const </span><span>buttonSampleGanSynthNote </span>= <span>document<br/></span><span>    </span>.<span>getElementById</span>(<span>"button-sample-gansynth-note"</span>);<br/><span>const </span><span>buttonSampleMusicVaeTrio </span>= <span>document<br/></span><span>    </span>.<span>getElementById</span>(<span>"button-sample-musicae-trio"</span>);<br/><span>const </span><span>containerPlots </span>= <span>document<br/>    </span>.<span>getElementById</span>(<span>"container-plots"</span>);<br/><span>const </span><span>canvasMusicVaePlot </span>= <span>document<br/>    </span>.<span>getElementById</span>(<span>"canvas-musicvae-plot"</span>);<br/><br/><span>// Starts the MusicVAE model and initializes it. When finished, enables<br/></span><span>// the button to start the sampling<br/></span><span>async function </span><span>startMusicVae</span>() {<br/>  <span>const </span><span>musicvae </span>= <strong><span>new </span>mm.MusicVAE(<span>"https://storage.googleapis.com/" </span>+</strong><br/><strong>      <span>"magentadata/js/checkpoints/music_vae/trio_4bar"</span>);</strong><br/>  <span>await </span><strong><span>musicvae</span>.<span>initialize</span>();</strong><br/>  <span>window</span>.<span>musicvae </span>= <span>musicvae</span>;<br/>  <span>buttonSampleMusicVaeTrio</span>.<span>disabled </span>= <span>false</span>;<br/>}<br/><br/><span>// Starts the GANSynth model and initializes it<br/></span><span>async function </span><span>startGanSynth</span>() {<br/>  <span>const </span><span>ganSynth </span>= <strong><span>new </span>mm.GANSynth(<span>"https://storage.googleapis.com/" </span>+</strong><br/><strong>      <span>"magentadata/js/checkpoints/gansynth/acoustic_only"</span>);</strong><br/>  <span>await </span><span>ganSynth</span>.<span>initialize</span>();<br/>  <span>window</span>.<span>ganSynth </span>= <span>ganSynth<br/></span>}</pre>
<p style="padding-left: 60px">Here, we only enable the <strong>MusicVAE sampling</strong> button. The <strong>GANSynth sampling</strong> button will get enabled when MusicVAE has completed its generation.</p>
<ol start="3">
<li>We keep the same <kbd>plotSpectra</kbd> method (from the previous example).</li>
<li>We keep the same <kbd>Player</kbd> class (from the previous example) for the sound synthesis. We can set <kbd>leadSynth = null</kbd> because it will get replaced by the GANSynth generation, but it is not necessary.</li>
<li>We keep the same <kbd>sampleMusicVaeTrio</kbd> method (from the previous example), but we also set the instantiated player as a global variable using <kbd>window.player = player</kbd>, since GANSynth will need to change the lead synth later.</li>
<li>We rewrite the <kbd>sampleGanNote</kbd> method (from the previous example) to add a sample player, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Samples a single note of 4 seconds from GANSynth and plays it repeatedly<br/></span><span>async function </span><span>sampleGanNote</span>() {<br/>  <span>const </span><span>lengthInSeconds </span>= <span>4.0</span>;<br/>  <span>const </span><span>sampleRate </span>= <span>16000</span>;<br/>  <span>const </span><span>length </span>= <span>lengthInSeconds </span>* <span>sampleRate</span>;<br/><br/>  <span>// The sampling returns a spectrogram, convert that to audio in<br/></span><span>  // a tone.js buffer<br/></span><span>  </span><span>const </span><span>specgrams </span>= <span>await </span><strong><span>ganSynth</span>.randomSample(<span>60</span>)</strong>;<br/>  <span>const </span><span>audio </span>= <span>await </span><span>ganSynth</span>.specgramsToAudio(<span>specgrams</span>);<br/>  <span>const </span><span>audioBuffer </span>= mm.Player.<span>tone</span>.<span>context</span>.<span>createBuffer</span>(<br/>      <span>1</span>, <span>length</span>, <span>sampleRate</span>);<br/>  <span>audioBuffer</span>.<span>copyToChannel</span>(<span>audio</span>, <span>0</span>, <span>0</span>);<br/><br/>  <span>// Plays the sample using tone.js by using C4 as a base note,<br/></span><span>  // since this is what we asked the model for (MIDI pitch 60).<br/></span><span>  // If the sequence contains other notes, the pitch will be<br/></span><span>  // changed automatically<br/></span><span>  </span><span>const </span><span>volume </span>= <span>new </span>mm.Player.<span>tone</span>.Volume(-<span>10</span>);<br/>  <span>const </span><span>instrument </span>= <strong><span>new </span>mm.Player.<span>tone</span>.Sampler({<span>"C4"</span>: <span>audioBuffer</span>})</strong>;<br/>  <span>instrument</span>.chain(<span>volume</span>, mm.Player.<span>tone</span>.Master);<br/>  <strong><span>window</span>.<span>player</span>.<span>leadSynth </span>= <span>instrument</span></strong>;<br/><br/>  <span>// Plots the resulting spectrograms<br/></span><span>  </span><span>await </span><span>plotSpectra</span>(<span>specgrams</span>, <span>0</span>);<br/>  <span>await </span><span>plotSpectra</span>(<span>specgrams</span>, <span>1</span>);<br/>}</pre>
<p style="padding-left: 60px">First, we sample a random instrument from GANSynth using <kbd>randomSample</kbd>, as in the previous example. Then, we need to play that sample in a Tone.js synth, so we use the <kbd>Sampler</kbd> class, which takes a dictionary containing a sample for each key. Because we sampled the model using the MIDI pitch 60, we are using a <kbd>C4</kbd> for the resulting audio buffer. Finally, we put that synth in our player using <kbd>window.player.leadSynth = instrument</kbd>.</p>
<ol start="7">
<li>Let's wrap up our example by binding the buttons to their corresponding actions and initializing the MusicVAE and GANSynth models, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Add on click handler to call the MusicVAE sampling<br/>buttonSampleMusicVaeTrio.addEventListener("click", (event) =&gt; {<br/>  <strong>sampleMusicVaeTrio()</strong>;<br/>  event.target.disabled = true;<br/>  buttonSampleGanSynthNote.disabled = false;<br/>});<br/><br/>// Add on click handler to call the GANSynth sampling<br/>buttonSampleGanSynthNote.addEventListener("click", () =&gt; {<br/>  <strong>sampleGanNote()</strong>;<br/>});<br/><br/>// Calls the initialization of MusicVAE and GanSynth<br/>try {<br/>  Promise.all([<strong>startMusicVae()</strong>, <strong>startGanSynth()</strong>]);<br/>} catch (error) {<br/>  console.error(error);<br/>}</span></pre>
<p style="padding-left: 60px">This code will start the models, bind the buttons, and update the button states.</p>
<p class="mce-root"/>
<ol start="8">
<li>Now that we have our web application ready, we can test our code. Let's open the HTML page we've created using a browser. We should see a page similar to the one shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/770e58ed-3a0b-41a1-8d4d-cbd2f754af69.png"/></p>
<p>By pressing the <strong><span class="packt_screen">Sample MusicVAE trio</span></strong> button, the MusicVAE should sample a sequence, plot it, and play it using the synths we've defined. Then, the <strong><span class="packt_screen">Sample GANSynth note for the lead synth</span></strong> button can be used to generate a new sound for the lead synth, which can be used multiple times.</p>
<p>To generate a new sequence, reload the page to start again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Web Workers API to offload computations from the UI thread</h1>
                </header>
            
            <article>
                
<p>As you might have noticed from the previous example when you use the <strong><span class="packt_screen">Sample GANSynth note for the lead synth</span></strong> button, the audio freezes (you won't hear any sound coming from MusicVAE) while GANSynth generates its first sample.</p>
<p class="mce-root"/>
<p>This is because JavaScript's concurrency is built on the event loop pattern, meaning that JavaScript is not multithreaded, and everything is executed in a single thread called the <strong>UI thread.</strong> This works well because JavaScript uses non-blocking I/O, meaning most of its costly operations complete immediately, and return their values using events and callbacks<strong>.</strong> Nonetheless, if a long computation is synchronous, it will block the UI thread while it executes, which is what happens when GANSynth generates its sample (see the previous <em>Introducing TensorFlow.js for machine learning in the browser</em> section, for more information on how Tensorflow handles computations using WebGL).</p>
<p>One solution to this is the <strong>Web Workers API</strong> (<a href="https://html.spec.whatwg.org/multipage/workers.html">html.spec.whatwg.org/multipage/workers.html</a>), specified by the <strong>Web Hypertext Application Technology Working Group</strong> (<strong>WHATWG</strong>), which enables offloading computations to another thread that won't affect the UI thread. A web worker is basically a JavaScript file that gets started from the main thread and executes in its own thread. It can send and receive messages from the main thread. The Web Workers API is mature and well supported across browsers. You can read more about web workers in the <em>Further reading</em> section.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_05.html</kbd> and <kbd>chapter_09_example_05.js</kbd> files in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>Unfortunately, at the time of writing, some parts of Magenta do not work well with web workers. We'll be showing an example using the MusicVAE model, but we can't show the same example using GANSynth, for example, because the model won't load in a web worker. We still provide this example, since it can serve as a base for later use:</p>
<ol>
<li>Let's write the main page code. We'll include only the JavaScript code from the full HTML page since we've covered the other parts in the previous sections. Proceed as follows:</li>
</ol>
<pre style="padding-left: 60px">  <span>// </span><span>Starts a new worker that will load the MusicVAE model<br/></span><span>  </span><strong><span>const </span><span>worker </span>= <span>new </span><span>Worker</span>(<span>"chapter_09_example_05.js"</span>);</strong><br/> <strong> <span>worker</span>.<span>onmessage </span>= <span>function </span>(event) {</strong><br/>    <strong><span>const </span><span>message </span>= event.<span>data</span>[<span>0</span>]</strong>;<br/>    <span>if </span>(<strong><span>message </span>=== <span>"initialized"</span></strong>) {<br/><span>      // When the worker sends the "initialized" message,<br/>      // we enable the button to sample the model<br/></span><span>      </span><span>buttonSampleMusicVaeTrio</span>.<span>disabled </span>= <span>false</span>;<br/>    }<br/>    <span>if </span>(<strong><span>message </span>=== <span>"sample"</span></strong>) {<br/><span>      // When the worked sends the "sample" message,<br/>      // we take the data (the note sequence sample)<br/>      // from the event, create and start a new player<br/>      // using the sequence<br/></span><strong><span>      </span><span>const </span><span>data </span>= event.<span>data</span>[<span>1</span>];</strong><br/><strong>      <span>const </span><span>sample </span>= <span>data</span>[<span>0</span>];</strong><br/>      <span>const </span><span>player </span>= <span>new </span>mm.Player();<br/>      mm.Player.<span>tone</span>.Transport.<span>loop </span>= <span>true</span>;<br/>      mm.Player.<span>tone</span>.Transport.<span>loopStart </span>= <span>0</span>;<br/>      mm.Player.<span>tone</span>.Transport.<span>loopEnd </span>= <span>8</span>;<br/>      <span>player</span>.<span>start</span>(<span>sample</span>, <span>120</span>);<br/>    }<br/>  };<br/><span>  // Add click handler to call the MusicVAE sampling,<br/>  // by posting a message to the web worker which<br/>  // sample and return the sequence using a message<br/>  const buttonSampleMusicVaeTrio = document<br/>      .getElementById("button-sample-musicae-trio");<br/>  </span><span>buttonSampleMusicVaeTrio</span>.<span>addEventListener</span>(<span>"click"</span>, (event) =&gt; {<br/>    <strong><span>worker</span>.<span>postMessage</span>([]);</strong><br/>    event.<span>target</span>.<span>disabled </span>= <span>true</span>;<br/>  });</pre>
<p style="padding-left: 60px">We've already covered most of the code shown in the preceding block in the previous examples. Let's break down the new content, covering the web worker creation and the message passing between the web worker and the main thread, as follows:</p>
<ul>
<li style="padding-left: 60px">First, we need to start the worker, which is done by using <kbd>new Worker("chapter_09_example_05.js")</kbd>. This will execute the content of the JavaScript file and return a handle we can assign to a variable.</li>
<li style="padding-left: 60px">Then, we bind the <kbd>onmessage</kbd> attribute on the worker, which will get called when the worker uses its <kbd>postMessage</kbd> function. In the <kbd>data</kbd> attribute of the <kbd>event</kbd> object, we can pass anything we want (see the worker's code described here next):</li>
<li style="padding-left: 90px">If the worker sends <kbd>initialized</kbd> as the first element of the <kbd>data</kbd> array, it means that the worker is initialized.</li>
<li style="padding-left: 90px">If the worker sends <kbd>sample</kbd> as the first element of the <kbd>data</kbd> array, it means the worker has sampled a MusicVAE sequence and is returning it as the second element of the <kbd>data</kbd> array.</li>
<li style="padding-left: 60px">Finally, when the HTML button is clicked, we call the <kbd>postMessage</kbd> method on the worker instance (without arguments, but it needs—at least—an empty array), which will start the sampling.</li>
</ul>
<p class="mce-root"/>
<p style="padding-left: 60px">Remember that the web workers have no shared state with the main thread, meaning all data sharing needs to happen using the <kbd>onmessage</kbd> and <kbd>postMessage</kbd> methods or functions exclusively.</p>
<ol start="2">
<li>Now, let's write the JavaScript worker's code (which sits at the same location as the HTML file), as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>importScripts</span>(<span>"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.4.0/dist/tf.min.js"</span>);<br/><span>importScripts</span>(<span>"https://cdn.jsdelivr.net/npm/@magenta/music@^1.12.0/es6/core.js"</span>);<br/><span>importScripts</span>(<span>"https://cdn.jsdelivr.net/npm/@magenta/music@^1.12.0/es6/music_vae.js"</span>);<br/><br/><span>async function </span><span>initialize</span>() {<br/>  <span>musicvae </span>= <span>new </span>music_vae.<span>MusicVAE</span>(<span>"https://storage.googleapis.com/" </span>+<br/>      <span>"magentadata/js/checkpoints/music_vae/trio_4bar"</span>);<br/>  <span>await </span><span>musicvae</span>.<span>initialize</span>();<br/>  <strong><span>postMessage</span>([<span>"initialized"</span>])</strong>;<br/>}<br/><br/><strong><span>onmessage </span>= <span>function </span>(event)</strong> {<br/>  <span>Promise</span>.<span>all</span>([<span>musicvae</span>.<span>sample</span>(<span>1</span>)])<br/>      .<span>then</span>(samples =&gt; <strong><span>postMessage</span>([<span>"sample"</span>, samples[<span>0</span>]])</strong>);<br/>};<br/><br/><span>try </span>{<br/>  <span>Promise</span>.<span>all</span>([<span>initialize</span>()]);<br/>} <span>catch </span>(error) {<br/>  <span>console</span>.<span>error</span>(error);<br/>}</pre>
<p>The first thing you notice here is that we are using Magenta's ES6 bundle since we cannot import everything in a web worker. By importing Tone.js, for example, we would get an error such as <strong>This browser does not support Tone.js</strong>. Also, remember that Magenta.js is not fully compatible yet with web workers, meaning importing GANSynth might result in an error.</p>
<p class="mce-root"/>
<p>Since we've already covered most of the code shown in the preceding block, we'll just talk about the web worker additions, as follows:</p>
<ul>
<li>First, we need to send an <kbd>initialized</kbd> message to the main thread using <kbd>postMessage</kbd> when the model is ready to roll.</li>
<li>Then, we bind on the module <kbd>onmessage</kbd> attribute, which will get called when the main thread sends the worker a message. Upon reception, we sample the MusicVAE model and then use <kbd>postMessage</kbd> to send the result back to the main thread.</li>
</ul>
<p>This covers the basic usage of creating a web worker and making it exchange data with the main thread.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using other Magenta.js models</h1>
                </header>
            
            <article>
                
<p>As always, we cannot cover all models here, but the usage of other models will be similar to the examples we've provided. There are a lot of Magenta.js examples and demos on the internet, and some are very impressive music-generation web applications.</p>
<p>We provide resources to find examples and demos in the <em>Further reading</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making Magenta.js interact with other apps</h1>
                </header>
            
            <article>
                
<p>Because Magenta.js sits in the browser, it is a bit harder to make it interact with other applications such as a DAW<strong> </strong>than a Magenta application, but as web standards evolve, this will become easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Web MIDI API</h1>
                </header>
            
            <article>
                
<p>The Web MIDI API (<a href="https://www.w3.org/TR/webmidi/">www.w3.org/TR/webmidi/</a>) is a W3C standard with a specification that isn't very mature, with the status of <em>W3C Working Draft March 17, 2015</em>. It isn't well supported across browsers, with Firefox and Edge having no support at all. It works pretty well in Chrome, though, so if you require your users to use that browser, your application might work. See the last section, <em>Further reading</em>, for more information.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_06.html</kbd> file, in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<p>We'll write a small example using the Web MIDI API, based on the previous example on MusicVAE and the trio sampling. You can copy the previous example and add the new content:</p>
<ol>
<li>First, let's add a <kbd>select</kbd> element to our page, like this:</li>
</ol>
<pre style="padding-left: 60px"><span>&lt;</span><span>label </span><span>for</span><span>="select-midi-output"</span><span>&gt;</span>Select MIDI output:<span>&lt;/</span><span>label</span><span>&gt;</span><br/><span>&lt;</span><span>select </span><span>disabled id</span><span>="select-midi-output"</span><span>&gt;</span><br/><span>&lt;/</span><span>select</span><span>&gt;</span></pre>
<ol start="2">
<li>Then, in the <kbd>startMusicVae</kbd> method, let's initialize the list of available MIDI outputs, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Starts a MIDI player, and for each available MIDI outputs,<br/></span><span>// adds an option to the select drop down.<br/></span><span>const </span><span>player </span>= <strong><span>new </span>mm.MIDIPlayer()</strong>;<br/><strong><span>player</span>.requestMIDIAccess()</strong><br/>    .<span>then</span>((<strong>outputs</strong>) =&gt; {<br/>        if (outputs &amp;&amp; outputs.length) {<br/>            const option = document.createElement("option");<br/>            selectMidiOutput.appendChild(option);<br/>            outputs.forEach(output =&gt; {<br/>                const option = document.createElement("option");<br/>                option.innerHTML = <strong>output.name</strong>;<br/>                selectMidiOutput.appendChild(option);<br/>            });<br/>            selectMidiOutput.disabled = false;<br/>        } else {<br/>            selectMidiOutput.disabled = true;<br/>        }<br/>    });<br/><span>window</span>.<span>player </span>= <span>player</span>;</pre>
<p style="padding-left: 60px">Here, we use the Magenta.js <kbd>MIDIPlayer</kbd> class, which makes usage of the <kbd>requestMIDIAccess</kbd> method easier than directly calling the Web MIDI API. Calling this method will return a list of <kbd>output</kbd> that we add, using the <kbd>name</kbd> attribute in the selection list.</p>
<ol start="3">
<li>Finally, in the <kbd>sampleMusicVaeTrio</kbd> method, we use the player to send the MIDI directly to that output, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>// Gets the selected MIDI output (if any) and uses the<br/></span><span>// output in the MIDI player<br/></span><span>const </span><span>midiOutputIndex </span>= <span>selectMidiOutput</span>.<span>selectedIndex</span>;<br/><span>if </span>(<span>midiOutputIndex</span>) {<br/>    <strong><span>player</span>.<span>outputs </span>= [<span>player</span>.availableOutputs[<span>midiOutputIndex </span>- <span>1</span>]];</strong><br/>    mm.Player.<span>tone</span>.Transport.<span>loop </span>= <span>true</span>;<br/>    mm.Player.<span>tone</span>.Transport.<span>loopStart </span>= <span>0</span>;<br/>    mm.Player.<span>tone</span>.Transport.<span>loopEnd </span>= <span>8</span>;<br/>    <span>player</span>.<span>start</span>(<span>sample</span>, <span>120</span>);<br/>}<br/><span>selectMidiOutput</span>.<span>disabled </span>= <span>true</span>;</pre>
<p style="padding-left: 60px">Here, we only need to set the <kbd>outputs</kbd> list with the element that was selected in the dropdown (if any).</p>
<ol start="4">
<li>To test our code, we can use our trusty FluidSynth, using the following:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 PATH_TO_SF2</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 PATH_TO_SF2</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 PATH_TO_SF2</kbd></li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">FluidSynth should start and show a terminal (notice we removed the <kbd>-n</kbd> and <kbd>-i</kbd> flags so that we can receive MIDI notes).</p>
<ol start="5">
<li>Now, let's open our web application. Once the model is initialized, we should see the FluidSynth MIDI input port in the <strong>Select MIDI output</strong>—select drop-down list. It should look like this: <strong>Synth input port (17921:0)</strong>. Choose the option, and then click on <strong>Sample MusicVAE trio</strong>. You should hear the sound coming from FluidSynth.</li>
</ol>
<p>You'll notice that all the notes are played as a piano sequence, even if we have three instruments. This is because the <kbd>MIDIPlayer</kbd> is pretty basic and won't send the percussion on the drums channel, as specified in the MIDI specification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Magenta.js server side with Node.js</h1>
                </header>
            
            <article>
                
<p>Magenta.js can also be used server side, using Node.js. Using Node.js is nice because you can have the same (or almost the same) code running server side and client side. Communication between client and server can be handled using WebSockets.</p>
<p>The WebSocket API (<a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">developer.mozilla.org/en-US/docs/Web/API/WebSockets_API</a>) is an API that makes it possible to open <strong>two-way communications</strong> between a client and a server. We won't be looking at WebSockets here, but they can be a good way of transferring the data back and forth between a server-side Magenta process (Magenta.js server side using Node.js, or Magenta in Python) and a client application. The easiest way of using WebSockets is to use a framework such as Socket.IO (<a href="https://socket.io/">socket.io/</a>).</p>
<p class="mce-root"/>
<p>Another advantage of using Node.js is that our program is running server side, which means it isn't dependent on the browser's implementation. A good example of this is that we could use a Node.js package to handle sending MIDI to other processes, such as <kbd>node-midi</kbd> (<a href="https://www.npmjs.com/package/midi">www.npmjs.com/package/midi</a>), which alleviates the necessity of using the Web MIDI API.</p>
<p>Let's show a simple example of Magenta.js running with Node.js. <span>The code shown here is similar to what we've already covered in JavaScript:</span></p>
<div class="packt_tip">You can find this code in the <kbd>chapter_08_example_07.js</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code—you should go and check it out.</div>
<ol>
<li>First, let's install Node.js (<a href="https://nodejs.org/en/download/">nodejs.org/en/download/</a>)</li>
<li>Then, let's install Magenta.js, using the <kbd>npm</kbd> command, which is the Node.js dependency manager, like this:</li>
</ol>
<pre style="padding-left: 60px"><strong>npm install --save @magenta/music</strong></pre>
<p style="padding-left: 60px">This will install Magenta.js and its dependencies in the <kbd>node_modules</kbd> directory. When Node.js runs, it looks in this directory to find the script's dependencies, for each <kbd>require</kbd> call.</p>
<ol start="3">
<li>We can now create a JavaScript file to sample a sequence, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>const </span><span>music_vae </span>= <strong>require</strong>(<span>"@magenta/music/node/music_vae"</span>);<br/><br/><span>// These hacks below are needed because the library uses performance<br/>// and fetch which </span><span>exist in browsers but not in node.<br/></span><span>const </span><span>globalAny </span>= <span>global</span>;<br/><span>globalAny</span>.<span>performance </span>= <span>Date</span>;<br/><span>globalAny</span>.<span>fetch </span>= require(<span>"node-fetch"</span>);<br/><br/><span>const </span><span>model </span>= <span>new </span><span>music_vae</span>.MusicVAE(<br/>    <span>"https://storage.googleapis.com/magentadata/js/checkpoints/" </span>+<br/>    <span>"music_vae/drums_2bar_lokl_small"</span>);<br/><span>model<br/></span><span>    </span>.<span>initialize</span>()<br/>    .<span>then</span>(() =&gt; <span>model</span>.<span>sample</span>(<span>1</span>))<br/>    .<span>then</span>(samples =&gt; {<br/>        <span>console</span>.<span>log</span>(samples[<span>0</span>])<br/>   });</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">This code is similar to the previous examples, the only addition being the <kbd>require</kbd> method, which is used in Node.js to import a dependency module.</p>
<ol start="4">
<li>To execute your Node.js application, use the <kbd>node</kbd> command (replacing <kbd>PATH_TO_JAVASCRIPT_FILE</kbd> by a proper value), as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>node PATH_TO_JAVASCRIPT_FILE</strong></pre>
<p>The sample should show on the console because we've used <kbd>console.log</kbd>. You will also notice a couple of messages on the console, as follows:</p>
<pre><strong>This browser does not support Tone.js</strong><br/><strong>Hi there. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.</strong></pre>
<p>This reminds us that Tone.js cannot be run on Node.js, because the Web Audio API is implemented client side. It also reminds us that Node.js can use CUDA libraries for <span><span>better </span></span>performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've looked at Tensorflow.js and Magenta.js, the JavaScript implementations of TensorFlow and Magenta. We've learned that TensorFlow.js is GPU accelerated using WebGL and that Magenta.js has a limited set of models available that can only be used for generation, not training. We've converted a Python-trained model from the previous chapter to a format that TensorFlow.js can load. We've also introduced Tone.js and the Web Audio API, which is used by Magenta.js to synthesize sound in the browser.</p>
<p>Then, we've created three music generation web applications. The first application used GANSynth to sample short audio notes. By doing so, we've learned how to import the required scripts, either using a big ES5 bundle or a smaller, split up, ES6 bundle. The second application used MusicVAE to sample a trio of instruments, with the drum kit, the bass, and the lead, and played the sequence in a loop. The third application used both models to generate sequences and audio together and introduced the usage of the Web Workers API to offload computations to another thread.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, we've talked about how to make Magenta.js interact with other applications. We've used the Web MIDI API to send the generated sequences to another synthesizer—for example, FluidSynth. We've also used Node.js to run a Magenta.js application server side.</p>
<p>Magenta.js is a great project because it makes it easy to create and share music-generation applications using web technologies. There are other ways of making Magenta fit in a broader context, such as using Magenta Studio (which makes Magenta run in Ableton Live) and using MIDI, which is a good way of controlling all types of devices, such as software and hardware synthesizers. We'll be showing those subjects in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Can a model be trained using Tensorflow.js? Using Magenta.js?</li>
<li>What does the Web Audio API do, and what is the easiest way of using it?</li>
<li>What is the generation method in GANSynth? What is the argument that needs to be provided?</li>
<li>What is the generation method in MusicVAE? How many instruments does it generate?</li>
<li>Why is the Web Workers API useful in JavaScript?</li>
<li>Name two ways of sending MIDI from a Magenta.js application to another application.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>MagentaMusic.js demos</strong>: A Magenta-maintained list of demos using the various models and core classes in Magenta.js (<a href="https://tensorflow.github.io/magenta-js/music/demos/">tensorflow.github.io/magenta-js/music/demos/</a>).</li>
<li><strong>Web apps built with Magenta.js</strong>: A community-driven list of demos using Magenta.js, with lots of cool stuff (<a href="https://magenta.tensorflow.org/demos/web/">magenta.tensorflow.org/demos/web/</a>).</li>
<li><strong>Monica Dinculescu—Why you should build silly things</strong>: Interesting talk on the importance of Magenta.js and sharing music-creation applications (<a href="https://www.youtube.com/watch?v=DkiFjzQgJtg">www.youtube.com/watch?v=DkiFjzQgJtg</a>).</li>
<li><strong>Celebrating Johann Sebastian Bach</strong>: A good example of a popular music-generation application (<a href="https://www.google.com/doodles/celebrating-johann-sebastian-bach">www.google.com/doodles/celebrating-johann-sebastian-bach</a>).</li>
<li><strong>WebGL Specifications</strong>: The WebGL specification (<a href="https://www.khronos.org/registry/webgl/specs/latest/">www.khronos.org/registry/webgl/specs/latest/</a>).</li>
<li><strong>Platform and environment</strong>: An interesting read on memory management and GPU computations using WebGL in TensorFlow.js (<a href="https://www.tensorflow.org/js/guide/platform_environment">www.tensorflow.org/js/guide/platform_environment</a>).</li>
<li><strong>Web Audio API</strong>: The Web Audio API specification from the W3C (<a href="https://webaudio.github.io/web-audio-api/">webaudio.github.io/web-audio-api/</a>).</li>
<li><strong>Web Audio API</strong>: An introduction to the Web Audio API (<a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API</a>).</li>
<li><strong>Web Workers</strong>: The Web Workers API specification from the WHATWG (<a href="https://html.spec.whatwg.org/multipage/workers.html">html.spec.whatwg.org/multipage/workers.html</a>).</li>
<li><strong>Concurrency model and the event loop</strong>: An introduction to the event loop pattern in JavaScript (<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop">developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop</a>).</li>
<li><strong>Using Web Workers</strong>: An introduction to the Web Workers API (<a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers</a>).</li>
<li><strong>Web MIDI API</strong>: The Web MIDI API specification for the W3C (<a href="https://webaudio.github.io/web-midi-api/">webaudio.github.io/web-midi-api/</a>).</li>
<li><strong>Web MIDI (MIDI Support in Web Browsers)</strong>: An introduction to the Web MIDI API from the MIDI Association, with examples <span>of </span>applications using it (<a href="https://www.midi.org/17-the-mma/99-web-midi">www.midi.org/17-the-mma/99-web-midi</a>).</li>
</ul>


            </article>

            
        </section>
    </body></html>