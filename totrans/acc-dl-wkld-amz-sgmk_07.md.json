["```py\n    from torch.utils.tensorboard import SummaryWriter\n    tb_writer = SummaryWriter(args.tb_s3_url)\n    ```", "```py\n    sample_inputs, _ = next(iter(dataloaders_dict[\"val\"]))\n    tb_writer.add_graph(model, sample_inputs, verbose=False, use_strict_trace=False)\n    ```", "```py\ntb_writer.add_histogram(\"conv1.weight\", model.conv1.weight, epoch)\ntb_writer.add_histogram(\"conv1.weight_grad\", model.conv1.weight.grad, epoch)\ntb_writer.add_histogram(\"fc.weight\", model.fc.weight, epoch)\ntb_writer.add_histogram(\"fc.weight_grad\", model.fc.weight.grad, epoch)\ntb_writer.add_scalar(f\"Loss/{phase}\", epoch_loss, epoch)\ntb_writer.add_scalar(f\"Accuracy/{phase}\", epoch_accuracy, epoch)\ntb_writer.add_hparams(hparam_dict=vars(args), metric_dict={\n                    f\"hparam/loss_{phase}\": epoch_loss,\n                    f\"hparam/accuracy_{phase}\": epoch_accuracy})\n```", "```py\ninstance_type = 'ml.p2.xlarge'\ninstance_count = 1\njob_name = \"pytorch-tb-profiling-12\"\ntb_debug_path = f\"s3://{bucket}/tensorboard/{job_name}\"\nestimator = PyTorch(\n          entry_point=\"train_resnet_tb.py\",\n          source_dir='1_sources',\n          role=role,\n          instance_type=instance_type,\n          sagemaker_session=sagemaker_session,\n          image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n          instance_count=instance_count,\n          hyperparameters={\n              \"batch-size\":64,\n              \"num-epochs\":10,\n              \"input-size\" : 224,\n              \"feature-extract\":False,\n              \"tb-s3-url\": tb_debug_path,\n              \"num-data-workers\": 4\n          },\n          disable_profiler=True,\n          debugger_hook_config=False,\n          base_job_name=job_name,\n      )\n```", "```py\ntensorboard --logdir ${tb_debug_path} \n```", "```py\nwith torch.profiler.profile(\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=5),\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n        os.path.join(os.environ[\"SM_OUTPUT_DATA_DIR\"], \"tb_profiler\")\n    ),\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True,\n) as prof:\n    for _, (inputs, labels) in enumerate(dataloaders[phase]):\n      # The rest of training loop without changes\n```", "```py\n    tb_profiler_path = f\"{estimator.latest_training_job.describe()['OutputDataConfig']['S3OutputPath']}{estimator.latest_training_job.describe()['TrainingJobName']}/output/output.tar.gz\"\n    ```", "```py\n    aws s3 cp ${ tb_profiler_path} .\n    mkdir profiler_output\n    tar -xf output.tar.gz -C profiler_output\n    tensorboard --logdir ./profiler_output\n    ```", "```py\n...\nmodel = initialize_resnet_model(\n    NUM_CLASSES, feature_extract=False, use_pretrained=True\n)\nmodel.to(torch.device(\"cuda\"))\noptimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nhook = smd.Hook.create_from_json_file()\nhook.register_hook(model)\nhook.register_loss(criterion)\n...\n```", "```py\n  for epoch in range(1, args.num_epochs + 1):\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()  # Set model to training mode\n                if hook:\n                    hook.set_mode(modes.TRAIN)\n            else:\n                model.eval()  # Set model to evaluate mode\n                if hook:\n                    hook.set_mode(modes.EVAL)\n            running_corrects = 0\n            running_loss = 0.0\n            step_counter = 0\n            epoch_start = time.time()\n            for _, (inputs, labels) in enumerate(\n            dataloaders[phase]):\n            # inside training loop\n...\n```", "```py\n    from sagemaker.debugger import (\n        Rule,\n        DebuggerHookConfig,\n        TensorBoardOutputConfig,\n        CollectionConfig,\n        rule_configs,\n        ProfilerRule\n    )\n    ```", "```py\n    actions = rule_configs.ActionList(\n        rule_configs.StopTraining())\n    rules = [\n        Rule.sagemaker(rule_configs.vanishing_gradient(), actions=actions),\n        Rule.sagemaker(rule_configs.overfit(), actions=actions),\n        Rule.sagemaker(rule_configs.overtraining(), actions=actions),\n        Rule.sagemaker(rule_configs.poor_weight_initialization(), actions=actions),\n    ]\n    ```", "```py\n    collection_configs=[\n            CollectionConfig(\n                name=\"weights\",\n                parameters={\n                    \"save_histogram\": \"True\"\n                    }\n                ),\n            CollectionConfig(name=\"losses\"),\n        ]\n    hook_config = DebuggerHookConfig(\n        hook_parameters={\"train.save_interval\": \"1\", \"eval.save_interval\": \"1\"},\n        collection_configs=collection_configs\n    )\n    ```", "```py\n    tb_debug_path = f\"s3://{bucket}/tensorboard/{job_name}\"\n    tensorboard_output_config = TensorBoardOutputConfig(\n        s3_output_path=tb_debug_path\n    )\n    debug_estimator = PyTorch(\n              entry_point=\"train_resnet_sm.py\",\n              source_dir='2_sources',\n              role=role,\n              instance_type=instance_type,\n              sagemaker_session=sagemaker_session,\n              image_uri=image_uri,\n              instance_count=instance_count,\n              disable_profiler=True,\n              rules=rules,\n              debugger_hook_config=hook_config,\n              tensorboard_output_config=tensorboard_output_config,\n              base_job_name=job_name,\n          )\n    ```", "```py\n    import smdebug.pytorch as smd\n    tensors_path = debug_estimator.latest_job_debugger_artifacts_path()\n    trial = smd.create_trial(tensors_path)\n    ```", "```py\n    print(f\"Persisted tensors: {trial.tensor_names()}\")\n    ```", "```py\n    print(f\"Loss values {trial.tensor('CrossEntropyLoss_output_0').values()}\")\n    ```", "```py\n    plot_tensor(trial, \"CrossEntropyLoss_output_0\")\n    ```", "```py\n    for s in debug_estimator.latest_training_job.rule_job_summary():\n        print(f\"Rule: {s['RuleConfigurationName']}\",\n              f\"status: {s['RuleEvaluationStatus']}\")\n    ```", "```py\nRule: VanishingGradient, status: NoIssuesFound\nRule: Overfit, status: NoIssuesFound \nRule: Overtraining, status: NoIssuesFound \nRule: PoorWeightInitialization, status: NoIssuesFound\n```", "```py\n    ! tensorboard --logdir  {tb_debug_path}\n    ```", "```py\nfrom sagemaker.debugger import (ProfilerConfig, \n                                FrameworkProfile, \n                                DetailedProfilingConfig, \n                                DataloaderProfilingConfig, \n                                PythonProfilingConfig,\n                                PythonProfiler, cProfileTimer)\nprofiler_config=ProfilerConfig(\n    system_monitor_interval_millis=500,\n    framework_profile_params=FrameworkProfile(\n        detailed_profiling_config=DetailedProfilingConfig(\n            start_step=2, \n            num_steps=1),\n        dataloader_profiling_config=DataloaderProfilingConfig(\n            start_step=2, \n            num_steps=1),\n        python_profiling_config=PythonProfilingConfig(\n            start_step=2, \n            num_steps=1, \n            python_profiler=PythonProfiler.CPROFILE, \n            cprofile_timer=cProfileTimer.TOTAL_TIME)))\n```", "```py\nprofiler_estimator = PyTorch(\n          entry_point=\"train_resnet_sm.py\",\n          source_dir='2_sources',\n          role=role,\n          instance_type='ml.p2.xlarge',\n          sagemaker_session=sagemaker_session,\n          image_uri=image_uri,\n          instance_count=instance_count,\n          hyperparameters={\n              \"num-data-workers\":8,\n          },\n          disable_profiler=False,\n          profiler_config=profiler_config,\n          rules=rules,\n        #  debugger_hook_config=hook_config,\n        #  tensorboard_output_config=tensorboard_output_config,\n          base_job_name=job_name,\n      )\n```", "```py\ntraining_job_name = profiler_estimator.latest_training_job.job_name\nregion = \"us-east-1\"\ntj = TrainingJob(training_job_name, region)\ntj.wait_for_sys_profiling_data_to_be_available()\n```", "```py\nfrom smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\nsystem_metrics_reader = tj.get_systems_metrics_reader()\nsystem_metrics_reader.refresh_event_file_list()\nview_timeline_charts = TimelineCharts(\n    system_metrics_reader,\n    framework_metrics_reader=None,\n    select_dimensions=[\"CPU\", \"GPU\"],\n    select_events=[\"total\"],\n)\n```", "```py\naws s3 cp s3://<JOB_BUCKET>/<JOB_NAME>/rule-output ./ --recursive\n```", "```py\nhyperparameter_ranges = {\n\"model_type\" : sagemaker.tuner.CategoricalParameter([\"resnet\", \"vgg16\", \"densenet\"]}]),\n\"learning_rate\" : sagemaker.tuner.ContinuousParameter(0.0001,0.1, scaling_type=\"Logarithmic\"),\n\"lr_scheduler_step_size\" : sagemaker.tuner.IntegerParameter(10,100, scaling_type=\"Linear\"),\n}\n```", "```py\n    metric_definitions = [\n        {\"Name\": \"train_loss\",\n         \"Regex\": \"Train Loss = (.*?);\"},\n        {\"Name\": \"val_loss\",\n         \"Regex\": \"Val Loss=(.*?);\"},\n        {\"Name\": \"train_accuracy\",\n         \"Regex\": \"Train Accuracy = (.*?);\"},\n        {\"Name\": \"val_accuracy\",\n         \"Regex\": \"Val Accuracy = (.*?);\"},]\n    objective_metric_name = \"val_accuracy\"\n    ```", "```py\n    estimator = PyTorch(\n              entry_point=\"train_script.py\",\n              role=role,\n              instance_type=instance_type,\n              sagemaker_session=sagemaker_session,\n              image_uri=image_uri,\n              instance_count=instance_count,\n              hyperparameters={\n                  \"batch-size\":64,\n                  \"num-epochs\":5,\n              })\n    ```", "```py\n    tuner = sagemaker.tuner.HyperparameterTuner(\n         estimator,\n         objective_metric_name,\n         hyperparameter_ranges,\n         metric_definitions,\n         objective_type=\"Maximize\",\n         max_jobs=200,\n         max_parallel_jobs=10)\n    ```", "```py\n    tuner.fit({\"train\":train_data_location, \"val\":val_data_location})\n    ```", "```py\n    tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n    tuner_results = tuner.dataframe()\n    ```"]