<html><head></head><body>
		<div id="_idContainer118">
			<h1 id="_idParaDest-204" class="chapter-number"><a id="_idTextAnchor226"/>13</h1>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor227"/>How Well Does It Work? – Evaluation</h1>
			<p>In this chapter, we will address the question of quantifying how well a <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) system works. Throughout this book, we assumed that we want the NLU systems that we develop to do a good job on the tasks that they are designed for. However, we haven’t dealt in detail with the tools that enable us to tell how well a system works – that is, how to evaluate it. This chapter will illustrate a number of evaluation techniques that will enable you to tell how well the system works, as well as to compare systems in terms of performance. We will also look at some ways to avoid drawing erroneous conclusions from <span class="No-Break">evaluation metrics.</span></p>
			<p>The topics we will cover in this chapter are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Why evaluate an <span class="No-Break">NLU system?</span></li>
				<li><span class="No-Break">Evaluation paradigms</span></li>
				<li><span class="No-Break">Data partitioning</span></li>
				<li><span class="No-Break">Evaluation metrics</span></li>
				<li><span class="No-Break">User testing</span></li>
				<li>Statistical significance <span class="No-Break">of differences</span></li>
				<li>Comparing three text <span class="No-Break">classification methods</span></li>
			</ul>
			<p>We will start by asking the question of why it’s important to evaluate <span class="No-Break">NLU systems.</span></p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor228"/>Why evaluate an NLU system?</h1>
			<p>There are many questions <a id="_idIndexMarker977"/>that we can ask about the overall quality of an NLU system, and evaluating it is the way that we answer these questions. How we evaluate depends on the goal of developing the system and what we want to learn about the system to make sure that the goal <span class="No-Break">is achieved.</span></p>
			<p>Different kinds of developers will have different goals. For example, consider the goals of the following types <span class="No-Break">of developers:</span></p>
			<ul>
				<li>I am a researcher, and I want to learn whether my ideas advance the science of NLU. Another way to put this is to ask how my work compares to the <strong class="bold">state of the art</strong> (<strong class="bold">SOTA</strong>) – that is, the best results that anyone has reported on a <span class="No-Break">particular task.</span></li>
				<li>I am a developer, and I want to make sure that my overall system performance is good enough for <span class="No-Break">an application.</span></li>
				<li>I am a developer, and I want to see how much my changes improve <span class="No-Break">a system.</span></li>
				<li>I am a developer, and I want to make sure my changes have not decreased a <span class="No-Break">system’s performance.</span></li>
				<li>I am a researcher or developer who wants to know how my system performs on different classes <span class="No-Break">of data.</span></li>
			</ul>
			<p>The most important question for all of these developers and researchers is, <em class="italic">how well does the system perform its </em><span class="No-Break"><em class="italic">intended function?</em></span></p>
			<p>This is the question we will focus on in this chapter, and we will address how each of these different kinds <a id="_idIndexMarker978"/>of developers discovers the information they need. However, there are other important NLU system properties that can be evaluated, and sometimes, these will be more important than overall system performance. It is worth mentioning them briefly so that you are aware of them. For example, other aspects of evaluation questions include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">The size of the machine learning model that supports the application</strong>: Today’s models can be very large, and there is significant research effort directed at making models smaller without significantly degrading their performance. If it is important to have a small model, you will want to look at the trade-offs between model size <span class="No-Break">and accuracy.</span></li>
				<li><strong class="bold">Training time</strong>: Some algorithms require training time on the order of several weeks on highly capable GPU processors, especially if they are training on large datasets. Reducing this time makes it much easier to experiment with alternative algorithms and tuning hyperparameters. In theory, larger models will provide better results, at the cost of more training time, but in practice, we need to ask how much difference do they make in the performance of any <span class="No-Break">particular task?</span></li>
				<li><strong class="bold">Amount of training data</strong>: Today’s <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) require enormous amounts of training data. In fact, in the current SOTA, this amount of data is prohibitively <a id="_idIndexMarker979"/>large for all but the largest organizations. However, as we saw in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, LLMs can be fine-tuned with application-specific data. The other consideration for training data is whether there is enough data available to train a system that <span class="No-Break">works well.</span></li>
				<li><strong class="bold">The expertise of developers</strong>: Depending on highly expert developers is expensive, so a development process that can be performed by less expert developers is usually desirable. The rule-based systems discussed in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>,<em class="italic"> </em>often require highly expert developers, which is one reason that they tend to be avoided if possible. On the other hand, experimenting with SOTA deep learning models can call for the knowledge of expert data scientists, who can also be expensive and hard <span class="No-Break">to find.</span></li>
				<li><strong class="bold">Cost of training</strong>: The training costs for very large models are in the range of millions of dollars, even if we are only taking into account the cost of computing resources. A lower training cost is clearly a desirable property of an <span class="No-Break">NLU system.</span></li>
				<li><strong class="bold">Environmental impact</strong>: Closely related to the cost of training is its environmental impact in terms of energy expenditure, which can be very high. Reducing this is <span class="No-Break">obviously desirable.</span></li>
				<li><strong class="bold">Processing time for inference</strong>: This question relates to how long it takes a trained system to process an input and deliver results. With today’s systems, this is not usually a problem, with the short inputs that are used with interactive systems such as chatbots or spoken dialog systems. Almost any modern approach will enable them to be processed quickly enough that users will not be annoyed. However, with offline applications such as analytics, where an application may need to extract information from many hours of audio or gigabytes of text, slow processing times will <span class="No-Break">add up.</span></li>
				<li><strong class="bold">Budget</strong>: Paid cloud-based LLMs such as GPT-4 usually provide very good results, but a local open source model such as BERT could be much cheaper and give results that are good enough for a <span class="No-Break">specific application.</span></li>
			</ul>
			<p>Even though these properties can be important to know when we’re deciding on which NLU approach to use, how well an NLU system works is probably the most important. As developers, we need the answers to such fundamental questions as <span class="No-Break">the following:</span></p>
			<ul>
				<li><em class="italic">Does this system perform its intended functions well enough to </em><span class="No-Break"><em class="italic">be useful?</em></span></li>
				<li><em class="italic">As changes are made in the system, is it </em><span class="No-Break"><em class="italic">getting better?</em></span></li>
				<li><em class="italic">How does this system’s performance compare to the performance of </em><span class="No-Break"><em class="italic">other systems?</em></span></li>
			</ul>
			<p>The answers to these questions require evaluation methods that assign numerical values to a system’s performance. Subjective or non-quantitative evaluation, where a few people look at a <a id="_idIndexMarker980"/>system’s performance and decide whether it <em class="italic">looks good</em> or not, is not precise enough to provide reliable answers to <span class="No-Break">those questions.</span></p>
			<p>We will start our discussion of evaluation by reviewing some overall approaches to evaluation, or <span class="No-Break"><em class="italic">evaluation paradigms</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor229"/>Evaluation paradigms</h1>
			<p>In this section, we will review some of the major evaluation paradigms that are used to quantify system performance and <span class="No-Break">compare systems.</span></p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor230"/>Comparing system results on standard metrics</h2>
			<p>This is the most <a id="_idIndexMarker981"/>common evaluation paradigm and probably the easiest to carry out. The system is simply given data to process, and its performance is evaluated quantitatively based on standard metrics. The upcoming <em class="italic">Evaluation metrics</em> section will delve into this topic in <a id="_idIndexMarker982"/>much <span class="No-Break">greater detail.</span></p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor231"/>Evaluating language output</h2>
			<p>Some NLU applications produce natural language output. These include applications such as translation <a id="_idIndexMarker983"/>or summarizing text. They differ from applications with a specific right or wrong answer, such as classification and slot filling, because there is no single correct answer – there could be many <span class="No-Break">good answers.</span></p>
			<p>One way to evaluate machine translation quality is for humans to look at the original text and the translation and judge how accurate it is, but this is usually too expensive to be used extensively. For that reason, metrics have been developed that can be applied automatically, although they are not as satisfactory as human evaluation. We will not cover these in detail here, but we will briefly list them so that you can investigate them if you need to evaluate <span class="No-Break">language output.</span></p>
			<p>The <strong class="bold">bilingual evaluation understudy</strong> (<strong class="bold">BLEU</strong>) metric is one well-established metric to evaluate translations. This metric is based on comparing machine translation results to human <a id="_idIndexMarker984"/>translations and measuring the difference. Because it is possible that a very good machine translation will be quite <a id="_idIndexMarker985"/>different from any particular human translation, the BLEU scores will not necessarily correspond to human <a id="_idIndexMarker986"/>judgments of translation <a id="_idIndexMarker987"/>quality. Other evaluation metrics for applications that produce language output include the <strong class="bold">metric for evaluation for translation with explicit ordering</strong> (<strong class="bold">METEOR</strong>), the <strong class="bold">recall-oriented understudy for gisting evaluation</strong> (<strong class="bold">ROUGE</strong>), and the <strong class="bold">cross-lingual optimized metric for evaluation of </strong><span class="No-Break"><strong class="bold">translation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">COMET</strong></span><span class="No-Break">).</span></p>
			<p>In the next section, we will discuss an approach to evaluation that involves removing part of a system to determine what effect it has on the results. Does it make the results better or worse, or does it not result in <span class="No-Break">any changes?</span></p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor232"/>Leaving out part of a system – ablation</h2>
			<p>If an experiment <a id="_idIndexMarker988"/>includes a pipeline of several operations, it’s often informative to compare results by removing steps in the pipeline. This is called <strong class="bold">ablation</strong>, and it is <a id="_idIndexMarker989"/>useful in two <span class="No-Break">different situations.</span></p>
			<p>The first case is when an experiment is being done for a research paper or an academic project that includes some innovative techniques in the pipeline. In that case, you want to be able to quantify what effect every step in the pipeline had on the final outcome. This will allow readers of the paper to evaluate the importance of each step, especially if the paper is attempting to show that one or more of the steps is a major innovation. If the system still performs well when the innovations are removed, then they are unlikely to be making a significant contribution to the system’s overall performance. Ablation studies will enable you to find out exactly what contributions are made by <span class="No-Break">each step.</span></p>
			<p>The second case for ablation is more practical and occurs when you’re working on a system that needs to be computationally efficient for deployment. By comparing versions of a system with and without specific steps in the pipeline, you can decide whether the amount of time that they take justifies the degree of improvement they make in the <span class="No-Break">system’s performance.</span></p>
			<p>An example of <a id="_idIndexMarker990"/>why you would want to consider doing an ablation study for this second reason could include finding out whether preprocessing steps such as stopword removal, lemmatization, or stemming make a difference in the <span class="No-Break">system’s performance.</span></p>
			<p>Another approach to evaluation involves a test where several independent systems process the same data and the results are compared. These are <span class="No-Break">shared tasks.</span></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor233"/>Shared tasks</h2>
			<p>The field of NLU has long benefited from comparing systems on <strong class="bold">shared tasks</strong>, where systems developed by <a id="_idIndexMarker991"/>different developers are all tested on a single set of shared data on a specific topic and the results are compared. In addition, the <a id="_idIndexMarker992"/>teams working on the shared task usually publish system descriptions that provide very useful insights into how their systems achieved <span class="No-Break">their results.</span></p>
			<p>The shared task paradigm has <span class="No-Break">two benefits:</span></p>
			<ul>
				<li>First, it enables developers who participate in the shared task to get precise information about how <a id="_idIndexMarker993"/>their system compares to others because the data is exactly <span class="No-Break">the same</span></li>
				<li>Second, the data used in the shared tasks is made available to the research community for use in developing <span class="No-Break">future systems</span></li>
			</ul>
			<p>Shared data can be useful for a long time because it is not affected by changes in NLU technology – for <a id="_idIndexMarker994"/>example, the <strong class="bold">air travel information system</strong> (<strong class="bold">ATIS</strong>) data from <a id="_idIndexMarker995"/>a travel planning task has been in use since the <span class="No-Break">early 1990s.</span></p>
			<p>The <em class="italic">NLP-progress</em> website at <a href="http://nlpprogress.com/">http://nlpprogress.com/</a> is a good source of information on shared tasks <a id="_idIndexMarker996"/>and shared <span class="No-Break">task data.</span></p>
			<p>In the next section, we will look at ways of dividing data into subsets in preparation <span class="No-Break">for evaluation.</span></p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor234"/>Data partitioning</h1>
			<p>In earlier chapters, we <a id="_idIndexMarker997"/>divided our datasets into subsets used for <em class="italic">training</em>, <em class="italic">validation</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">testing</em></span><span class="No-Break">.</span></p>
			<p>As a reminder, training data is used to develop the NLU model that is used to perform the eventual task of the NLU application, whether that is classification, slot-filling, intent recognition, or most other <span class="No-Break">NLU tasks.</span></p>
			<p><strong class="bold">Validation data</strong> (sometimes called <strong class="bold">development test</strong> data) is used during training to assess the model on <a id="_idIndexMarker998"/>data that was not used in training. This is important because if the system is tested on the <a id="_idIndexMarker999"/>training data, it could get a good result simply by, in effect, memorizing the training data. This would be misleading because that kind of system isn’t very useful – we want the system to generalize or work well on the new data that it’s going to get when it is deployed. Validation data can also be used to help tune hyperparameters in machine learning applications, but this means that during development, the system has been exposed a bit to the validation data, and as a consequence, that data is not as novel as we would like it <span class="No-Break">to be.</span></p>
			<p>For that reason, another set of completely new data is typically held out for a final test; this is the test data. In preparation <a id="_idIndexMarker1000"/>for system development, the full dataset is partitioned into training, validation, and test data. Typically, around 80% of the full dataset is allocated to training, 10% is allocated to validation, and 10% is allocated <span class="No-Break">to testing.</span></p>
			<p>There are three general ways that data can <span class="No-Break">be partitioned:</span></p>
			<ul>
				<li>In some cases, the dataset is already partitioned into training, validation, and testing sets. This is most common in generally available datasets such as those we have used in this book or shared tasks. Sometimes, the data is only partitioned into training and testing. If so, a subset of the training data should be used for validation. Keras provides a useful utility, <strong class="source-inline">text_dataset_from_directory</strong>, that loads a dataset from a directory, using subdirectory names for the supervised categories of the text in the directories, and partitions a validation subset. In the following code, the training data is loaded from the <strong class="source-inline">aclImdb/train</strong> directory, and then 20% of the data is split out to be used as <span class="No-Break">validation data:</span><pre class="source-code">
raw_train_ds = tf.keras.utils.text_dataset_from_directory(</pre><pre class="source-code">
     'aclImdb/train',</pre><pre class="source-code">
     batch_size=batch_size,</pre><pre class="source-code">
     validation_split=0.2,</pre><pre class="source-code">
     subset='training',</pre><pre class="source-code">
     seed=seed)</pre></li>
				<li>Of course, making use of a previous partition is only useful when you work with generally available datasets that have already been partitioned. If you have your own data, you will need to do your own partitioning. Some of the libraries that we’ve been working with have functions that can automatically partition the data when it is loaded. For example, scikit-learn, TensorFlow, and Keras have <strong class="source-inline">train_test_split</strong> functions that can be used <span class="No-Break">for this.</span></li>
				<li>You can also manually write Python code to partition your dataset. Normally, it would be preferable to work with pretested code from a library, so writing your own Python code for this would not be recommended unless you are unable to find a suitable library, or you are simply interested in the exercise of learning more about the <span class="No-Break">partitioning process.</span></li>
			</ul>
			<p>The final data partitioning <a id="_idIndexMarker1001"/>strategy is called <strong class="bold">k-fold cross-validation</strong>. This strategy involves partitioning the whole dataset into <em class="italic">k</em> subsets, or <em class="italic">folds</em>, and then treating each of the folds as test <a id="_idIndexMarker1002"/>data for an evaluation of the system. The overall system score in <em class="italic">k</em>-fold cross-validation is the average score of all of <span class="No-Break">the tests.</span></p>
			<p>The advantage of this approach is that it reduces the chances of an accidental difference between the test and <a id="_idIndexMarker1003"/>training data, leading to a model that gives poor predictions for new test examples. It is harder for an accidental difference to affect the results in <em class="italic">k</em>-fold cross-validation because there is no strict line between training and test data; the data in each fold takes a turn at being the test data. The disadvantage of this approach is that it multiplies the amount of time it takes to test the system by <em class="italic">k</em>, which could become very large if the dataset is <span class="No-Break">also large.</span></p>
			<p>Data partitioning is necessary for almost every evaluation metric that we will discuss in this chapter, with the exception of <span class="No-Break">user testing.</span></p>
			<p>In the next section, we will look at some of the most common specific quantitative metrics. In <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we went over the most basic and intuitive metric, accuracy. Here, we will review other metrics that can usually provide better insights than accuracy and explain how to <span class="No-Break">use them.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor235"/>Evaluation metrics</h1>
			<p>There are two important concepts that we should keep in mind when selecting an evaluation metric for NLP systems or, more generally, any system that we want <span class="No-Break">to evaluate:</span></p>
			<ul>
				<li><strong class="bold">Validity</strong>: The first is validity, which <a id="_idIndexMarker1004"/>means that the metric corresponds to what we think of intuitively as the actual property we want to know about. For example, we wouldn’t want to pick the length of a text as a measurement for its positive or negative sentiment <a id="_idIndexMarker1005"/>because the length of a text would not be a valid measure of <span class="No-Break">its sentiment.</span></li>
				<li><strong class="bold">Reliability</strong>: The other important concept is reliability, which means that if we measure the <a id="_idIndexMarker1006"/>same thing repeatedly, we always get the <span class="No-Break">same result.</span></li>
			</ul>
			<p>In the next sections, we will look at some of the most commonly used metrics in NLU that are considered to be both valid <span class="No-Break">and reliable.</span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor236"/>Accuracy and error rate</h2>
			<p>In <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we defined accuracy as the <a id="_idIndexMarker1007"/>number of correct system responses divided by the overall <a id="_idIndexMarker1008"/>number of inputs. Similarly, we defined <strong class="bold">error rate</strong> as incorrect <a id="_idIndexMarker1009"/>responses divided by the number of inputs. Note that you might encounter a <strong class="bold">word error rate</strong> when you read reports of speech recognition <a id="_idIndexMarker1010"/>results. Because the word error rate is calculated according to a different formula that takes into account different kinds of common speech recognition errors, this is a different metric, which we will not <span class="No-Break">cover here.</span></p>
			<p>The next section will go over some more detailed metrics, precision, recall, and F<span class="subscript">1</span> score, which were briefly mentioned in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></p>
			<p>These metrics usually provide more insight into a system’s processing results <span class="No-Break">than accuracy.</span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor237"/>Precision, recall, and F<span class="subscript">1</span></h2>
			<p><strong class="bold">Accuracy</strong> can give misleading results under certain conditions. For example, we might have a dataset where <a id="_idIndexMarker1011"/>there are 100 items but the classes are <a id="_idIndexMarker1012"/>unbalanced, and the vast majority of cases (say, 90) belong to one class, which we call the majority class. This is very common in real datasets. In that situation, 90% accuracy could be obtained by automatically assigning every case to the majority class, but the 10 remaining instances that belong to the other class (let’s assume we only have two classes for simplicity) would always be wrong. Intuitively, accuracy would seem to be an invalid and misleading <a id="_idIndexMarker1013"/>metric for this situation. To provide a more valid metric, some refinements were introduced – most importantly, the concepts of recall <span class="No-Break">and precision.</span></p>
			<p><strong class="bold">Recall</strong> means that a system finds every example of a class and does not miss any. In our example, it correctly <a id="_idIndexMarker1014"/>finds all 90 instances of the majority class but <a id="_idIndexMarker1015"/>no instances of the <span class="No-Break">other class.</span></p>
			<p>The formula for recall is shown in the following equation, where <em class="italic">true positives</em> are instances of the class that were correctly identified, and <em class="italic">false negatives</em> are instances of that class that were missed. In our initial example, assuming that there are 100 items in the dataset, the system correctly found 90 (the majority class) but missed the 10 examples of the other class. Therefore, the recall score of the majority class is 1.0, but the recall score of the other class <span class="No-Break">is 0:</span></p>
			<p>                                                          <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">____________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>On the other hand, <strong class="bold">precision</strong> means that whatever was identified was correct. Perfect precision means <a id="_idIndexMarker1016"/>that no items were misidentified, although some items <a id="_idIndexMarker1017"/>might have been missed. The following is the formula for precision, where <em class="italic">false positives</em> are instances that were misidentified. In this example, the false positives are the 10 items that were incorrectly identified as belonging to the majority class. In our example, the precision score for the majority class is 1.0, but for the other class, it is 0 because there are no <em class="italic">true positives</em>. The precision and recall scores in this example give us more detail about the kinds of mistakes that the system <span class="No-Break">has made:</span></p>
			<p>                                                         <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>Finally, there is another important metric, <strong class="bold">F</strong><span class="subscript">1</span>, that combines precision and recall scores. This is a useful metric <a id="_idIndexMarker1018"/>because we often want to have a single overall <a id="_idIndexMarker1019"/>metric to describe a system’s performance. This is probably the most commonly used metric in NLU. The formula for F1 is <span class="No-Break">as follows:</span></p>
			<p>                                                          <span class="_-----MathTools-_Math_Variable">           </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.1</em> shows correct and incorrect classifications graphically for one class. Items in the ellipse are identified as <strong class="bold">Class 1</strong>. The round markers inside the ellipse are true positives – items identified as <strong class="bold">Class 1</strong> that are actually <strong class="bold">Class 1</strong>. The square markers inside the ellipse are false <a id="_idIndexMarker1020"/>positives – items in <strong class="bold">Class 2</strong> that are incorrectly <a id="_idIndexMarker1021"/>identified as <strong class="bold">Class 1</strong>. Round markers outside of the ellipse are false negatives – items that are actually in <strong class="bold">Class 1</strong> but were <span class="No-Break">not recognized:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B19005_13_01.jpg" alt="Figure 13.1 – The classification for one class"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – The classification for one class</p>
			<p>There is an assumption with the <strong class="source-inline">F</strong><span class="subscript">1</span> metric that the recall and precision scores are equally important. This is true in many cases, but not always. For example, consider an application whose goal is to detect mentions of its company’s products in tweets. The developers of this system might consider it to be very important not to miss any tweets about their products. In that case, they will want to emphasize recall at the expense of precision, which means that they might get a lot of false positives – tweets that the system categorized as being about their product but actually were not. There are more general versions of the <strong class="source-inline">F</strong><span class="subscript">1</span> metric that <a id="_idIndexMarker1022"/>can be used to weight recall and precision if <a id="_idIndexMarker1023"/>you develop a system where recall and precision are not <span class="No-Break">equally important.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor238"/>The receiver operating characteristic and area under the curve</h2>
			<p>The decision of the class to which a test item belongs depends on its score for that class. If the item’s score exceeds a given threshold score (selected by the developer), then the system decides that <a id="_idIndexMarker1024"/>the item does actually belong in that class. We can see that <a id="_idIndexMarker1025"/>true positive and false positive scores can be affected <a id="_idIndexMarker1026"/>by this threshold. If we make the threshold very high, few items will fall into that class, and the true positives will be lower. On the other hand, if the threshold is very low, the system will decide that many items fall into that class, the system will accept many items that don’t belong, and the false positives will be very high. What we really want to know is how good the system is at discriminating between classes at <span class="No-Break">every threshold.</span></p>
			<p>A good way to visualize the trade-off between false positives (precision failures) and false negatives (recall failures) is a graph called the <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) and its related metric, the <strong class="bold">area under the curve</strong> (<strong class="bold">AUC</strong>). The ROC curve is a measurement of <a id="_idIndexMarker1027"/>how good a system is overall at discriminating between classes. The best way to understand this is to look at an example of an ROC curve, which we can see in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.2</em>. This figure is based on randomly <span class="No-Break">generated data:</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B19005_13_02.jpg" alt="Figure 13.2 – The ROC curve for a perfect classifier compared to a random classifier"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – The ROC curve for a perfect classifier compared to a random classifier</p>
			<p>In <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.2</em>, we can see that at point (<strong class="bold">0</strong>, <strong class="bold">1)</strong>, the system has no false positives, and all of the true positives are <a id="_idIndexMarker1028"/>correctly detected. If we set the acceptance threshold to <strong class="bold">1</strong>, the <a id="_idIndexMarker1029"/>system will still accept all of the true positives, and <a id="_idIndexMarker1030"/>there will still be no false positives. On the other <a id="_idIndexMarker1031"/>hand, if the classifier can’t tell the classes apart at all, we would get something like the <strong class="bold">Random Classifier</strong> line. No matter where we set the threshold, the system will still make many mistakes, as if it was randomly assigning classes <span class="No-Break">to inputs.</span></p>
			<p>A common way to summarize a system’s ability to discriminate between classes is the area under the ROC curve, or AUC. The perfect classifier’s AUC score will be <strong class="bold">1.0</strong>, a random classifier’s AUC score will be around <strong class="bold">0.5</strong>, and if the classifier performs at a level that is worse than random, the AUC score will be less than <strong class="bold">0.5</strong>. Note that we look at a binary classification problem here because it is simpler to explain, but there are techniques to apply these ideas to multi-class problems as well. We will not go into these here, but a <a id="_idIndexMarker1032"/>good discussion of the techniques to look at ROC curves <a id="_idIndexMarker1033"/>for multi-class datasets can <a id="_idIndexMarker1034"/>be found in the scikit-learn documentation <span class="No-Break">at </span><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"><span class="No-Break">https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor239"/>Confusion matrix</h2>
			<p>Another important evaluation tool is a <strong class="bold">confusion matrix</strong>, which shows how often each class is confused <a id="_idIndexMarker1035"/>with each other class. This will be much <a id="_idIndexMarker1036"/>clearer with an example, so we will postpone discussing confusion matrices until we go over the example at the end of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor240"/>User testing</h1>
			<p>In addition to direct system measurements, it is also possible to evaluate systems with user testing, where test users who are representative of a system’s intended users interact <span class="No-Break">with it.</span></p>
			<p><strong class="bold">User testing</strong> is a time-consuming and expensive type of testing, but sometimes, it is the only way that you can <a id="_idIndexMarker1037"/>find out qualitative aspects of system performance – for example, how easy it is for users to complete tasks with a system, or how much they enjoy using it. Clearly, user testing can only be done on aspects of the system that users can perceive, such as conversations, and users should be only expected to evaluate the system as a whole – that is, users can’t be expected to reliably discriminate between the performance of the speech recognition and the NLU components of <span class="No-Break">the system.</span></p>
			<p>Carrying out a valid and reliable evaluation with users is actually a psychological experiment. This is a complex topic, and it’s easy to make mistakes that make it impossible to draw conclusions from the results. For those reasons, providing complete instructions to conduct user testing is outside of the scope of this book. However, you can do some exploratory user testing by having a few users interact with a system and measuring their experiences. Some simple measurements that you can collect in user testing include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Having users fill out a simple questionnaire about <span class="No-Break">their experiences.</span></li>
				<li>Measuring how long users spend interacting with the system. The amount of time users spend interacting with the system can be a positive or negative measurement, depending on the purpose of the system, such as <span class="No-Break">the following:</span><ul><li>If you are developing a social system that’s just supposed to be fun, you will want them to spend more time interacting with <span class="No-Break">the system</span></li><li>On the other hand, if you are developing a task-oriented system that will help users complete a task, you will usually want the users to spend less time interacting with <span class="No-Break">the system</span></li></ul></li>
			</ul>
			<p>However, you also have to be cautious with <span class="No-Break">user testing:</span></p>
			<ul>
				<li>It is important for users to be representative of the actual intended user population. If they are not, the results can be very misleading. For example, a company chatbot intended for customer support should be tested on customers, not employees, since employees have much more knowledge about the company than customers and will ask <span class="No-Break">different questions.</span></li>
				<li>Keep questionnaires simple, and don’t ask users to provide information that isn’t relevant to what you are trying to learn. Users who are bored or impatient with the questionnaire will not provide <span class="No-Break">useful information.</span></li>
				<li>If the results of user testing are very important to the project, you should find a human factors engineer – that is, someone with experience designing experiments with human users – to design the testing process so that the results are valid <span class="No-Break">and reliable.</span></li>
			</ul>
			<p>So far, we have looked at several different ways to measure the performance of systems. Applying each <a id="_idIndexMarker1038"/>of these techniques will result in one or more numerical values, or <em class="italic">metrics</em> that quantify a system’s performance. Sometimes, when we use these metrics to compare several systems, or several versions of the same system, we will find that the different values of the metrics are small. It is then worth asking whether small differences are really meaningful. This question is addressed by the topic of statistical significance, which we will cover in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor241"/>Statistical significance of differences</h1>
			<p>The last general topic we will cover in evaluation is the topic of determining whether the differences <a id="_idIndexMarker1039"/>between the results of experiments we have done reflect a real difference between the experimental conditions, or whether they reflect differences that are due to chance. This is called <strong class="bold">statistical significance</strong>. Whether a difference in the values of the metrics represents <a id="_idIndexMarker1040"/>a real difference between systems isn’t something that we can know for certain, but what we can know is how likely it is that a difference that we’re interested in is due to chance. Let’s suppose we have the situation with our data that’s shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B19005_13_03.jpg" alt="Figure 13.3 – Two distributions of measurement values – do they reflect a real difference between the things they’re measuring?"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Two distributions of measurement values – do they reflect a real difference between the things they’re measuring?</p>
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.3</em> shows two sets of measurements, one with a mean of <strong class="bold">0</strong>, on the left, and one with a mean of <strong class="bold">0.75</strong>, on the right. Here, we might be comparing the performance of two classification algorithms on different datasets, for example. It looks like there is a real difference between the algorithms, but could this difference be an accident? We usually consider <a id="_idIndexMarker1041"/>that if the probability of a difference of this size occurring by chance is once out of 20, then the difference is considered to be statistically significant, or not due to chance. Of course, that means that 1 out of 20 statistically significant results is probably actually due to chance. This probability is determined by standard statistical formulas such as the <em class="italic">t-statistic</em> or the <em class="italic">analysis </em><span class="No-Break"><em class="italic">of variance</em></span><span class="No-Break">.</span></p>
			<p>If you read a paper that performs a significance test on its results and it states something such as <em class="italic">p&lt;.05</em>, the <em class="italic">p</em> refers to the probability of the difference being due to chance. This kind of statistical analysis is most commonly performed with data where knowing whether differences are statistically significant is very important, such as academic papers for presentation at conferences or for publication in academic journals. We will not cover the process of how statistical significance is calculated here, since it can become quite a complex process, but you should be aware of what it means if you have a need <span class="No-Break">for it.</span></p>
			<p>It is also important to consider that a result can be statistically significant but not really meaningful in realistic situations. If the results of one classification algorithm are only slightly better than another but the better algorithm is much more complicated to compute, it might not <a id="_idIndexMarker1042"/>be worth bothering with the better algorithm. These trade-offs are something that has to be considered from the perspective of how the algorithm will be used. Even a small, but significant, difference could be important in an academic paper but not important at all in a <span class="No-Break">deployed application.</span></p>
			<p>Now that we have reviewed several approaches to evaluating systems, let’s take a look at applying them in practice. In the next section, we will work through a case study that compares three different approaches to text classification on the <span class="No-Break">same data.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor242"/>Comparing three text classification methods</h1>
			<p>One of the most useful things we can do with evaluation techniques is to decide which of several <a id="_idIndexMarker1043"/>approaches to use in an application. Are the traditional approaches such as <strong class="bold">term frequency - inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>), <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), and <strong class="bold">conditional random fields</strong> (<strong class="bold">CRFs</strong>) good <a id="_idIndexMarker1044"/>enough for our task, or will it be necessary to use deep learning and transformer approaches that have better results at the cost of longer <span class="No-Break">training time?</span></p>
			<p>In this section, we will compare the performance of three approaches on a larger version of the movie review dataset that we looked at in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>. We will look at using a small BERT model, TF-IDF vectorization with the Naïve Bayes classification, and a larger <span class="No-Break">BERT model.</span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor243"/>A small transformer system</h2>
			<p>We will start by <a id="_idIndexMarker1045"/>looking at the BERT system that <a id="_idIndexMarker1046"/>we developed in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>. We will <a id="_idIndexMarker1047"/>use the same BERT model as in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, which <a id="_idIndexMarker1048"/>is one of the smallest BERT <a id="_idIndexMarker1049"/>models, <strong class="source-inline">small_bert/bert_en_uncased_L-2_H-128_A-2</strong>, with two layers, a hidden size of <strong class="source-inline">128</strong>, and <a id="_idIndexMarker1050"/>two <span class="No-Break">attention heads.</span></p>
			<p>We will be <a id="_idIndexMarker1051"/>making a few changes in order to better evaluate this <span class="No-Break">model’s performance.</span></p>
			<p>First, we will add new metrics, precision and recall, to the <strong class="source-inline">BinaryAccuracy</strong> metric that we used in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
metrics = [tf.metrics.Precision(),tf.metrics.Recall(),tf.metrics.BinaryAccuracy()]</pre>
			<p>With these metrics, the <strong class="source-inline">history</strong> object will include the changes in precision and recall during the 10-epoch training process. We can look at these with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
history_dict = history.history
print(history_dict.keys())
acc = history_dict['binary_accuracy']
precision = history_dict['precision_1']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_recall = history_dict['val_recall_1']
recall = history_dict['recall_1']
val_loss = history_dict['val_loss']
val_precision = history_dict['val_precision_1']
precision = history_dict['precision_1']
val_recall = history_dict['val_recall_1']</pre>
			<p>As shown in the preceding code snippet, the first steps are to pull out the results we are interested in from the <strong class="source-inline">history</strong> object. The next step is to plot the results as they change over training epochs, which is calculated in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()
plt.subplot(4, 1, 1)
# r is for "solid red line"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()</pre>
			<p>The preceding code <a id="_idIndexMarker1052"/>shows the plotting process for <a id="_idIndexMarker1053"/>the training and validation loss, which <a id="_idIndexMarker1054"/>results in the top plot in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.4</em>. The <a id="_idIndexMarker1055"/>rest of the <a id="_idIndexMarker1056"/>plots in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.4</em> and the plots in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.5</em> are <a id="_idIndexMarker1057"/>calculated in the same manner, but <a id="_idIndexMarker1058"/>we will not show the full code here, since it is nearly the same as the code for the <span class="No-Break">first plot.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.4</em> shows decreasing loss and increasing accuracy over training epochs, which we saw previously. We can see that loss for both the training and validation data is leveling off between <strong class="bold">0.40</strong> and <strong class="bold">0.45</strong>. At epoch <strong class="bold">7</strong>, it looks like additional training is unlikely to <span class="No-Break">improve performance:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B19005_13_04.jpg" alt="Figure 13.4 – Loss and accuracy over 10 training epochs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Loss and accuracy over 10 training epochs</p>
			<p>Since we have <a id="_idIndexMarker1059"/>added the precision and <a id="_idIndexMarker1060"/>recall metrics, we can also <a id="_idIndexMarker1061"/>see that these metrics level off at <a id="_idIndexMarker1062"/>around <strong class="bold">0.8</strong> at around <strong class="bold">7</strong> epochs of <a id="_idIndexMarker1063"/>training, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B19005_13_05.jpg" alt="Figure 13.5 – Precision and recall over 10 training epochs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Precision and recall over 10 training epochs</p>
			<p>We can <a id="_idIndexMarker1064"/>also look at the confusion matrix and <a id="_idIndexMarker1065"/>classification <a id="_idIndexMarker1066"/>report for more <a id="_idIndexMarker1067"/>information with the <a id="_idIndexMarker1068"/>following code, using functions <a id="_idIndexMarker1069"/><span class="No-Break">from scikit-learn:</span></p>
			<pre class="source-code">
# Displaying the confusion matrix
%matplotlib inline
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,f1_score,classification_report
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 12})
disp = ConfusionMatrixDisplay(confusion_matrix = conf_matrix,
                                             display_labels = class_names)
print(class_names)
disp.plot(xticks_rotation=75,cmap=plt.cm.Blues)
plt.show()</pre>
			<p>This will <a id="_idIndexMarker1070"/>plot the confusion matrix as shown <a id="_idIndexMarker1071"/>in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B19005_13_06.jpg" alt="Figure 13.6 – The confusion matrix for a small BERT model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – The confusion matrix for a small BERT model</p>
			<p>The dark <a id="_idIndexMarker1072"/>cells in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.6</em> show the number of <a id="_idIndexMarker1073"/>correct classifications, where actual negative and <a id="_idIndexMarker1074"/>positive reviews were assigned to <a id="_idIndexMarker1075"/>the right classes. We can see that there <a id="_idIndexMarker1076"/>are quite a few incorrect classifications. We can see more detail by printing a summary classification report with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
print(classification_report(y_test, y_pred, target_names=class_names))
['neg', 'pos']</pre>
			<p>The classification <a id="_idIndexMarker1077"/>report shows the <strong class="source-inline">recall</strong>, <strong class="source-inline">precision</strong>, and <strong class="source-inline">F1</strong> scores for <span class="No-Break">both classes:</span></p>
			<pre class="source-code">
                precision   recall      f1-score  support
neg             0.82        0.80        0.81      12501
pos             0.80        0.82        0.81      12500
accuracy                                0.81      25001
macro avg       0.81        0.81        0.81      25001
weighted avg    0.81        0.81        0.81      25001</pre>
			<p>From this report, we <a id="_idIndexMarker1078"/>can see that the system is <a id="_idIndexMarker1079"/>almost equally good at recognizing <a id="_idIndexMarker1080"/>positive and negative reviews. The <a id="_idIndexMarker1081"/>system correctly classifies many <a id="_idIndexMarker1082"/>reviews but is still making <span class="No-Break">many errors.</span></p>
			<p>Now, we will <a id="_idIndexMarker1083"/>compare the BERT test with one of our earlier tests, based on TF-IDF vectorization and Naïve <span class="No-Break">Bayes classification.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor244"/>TF-IDF evaluation</h2>
			<p>In <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we learned about vectorizing <a id="_idIndexMarker1084"/>with TF-IDF and classifying with Naïve Bayes. We illustrated the process with the movie review corpus. We want to compare these very traditional techniques with the newer transformer-based LLMs, such as BERT, that we just saw. How much better are transformers than the traditional <a id="_idIndexMarker1085"/>approaches? Do the transformers <a id="_idIndexMarker1086"/>justify their much bigger size and <a id="_idIndexMarker1087"/>longer training time? To make the comparison between BERT and TF-IDF/Naïve Bayes fair, we will use the larger <strong class="source-inline">aclimdb</strong> movie review dataset that we used <a id="_idIndexMarker1088"/>in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
			<p>We will use <a id="_idIndexMarker1089"/>the same code we used in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> to set up the system and perform the TF-IDF/Naïve Bayes classification, so we won’t repeat it <a id="_idIndexMarker1090"/>here. We will just add the final <a id="_idIndexMarker1091"/>code to show the confusion matrix and display the <span class="No-Break">results graphically:</span></p>
			<pre class="source-code">
# View the results as a confusion matrix
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(labels_test, labels_pred,normalize=None)
print(conf_matrix)
[[9330 3171]
 [3444 9056]]</pre>
			<p>The text <a id="_idIndexMarker1092"/>version of the confusion matrix is <span class="No-Break">the array:</span></p>
			<p><strong class="source-inline">[[</strong><span class="No-Break"><strong class="source-inline">9330 3171]</strong></span></p>
			<p><strong class="source-inline">[</strong><span class="No-Break"><strong class="source-inline">3444 9056]]</strong></span></p>
			<p>However, this is not very easy to understand. We can display it more clearly by using <strong class="source-inline">matplotlib</strong> in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# Displaying the confusion matrix
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,f1_score,classification_report
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 12})
disp = ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = class_names)
print(class_names)
disp.plot(xticks_rotation=75,cmap=plt.cm.Blues)
plt.show()</pre>
			<p>The result is shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B19005_13_07.jpg" alt="Figure 13.7 – The confusion matrix for TF﻿-IDF/the Naïve Bayes classification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – The confusion matrix for TF-IDF/the Naïve Bayes classification</p>
			<p>The dark <a id="_idIndexMarker1093"/>cells in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.7</em> show the correct classifications, where actual negative and positive reviews <a id="_idIndexMarker1094"/>were assigned to the right classes. We can also see <a id="_idIndexMarker1095"/>that <strong class="bold">3171</strong> actual negative reviews were <a id="_idIndexMarker1096"/>misclassified as positive and <strong class="bold">3444</strong> actual <a id="_idIndexMarker1097"/>positive reviews were <a id="_idIndexMarker1098"/>misclassified <span class="No-Break">as negative.</span></p>
			<p>To see the <a id="_idIndexMarker1099"/>recall, precision, and F<span class="subscript">1</span> scores, we can print the <span class="No-Break">classification report:</span></p>
			<pre class="source-code">
print(classification_report(labels_test, labels_pred, target_names=class_names))</pre>
			<p>The resulting classification report shows the recall, precision, and F<span class="subscript">1</span> scores, along with the accuracy, the number of items in each class (<strong class="source-inline">support</strong>), and <span class="No-Break">other statistics:</span></p>
			<pre class="source-code">
             precision     recall     f1-score    support
neg             0.73        0.75        0.74      12501
pos             0.74        0.72        0.73      12500
accuracy                                0.74      25001
macro avg       0.74        0.74        0.74      25001
weighted avg    0.74        0.74        0.74      25001</pre>
			<p>From this report, we can see that the system is slightly better at recognizing negative reviews. The system correctly classifies many reviews but is still making many errors. Comparing these <a id="_idIndexMarker1100"/>results to the results for the <a id="_idIndexMarker1101"/>BERT system in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.6</em>, we can see <a id="_idIndexMarker1102"/>that the BERT results are quite a bit <a id="_idIndexMarker1103"/>better. The BERT F<span class="subscript">1</span> score <a id="_idIndexMarker1104"/>is <strong class="bold">0.81</strong>, while the TF-IDF/Naïve <a id="_idIndexMarker1105"/>Bayes F<span class="subscript">1</span> score is <strong class="bold">0.74</strong>. For this task, the BERT system <a id="_idIndexMarker1106"/>would be the better choice, but can we <span class="No-Break">do better?</span></p>
			<p>In the next section, we will ask another question that compares two systems. The question concerns what might happen with other BERT transformer models. Larger models will almost always have better performance than smaller models but will take more training time. How much better will they be in terms <span class="No-Break">of performance?</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor245"/>A larger BERT model</h2>
			<p>So far, we have compared a very small BERT model to a system based on TF-IDF and Naïve Bayes. We <a id="_idIndexMarker1107"/>saw from the comparison between <a id="_idIndexMarker1108"/>the two systems’ classification <a id="_idIndexMarker1109"/>reports that the BERT system was definitely better than the TF-IDF/Naïve Bayes system. For this task, BERT is better as far as correct classification goes. On the other hand, BERT is much slower <span class="No-Break">to train.</span></p>
			<p>Can we get <a id="_idIndexMarker1110"/>even better performance with another <a id="_idIndexMarker1111"/>transformer system, such as another <a id="_idIndexMarker1112"/>variation of BERT? We can find this out by <a id="_idIndexMarker1113"/>comparing our system’s results to the results from other variations of BERT. This is easy to do because the BERT system includes many more models of various sizes and complexities, which can be found <span class="No-Break">at </span><a href="https://tfhub.dev/google/collections/bert/1"><span class="No-Break">https://tfhub.dev/google/collections/bert/1</span></a><span class="No-Break">.</span></p>
			<p>Let’s compare another model to the BERT system we just tested. The system that we’ve tested is one of the smaller BERT systems, <strong class="source-inline">small_bert/bert_en_uncased_L-2_H-128_A-2</strong>. The name encodes its important properties – two layers, a hidden size of <strong class="source-inline">128</strong>, and <a id="_idIndexMarker1114"/>two attention heads. We might be <a id="_idIndexMarker1115"/>interested in finding out what happens <a id="_idIndexMarker1116"/>with a larger model. Let’s try <a id="_idIndexMarker1117"/>one, <strong class="source-inline">small_bert/bert_en_uncased_L-4_H-512_A-8</strong>, which is nevertheless not too large to <a id="_idIndexMarker1118"/>train on a CPU (rather than a GPU). This <a id="_idIndexMarker1119"/>model is still rather small, with four <a id="_idIndexMarker1120"/>layers, a hidden size of <strong class="source-inline">512</strong>, and eight attention heads. Trying a different model is quite easy to do, with just a minor modification of the code that sets up the BERT model for use by including the information for the <span class="No-Break">new model:</span></p>
			<pre class="source-code">
bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
map_name_to_handle = {
     'small_bert/bert_en_uncased_L-4_H-512_A-8' :
           'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
}
map_model_to_preprocess = {
      'small_bert/bert_en_uncased_L-4_H-512_A-8':
           'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}</pre>
			<p>As the code shows, all we have to do is select the model’s name, map it to the URL where it is located, and assign it to a preprocessor. The rest of the code will be the same. <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.8</em> shows the confusion matrix for the larger <span class="No-Break">BERT model:</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B19005_13_08.jpg" alt="Figure 13.8 – The confusion matrix for the larger BERT model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – The confusion matrix for the larger BERT model</p>
			<p>The performance of this model, as shown in the confusion matrix, is superior to that of the smaller BERT model as well as the TF-IDF/Naïve Bayes model. The classification report is shown in the <a id="_idIndexMarker1121"/>following snippet, and it also shows that <a id="_idIndexMarker1122"/>this model performs the best of the <a id="_idIndexMarker1123"/>three models we have looked at in this <a id="_idIndexMarker1124"/>section, with an average <strong class="source-inline">F1</strong><span class="subscript"> </span>score <a id="_idIndexMarker1125"/>of <strong class="source-inline">0.85</strong>, compared to <strong class="source-inline">0.81</strong> for the smaller <a id="_idIndexMarker1126"/>BERT model and <strong class="source-inline">0.74</strong> for <a id="_idIndexMarker1127"/>the TF-IDF/Naïve <span class="No-Break">Bayes model.</span></p>
			<pre class="source-code">
            precision   recall      f1-score    support
neg           0.86        0.85        0.85      12501
pos           0.85        0.86        0.86      12500
accuracy                              0.85      25001
macro avg     0.85        0.85        0.85      25001
weighted avg  0.85        0.85        0.85      25001</pre>
			<p>The training time for this model on the <strong class="source-inline">aclimdb</strong> dataset with 25,000 items was about eight hours on a standard CPU, which is probably acceptable for most applications. Is this performance good enough? Should we explore even bigger models? There is clearly quite a bit of room for improvement, and there are many other BERT models that we can experiment with. The decision of whether the results are acceptable or not is up to the developers of the application and depends on the importance of getting correct answers and <a id="_idIndexMarker1128"/>avoiding wrong answers. It can be <a id="_idIndexMarker1129"/>different for every application. You <a id="_idIndexMarker1130"/>are encouraged to experiment with <a id="_idIndexMarker1131"/>some of the larger models and consider <a id="_idIndexMarker1132"/>whether the improved <a id="_idIndexMarker1133"/>performance justifies the additional <a id="_idIndexMarker1134"/><span class="No-Break">training time.</span></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor246"/>Summary</h1>
			<p>In this chapter, you learned about a number of important topics related to evaluating NLU systems. You learned how to separate data into different subsets for training and testing, and you learned about the most commonly used NLU performance metrics – accuracy, precision, recall, F<span class="subscript">1</span>, AUC, and confusion matrices – and how to use these metrics to compare systems. You also learned about related topics, such as comparing systems with ablation, evaluation with shared tasks, statistical significance testing, and <span class="No-Break">user testing.</span></p>
			<p>The next chapter will start <em class="italic">Part 3</em> of this book, where we cover systems in action – applying NLU at scale. We will start <em class="italic">Part 3</em> by looking at what to do if a system isn’t working. If the original model isn’t adequate or the system models a real-world situation that changes, what has to be changed? The chapter discusses topics such as adding new data and changing the structure of <span class="No-Break">the application.</span></p>
		</div>
	

		<div id="_idContainer119" class="Content">
			<h1 id="_idParaDest-225"><a id="_idTextAnchor247"/>Part 3: Systems in Action – Applying Natural Language Understanding at Scale</h1>
			<p>In <em class="italic">Part 3</em>, you will learn about applying natural language understanding in running applications. This part will cover adding new data to existing applications, dealing with volatile applications, adding and removing categories, and will include a final chapter summarizing the book and looking to the future of natural <span class="No-Break">language understanding.</span></p>
			<p>We focus on getting NLU systems out of the lab and making them do real work solving <span class="No-Break">practical problems.</span></p>
			<p>This part comprises the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19005_14.xhtml#_idTextAnchor248"><em class="italic">Chapter 14</em></a>, <em class="italic">What to Do If the System Isn’t Working</em></li>
				<li><a href="B19005_15.xhtml#_idTextAnchor262"><em class="italic">Chapter 15</em></a>, <em class="italic">Summary and Looking to the Future</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer120" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer121">
			</div>
		</div>
	</body></html>