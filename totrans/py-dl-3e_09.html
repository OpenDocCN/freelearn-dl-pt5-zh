<html><head></head><body>
<div id="_idContainer1080">
<h1 class="chapter-number" id="_idParaDest-162" lang="en-GB"><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-163" lang="en-GB"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.2.1">Advanced Applications of Large Language Models</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In the previous two chapters, we introduced the transformer architecture and learned about</span><a id="_idIndexMarker1294"/><span class="koboSpan" id="kobo.4.1"> its latest large-scale incarnations, known as </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">large language models</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">LLMs</span></strong><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">We discussed them in the context of </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">natural language processing</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.11.1">NLP</span></strong><span class="koboSpan" id="kobo.12.1">) tasks. </span><span class="koboSpan" id="kobo.12.2">NLP was the original transformer application and is still the field at the forefront of LLM </span><a id="_idIndexMarker1295"/><span class="koboSpan" id="kobo.13.1">development today. </span><span class="koboSpan" id="kobo.13.2">However, the success of the architecture has led the research community to explore the application of transformers in other areas, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">computer vision.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.15.1">In this chapter, we’ll focus on these areas. </span><span class="koboSpan" id="kobo.15.2">We’ll discuss transformers as replacements for convolutional networks (CNNs, </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.16.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.17.1">) for tasks such as image classification and object detection. </span><span class="koboSpan" id="kobo.17.2">We’ll also learn how to use them as generative models for images instead of text, as we have done until now. </span><span class="koboSpan" id="kobo.17.3">We’ll also implement a model fine-tuning example – something we failed to do in </span><a href="B19627_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.18.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">And finally, we’ll implement a novel </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">LLM-driven application.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.21.1">In this chapter, we will cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">main topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.23.1">Classifying images with </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">Vision Transformer</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.25.1">Detection transformer</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.26.1">Generating images with </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">stable diffusion</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">Fine-tuning transformers</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.29.1">Harnessing the power of LLMs </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">with LangChain</span></span></li>
</ul>
<h1 id="_idParaDest-164" lang="en-GB"><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.31.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.32.1">We’ll implement the example in this chapter using Python, PyTorch, the Hugging Face Transformers library (</span><a href="https://github.com/huggingface/transformers"><span class="koboSpan" id="kobo.33.1">https://github.com/huggingface/transformers</span></a><span class="koboSpan" id="kobo.34.1">), and the LangChain framework (</span><a href="https://www.langchain.com/"><span class="koboSpan" id="kobo.35.1">https://www.langchain.com/</span></a><span class="koboSpan" id="kobo.36.1">, </span><a href="https://github.com/langchain-ai/langchain"><span class="koboSpan" id="kobo.37.1">https://github.com/langchain-ai/langchain</span></a><span class="koboSpan" id="kobo.38.1">). </span><span class="koboSpan" id="kobo.38.2">If you don’t have an environment with these tools, fret not – the example is available as a Jupyter Notebook on Google Colab. </span><span class="koboSpan" id="kobo.38.3">The code examples can be found in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09"><span class="No-Break"><span class="koboSpan" id="kobo.40.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.41.1">.</span></span></p>
<h1 id="_idParaDest-165" lang="en-GB"><a id="_idTextAnchor239"/><span class="koboSpan" id="kobo.42.1">Classifying images with Vision Transformer</span></h1>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.43.1">Vision Transformer</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">ViT</span></strong><span class="koboSpan" id="kobo.46.1">, </span><em class="italic"><span class="koboSpan" id="kobo.47.1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span></em><span class="koboSpan" id="kobo.48.1">, </span><a href="https://arxiv.org/abs/2010.11929"><span class="koboSpan" id="kobo.49.1">https://arxiv.org/abs/2010.11929</span></a><span class="koboSpan" id="kobo.50.1">) proves the adaptability of the attention mechanism</span><a id="_idIndexMarker1296"/><span class="koboSpan" id="kobo.51.1"> by introducing a clever technique for processing</span><a id="_idIndexMarker1297"/><span class="koboSpan" id="kobo.52.1"> images. </span><span class="koboSpan" id="kobo.52.2">One way to use transformers</span><a id="_idIndexMarker1298"/><span class="koboSpan" id="kobo.53.1"> for image inputs is to encode each pixel with four variables – pixel intensity, row, column, and channel location. </span><span class="koboSpan" id="kobo.53.2">Each pixel encoding is an input to a simple </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">neural network</span></strong><span class="koboSpan" id="kobo.55.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.56.1">NN</span></strong><span class="koboSpan" id="kobo.57.1">), which outputs a </span><span class="koboSpan" id="kobo.58.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/845.png" style="vertical-align:-0.340em;height:1.051em;width:1.834em"/></span><span class="koboSpan" id="kobo.59.1">-dimensional embedding vector. </span><span class="koboSpan" id="kobo.59.2">We can represent the three-dimensional image</span><a id="_idIndexMarker1299"/><span class="koboSpan" id="kobo.60.1"> as a one-dimensional sequence of these embedding vectors. </span><span class="koboSpan" id="kobo.60.2">It acts as an input to the model in the same way as a token embedding sequence does. </span><span class="koboSpan" id="kobo.60.3">Each pixel will attend to every other pixel in the </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">attention blocks.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.62.1">This approach has some disadvantages related to the length of the input sequence (context window). </span><span class="koboSpan" id="kobo.62.2">Unlike a one-dimensional text sequence, an image has a two-dimensional structure (the color channel doesn’t increase the number of pixels). </span><span class="koboSpan" id="kobo.62.3">Therefore, the input sequence length increases quadratically as the image size increases. </span><span class="koboSpan" id="kobo.62.4">Even a small 64×64 image would result in an input sequence with a length of 64*64=4,096. </span><span class="koboSpan" id="kobo.62.5">On one hand, this makes the model computationally intensive. </span><span class="koboSpan" id="kobo.62.6">On the other hand, as each pixel attends to the entire long sequence, it will be hard for the model to learn the structure of the image. </span><span class="koboSpan" id="kobo.62.7">CNNs approach this problem by using filters, which restrict the input size of a unit only to its immediate surrounding area (receptive field). </span><span class="koboSpan" id="kobo.62.8">To understand how ViT solves this problem, let’s start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1018">
<span class="koboSpan" id="kobo.64.1"><img alt="Figure 9.1 – Vision Transformer. Inspired by https://arxiv.org/abs/2010.11929" src="image/B19627_09_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.65.1">Figure 9.1 – Vision Transformer. </span><span class="koboSpan" id="kobo.65.2">Inspired by </span><a href="https://arxiv.org/abs/2010.11929"><span class="koboSpan" id="kobo.66.1">https://arxiv.org/abs/2010.11929</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.67.1">Let’s denote the input image</span><a id="_idIndexMarker1300"/><span class="koboSpan" id="kobo.68.1"> resolution with </span><em class="italic"><span class="koboSpan" id="kobo.69.1">(H, W)</span></em><span class="koboSpan" id="kobo.70.1"> and the number of channels with </span><em class="italic"><span class="koboSpan" id="kobo.71.1">C</span></em><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">Then, we can represent the input</span><a id="_idIndexMarker1301"/><span class="koboSpan" id="kobo.73.1"> image as a tensor, </span><span class="koboSpan" id="kobo.74.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/846.png" style="vertical-align:-0.027em;height:0.747em;width:5.238em"/></span><span class="koboSpan" id="kobo.75.1">. </span><span class="koboSpan" id="kobo.75.2">ViT splits the image into a sequence of two-dimension square patches, </span><span class="koboSpan" id="kobo.76.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/847.png" style="vertical-align:-0.482em;height:1.224em;width:5.206em"/></span><span class="koboSpan" id="kobo.77.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.78.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.79.1">.1</span></em><span class="koboSpan" id="kobo.80.1">). </span><span class="koboSpan" id="kobo.80.2">Here, </span><em class="italic"><span class="koboSpan" id="kobo.81.1">(P, P)</span></em><span class="koboSpan" id="kobo.82.1"> is the resolution of each image patch (</span><em class="italic"><span class="koboSpan" id="kobo.83.1">P=16</span></em><span class="koboSpan" id="kobo.84.1">) and </span><span class="koboSpan" id="kobo.85.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/848.png" style="vertical-align:-0.195em;height:0.897em;width:7.064em"/></span><span class="koboSpan" id="kobo.86.1"> is the number of patches (which is also the input sequence length). </span><span class="koboSpan" id="kobo.86.2">The sequence of patches serves as input to the model, in the same way as a token </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">sequence does.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.88.1">Next, the input patches, </span><span class="koboSpan" id="kobo.89.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/849.png" style="vertical-align:-0.482em;height:0.923em;width:0.825em"/></span><span class="koboSpan" id="kobo.90.1">, serve as input</span><a id="_idIndexMarker1302"/><span class="koboSpan" id="kobo.91.1"> to a linear projection, which outputs a </span><span class="koboSpan" id="kobo.92.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/693.png" style="vertical-align:-0.340em;height:1.051em;width:1.875em"/></span><span class="koboSpan" id="kobo.93.1">-dimensional </span><strong class="bold"><span class="koboSpan" id="kobo.94.1">patch embedding</span></strong><span class="koboSpan" id="kobo.95.1"> vector for each patch. </span><span class="koboSpan" id="kobo.95.2">The patch embeddings form the input sequence, </span><span class="koboSpan" id="kobo.96.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/851.png" style="vertical-align:-0.340em;height:0.803em;width:0.773em"/></span><span class="koboSpan" id="kobo.97.1">. </span><span class="koboSpan" id="kobo.97.2">We can summarize the patch-to-embedding process with the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.99.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;E&lt;/mml:mi&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/852.png" style="vertical-align:-0.532em;height:1.321em;width:14.569em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.100.1">Here, </span><span class="koboSpan" id="kobo.101.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;E&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/853.png" style="vertical-align:-0.027em;height:0.799em;width:5.980em"/></span><span class="koboSpan" id="kobo.102.1"> is the linear projection and </span><span class="koboSpan" id="kobo.103.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/854.png" style="vertical-align:-0.482em;height:1.229em;width:6.772em"/></span><span class="koboSpan" id="kobo.104.1"> is the static positional encoding (the same as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">original transformer).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.106.1">Once we have the embedding sequence, ViT processes it with a standard encoder-only pre-normalization transformer, similar to BERT (</span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.107.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.108.1">). </span><span class="koboSpan" id="kobo.108.2">It comes in three variants, displayed </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1028">
<span class="koboSpan" id="kobo.110.1"><img alt="Figure 9.2 – ViT variants. Based on https://arxiv.org/abs/2010.11929" src="image/B19627_09_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.111.1">Figure 9.2 – ViT variants. </span><span class="koboSpan" id="kobo.111.2">Based on https://arxiv.org/abs/2010.11929</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.112.1">The encoder architecture uses unmasked self-attention, which allows a token to attend to the full sequence rather than the preceding tokens only. </span><span class="koboSpan" id="kobo.112.2">This makes sense because the preceding or the next element doesn’t carry the same meaning in the relationship between pixels of an image as the order of the elements in a text sequence. </span><span class="koboSpan" id="kobo.112.3">The similarities between the two models don’t end here. </span><span class="koboSpan" id="kobo.112.4">Like BERT, the input sequence starts with a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">[CLS]</span></strong><span class="koboSpan" id="kobo.114.1"> (</span><span class="koboSpan" id="kobo.115.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/855.png" style="vertical-align:-0.340em;height:0.781em;width:1.069em"/></span><span class="koboSpan" id="kobo.116.1">) token (for classification tasks). </span><span class="koboSpan" id="kobo.116.2">The model output for the </span><span class="koboSpan" id="kobo.117.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/856.png" style="vertical-align:-0.340em;height:0.781em;width:1.091em"/></span><span class="koboSpan" id="kobo.118.1"> token is the output for the full image. </span><span class="koboSpan" id="kobo.118.2">In this way, the </span><span class="koboSpan" id="kobo.119.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/856.png" style="vertical-align:-0.340em;height:0.781em;width:1.091em"/></span><span class="koboSpan" id="kobo.120.1"> token attends to the entire input sequence (that is, the entire image). </span><span class="koboSpan" id="kobo.120.2">Alternatively, if we take the model output for any other patch, we will introduce an imbalance between the selected patch and the others of </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">the sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.122.1">Following the example of BERT, ViT</span><a id="_idIndexMarker1303"/><span class="koboSpan" id="kobo.123.1"> has pre-training and fine-tuning</span><a id="_idIndexMarker1304"/><span class="koboSpan" id="kobo.124.1"> phases. </span><span class="koboSpan" id="kobo.124.2">Pre-training uses large general-purpose image datasets (such as ImageNet) while fine-tuning trains the model on smaller </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">task-specific datasets.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.126.1">The model ends with a </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">classification head</span></strong><span class="koboSpan" id="kobo.128.1">, which contains one hidden layer during pre-training and no hidden</span><a id="_idIndexMarker1305"/><span class="koboSpan" id="kobo.129.1"> layers </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">during fine-tuning.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.131.1">One issue with ViT is that it performs best when pre-training with very large datasets, such as JFT-300M with 300M labeled images (</span><em class="italic"><span class="koboSpan" id="kobo.132.1">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</span></em><span class="koboSpan" id="kobo.133.1">, </span><a href="https://arxiv.org/abs/1707.02968"><span class="koboSpan" id="kobo.134.1">https://arxiv.org/abs/1707.02968</span></a><span class="koboSpan" id="kobo.135.1">). </span><span class="koboSpan" id="kobo.135.2">This makes the training a lot more computationally intensive versus a comparable CNN. </span><span class="koboSpan" id="kobo.135.3">Many further variants of ViT try to solve this challenge and propose other improvements to the original model. </span><span class="koboSpan" id="kobo.135.4">You can find out more in </span><em class="italic"><span class="koboSpan" id="kobo.136.1">A Survey on Visual Transformer</span></em><span class="koboSpan" id="kobo.137.1"> (</span><a href="https://arxiv.org/abs/2012.12556"><span class="koboSpan" id="kobo.138.1">https://arxiv.org/abs/2012.12556</span></a><span class="koboSpan" id="kobo.139.1">), which is regularly updated</span><a id="_idIndexMarker1306"/><span class="koboSpan" id="kobo.140.1"> with the latest advancements</span><a id="_idIndexMarker1307"/><span class="koboSpan" id="kobo.141.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">the field.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.143.1">With that, let’s see how to use ViT </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">in practice.</span></span></p>
<h2 id="_idParaDest-166" lang="en-GB"><a id="_idTextAnchor240"/><span class="koboSpan" id="kobo.145.1">Using ViT with Hugging Face Transformers</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.146.1">In this section, we’ll implement</span><a id="_idIndexMarker1308"/><span class="koboSpan" id="kobo.147.1"> a basic example of ViT image classification</span><a id="_idIndexMarker1309"/><span class="koboSpan" id="kobo.148.1"> with the help of Hugging Face Transformers and its </span><strong class="source-inline"><span class="koboSpan" id="kobo.149.1">pipeline</span></strong><span class="koboSpan" id="kobo.150.1"> abstraction, which we introduced in </span><a href="B19627_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.152.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.154.1">Import the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">pipeline</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.156.1"> abstraction:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.157.1">
from transformers import pipeline</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.158.1">Create an image classification pipeline instance. </span><span class="koboSpan" id="kobo.158.2">The pipeline uses the </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">ViT-Base model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.160.1">
img_classification_pipeline = pipeline(
     task="image-classification",
     model="google/vit-base-patch16-224")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.161.1">Run the instance with an image of a bicycle </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">from Wikipedia:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.163.1">
img_classification_pipeline("https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Left_side_of_Flying_Pigeon.jpg/640px-Left_side_of_Flying_Pigeon.jpg")</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.164.1">This code outputs the following top-5 class probability distribution (only the first class </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">is displayed):</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.166.1">
[{'score': 0.4616938531398773, 'label': 'tricycle,
     trike, velocipede'}]</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.167.1">This example is simple enough, but let’s dive in and analyze the ViT model itself. </span><span class="koboSpan" id="kobo.167.2">We can do this with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">print(img_classification_pipeline.model)</span></strong><span class="koboSpan" id="kobo.169.1"> command, which outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">the following:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.171.1">
ViTForImageClassification(
  (vit): ViTModel(
     (embeddings): ViTEmbeddings(
        (patch_embeddings): ViTPatchEmbeddings(
           (projection): Conv2d(3, 768,
                        kernel_size=(16, 16),
                        stride=(16, 16))
        )
        (dropout): Dropout(p=0.0)
     )
     (encoder): ViTEncoder(
        (layer): ModuleList(
           (0-11): 12 x ViTLayer(
              (attention): ViTAttention(
                 (attention): ViTSelfAttention(
                    (query): Linear(in_f=768,
                              out_f=768)
                    (key): Linear(in_f=768, out_f=768)
                    (value): Linear(in_f=768,
                              out_f=768)
                    (dropout): Dropout(p=0.0)
                 )
                 (output): ViTSelfOutput(
                    (dense): Linear(in_f=768,
                              out_f=768)
                    (dropout): Dropout(p=0.0)
                 )
              )
              (intermediate): ViTIntermediate(
                (dense): Linear(in_f=768, out_f=3072)
                (intermediate_act_fn):GELUActivation()
              )
              (output): ViTOutput(
                (dense): Linear(in_f=3072, out_f=768)
                (dropout): Dropout(p=0.0)
              )
              (layernorm_before): LayerNorm((768,))
              (layernorm_after): LayerNorm((768,))
           )
        )
     )
     (layernorm): LayerNorm((768,))
  )
  (classifier): Linear(in_f=768, out_f=1000)
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.172.1">The model works with 224×224 input</span><a id="_idIndexMarker1310"/><span class="koboSpan" id="kobo.173.1"> images. </span><span class="koboSpan" id="kobo.173.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">in_f</span></strong><span class="koboSpan" id="kobo.175.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">out_f</span></strong><span class="koboSpan" id="kobo.177.1"> are shortened for </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">in_features</span></strong><span class="koboSpan" id="kobo.179.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">out_features</span></strong><span class="koboSpan" id="kobo.181.1">, respectively. </span><span class="koboSpan" id="kobo.181.2">Unlike</span><a id="_idIndexMarker1311"/><span class="koboSpan" id="kobo.182.1"> other models, ViT uses bias in all </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">Linear</span></strong><span class="koboSpan" id="kobo.184.1"> layers (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">bias=True</span></strong><span class="koboSpan" id="kobo.186.1"> input parameter is not displayed). </span><span class="koboSpan" id="kobo.186.2">Let’s discuss the components of the model in the order that </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">they appear:</span></span></p><ul><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.188.1">ViTEmbeddings</span></strong><span class="koboSpan" id="kobo.189.1">: The patch embedding block. </span><span class="koboSpan" id="kobo.189.2">It contains a 2D convolution with a 16×16 filter size, stride of 16, three input channels (one for each color), and 768 output channels (</span><span class="koboSpan" id="kobo.190.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;768&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/858.png" style="vertical-align:-0.340em;height:1.051em;width:4.992em"/></span><span class="koboSpan" id="kobo.191.1">). </span><span class="koboSpan" id="kobo.191.2">Applying the convolutional filter at each location produces one 768-dimensional patch embedding per location of the input image. </span><span class="koboSpan" id="kobo.191.3">Since the patches form a two-dimensional grid (the same as the input), the output is flattened to a one-dimensional sequence. </span><span class="koboSpan" id="kobo.191.4">This block also adds positional encoding information, which is not reflected in its string representation. </span><span class="koboSpan" id="kobo.191.5">The dropout probability of all dropout instances is 0 because the model runs in inference rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">training mode.</span></span></li><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">ViTEncoder</span></strong><span class="koboSpan" id="kobo.194.1">: The main encoder model contains 12 </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">ViTLayer</span></strong><span class="koboSpan" id="kobo.196.1"> pre-ln (</span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">LayerNorm</span></strong><span class="koboSpan" id="kobo.198.1">) encoder block instances. </span><span class="koboSpan" id="kobo.198.2">Each contains </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">the following:</span></span><ul><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">ViTAttention</span></strong><span class="koboSpan" id="kobo.201.1"> attention block: </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">ViTSelfAttention</span></strong><span class="koboSpan" id="kobo.203.1"> multi-head attention and its output linear projection, </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">ViTSelfOutput</span></strong><span class="koboSpan" id="kobo.205.1">. </span><span class="koboSpan" id="kobo.205.2">All </span><strong class="bold"><span class="koboSpan" id="kobo.206.1">fully connected</span></strong><span class="koboSpan" id="kobo.207.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.208.1">FC</span></strong><span class="koboSpan" id="kobo.209.1">) layers in the block have a size equal to </span><span class="koboSpan" id="kobo.210.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;768&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/859.png" style="vertical-align:-0.340em;height:1.051em;width:5.014em"/></span><span class="koboSpan" id="kobo.211.1">, despite the multiple</span><a id="_idIndexMarker1312"/><span class="koboSpan" id="kobo.212.1"> attention heads. </span><span class="koboSpan" id="kobo.212.2">This is possible because each of the </span><em class="italic"><span class="koboSpan" id="kobo.213.1">N</span></em><span class="koboSpan" id="kobo.214.1"> attention heads has a size </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.216.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/860.png" style="vertical-align:-0.340em;height:1.051em;width:3.019em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.218.1">Feed-forward network</span></strong><span class="koboSpan" id="kobo.219.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.220.1">FFN</span></strong><span class="koboSpan" id="kobo.221.1">): A combination of two linear</span><a id="_idIndexMarker1313"/><span class="koboSpan" id="kobo.222.1"> layers, </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">ViTIntermediate</span></strong><span class="koboSpan" id="kobo.224.1"> plus </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">GELUActivation</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.226.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">ViTOutput</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">.</span></span></li></ul></li><li lang="en-GB"><span class="koboSpan" id="kobo.229.1">Classification head (</span><strong class="source-inline"><span class="koboSpan" id="kobo.230.1">classifier</span></strong><span class="koboSpan" id="kobo.231.1">): In inference mode, the classification head has only </span><a id="_idIndexMarker1314"/><span class="koboSpan" id="kobo.232.1">one </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">Linear</span></strong><span class="koboSpan" id="kobo.234.1"> layer with 1,000 outputs (because the model was fine-tuned</span><a id="_idIndexMarker1315"/><span class="koboSpan" id="kobo.235.1"> on the </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">ImageNet dataset).</span></span></li></ul></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.237.1">Next, let’s see how object detection with </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">transformers works.</span></span></p>
<h1 id="_idParaDest-167" lang="en-GB"><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.239.1">Understanding the DEtection TRansformer</span></h1>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.240.1">DEtection TRansformer</span></strong><span class="koboSpan" id="kobo.241.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.242.1">DETR</span></strong><span class="koboSpan" id="kobo.243.1">, </span><em class="italic"><span class="koboSpan" id="kobo.244.1">End-to-End Object Detection with Transformers</span></em><span class="koboSpan" id="kobo.245.1">, </span><a href="https://arxiv.org/abs/2005.12872"><span class="koboSpan" id="kobo.246.1">https://arxiv.org/abs/2005.12872</span></a><span class="koboSpan" id="kobo.247.1">) introduces a novel</span><a id="_idIndexMarker1316"/><span class="koboSpan" id="kobo.248.1"> transformer-based object </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">detection algorithm.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.250.1">A quick recap of the YOLO object detection algorithm</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.251.1">We first introduced YOLO in </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.252.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.253.1">. </span><span class="koboSpan" id="kobo.253.2">It has three main</span><a id="_idIndexMarker1317"/><span class="koboSpan" id="kobo.254.1"> components. </span><span class="koboSpan" id="kobo.254.2">The first is the backbone – that is, a CNN model that extracts features from the input image. </span><span class="koboSpan" id="kobo.254.3">Next is the neck – an intermediate part of the model that connects the backbone to the head. </span><span class="koboSpan" id="kobo.254.4">Finally, the head outputs the detected objects using a multi-step algorithm. </span><span class="koboSpan" id="kobo.254.5">More specifically, it splits the image into a grid of cells. </span><span class="koboSpan" id="kobo.254.6">Each cell contains several pre-defined anchor boxes with different shapes. </span><span class="koboSpan" id="kobo.254.7">The model predicts whether any of the anchor boxes contains an object and the coordinates of the object’s bounding box. </span><span class="koboSpan" id="kobo.254.8">Many of the boxes will overlap and predict the same object. </span><span class="koboSpan" id="kobo.254.9">The model filters the overlapping objects with the help of intersection-over-union and </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">non-maximum suppression.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.256.1">Like YOLO, DetR starts with a CNN backbone. </span><span class="koboSpan" id="kobo.256.2">However, it replaces the neck and the head with a full post-normalization transformer encoder-decoder. </span><span class="koboSpan" id="kobo.256.3">This negates the need for hand-designed components such as the non-maximum suppression procedure or anchor boxes. </span><span class="koboSpan" id="kobo.256.4">Instead, the model outputs a set of bounding boxes and class labels for the detected objects. </span><span class="koboSpan" id="kobo.256.5">To understand how it works, we’ll start with the following figure, which displays the components </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">of DetR:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1035">
<span class="koboSpan" id="kobo.258.1"><img alt="Figure 9.3 – DetR architecture. Inspired by https://arxiv.org/abs/2005.12872" src="image/B19627_09_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.259.1">Figure 9.3 – DetR architecture. </span><span class="koboSpan" id="kobo.259.2">Inspired by https://arxiv.org/abs/2005.12872</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.260.1">First, the backbone CNN extracts</span><a id="_idIndexMarker1318"/><span class="koboSpan" id="kobo.261.1"> the features from the input image, the same as in YOLO. </span><span class="koboSpan" id="kobo.261.2">Its outputs are the feature maps of the last convolutional layer. </span><span class="koboSpan" id="kobo.261.3">The original input image is a tensor with a shape of </span><span class="koboSpan" id="kobo.262.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/861.png" style="vertical-align:-0.483em;height:1.203em;width:6.045em"/></span><span class="koboSpan" id="kobo.263.1">, where </span><span class="koboSpan" id="kobo.264.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/862.png" style="vertical-align:-0.340em;height:1.004em;width:2.796em"/></span><span class="koboSpan" id="kobo.265.1"> is the number of color channels and </span><span class="koboSpan" id="kobo.266.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/863.png" style="vertical-align:-0.340em;height:0.988em;width:0.950em"/></span><span class="koboSpan" id="kobo.267.1">/</span><span class="koboSpan" id="kobo.268.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/864.png" style="vertical-align:-0.340em;height:0.988em;width:1.153em"/></span><span class="koboSpan" id="kobo.269.1"> are the image dimensions. </span><span class="koboSpan" id="kobo.269.2">The last convolution output is a tensor with a shape of </span><span class="koboSpan" id="kobo.270.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/865.png" style="vertical-align:-0.340em;height:1.060em;width:5.228em"/></span><span class="koboSpan" id="kobo.271.1">. </span><span class="koboSpan" id="kobo.271.2">Typically, the number of output feature maps is </span><em class="italic"><span class="koboSpan" id="kobo.272.1">C=2048</span></em><span class="koboSpan" id="kobo.273.1">, and their height and width are </span><span class="koboSpan" id="kobo.274.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/866.png" style="vertical-align:-0.340em;height:1.004em;width:4.683em"/></span><span class="koboSpan" id="kobo.275.1"> and </span><span class="koboSpan" id="kobo.276.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/867.png" style="vertical-align:-0.340em;height:1.004em;width:5.097em"/></span><span class="koboSpan" id="kobo.277.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">respectively.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.279.1">However, the three-dimensional (excluding the batch dimension) backbone output is incompatible with the expected input tensor of the encoder, which should be a one-dimensional input sequence of </span><span class="koboSpan" id="kobo.280.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/868.png" style="vertical-align:-0.340em;height:1.051em;width:1.787em"/></span><span class="koboSpan" id="kobo.281.1">-sized embedding tensors (</span><span class="koboSpan" id="kobo.282.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/869.png" style="vertical-align:-0.340em;height:1.051em;width:3.848em"/></span><span class="koboSpan" id="kobo.283.1">). </span><span class="koboSpan" id="kobo.283.2">To solve this, the model applies 1×1 bottleneck convolution, which downsamples the number of channels from </span><em class="italic"><span class="koboSpan" id="kobo.284.1">C</span></em><span class="koboSpan" id="kobo.285.1"> to </span><span class="koboSpan" id="kobo.286.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/870.png" style="vertical-align:-0.340em;height:1.051em;width:1.858em"/></span><span class="koboSpan" id="kobo.287.1">, followed by a flattening operation. </span><span class="koboSpan" id="kobo.287.2">The transformed tensor becomes </span><span class="koboSpan" id="kobo.288.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/871.png" style="vertical-align:-0.340em;height:1.088em;width:5.791em"/></span><span class="koboSpan" id="kobo.289.1">, which we can use as a transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">input sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.291.1">Next, let’s focus on the actual transformer, which is displayed in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1047">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 9.4 – DetR transformer in detail. Inspired by https://arxiv.org/abs/2005.12872" src="image/B19627_09_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 9.4 – DetR transformer in detail. </span><span class="koboSpan" id="kobo.294.2">Inspired by https://arxiv.org/abs/2005.12872</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.295.1">The encoder maps the input sequence to a sequence of continuous representations, just like the original encoder (</span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.296.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.297.1">). </span><span class="koboSpan" id="kobo.297.2">One difference is that the model adds fixed absolute positional encodings to each </span><strong class="bold"><span class="koboSpan" id="kobo.298.1">Q</span></strong><span class="koboSpan" id="kobo.299.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.300.1">K</span></strong><span class="koboSpan" id="kobo.301.1"> tensor of all attention layers of the encoder, as opposed to static positional encodings</span><a id="_idIndexMarker1319"/><span class="koboSpan" id="kobo.302.1"> added only to the initial input tensor of the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">original transformer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.304.1">The decoder is where it gets more interesting. </span><span class="koboSpan" id="kobo.304.2">First, let’s note that the fixed positional encodings also participate in the decoder’s encoder-decoder attention block. </span><span class="koboSpan" id="kobo.304.3">Since they participate in all self-attention blocks of the encoder, we propagate them to the encoder-decoder attention to level the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">playing field.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.306.1">Next, the encoder</span><a id="_idIndexMarker1320"/><span class="koboSpan" id="kobo.307.1"> takes as input a sequence of </span><em class="italic"><span class="koboSpan" id="kobo.308.1">N</span></em> <strong class="bold"><span class="koboSpan" id="kobo.309.1">object queries</span></strong><span class="koboSpan" id="kobo.310.1">, represented by tensors, </span><span class="koboSpan" id="kobo.311.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/872.png" style="vertical-align:-0.340em;height:1.088em;width:5.393em"/></span><span class="koboSpan" id="kobo.312.1">. </span><br/><span class="koboSpan" id="kobo.313.1">We can think of them as slots, which the model uses to detect objects. </span><span class="koboSpan" id="kobo.313.2">The model output for each input object query represents the properties (bounding box and class) of one detected object. </span><span class="koboSpan" id="kobo.313.3">Having </span><em class="italic"><span class="koboSpan" id="kobo.314.1">N</span></em><span class="koboSpan" id="kobo.315.1"> object queries means that the model can detect </span><em class="italic"><span class="koboSpan" id="kobo.316.1">N</span></em><span class="koboSpan" id="kobo.317.1"> objects at most. </span><span class="koboSpan" id="kobo.317.2">Because of this, the paper’s authors propose to use </span><em class="italic"><span class="koboSpan" id="kobo.318.1">N</span></em><span class="koboSpan" id="kobo.319.1">, which is significantly larger than the typical number of objects in an image. </span><span class="koboSpan" id="kobo.319.2">Unlike the original transformer, the decoder’s attention here isn’t masked, so it can detect all objects in parallel rather </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">than sequentially.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.321.1">At the start of the training</span><a id="_idIndexMarker1321"/><span class="koboSpan" id="kobo.322.1"> process, the object query tensors are initialized randomly. </span><span class="koboSpan" id="kobo.322.2">The training itself updates both the model weights and the query tensors – that is, the model learns the object queries alongside the model weights. </span><span class="koboSpan" id="kobo.322.3">They act as learned positional encodings of the detected objects and serve the same purpose as the initial fixed input positional encodings. </span><span class="koboSpan" id="kobo.322.4">Because of this, we add the object queries to the encoder-decoder attention and the self-attention layers of the decoder blocks in the same way we add the input positional encodings to the encoder. </span><span class="koboSpan" id="kobo.322.5">This architecture has a sort of </span><em class="italic"><span class="koboSpan" id="kobo.323.1">bug</span></em><span class="koboSpan" id="kobo.324.1"> – the very first self-attention layer of the first decoder block will take as input the same object query twice, making it useless. </span><span class="koboSpan" id="kobo.324.2">Empirical experiments show that this doesn’t degrade the model performance. </span><span class="koboSpan" id="kobo.324.3">For the sake of simplicity, the implementation doesn’t have a unique first decoder block without self-attention but uses the standard decoder </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">block instead.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.326.1">Encoding configurations</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.327.1">The model can work</span><a id="_idIndexMarker1322"/><span class="koboSpan" id="kobo.328.1"> with multiple configurations of the fixed and learned encodings: </span></p>
<ul>
<li class="callout" lang="en-GB"><span class="koboSpan" id="kobo.329.1">add both types of encodings only to input data; </span></li>
<li class="callout" lang="en-GB"><span class="koboSpan" id="kobo.330.1">add the fixed encodings to the input data and the learned encodings to the input and all decoder attention layers; </span></li>
<li class="callout" lang="en-GB"><span class="koboSpan" id="kobo.331.1">add the fixed encodings to the data and all encoder attention layers and the learned encodings only to the decoder input; </span></li>
<li class="callout" lang="en-GB"><span class="koboSpan" id="kobo.332.1">add both types of encodings to the input data and every attention layer of the encoder and the decoder. </span></li>
</ul>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.333.1">The model works best in the fourth configuration, but for the sake of simplicity, it can be implemented in </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">the first.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.335.1">The object queries make it possible to not impose prior geometric limitations such as grid cells and anchor boxes in YOLO. </span><span class="koboSpan" id="kobo.335.2">Instead, we specify only the maximum number of objects to detect and let the model do its magic. </span><span class="koboSpan" id="kobo.335.3">The learned queries tend to specialize over different regions of the image. </span><span class="koboSpan" id="kobo.335.4">However, this is a result of the training and the properties of the training dataset, as opposed to manually </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">crafted features.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.337.1">The model ends with a combination of two heads: a three-layer perceptron with ReLU activations and a separate FC layer. </span><span class="koboSpan" id="kobo.337.2">The perceptron</span><a id="_idIndexMarker1323"/><span class="koboSpan" id="kobo.338.1"> is called an FFN, which differs from the FFNs in the transformer blocks. </span><span class="koboSpan" id="kobo.338.2">It predicts the detected object bounding box height, width, and normalized center coordinates concerning the input image. </span><span class="koboSpan" id="kobo.338.3">The FC has softmax activation and predicts the class of the object. </span><span class="koboSpan" id="kobo.338.4">Like YOLO, it includes an additional special background class, which indicates that no object is detected within the slot. </span><span class="koboSpan" id="kobo.338.5">Having this class is even more necessary because some slots will inevitably be empty, as </span><em class="italic"><span class="koboSpan" id="kobo.339.1">N</span></em><span class="koboSpan" id="kobo.340.1"> is much larger than the number of objects in </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">the image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.342.1">Predicting a set of unrestricted bounding boxes poses a challenge to the training because it is not trivial to match the predicted boxes with the ground-truth ones. </span><span class="koboSpan" id="kobo.342.2">The first step is to pad the ground-truth boxes for each image with dummy entries, so the number of ground-truth boxes becomes </span><a id="_idIndexMarker1324"/><span class="koboSpan" id="kobo.343.1">equal to the number of predicted ones, </span><em class="italic"><span class="koboSpan" id="kobo.344.1">N</span></em><span class="koboSpan" id="kobo.345.1">. </span><span class="koboSpan" id="kobo.345.2">Next, the training uses one-to-one </span><strong class="bold"><span class="koboSpan" id="kobo.346.1">bipartite matching</span></strong><span class="koboSpan" id="kobo.347.1"> between the predicted and ground-truth boxes. </span><span class="koboSpan" id="kobo.347.2">Finally, the algorithm supervises each predicted box to be closer to the ground-truth box it was matched to. </span><span class="koboSpan" id="kobo.347.3">You can check out the paper</span><a id="_idIndexMarker1325"/><span class="koboSpan" id="kobo.348.1"> for more details on </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">the training.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.350.1">DetR for image segmentation</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.351.1">The authors of DetR extend the model</span><a id="_idIndexMarker1326"/><span class="koboSpan" id="kobo.352.1"> for image segmentation. </span><span class="koboSpan" id="kobo.352.2">The relationship</span><a id="_idIndexMarker1327"/><span class="koboSpan" id="kobo.353.1"> between DetR for detection and segmentation is similar to the one between Faster R-CNN and Mask R-CNN (</span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.354.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.355.1">). </span><span class="koboSpan" id="kobo.355.2">DetR for segmentation adds a third head that’s implemented with upsampling convolutions. </span><span class="koboSpan" id="kobo.355.3">It produces binary segmentation masks for each detected object in parallel. </span><span class="koboSpan" id="kobo.355.4">The final result merges all masks using </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">pixel-wise argmax.</span></span></p>
<h2 id="_idParaDest-168" lang="en-GB"><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.357.1">Using DetR with Hugging Face Transformers</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.358.1">In this section, we’ll implement</span><a id="_idIndexMarker1328"/><span class="koboSpan" id="kobo.359.1"> a basic example of DetR object detection</span><a id="_idIndexMarker1329"/><span class="koboSpan" id="kobo.360.1"> with the help of Hugging Face Transformers and its </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">pipeline</span></strong><span class="koboSpan" id="kobo.362.1"> abstraction, which we introduced in </span><a href="B19627_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.363.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.364.1">. </span><span class="koboSpan" id="kobo.364.2">This example follows the ViT pattern, so we’ll include the full code without any comments. </span><span class="koboSpan" id="kobo.364.3">Here </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">it is:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.366.1">
from transformers import pipeline
obj_detection_pipeline = pipeline(
     task="object-detection",
     model="facebook/detr-resnet-50")
obj_detection_pipeline("https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Left_side_of_Flying_Pigeon.jpg/640px-Left_side_of_Flying_Pigeon.jpg")</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.367.1">The last call returns</span><a id="_idIndexMarker1330"/><span class="koboSpan" id="kobo.368.1"> a list of detected objects</span><a id="_idIndexMarker1331"/><span class="koboSpan" id="kobo.369.1"> in the </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">following form:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.371.1">
{'score': 0.997983455657959,
  'label': 'bicycle',
  'box': {'xmin': 16, 'ymin': 14, 'xmax': 623, 'ymax': 406}}</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.372.1">Next, we can see the model definition with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">print(obj_detection_pipeline.model)</span></strong><span class="koboSpan" id="kobo.374.1"> command. </span><span class="koboSpan" id="kobo.374.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.375.1">in_f</span></strong><span class="koboSpan" id="kobo.376.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.377.1">out_f</span></strong><span class="koboSpan" id="kobo.378.1"> are shortened for </span><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">in_features</span></strong><span class="koboSpan" id="kobo.380.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">out_features</span></strong><span class="koboSpan" id="kobo.382.1">, respectively. </span><span class="koboSpan" id="kobo.382.2">DetR uses bias in all </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">Linear</span></strong><span class="koboSpan" id="kobo.384.1"> layers (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">bias=True</span></strong><span class="koboSpan" id="kobo.386.1"> input parameter is not displayed). </span><span class="koboSpan" id="kobo.386.2">We’ll omit the </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">backbone definition.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.388.1">Let’s discuss the model elements in the order that they appear, starting with the 1×1 bottleneck convolution (we </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">have </span></span><span class="No-Break"><span class="koboSpan" id="kobo.390.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;256&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/873.png" style="vertical-align:-0.340em;height:1.051em;width:4.838em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">):</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.392.1">
(input_projection): Conv2d(2048, 256,
                   kernel_size=(1, 1),
                   stride=(1, 1))</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.393.1">Next, we have the object query embedding (</span><em class="italic"><span class="koboSpan" id="kobo.394.1">N=100</span></em><span class="koboSpan" id="kobo.395.1">). </span><span class="koboSpan" id="kobo.395.2">As we mentioned, the object queries are learned alongside the weight updates </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">during training:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.397.1">
(query_position_embeddings): Embedding(100, 256)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.398.1">The following is the encoder with six post-ln encoder blocks, ReLU activation, and FFN with one 2,048-dimensional hidden layer. </span><span class="koboSpan" id="kobo.398.2">Note that the positional encodings are not displayed (the same applies to </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">the decoder):</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.400.1">
(encoder): DetrEncoder(
  (layers): ModuleList(
     (0-5): 6 x DetrEncoderLayer(
        (self_attn): DetrAttention(
           (k_proj): Linear(in_f=256, out_f=256)
           (v_proj): Linear(in_f=256, out_f=256)
           (q_proj): Linear(in_f=256, out_f=256)
           (out_proj): Linear(in_f=256, out_f=256)
        )
        (self_attn_layer_norm): LayerNorm((256,))
        (activation_fn): ReLU()
        (fc1): Linear(in_f=256, out_f=2048)
        (fc2): Linear(in_f=2048, out_f=256)
        (final_layer_norm): LayerNorm((256,))
     )
  )
)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.401.1">Then, we have the decoder</span><a id="_idIndexMarker1332"/><span class="koboSpan" id="kobo.402.1"> with six post-ln decoder</span><a id="_idIndexMarker1333"/><span class="koboSpan" id="kobo.403.1"> blocks and the same properties as </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">the encoder:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.405.1">
(decoder): DetrDecoder(
  (layers): ModuleList(
     (0-5): 6 x DetrDecoderLayer(
        (self_attn): DetrAttention(
           (k_proj): Linear(in_f=256, out_f=256)
           (v_proj): Linear(in_f=256, out_f=256)
           (q_proj): Linear(in_f=256, out_f=256)
           (out_proj): Linear(in_f=256, out_f=256)
        )
        (activation_fn): ReLU()
        (self_attn_layer_norm): LayerNorm((256,))
        (encoder_attn): DetrAttention(
           (k_proj): Linear(in_f=256, out_f=256)
           (v_proj): Linear(in_f=256, out_f=256)
           (q_proj): Linear(in_f=256, out_f=256)
           (out_proj): Linear(in_f=256, out_f=256)
        )
        (encoder_attn_layer_norm): LayerNorm((256,))
        (fc1): Linear(in_f=256, out_f=2048)
        (fc2): Linear(in_f=2048, out_f=256)
        (final_layer_norm): LayerNorm((256,))
     )
  )
  (layernorm): LayerNorm((256,))
)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.406.1">Finally, we have the output FFN</span><a id="_idIndexMarker1334"/><span class="koboSpan" id="kobo.407.1"> and linear layer. </span><span class="koboSpan" id="kobo.407.2">The FFN</span><a id="_idIndexMarker1335"/><span class="koboSpan" id="kobo.408.1"> outputs four values (the bounding box coordinates), and the linear layer can detect 91 classes and </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">the background:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.410.1">
(class_labels_classifier): Linear(in_f=256, out_f=92)
(bbox_predictor): DetrMLPPredictionHead(
  (layers): ModuleList(
     (0-1): 2 x Linear(in_f=256, out_f=256)
     (2): Linear(in_f=256, out_f=4)
  )
)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.411.1">Next, let’s see how</span><a id="_idIndexMarker1336"/><span class="koboSpan" id="kobo.412.1"> we can generate</span><a id="_idIndexMarker1337"/><span class="koboSpan" id="kobo.413.1"> new images </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">with transformers.</span></span></p>
<h1 id="_idParaDest-169" lang="en-GB"><a id="_idTextAnchor243"/><span class="koboSpan" id="kobo.415.1">Generating images with stable diffusion</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.416.1">In this section, we’ll introduce </span><strong class="bold"><span class="koboSpan" id="kobo.417.1">stable diffusion</span></strong><span class="koboSpan" id="kobo.418.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.419.1">SD</span></strong><span class="koboSpan" id="kobo.420.1">, </span><em class="italic"><span class="koboSpan" id="kobo.421.1">High-Resolution Image Synthesis with Latent Diffusion Models</span></em><span class="koboSpan" id="kobo.422.1">, </span><a href="https://arxiv.org/abs/2112.10752"><span class="koboSpan" id="kobo.423.1">https://arxiv.org/abs/2112.10752</span></a><span class="koboSpan" id="kobo.424.1">, </span><a href="https://github.com/Stability-AI/stablediffusion"><span class="koboSpan" id="kobo.425.1">https://github.com/Stability-AI/stablediffusion</span></a><span class="koboSpan" id="kobo.426.1">). </span><span class="koboSpan" id="kobo.426.2">This is a generative model that can synthesize images based</span><a id="_idIndexMarker1338"/><span class="koboSpan" id="kobo.427.1"> on text prompts or other types</span><a id="_idIndexMarker1339"/><span class="koboSpan" id="kobo.428.1"> of data (in this section, we’ll focus on the text-to-image scenario). </span><span class="koboSpan" id="kobo.428.2">To understand how it works, let’s start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1050">
<span class="koboSpan" id="kobo.430.1"><img alt="Figure 9.5 – Stable diffusion model and training. Inspired by https://arxiv.org/abs/2112.10752" src="image/B19627_09_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.431.1">Figure 9.5 – Stable diffusion model and training. </span><span class="koboSpan" id="kobo.431.2">Inspired by https://arxiv.org/abs/2112.10752</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.432.1">SD combines an autoencoder (</span><strong class="bold"><span class="koboSpan" id="kobo.433.1">AE</span></strong><span class="koboSpan" id="kobo.434.1">, the </span><em class="italic"><span class="koboSpan" id="kobo.435.1">Pixel space</span></em><span class="koboSpan" id="kobo.436.1"> section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.437.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.438.1">.5</span></em><span class="koboSpan" id="kobo.439.1">), denoising diffusion probabilistic models (</span><strong class="bold"><span class="koboSpan" id="kobo.440.1">DDPM</span></strong><span class="koboSpan" id="kobo.441.1"> or simply </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">DM</span></strong><span class="koboSpan" id="kobo.443.1">, the </span><em class="italic"><span class="koboSpan" id="kobo.444.1">Latent distribution space</span></em><span class="koboSpan" id="kobo.445.1"> section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.446.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.447.1">.5</span></em><span class="koboSpan" id="kobo.448.1"> and </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.449.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.450.1">), and transformers (the </span><em class="italic"><span class="koboSpan" id="kobo.451.1">Conditioning</span></em><span class="koboSpan" id="kobo.452.1"> section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.453.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.454.1">.5</span></em><span class="koboSpan" id="kobo.455.1">). </span><span class="koboSpan" id="kobo.455.2">Before we dive into each</span><a id="_idIndexMarker1340"/><span class="koboSpan" id="kobo.456.1"> of these components, let’s outline their role in the training and inference pipelines of SD. </span><span class="koboSpan" id="kobo.456.2">Training</span><a id="_idIndexMarker1341"/><span class="koboSpan" id="kobo.457.1"> involves all of them – AE encoder, forward diffusion, reverse diffusion (</span><strong class="bold"><span class="koboSpan" id="kobo.458.1">U-Net</span></strong><span class="koboSpan" id="kobo.459.1">, </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.460.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.461.1">), AE decoder, and conditioning. </span><span class="koboSpan" id="kobo.461.2">Inference (generating images from text) only involves reverse diffusion, AE decoder, and conditioning. </span><span class="koboSpan" id="kobo.461.3">Don’t worry if you don’t understand everything you just read, as we’ll go into more detail in the following </span><a id="_idIndexMarker1342"/><span class="koboSpan" id="kobo.462.1">sections. </span><span class="koboSpan" id="kobo.462.2">We’ll start with the AE, continue</span><a id="_idIndexMarker1343"/><span class="koboSpan" id="kobo.463.1"> with the conditioning transformer, and combine it when we discuss the </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">diffusion process.</span></span></p>
<h2 id="_idParaDest-170" lang="en-GB"><a id="_idTextAnchor244"/><span class="koboSpan" id="kobo.465.1">Autoencoder</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.466.1">Although we mentioned</span><a id="_idIndexMarker1344"/><span class="koboSpan" id="kobo.467.1"> AEs briefly in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.468.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.469.1">, we’ll introduce this architecture in more detail here, starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1051">
<span class="koboSpan" id="kobo.471.1"><img alt="Figure 9.6 – An ﻿AE﻿" src="image/B19627_09_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.472.1">Figure 9.6 – An AE</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.473.1">An AE is a feed-forward neural network that tries to reproduce its input. </span><span class="koboSpan" id="kobo.473.2">In other words, an AE’s target value (label), </span><strong class="bold"><span class="koboSpan" id="kobo.474.1">y</span></strong><span class="koboSpan" id="kobo.475.1">, equals the input data, </span><strong class="bold"><span class="koboSpan" id="kobo.476.1">x</span></strong><span class="koboSpan" id="kobo.477.1">. </span><span class="koboSpan" id="kobo.477.2">We can formally say that it tries to learn an identity function, </span><span class="koboSpan" id="kobo.478.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/874.png" style="vertical-align:-0.402em;height:1.113em;width:5.279em"/></span><span class="koboSpan" id="kobo.479.1"> (a function that repeats its input). </span><span class="koboSpan" id="kobo.479.2">In its most basic form, an AE consists of hidden </span><br/><span class="koboSpan" id="kobo.480.1">(or bottleneck) and output layers (</span><strong class="bold"><span class="koboSpan" id="kobo.481.1">W</span></strong><span class="koboSpan" id="kobo.482.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.483.1">W</span></strong><span class="koboSpan" id="kobo.484.1">’ are the weight matrices of these layers). </span><span class="koboSpan" id="kobo.484.2">Like U-Net, we can think of the autoencoder</span><a id="_idIndexMarker1345"/><span class="koboSpan" id="kobo.485.1"> as a virtual composition of </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">two components:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.487.1">Encoder</span></strong><span class="koboSpan" id="kobo.488.1">: This maps the input data to the network’s internal latent representation. </span><span class="koboSpan" id="kobo.488.2">For the sake</span><a id="_idIndexMarker1346"/><span class="koboSpan" id="kobo.489.1"> of simplicity, in this example, the encoder</span><a id="_idIndexMarker1347"/><span class="koboSpan" id="kobo.490.1"> is a single FC bottleneck layer. </span><span class="koboSpan" id="kobo.490.2">The internal state is just its activation tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.491.1">z</span></strong><span class="koboSpan" id="kobo.492.1">. </span><span class="koboSpan" id="kobo.492.2">The encoder can have multiple hidden layers, including convolutional ones (as in SD). </span><span class="koboSpan" id="kobo.492.3">In this case, </span><strong class="bold"><span class="koboSpan" id="kobo.493.1">z</span></strong><span class="koboSpan" id="kobo.494.1"> is the activation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">last layer.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.496.1">Decoder</span></strong><span class="koboSpan" id="kobo.497.1">: This tries to reconstruct the input from the network’s internal state, </span><strong class="bold"><span class="koboSpan" id="kobo.498.1">z</span></strong><span class="koboSpan" id="kobo.499.1">. </span><span class="koboSpan" id="kobo.499.2">The decoder can also have a complex</span><a id="_idIndexMarker1348"/><span class="koboSpan" id="kobo.500.1"> structure that typically mirrors</span><a id="_idIndexMarker1349"/><span class="koboSpan" id="kobo.501.1"> the encoder. </span><span class="koboSpan" id="kobo.501.2">While U-Net tries to translate the input image into a target image of some other domain (for example, a segmentation map), the autoencoder simply tries to reconstruct </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">its input.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.503.1">We can train the autoencoder</span><a id="_idIndexMarker1350"/><span class="koboSpan" id="kobo.504.1"> by minimizing a loss function, known as the </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">reconstruction error</span></strong><span class="koboSpan" id="kobo.506.1">. </span><span class="koboSpan" id="kobo.506.2">It measures the distance between the original input and </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">its reconstruction.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.508.1">The latent tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.509.1">z</span></strong><span class="koboSpan" id="kobo.510.1">, is the focus of the entire AE. </span><span class="koboSpan" id="kobo.510.2">The key is that the bottleneck layer has fewer units than the input/output ones. </span><span class="koboSpan" id="kobo.510.3">Because the model tries to reconstruct its input from a smaller feature space, we force it to learn only the most important features of the data. </span><span class="koboSpan" id="kobo.510.4">Think of the compact data representation as a form of compression (but not lossless). </span><span class="koboSpan" id="kobo.510.5">We can use only the encoder part of the model to generate latent tensors for downstream tasks. </span><span class="koboSpan" id="kobo.510.6">Alternatively, we can use only the decoder to synthesize new images from generated </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">latent tensors.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.512.1">During training, the encoder maps the input sample to the latent space, where each latent attribute has a discrete value. </span><span class="koboSpan" id="kobo.512.2">An input sample can have only one latent representation. </span><span class="koboSpan" id="kobo.512.3">Therefore, the decoder can reconstruct the input in only one possible way. </span><span class="koboSpan" id="kobo.512.4">In other words, we can generate a single reconstruction of one input sample. </span><span class="koboSpan" id="kobo.512.5">However, we want to generate new images conditioned on text prompts rather than recreating the original ones. </span><span class="koboSpan" id="kobo.512.6">One possible solution to this task is </span><strong class="bold"><span class="koboSpan" id="kobo.513.1">variational autoencoders</span></strong><span class="koboSpan" id="kobo.514.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.515.1">VAEs</span></strong><span class="koboSpan" id="kobo.516.1">). </span><span class="koboSpan" id="kobo.516.2">A VAE can describe the latent representation in probabilistic terms. </span><span class="koboSpan" id="kobo.516.3">Instead</span><a id="_idIndexMarker1351"/><span class="koboSpan" id="kobo.517.1"> of discrete values, we’ll have a probability distribution for each latent attribute, making the latent space continuous. </span><span class="koboSpan" id="kobo.517.2">We can modify the latent tensor to influence the probability distribution (that is, the properties) of the generated image. </span><span class="koboSpan" id="kobo.517.3">In SD, the DM component, combined with the conditioning text prompts, acts as </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">this modifier.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.519.1">With this short detour completed, let’s discuss the role of the convolutional encoder in SD (the </span><em class="italic"><span class="koboSpan" id="kobo.520.1">Pixel space</span></em><span class="koboSpan" id="kobo.521.1"> section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.522.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.523.1">.5</span></em><span class="koboSpan" id="kobo.524.1">). </span><span class="koboSpan" id="kobo.524.2">During training, the AE encoder creates a compressed initial latent representation tensor, </span><span class="koboSpan" id="kobo.525.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/875.png" style="vertical-align:-0.027em;height:0.775em;width:4.560em"/></span><span class="koboSpan" id="kobo.526.1">, of the input image, </span><span class="koboSpan" id="kobo.527.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/876.png" style="vertical-align:-0.027em;height:0.738em;width:4.964em"/></span><span class="koboSpan" id="kobo.528.1">. </span><span class="koboSpan" id="kobo.528.2">More specifically, the encoder downsamples the image by a factor, </span><em class="italic"><span class="koboSpan" id="kobo.529.1">f = H/h = W/w</span></em><span class="koboSpan" id="kobo.530.1">, where </span><span class="koboSpan" id="kobo.531.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/877.png" style="vertical-align:-0.257em;height:0.968em;width:2.699em"/></span><span class="koboSpan" id="kobo.532.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.533.1">m</span></em><span class="koboSpan" id="kobo.534.1"> is an integer selected by empirical experiments). </span><span class="koboSpan" id="kobo.534.2">Then, the entire diffusion process (forward and reverse) works with the compressed </span><strong class="bold"><span class="koboSpan" id="kobo.535.1">z</span></strong><span class="koboSpan" id="kobo.536.1"> rather than the original image, </span><strong class="bold"><span class="koboSpan" id="kobo.537.1">x</span></strong><span class="koboSpan" id="kobo.538.1">. </span><span class="koboSpan" id="kobo.538.2">Only when the reverse diffusion ends does the AE decoder upsample the newly generated representation, </span><strong class="bold"><span class="koboSpan" id="kobo.539.1">z</span></strong><span class="koboSpan" id="kobo.540.1">, into the final generated image, </span><span class="koboSpan" id="kobo.541.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;~&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/878.png" style="vertical-align:-0.000em;height:0.630em;width:0.577em"/></span><span class="koboSpan" id="kobo.542.1">. </span><span class="koboSpan" id="kobo.542.2">In this way, the smaller </span><strong class="bold"><span class="koboSpan" id="kobo.543.1">z</span></strong><span class="koboSpan" id="kobo.544.1"> allows the use of a smaller and more computationally efficient U-Net, which benefits both the training</span><a id="_idIndexMarker1352"/><span class="koboSpan" id="kobo.545.1"> and the inference. </span><span class="koboSpan" id="kobo.545.2">The paper’s authors refer to this combination of AEs and diffusion models as </span><strong class="bold"><span class="koboSpan" id="kobo.546.1">latent </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.547.1">diffusion models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.549.1">The AE training is separate from the U-Net training. </span><span class="koboSpan" id="kobo.549.2">Because of this, we can train the AE once and then use it for multiple downstream</span><a id="_idIndexMarker1353"/><span class="koboSpan" id="kobo.550.1"> tasks with different </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">U-Net configurations.</span></span></p>
<h2 id="_idParaDest-171" lang="en-GB"><a id="_idTextAnchor245"/><span class="koboSpan" id="kobo.552.1">Conditioning transformer</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.553.1">The conditioning transformer, </span><span class="koboSpan" id="kobo.554.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/879.png" style="vertical-align:-0.340em;height:0.801em;width:0.702em"/></span><span class="koboSpan" id="kobo.555.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.556.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.557.1">.5</span></em><span class="koboSpan" id="kobo.558.1">), produces a latent representation of the text description</span><a id="_idIndexMarker1354"/><span class="koboSpan" id="kobo.559.1"> of the desired image. </span><span class="koboSpan" id="kobo.559.2">SD provides this representation to the U-Net so that it can influence its output. </span><span class="koboSpan" id="kobo.559.3">For this to work, the text latent representation has to live in the same semantic (not just dimensional) space as the image latent representation</span><a id="_idIndexMarker1355"/><span class="koboSpan" id="kobo.560.1"> of the U-Net. </span><span class="koboSpan" id="kobo.560.2">To achieve this, the latest version of SD, 2.1, uses the OpenCLIP open source model as a conditioning transformer (</span><em class="italic"><span class="koboSpan" id="kobo.561.1">Reproducible scaling laws for contrastive language-image learning</span></em><span class="koboSpan" id="kobo.562.1">, https://arxiv.org/abs/2212.07143). </span><strong class="bold"><span class="koboSpan" id="kobo.563.1">CLIP</span></strong><span class="koboSpan" id="kobo.564.1"> stands for </span><strong class="bold"><span class="koboSpan" id="kobo.565.1">contrastive language-image pre-training</span></strong><span class="koboSpan" id="kobo.566.1">. </span><span class="koboSpan" id="kobo.566.2">This technique was introduced by OpenAI (</span><em class="italic"><span class="koboSpan" id="kobo.567.1">Learning Transferable Visual Models From Natural Language Supervision</span></em><span class="koboSpan" id="kobo.568.1">, https://arxiv.org/abs/2103.00020). </span><span class="koboSpan" id="kobo.568.2">Let’s discuss it in more detail, starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer1058">
<span class="koboSpan" id="kobo.570.1"><img alt="Figure 9.7 – CLIP. Inspired by https://arxiv.org/abs/2103.00020" src="image/B19627_09_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.571.1">Figure 9.7 – CLIP. </span><span class="koboSpan" id="kobo.571.2">Inspired by https://arxiv.org/abs/2103.00020</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.572.1">It has two </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">main components:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.574.1">Text encoder</span></strong><span class="koboSpan" id="kobo.575.1">: This is a transformer that takes a token-encoded text sequence as input</span><a id="_idIndexMarker1356"/><span class="koboSpan" id="kobo.576.1"> and outputs an embedding vector, </span><strong class="bold"><span class="koboSpan" id="kobo.577.1">t</span></strong><span class="koboSpan" id="kobo.578.1">. </span><span class="koboSpan" id="kobo.578.2">The output is simply the activation of the last layer of the last transformer block rather than a task-specific head. </span><span class="koboSpan" id="kobo.578.3">The text encoder follows the principles we described in the last two chapters. </span><span class="koboSpan" id="kobo.578.4">For example, OpenAI CLIP</span><a id="_idIndexMarker1357"/><span class="koboSpan" id="kobo.579.1"> uses a transformer-decoder with </span><strong class="bold"><span class="koboSpan" id="kobo.580.1">byte-pair encoding</span></strong><span class="koboSpan" id="kobo.581.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.582.1">BPE</span></strong><span class="koboSpan" id="kobo.583.1">) tokenization. </span><span class="koboSpan" id="kobo.583.2">The input token sequence could include the special </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">[EOS]</span></strong><span class="koboSpan" id="kobo.585.1"> token. </span><span class="koboSpan" id="kobo.585.2">The model output at this token serves as an embedding vector of the entire sequence. </span><span class="koboSpan" id="kobo.585.3">In the context of SD, we’re only interested in the text encoder, and all other components of the CLIP system are only necessary for </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">its training.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.587.1">Image encoder</span></strong><span class="koboSpan" id="kobo.588.1">: This is either a ViT or a CNN (most often ResNet). </span><span class="koboSpan" id="kobo.588.2">It takes an image as input</span><a id="_idIndexMarker1358"/><span class="koboSpan" id="kobo.589.1"> and outputs its embedding vector, </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">i</span></strong><span class="koboSpan" id="kobo.591.1">. </span><span class="koboSpan" id="kobo.591.2">Like the text encoder, this is the activation of the highest layer of the model and not a </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">task-specific head.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.593.1">For CLIP to work, the embedding</span><a id="_idIndexMarker1359"/><span class="koboSpan" id="kobo.594.1"> vectors of the two encoders must have the same size, </span><span class="koboSpan" id="kobo.595.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/870.png" style="vertical-align:-0.340em;height:1.051em;width:1.858em"/></span><span class="koboSpan" id="kobo.596.1">. </span><span class="koboSpan" id="kobo.596.2">When necessary (for example, in the case of the CNN image encoder), the encoder’s output tensor is flattened to a one-dimensional vector. </span><span class="koboSpan" id="kobo.596.3">If the dimensions of the two encoders still differ, we can add linear projections (FC layers) to </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">equalize them.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.598.1">Next, let’s focus on the actual pre-training algorithm. </span><span class="koboSpan" id="kobo.598.2">The training set contains </span><em class="italic"><span class="koboSpan" id="kobo.599.1">N</span></em><span class="koboSpan" id="kobo.600.1"> text-image pairs, where the text of each pair describes the content of its corresponding image. </span><span class="koboSpan" id="kobo.600.2">We feed all text representations to the text encoder and the images to the image encoder to produce the </span><span class="koboSpan" id="kobo.601.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/881.png" style="vertical-align:-0.340em;height:0.932em;width:1.345em"/></span><span class="koboSpan" id="kobo.602.1"> and </span><span class="koboSpan" id="kobo.603.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/882.png" style="vertical-align:-0.340em;height:0.996em;width:1.352em"/></span><span class="koboSpan" id="kobo.604.1"> embeddings, respectively. </span><span class="koboSpan" id="kobo.604.2">Then, we compute a cosine similarity between every two embedding vectors (a total of </span><em class="italic"><span class="koboSpan" id="kobo.605.1">N×N</span></em><span class="koboSpan" id="kobo.606.1"> similarity measurements). </span><span class="koboSpan" id="kobo.606.2">Within these measurements, we have </span><em class="italic"><span class="koboSpan" id="kobo.607.1">N</span></em><span class="koboSpan" id="kobo.608.1"> correctly matching text-image pairs (the table diagonal of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.609.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.610.1">.5</span></em><span class="koboSpan" id="kobo.611.1">) and </span><em class="italic"><span class="koboSpan" id="kobo.612.1">N×N-N</span></em><span class="koboSpan" id="kobo.613.1"> incorrect pairs (all pairs outside the table diagonal). </span><span class="koboSpan" id="kobo.613.2">The training updates the weights of the two encoders so that the similarity scores for the correct pairs are maximized and the incorrect ones are minimized. </span><span class="koboSpan" id="kobo.613.3">Should the training prove successful, we’ll have similar embeddings for text prompts that correctly describe what’s on the image and dissimilar embeddings in all other cases. </span><span class="koboSpan" id="kobo.613.4">During SD training, we optimize the text encoder alongside the U-Net (but not the full </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">CLIP system).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.615.1">Now that we know how to produce semantically</span><a id="_idIndexMarker1360"/><span class="koboSpan" id="kobo.616.1"> correct text embeddings, we can proceed with the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">diffusion model.</span></span></p>
<h2 id="_idParaDest-172" lang="en-GB"><a id="_idTextAnchor246"/><span class="koboSpan" id="kobo.618.1">Diffusion model</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.619.1">DM is a type of generative model</span><a id="_idIndexMarker1361"/><span class="koboSpan" id="kobo.620.1"> that has forward and reverse phases. </span><span class="koboSpan" id="kobo.620.2">Forward diffusion starts with the latent vector, </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">z</span></strong><span class="koboSpan" id="kobo.622.1">, produced by the AE encoder (which takes an image, </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">x</span></strong><span class="koboSpan" id="kobo.624.1">, as input). </span><span class="koboSpan" id="kobo.624.2">Then, it gradually adds random Gaussian noise to </span><strong class="bold"><span class="koboSpan" id="kobo.625.1">z</span></strong><span class="koboSpan" id="kobo.626.1"> through a series of </span><em class="italic"><span class="koboSpan" id="kobo.627.1">T</span></em><span class="koboSpan" id="kobo.628.1"> steps until the final (latent) representation, </span><span class="koboSpan" id="kobo.629.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/883.png" style="vertical-align:-0.333em;height:0.796em;width:0.844em"/></span><span class="koboSpan" id="kobo.630.1">, </span><br/><span class="koboSpan" id="kobo.631.1">is pure noise. </span><span class="koboSpan" id="kobo.631.2">Forward diffusion uses an accelerated algorithm, which produces </span><span class="koboSpan" id="kobo.632.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/884.png" style="vertical-align:-0.333em;height:0.796em;width:0.884em"/></span><span class="koboSpan" id="kobo.633.1"> in a single step instead of </span><em class="italic"><span class="koboSpan" id="kobo.634.1">T</span></em><span class="koboSpan" id="kobo.635.1"> steps (</span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.636.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.637.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.638.1">Reverse diffusion does the opposite and starts with pure noise. </span><span class="koboSpan" id="kobo.638.2">It gradually tries to restore the original latent tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.639.1">z</span></strong><span class="koboSpan" id="kobo.640.1">, by removing small amounts of noise in a series of </span><em class="italic"><span class="koboSpan" id="kobo.641.1">T</span></em><span class="koboSpan" id="kobo.642.1"> denoising steps. </span><span class="koboSpan" id="kobo.642.2">In practice, we’re interested in reverse diffusion to generate images based on latent representations (forward diffusion only participates in the training). </span><span class="koboSpan" id="kobo.642.3">It is usually implemented with a U-Net type of CNN (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.643.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.644.1">.5</span></em><span class="koboSpan" id="kobo.645.1">, </span><span class="koboSpan" id="kobo.646.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/885.png" style="vertical-align:-0.340em;height:0.799em;width:0.764em"/></span><span class="koboSpan" id="kobo.647.1">). </span><br/><span class="koboSpan" id="kobo.648.1">It takes the noise tensor, </span><span class="koboSpan" id="kobo.649.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/540.png" style="vertical-align:-0.340em;height:0.803em;width:0.673em"/></span><span class="koboSpan" id="kobo.650.1">, at step </span><em class="italic"><span class="koboSpan" id="kobo.651.1">t</span></em><span class="koboSpan" id="kobo.652.1"> as input and outputs an approximation of the noise added to the original latent tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.653.1">z</span></strong><span class="koboSpan" id="kobo.654.1"> (that is, only the noise and not the tensor itself). </span><span class="koboSpan" id="kobo.654.2">Then, we subtract the predicted noise from the current U-Net input, </span><span class="koboSpan" id="kobo.655.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/887.png" style="vertical-align:-0.340em;height:0.803em;width:0.877em"/></span><span class="koboSpan" id="kobo.656.1">, and feed the result as new input to the U-Net, </span><span class="koboSpan" id="kobo.657.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/888.png" style="vertical-align:-0.340em;height:0.803em;width:1.291em"/></span><span class="koboSpan" id="kobo.658.1">. </span><br/><span class="koboSpan" id="kobo.659.1">During training, the cost function measures the difference between the predicted and actual noise and accordingly updates the U-Net weights after each denoising step. </span><span class="koboSpan" id="kobo.659.2">This process continues until (hopefully) only the original tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.660.1">z</span></strong><span class="koboSpan" id="kobo.661.1">, remains. </span><span class="koboSpan" id="kobo.661.2">Then, the AE decoder uses it to produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">final image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.663.1">The pure form of DM has no way</span><a id="_idIndexMarker1362"/><span class="koboSpan" id="kobo.664.1"> to influence the properties of the generated image (this is known as conditioning) because we start with random noise, which results in random images. </span><span class="koboSpan" id="kobo.664.2">SD allows us to do just that – a way to condition the U-Net to generate images based on specific text prompts or other data types. </span><span class="koboSpan" id="kobo.664.3">To do this, we need to integrate the output embedding of the conditioning transformer with the denoising U-Net. </span><span class="koboSpan" id="kobo.664.4">Let’s assume that we have a text prompt, </span><strong class="bold"><span class="koboSpan" id="kobo.665.1">y</span></strong><span class="koboSpan" id="kobo.666.1">. </span><span class="koboSpan" id="kobo.666.2">We feed it to the conditioning transformer, </span><span class="koboSpan" id="kobo.667.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/889.png" style="vertical-align:-0.340em;height:0.801em;width:0.763em"/></span><span class="koboSpan" id="kobo.668.1">, which produces an output, </span><span class="koboSpan" id="kobo.669.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/890.png" style="vertical-align:-0.340em;height:1.088em;width:5.841em"/></span><span class="koboSpan" id="kobo.670.1">, where </span><span class="koboSpan" id="kobo.671.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/891.png" style="vertical-align:-0.340em;height:1.051em;width:0.764em"/></span><span class="koboSpan" id="kobo.672.1"> is the embedding size of the conditioning transformer, and </span><em class="italic"><span class="koboSpan" id="kobo.673.1">M</span></em><span class="koboSpan" id="kobo.674.1"> is the number of tokens of the input sequence. </span><span class="koboSpan" id="kobo.674.2">Usually, </span><em class="italic"><span class="koboSpan" id="kobo.675.1">M=1</span></em><span class="koboSpan" id="kobo.676.1"> because we only take the output at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">[EOS]</span></strong><span class="koboSpan" id="kobo.678.1"> token. </span><span class="koboSpan" id="kobo.678.2">Then, we map</span><a id="_idIndexMarker1363"/><span class="koboSpan" id="kobo.679.1"> its output to the intermediate layers of the U-Net via a </span><strong class="bold"><span class="koboSpan" id="kobo.680.1">cross-attention</span></strong><span class="koboSpan" id="kobo.681.1"> layer. </span><span class="koboSpan" id="kobo.681.2">In this layer, the key and value tensors represent the conditioning transformer outputs, and the query tensors represent the intermediate U-Net layers (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.682.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.683.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.685.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/892.png" style="vertical-align:-0.427em;height:1.162em;width:6.765em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.686.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/893.png" style="vertical-align:-0.340em;height:1.075em;width:6.486em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.687.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/894.png" style="vertical-align:-0.340em;height:1.075em;width:6.494em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.688.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.689.1">i</span></em><span class="koboSpan" id="kobo.690.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.691.1">i</span></em><span class="koboSpan" id="kobo.692.1">-th intermediate U-Net layer, </span><span class="koboSpan" id="kobo.693.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/895.png" style="vertical-align:-0.390em;height:1.138em;width:5.776em"/></span><span class="koboSpan" id="kobo.694.1"> is the flattened activation of that layer, and </span><span class="koboSpan" id="kobo.695.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/896.png" style="vertical-align:-0.402em;height:1.113em;width:1.011em"/></span><span class="koboSpan" id="kobo.696.1"> is the flattened activation tensor size. </span><span class="koboSpan" id="kobo.697.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/897.png" style="vertical-align:-0.427em;height:1.174em;width:5.257em"/></span><span class="koboSpan" id="kobo.698.1">, </span><span class="koboSpan" id="kobo.699.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/898.png" style="vertical-align:-0.333em;height:1.081em;width:5.077em"/></span><span class="koboSpan" id="kobo.700.1">, and </span><span class="koboSpan" id="kobo.701.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/899.png" style="vertical-align:-0.339em;height:1.086em;width:5.077em"/></span><span class="koboSpan" id="kobo.702.1"> are learnable projection matrices, where </span><em class="italic"><span class="koboSpan" id="kobo.703.1">d</span></em><span class="koboSpan" id="kobo.704.1"> is the chosen size of the actual cross-attention embedding. </span><span class="koboSpan" id="kobo.704.2">We have a unique set of three matrices for each of the </span><em class="italic"><span class="koboSpan" id="kobo.705.1">i</span></em><span class="koboSpan" id="kobo.706.1"> intermediate U-Net layers</span><a id="_idIndexMarker1364"/><span class="koboSpan" id="kobo.707.1"> with cross-attention. </span><span class="koboSpan" id="kobo.707.2">In its simplest form, we can add one or more cross-attention blocks after the output of an intermediate U-Net layer. </span><span class="koboSpan" id="kobo.707.3">The blocks can have a residual connection, which preserves the unmodified intermediate layer output and augments it with the attention vector. </span><span class="koboSpan" id="kobo.707.4">Note that the output of the intermediate convolutional layers has four dimensions: </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">[batch, channel, height, width]</span></strong><span class="koboSpan" id="kobo.709.1">. </span><span class="koboSpan" id="kobo.709.2">However, the standard attention blocks use two-dimensional input: </span><strong class="source-inline"><span class="koboSpan" id="kobo.710.1">[batch, dim]</span></strong><span class="koboSpan" id="kobo.711.1">. </span><span class="koboSpan" id="kobo.711.2">One solution is to flatten the convolutional output before feeding it to the attention block. </span><span class="koboSpan" id="kobo.711.3">Alternatively, we can preserve the channel dimension and only flatten the height and width: </span><strong class="source-inline"><span class="koboSpan" id="kobo.712.1">[batch, channel, height*width]</span></strong><span class="koboSpan" id="kobo.713.1">. </span><span class="koboSpan" id="kobo.713.2">In this case, we can assign one attention head to the output of each </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">convolutional channel.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.715.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.716.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.717.1">.5</span></em><span class="koboSpan" id="kobo.718.1"> has a </span><em class="italic"><span class="koboSpan" id="kobo.719.1">switch</span></em><span class="koboSpan" id="kobo.720.1"> component, which allows us to concatenate the text prompt representation and the U-Net input rather than using cross-attention in the intermediate layers. </span><span class="koboSpan" id="kobo.720.2">This use case is for tasks other than text-to-image, which is the focus</span><a id="_idIndexMarker1365"/><span class="koboSpan" id="kobo.721.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">this section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.723.1">With that, let’s see how to use SD </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">in practice.</span></span></p>
<h2 id="_idParaDest-173" lang="en-GB"><a id="_idTextAnchor247"/><span class="koboSpan" id="kobo.725.1">Using stable diffusion with Hugging Face Transformers</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.726.1">In this section, we’ll use SD</span><a id="_idIndexMarker1366"/><span class="koboSpan" id="kobo.727.1"> to generate an image conditioned </span><a id="_idIndexMarker1367"/><span class="koboSpan" id="kobo.728.1">on a text prompt. </span><span class="koboSpan" id="kobo.728.2">In addition to the Transformers library, we’ll also need </span><strong class="bold"><span class="koboSpan" id="kobo.729.1">Diffusers</span></strong><span class="koboSpan" id="kobo.730.1"> (https://github.com/huggingface/diffusers) – a library for pre-trained diffusion </span><a id="_idIndexMarker1368"/><span class="koboSpan" id="kobo.731.1">models for generating images and audio. </span><span class="koboSpan" id="kobo.731.2">Please note that the diffusers SD implementation requires the presence of a GPU. </span><span class="koboSpan" id="kobo.731.3">You can run this example in the Google Colab notebook with GPU enabled. </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.733.1">Do the </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">necessary imports:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.735.1">
import torch
from diffusers import StableDiffusionPipeline</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.736.1">Instantiate an SD pipeline (</span><strong class="source-inline"><span class="koboSpan" id="kobo.737.1">sd_pipe</span></strong><span class="koboSpan" id="kobo.738.1">) with SD version 2.1. </span><span class="koboSpan" id="kobo.738.2">We don’t use the main transformers </span><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">pipeline</span></strong><span class="koboSpan" id="kobo.740.1"> abstraction, which we used in the preceding examples. </span><span class="koboSpan" id="kobo.740.2">Instead, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.741.1">StableDiffusionPipeline</span></strong><span class="koboSpan" id="kobo.742.1">, which comes from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.743.1">diffusers</span></strong><span class="koboSpan" id="kobo.744.1"> library. </span><span class="koboSpan" id="kobo.744.2">We’ll also move the model to a </span><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">cuda</span></strong><span class="koboSpan" id="kobo.746.1"> device (an NVIDIA GPU) if it </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">is available:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.748.1">
sd_pipe = StableDiffusionPipeline.from_pretrained(
     "stabilityai/stable-diffusion-2-1",
     torch_dtype=torch.float16)
sd_pipe.to('cuda')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.749.1">Let’s run </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">sd_pipe</span></strong><span class="koboSpan" id="kobo.751.1"> for 100 denoising steps with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">text prompt:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.753.1">
prompt = \
  "High quality photo of a racing car on a track"
image = sd_pipe(
     prompt,
     num_inference_steps=100).images[0]</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.754.1">The generated </span><strong class="source-inline"><span class="koboSpan" id="kobo.755.1">image</span></strong><span class="koboSpan" id="kobo.756.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">as follows:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer1079">
<span class="koboSpan" id="kobo.758.1"><img alt="Figure 9.8 – SD-generated image" src="image/B19627_09_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.759.1">Figure 9.8 – SD-generated image</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.760.1">Unfortunately, the AE, U-Net, and</span><a id="_idIndexMarker1369"/><span class="koboSpan" id="kobo.761.1"> conditioning transformer</span><a id="_idIndexMarker1370"/><span class="koboSpan" id="kobo.762.1"> descriptions are large, and it would be impractical to include them here. </span><span class="koboSpan" id="kobo.762.2">Still, they are available in the Jupyter Notebook. </span><span class="koboSpan" id="kobo.762.3">Nevertheless, we can see a shortened summary of the entire SD pipeline with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">print(sd_pipe)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.764.1"> command:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.765.1">
StableDiffusionPipeline {
  "safety_checker": [null, null],
  "tokenizer": ["transformers", "CLIPTokenizer"],
  "text_encoder": ["transformers", "CLIPTextModel"],
  "unet": ["diffusers", "UNet2DConditionModel"],
  "vae": ["diffusers", "AutoencoderKL"],
  "scheduler": ["diffusers", "DDIMScheduler"]
}</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.766.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.767.1">transformers</span></strong><span class="koboSpan" id="kobo.768.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.769.1">diffusers</span></strong><span class="koboSpan" id="kobo.770.1"> refer to the package of origin for the </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">given component.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.772.1">The first component is an optional </span><strong class="source-inline"><span class="koboSpan" id="kobo.773.1">safety_checker</span></strong><span class="koboSpan" id="kobo.774.1"> (not initialized), which</span><a id="_idIndexMarker1371"/><span class="koboSpan" id="kobo.775.1"> can identify </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">not-safe-for-work</span></strong><span class="koboSpan" id="kobo.777.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.778.1">NSFW</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">) images.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.780.1">Next, we have a BPE-based </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">CLIPTokenizer</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.782.1">tokenizer</span></strong><span class="koboSpan" id="kobo.783.1">, with a token vocabulary size of around 50,000 tokens. </span><span class="koboSpan" id="kobo.783.2">It tokenizes the text prompt and feeds it to </span><strong class="source-inline"><span class="koboSpan" id="kobo.784.1">text_encoder</span></strong><span class="koboSpan" id="kobo.785.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.786.1">CLIPTextModel</span></strong><span class="koboSpan" id="kobo.787.1">. </span><span class="koboSpan" id="kobo.787.2">The Hugging Face </span><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">CLIPTextModel</span></strong><span class="koboSpan" id="kobo.789.1"> duplicates the OpenAI CLIP transformer-decoder (the model card is available </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">at </span></span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">https://huggingface.co/openai/clip-vit-large-patch14</span></span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.793.1">Then, we have </span><strong class="source-inline"><span class="koboSpan" id="kobo.794.1">UNet2DConditionModel</span></strong><span class="koboSpan" id="kobo.795.1">. </span><span class="koboSpan" id="kobo.795.2">The convolutional portions</span><a id="_idIndexMarker1372"/><span class="koboSpan" id="kobo.796.1"> of the U-Net use residual</span><a id="_idIndexMarker1373"/><span class="koboSpan" id="kobo.797.1"> blocks (</span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.798.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.799.1">). </span><span class="koboSpan" id="kobo.799.2">It has four downsampling blocks with a downsampling factor of 2 (implemented with convolutions with stride 2). </span><span class="koboSpan" id="kobo.799.3">The first three include </span><strong class="source-inline"><span class="koboSpan" id="kobo.800.1">text_encoder</span></strong><span class="koboSpan" id="kobo.801.1"> cross-attention layers. </span><span class="koboSpan" id="kobo.801.2">Then, we have a single mid-block, which preserves input size and contains one residual and one cross-attention sublayer. </span><span class="koboSpan" id="kobo.801.3">The model </span><a id="_idIndexMarker1374"/><span class="koboSpan" id="kobo.802.1">ends with four skip-connected upsampling blocks, symmetrical to the downsampling sequence. </span><span class="koboSpan" id="kobo.802.2">The last three blocks also include cross-attention layers. </span><span class="koboSpan" id="kobo.802.3">The model uses </span><strong class="bold"><span class="koboSpan" id="kobo.803.1">sigmoid linear unit</span></strong><span class="koboSpan" id="kobo.804.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.805.1">SiLU</span></strong><span class="koboSpan" id="kobo.806.1">, </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.807.1">Chapter </span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.808.1">3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.809.1">) activations.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.810.1">Next, we have the convolutional autoencoder, </span><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">AutoencoderKL</span></strong><span class="koboSpan" id="kobo.812.1">, with four downsampling residual blocks, one residual mid-block (the same as the one in U-Net), four upsampling residual blocks (symmetrical to the downsampling sequence), and </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">SiLU activations.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.814.1">Finally, let’s focus on </span><strong class="source-inline"><span class="koboSpan" id="kobo.815.1">scheduler</span></strong><span class="koboSpan" id="kobo.816.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.817.1">DDIMScheduler</span></strong><span class="koboSpan" id="kobo.818.1">, which is part of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.819.1">diffusers</span></strong><span class="koboSpan" id="kobo.820.1"> library. </span><span class="koboSpan" id="kobo.820.2">It is one of multiple available schedulers. </span><span class="koboSpan" id="kobo.820.3">During training, a scheduler adds noise to a sample to train the DM. </span><span class="koboSpan" id="kobo.820.4">It defines how to update the latent tensor based on the U-Net output </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">during inference.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.822.1">Stable Diffusion XL</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.823.1">Recently, Stability AI released Stable Diffusion XL (</span><em class="italic"><span class="koboSpan" id="kobo.824.1">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</span></em><span class="koboSpan" id="kobo.825.1">, </span><a href="https://arxiv.org/abs/2307.01952"><span class="koboSpan" id="kobo.826.1">https://arxiv.org/abs/2307.01952</span></a><span class="koboSpan" id="kobo.827.1">). </span><span class="koboSpan" id="kobo.827.2">SDXL uses a three times larger U-Net. </span><span class="koboSpan" id="kobo.827.3">The larger size is due to more attention blocks and a larger attention context (the new version </span><a id="_idIndexMarker1375"/><span class="koboSpan" id="kobo.828.1">uses the concatenated outputs of two different text encoders). </span><span class="koboSpan" id="kobo.828.2">It also utilizes an optional </span><strong class="bold"><span class="koboSpan" id="kobo.829.1">refinement model</span></strong><span class="koboSpan" id="kobo.830.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.831.1">refiner</span></strong><span class="koboSpan" id="kobo.832.1">) – a second U-Net in the same latent space as the first, specializing in high-quality, high-resolution data. </span><span class="koboSpan" id="kobo.832.2">It takes the output latent representation, </span><strong class="bold"><span class="koboSpan" id="kobo.833.1">z</span></strong><span class="koboSpan" id="kobo.834.1">, of the first U-Net as input and uses the same conditioning </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">text prompt.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.836.1">With that, we’ve concluded</span><a id="_idIndexMarker1376"/><span class="koboSpan" id="kobo.837.1"> our introduction</span><a id="_idIndexMarker1377"/><span class="koboSpan" id="kobo.838.1"> to SD and the larger topic of transformers for computer vision. </span><span class="koboSpan" id="kobo.838.2">Next, let’s see how to fine-tune a </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">transformer-based model.</span></span></p>
<h1 id="_idParaDest-174" lang="en-GB"><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.840.1">Exploring fine-tuning transformers</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.841.1">In this section, we’ll use PyTorch to fine-tune</span><a id="_idIndexMarker1378"/><span class="koboSpan" id="kobo.842.1"> a pre-trained transformer. </span><span class="koboSpan" id="kobo.842.2">More</span><a id="_idIndexMarker1379"/><span class="koboSpan" id="kobo.843.1"> specifically, we’ll fine-tune a </span><strong class="bold"><span class="koboSpan" id="kobo.844.1">DistilBERT</span></strong><span class="koboSpan" id="kobo.845.1"> transformer-encoder (</span><em class="italic"><span class="koboSpan" id="kobo.846.1">DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter</span></em><span class="koboSpan" id="kobo.847.1">, </span><a href="https://arxiv.org/abs/1910.01108"><span class="koboSpan" id="kobo.848.1">https://arxiv.org/abs/1910.01108</span></a><span class="koboSpan" id="kobo.849.1">) to classify whether a movie review is positive or negative. </span><span class="koboSpan" id="kobo.849.2">We’ll use the Rotten Tomatoes dataset (</span><a href="https://huggingface.co/datasets/rotten_tomatoes"><span class="koboSpan" id="kobo.850.1">https://huggingface.co/datasets/rotten_tomatoes</span></a><span class="koboSpan" id="kobo.851.1">), which contains around 10,000 reviews, split equally between positive and negative (https://huggingface.co/datasets/rotten_tomatoes), licensed under Apache 2.0, derived from </span><a href="https://huggingface.co/datasets/rotten_tomatoes/blob/main/rotten_tomatoes.py"><span class="koboSpan" id="kobo.852.1">https://huggingface.co/datasets/rotten_tomatoes/blob/main/rotten_tomatoes.py</span></a><span class="koboSpan" id="kobo.853.1">. </span><span class="koboSpan" id="kobo.853.2">We’ll implement the example with the help of the Transformers library’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.854.1">Trainer</span></strong><span class="koboSpan" id="kobo.855.1"> class (</span><a href="https://huggingface.co/docs/transformers/main_classes/trainer"><span class="koboSpan" id="kobo.856.1">https://huggingface.co/docs/transformers/main_classes/trainer</span></a><span class="koboSpan" id="kobo.857.1">), which implements the basic training loop, model evaluation, distributed training on multiple GPUs/TPUs, mixed precision, and other training features. </span><span class="koboSpan" id="kobo.857.2">This is opposed to implementing the training from scratch, as we’ve been doing until now in our PyTorch examples. </span><span class="koboSpan" id="kobo.857.3">We’ll also need the </span><strong class="bold"><span class="koboSpan" id="kobo.858.1">Datasets</span></strong><span class="koboSpan" id="kobo.859.1"> (</span><a href="https://github.com/huggingface/datasets"><span class="koboSpan" id="kobo.860.1">https://github.com/huggingface/datasets</span></a><span class="koboSpan" id="kobo.861.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.862.1">Evaluate</span></strong><span class="koboSpan" id="kobo.863.1"> (</span><a href="https://github.com/huggingfahttps://github.com/huggingface/evaluate"><span class="koboSpan" id="kobo.864.1">https://github.com/huggingfahttps://github.com/huggingface/evaluate</span></a><span class="koboSpan" id="kobo.865.1">) packages. </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.867.1">Load the dataset, which is split into </span><strong class="source-inline"><span class="koboSpan" id="kobo.868.1">train</span></strong><span class="koboSpan" id="kobo.869.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.870.1">validation</span></strong><span class="koboSpan" id="kobo.871.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.872.1">test</span></strong><span class="koboSpan" id="kobo.873.1"> portions: </span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.874.1">
from datasets import load_dataset
dataset = load_dataset('rotten_tomatoes')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.875.1">Load the DistilBERT WordPiece </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">subword </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.877.1">tokenizer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.879.1">
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.880.1">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.881.1">tokenizer</span></strong><span class="koboSpan" id="kobo.882.1"> to tokenize</span><a id="_idIndexMarker1380"/><span class="koboSpan" id="kobo.883.1"> the dataset. </span><span class="koboSpan" id="kobo.883.2">In addition, it’ll pad or truncate each sample to the maximum length accepted by the model. </span><span class="koboSpan" id="kobo.883.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">batched=True</span></strong><span class="koboSpan" id="kobo.885.1"> mapping speeds up processing by combining the data in batches (as opposed to single samples). </span><span class="koboSpan" id="kobo.885.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.886.1">Tokenizers</span></strong><span class="koboSpan" id="kobo.887.1"> library works faster with batches because it parallelizes the tokenization of all the examples in </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">a batch:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.889.1">
tok_dataset = dataset.map(
    lambda x: tokenizer(
        text=x['text'],
        padding='max_length',
        truncation=True),
    batched=True)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.890.1">Load the </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">transformer </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.892.1">model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.894.1">
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(
    'distilbert-base-uncased')</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.895.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.896.1">AutoModelForSequenceClassification</span></strong><span class="koboSpan" id="kobo.897.1"> class loads DistilBERT configuration for binary classification – the model head has a hidden layer and an output layer with two units. </span><span class="koboSpan" id="kobo.897.2">This configuration works for our task because we have to classify the movie reviews into </span><span class="No-Break"><span class="koboSpan" id="kobo.898.1">two categories.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.899.1">Initialize </span><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">TrainingArguments</span></strong><span class="koboSpan" id="kobo.901.1"> of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.902.1">Trainer</span></strong><span class="koboSpan" id="kobo.903.1"> instance. </span><span class="koboSpan" id="kobo.903.2">We’ll specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">output_dir</span></strong><span class="koboSpan" id="kobo.905.1"> for the location of the model predictions and checkpoints. </span><span class="koboSpan" id="kobo.905.2">We’ll also run the evaluation</span><a id="_idIndexMarker1381"/><span class="koboSpan" id="kobo.906.1"> once </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">per epoch:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.908.1">
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir='test_trainer',
    evaluation_strategy='epoch')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.909.1">Initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.910.1">accuracy</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.911.1">evaluation metric:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.912.1">
import evaluate
accuracy = evaluate.load('accuracy')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.913.1">Initialize </span><strong class="source-inline"><span class="koboSpan" id="kobo.914.1">trainer</span></strong><span class="koboSpan" id="kobo.915.1"> with all the necessary components for training </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">and evaluation:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.917.1">
from transformers import Trainer
import numpy as np
trainer = Trainer(
    model=model,
    train_dataset=tok_dataset['train'],
    eval_dataset=tok_dataset['test'],
    args=training_args,
    compute_metrics=
        lambda x: accuracy.compute(
            predictions=x[0],
            references=x[1]),
    preprocess_logits_for_metrics=
        lambda x, _: np.argmax(x.cpu(), axis=-1)
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.918.1">It accepts the model, train, and evaluation datasets and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">training_args</span></strong><span class="koboSpan" id="kobo.920.1"> instance. </span><span class="koboSpan" id="kobo.920.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">compute_metrics</span></strong><span class="koboSpan" id="kobo.922.1"> function will compute the validation accuracy after each epoch. </span><strong class="source-inline"><span class="koboSpan" id="kobo.923.1">preprocess_logits_for_metrics</span></strong><span class="koboSpan" id="kobo.924.1"> will convert the one-hot encoded model output (</span><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">x[0]</span></strong><span class="koboSpan" id="kobo.926.1">) to indexed labels so that it can match the format of the ground-truth labels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.927.1">x[1]</span></strong><span class="koboSpan" id="kobo.928.1">) in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.929.1">compute_metrics</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.930.1"> function.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.931.1">Finally, we can run </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">the training:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.933.1">
trainer.train()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.934.1">The model will achieve</span><a id="_idIndexMarker1382"/><span class="koboSpan" id="kobo.935.1"> around 85% accuracy in </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">three epochs.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.937.1">Next, let’s see how to harness the power of LLMs with the </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">LangChain framework.</span></span></p>
<h1 id="_idParaDest-175" lang="en-GB"><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.939.1">Harnessing the power of LLMs with LangChain</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.940.1">LLMs are powerful</span><a id="_idIndexMarker1383"/><span class="koboSpan" id="kobo.941.1"> tools, yet they have some limitations. </span><span class="koboSpan" id="kobo.941.2">One of them is the context window length. </span><span class="koboSpan" id="kobo.941.3">For example, the maximum input sequence of Llama 2 is 4,096 tokens and even less in terms of words. </span><span class="koboSpan" id="kobo.941.4">As a reference, most of the chapters in this book hover around 10,000 words. </span><span class="koboSpan" id="kobo.941.5">Many tasks wouldn’t fit this length. </span><span class="koboSpan" id="kobo.941.6">Another LLM limitation is that its entire knowledge is stored within the model weights at training time. </span><span class="koboSpan" id="kobo.941.7">It has no direct way to interact with external data</span><a id="_idIndexMarker1384"/><span class="koboSpan" id="kobo.942.1"> sources, such as databases or service APIs. </span><span class="koboSpan" id="kobo.942.2">Therefore, the knowledge can be outdated or insufficient. </span><span class="koboSpan" id="kobo.942.3">The </span><strong class="bold"><span class="koboSpan" id="kobo.943.1">LangChain</span></strong><span class="koboSpan" id="kobo.944.1"> framework can help us alleviate these issues. </span><span class="koboSpan" id="kobo.944.2">It does so with the </span><span class="No-Break"><span class="koboSpan" id="kobo.945.1">following modules:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.946.1">Model I/O</span></strong><span class="koboSpan" id="kobo.947.1">: The framework differentiates between classic LLMs and chat models. </span><span class="koboSpan" id="kobo.947.2">In the first case, we can prompt</span><a id="_idIndexMarker1385"/><span class="koboSpan" id="kobo.948.1"> the model with a single prompt, and it will generate a response. </span><span class="koboSpan" id="kobo.948.2">The second case is more interactive – it presumes a back-and-forth communication between the human and the model in a chat form. </span><span class="koboSpan" id="kobo.948.3">Internally, both are LLMs; the difference comes from using different APIs. </span><span class="koboSpan" id="kobo.948.4">Regardless of the model type, a token sequence is the only way to feed it with input data. </span><span class="koboSpan" id="kobo.948.5">The I/O module provides helper prompt templates for different use cases. </span><span class="koboSpan" id="kobo.948.6">For example, the chat template maintains an explicit list of all messages instead of concatenating them in a single sequence. </span><span class="koboSpan" id="kobo.948.7">We also have a few-shot template, which provides an interface to include one or more instructive input/output examples within the </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">input query.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.950.1">The module can also parse the model output (a token sequence converted into words). </span><span class="koboSpan" id="kobo.950.2">For example, if the output</span><a id="_idIndexMarker1386"/><span class="koboSpan" id="kobo.951.1"> is a JSON string, a JSON parser can convert it into an actual </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">JSON object.</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.953.1">Retrieval</span></strong><span class="koboSpan" id="kobo.954.1">: This retrieves external data</span><a id="_idIndexMarker1387"/><span class="koboSpan" id="kobo.955.1"> and feeds it to the model input sequence. </span><span class="koboSpan" id="kobo.955.2">Its most basic function is to parse file formats such as CSV and JSON. </span><span class="koboSpan" id="kobo.955.3">It can also split larger documents into chunks if they don’t fit within the context </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">window size.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.957.1">Vector databases</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.958.1">The primary output</span><a id="_idIndexMarker1388"/><span class="koboSpan" id="kobo.959.1"> of an LLM and other neural networks (before any task-specific heads) are embedding vectors, which we use for downstream tasks such as classification or text generation. </span><span class="koboSpan" id="kobo.959.2">The universal nature of this data format has led to the creation of vector-specific databases (or stores). </span><span class="koboSpan" id="kobo.959.3">As the name suggests, these stores only work with vectors and support fast vector operations, such as different similarity measures over the whole database. </span><span class="koboSpan" id="kobo.959.4">We can query an input embedding vector against all other database vectors and find the most similar ones. </span><span class="koboSpan" id="kobo.959.5">This concept is similar to the </span><strong class="bold"><span class="koboSpan" id="kobo.960.1">Q</span></strong><span class="koboSpan" id="kobo.961.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.962.1">K</span></strong><span class="koboSpan" id="kobo.963.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.964.1">V</span></strong><span class="koboSpan" id="kobo.965.1"> attention mechanism but in an external database form, which allows it to work with a larger dataset than </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">in-memory attention.</span></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.967.1">This retrieval module has integrations with multiple vector databases. </span><span class="koboSpan" id="kobo.967.2">This way, we can use the LLM to generate and store document embeddings (the document acts as an input sequence). </span><span class="koboSpan" id="kobo.967.3">Later, we can query the LLM to generate a new embedding for a given query and compare this query against the database to find the nearest matches. </span><span class="koboSpan" id="kobo.967.4">In this scenario, the role of the LLM is limited to generating </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">vector</span></span><span class="No-Break"><a id="_idIndexMarker1389"/></span><span class="No-Break"><span class="koboSpan" id="kobo.969.1"> embeddings.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.970.1">Chains</span></strong><span class="koboSpan" id="kobo.971.1">: These are mechanisms that combine multiple LangChain components to create a single</span><a id="_idIndexMarker1390"/><span class="koboSpan" id="kobo.972.1"> application. </span><span class="koboSpan" id="kobo.972.2">For example, we can create a chain that takes user input, formats it with a special prompt template, feeds it to an LLM, and parses the LLM output to JSON. </span><span class="koboSpan" id="kobo.972.3">We can branch chains or combine </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">multiple chains.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.974.1">Memory</span></strong><span class="koboSpan" id="kobo.975.1">: This maintains the input token sequence throughout a chain of steps or model interactions</span><a id="_idIndexMarker1391"/><span class="koboSpan" id="kobo.976.1"> with the outside world, which can modify and extend the sequence dynamically. </span><span class="koboSpan" id="kobo.976.2">It can also use the emerging LLM abilities to create a shortened summary of the current historical sequence. </span><span class="koboSpan" id="kobo.976.3">The shortened version replaces the original in the input token sequence for future inputs. </span><span class="koboSpan" id="kobo.976.4">This compression allows us to use the context window more efficiently and store </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">more information.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.978.1">Agents</span></strong><span class="koboSpan" id="kobo.979.1">: An agent is an entity that can</span><a id="_idIndexMarker1392"/><span class="koboSpan" id="kobo.980.1"> take actions that interact with the environment. </span><span class="koboSpan" id="kobo.980.2">In the current context, an LLM acts as the agent’s reasoning engine to determine which actions the agent is to take and in which order. </span><span class="koboSpan" id="kobo.980.3">To help</span><a id="_idIndexMarker1393"/><span class="koboSpan" id="kobo.981.1"> with this task, the agent/LLM can use special functions called </span><strong class="bold"><span class="koboSpan" id="kobo.982.1">tools</span></strong><span class="koboSpan" id="kobo.983.1">. </span><span class="koboSpan" id="kobo.983.2">These can be generic utilities (for example, API calls), other chains, or </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">even agents.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.985.1">Callbacks</span></strong><span class="koboSpan" id="kobo.986.1">: We can use callbacks</span><a id="_idIndexMarker1394"/><span class="koboSpan" id="kobo.987.1"> to plug into various points of the LLM application, which is useful for logging, monitoring, </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">or streaming.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.989.1">Next, let’s solidify our understanding of LangChain with </span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">an example.</span></span></p>
<h2 id="_idParaDest-176" lang="en-GB"><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.991.1">Using LangChain in practice</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.992.1">In this section, we’ll use LangChain, LangChain Experimental (</span><a href="https://github.com/langchain-ai/langchain/tree/master/libs/experimental"><span class="koboSpan" id="kobo.993.1">https://github.com/langchain-ai/langchain/tree/master/libs/experimental</span></a><span class="koboSpan" id="kobo.994.1">), and OpenAI’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.995.1">gpt-3.5-turbo</span></strong><span class="koboSpan" id="kobo.996.1"> model</span><a id="_idIndexMarker1395"/><span class="koboSpan" id="kobo.997.1"> to answer the question: </span><em class="italic"><span class="koboSpan" id="kobo.998.1">What are the sum of the elevations of the deepest section of the ocean and the highest peak on Earth? </span><span class="koboSpan" id="kobo.998.2">Use metric units only</span></em><span class="koboSpan" id="kobo.999.1">. </span><span class="koboSpan" id="kobo.999.2">To make things more interesting, we won’t let an LLM generate the output one word at a time. </span><span class="koboSpan" id="kobo.999.3">Instead, we’ll ask it to break up the solution into steps and use data lookup and calculations to find the </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">right answer.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1001.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1002.1">This example is partially based on https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute. </span><span class="koboSpan" id="kobo.1002.2">It requires access</span><a id="_idIndexMarker1396"/><span class="koboSpan" id="kobo.1003.1"> to the</span><a id="_idIndexMarker1397"/><span class="koboSpan" id="kobo.1004.1"> OpenAI API (</span><a href="https://platform.openai.com/"><span class="koboSpan" id="kobo.1005.1">https://platform.openai.com/</span></a><span class="koboSpan" id="kobo.1006.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">SerpAPI (</span></span><a href="https://serpapi.com/"><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">https://serpapi.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">).</span></span></p>
<p lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1011.1">Initialize LangChain’s API wrapper</span><a id="_idIndexMarker1398"/><span class="koboSpan" id="kobo.1012.1"> for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1013.1">gpt-3.5-turbo</span></strong><span class="koboSpan" id="kobo.1014.1"> model. </span><span class="koboSpan" id="kobo.1014.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1015.1">temperature</span></strong><span class="koboSpan" id="kobo.1016.1"> parameter (in the [0,1] range) determines how the model selects the next token. </span><span class="koboSpan" id="kobo.1016.2">For example, if </span><strong class="source-inline"><span class="koboSpan" id="kobo.1017.1">temperature=0</span></strong><span class="koboSpan" id="kobo.1018.1">, it will always output the highest probability token. </span><span class="koboSpan" id="kobo.1018.2">The closer </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">temperature</span></strong><span class="koboSpan" id="kobo.1020.1"> is to 1, the more likely it is that the model selects a token with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">lower probability:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1022.1">
from langchain.chat_models import ChatOpenAI
model = ChatOpenAI(temperature=0)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1023.1">Define the tools that will help us solve this task. </span><span class="koboSpan" id="kobo.1023.2">First is the search tool, which uses SerpAPI to perform Google searches. </span><span class="koboSpan" id="kobo.1023.3">This allows the LLM to query Google for the elevations of the deepest part of the ocean and the highest mountain in </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">our question:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1025.1">
# Tools
from langchain.agents.tools import Tool
# Search tool
from langchain import SerpAPIWrapper
search = Tool(
    name='Search',
    func=SerpAPIWrapper().run,
    description='Google search tool')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1026.1">Next is the calculator tool, which will allow the LLM to compute the sum of the elevations. </span><span class="koboSpan" id="kobo.1026.2">This tool uses a special few-shot learning LangChain </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">PromptTemplate</span></strong><span class="koboSpan" id="kobo.1028.1"> to query the LLM to calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">mathematical equations:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1030.1">
from langchain import LLMMathChain
llm_math_chain = LLMMathChain.from_llm(
    llm=model,
    verbose=True)
calculator = Tool(
    name='Calculator',
    func=llm_math_chain.run,
    description='Calculator tool')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1031.1">Initialize a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.1032.1">PlanAndExecute</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">agent</span></strong><span class="koboSpan" id="kobo.1034.1">. </span><span class="koboSpan" id="kobo.1034.2">It accepts the LLM, the tools</span><a id="_idIndexMarker1399"/><span class="koboSpan" id="kobo.1035.1"> we just defined, as well as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1036.1">planner</span></strong><span class="koboSpan" id="kobo.1037.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1038.1">executor</span></strong><span class="koboSpan" id="kobo.1039.1"> agents, </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">as arguments:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1041.1">
from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner
agent = PlanAndExecute(
    planner=load_chat_planner(
        llm=model),
    executor=load_agent_executor(
        llm=model,
        tools=[search, calculator],
        verbose=True),
    verbose=True)</span></pre><p class="list-inset" lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.1042.1">planner</span></strong><span class="koboSpan" id="kobo.1043.1"> uses a special LangChain text prompt template that queries the LLM model to break up the solution of the task into subtasks (steps). </span><span class="koboSpan" id="kobo.1043.2">The model generates a list-friendly formatted string, which the planner parses and returns as the list of steps that </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">executor</span></strong><span class="koboSpan" id="kobo.1045.1"> (itself an </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1046.1">agent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">) executes.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1048.1">Finally, we can </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">run </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1050.1">agent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1052.1">
agent.run('What is the sum of the elevations of the deepest section of the ocean and the highest peak on Earth? </span><span class="koboSpan" id="kobo.1052.2">Use metric units only.')</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1053.1">The final answer is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1054.1">The depth of the deepest section of the ocean in metric units is 19,783 meters</span></strong><span class="koboSpan" id="kobo.1055.1">. </span><span class="koboSpan" id="kobo.1055.2">Although the text description is off the mark, the computation </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">seems correct.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1057.1">Let’s analyze part of the steps </span><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">planner</span></strong><span class="koboSpan" id="kobo.1059.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">executor</span></strong><span class="koboSpan" id="kobo.1061.1"> take to reach the result. </span><span class="koboSpan" id="kobo.1061.2">First, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1062.1">planner</span></strong><span class="koboSpan" id="kobo.1063.1"> takes our initial query</span><a id="_idIndexMarker1400"/><span class="koboSpan" id="kobo.1064.1"> and asks the LLM to break it up into the following list </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">of steps:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.1066.1">
1. </span><span class="koboSpan" id="kobo.1066.2">'Find the depth of the deepest section of the ocean in metric units.'
</span><span class="koboSpan" id="kobo.1066.3">2. </span><span class="koboSpan" id="kobo.1066.4">'Find the elevation of the highest peak on Earth in metric units.'
</span><span class="koboSpan" id="kobo.1066.5">3. </span><span class="koboSpan" id="kobo.1066.6">'Add the depth of the deepest section of the ocean to the elevation of the highest peak on Earth.'
</span><span class="koboSpan" id="kobo.1066.7">4. </span><span class="koboSpan" id="kobo.1066.8">'Round the sum to an appropriate number of decimal places.'
</span><span class="koboSpan" id="kobo.1066.9">5. </span><span class="koboSpan" id="kobo.1066.10">"Given the above steps taken, respond to the user's original question. </span><span class="koboSpan" id="kobo.1066.11">\n"</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1067.1">Next, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1068.1">agent</span></strong><span class="koboSpan" id="kobo.1069.1"> iterates over each step and tasks </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">executor</span></strong><span class="koboSpan" id="kobo.1071.1"> to perform it. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1072.1">executor</span></strong><span class="koboSpan" id="kobo.1073.1"> has an internal LLM planner, which can also break up the current step into subtasks. </span><span class="koboSpan" id="kobo.1073.2">In addition to the step description, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1074.1">executor</span></strong><span class="koboSpan" id="kobo.1075.1"> uses a special text prompt, instructing its LLM </span><strong class="source-inline"><span class="koboSpan" id="kobo.1076.1">model</span></strong><span class="koboSpan" id="kobo.1077.1"> to identify the list of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1078.1">tools</span></strong><span class="koboSpan" id="kobo.1079.1"> (a tool has a name) it can use for each step. </span><span class="koboSpan" id="kobo.1079.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">executor</span></strong><span class="koboSpan" id="kobo.1081.1"> returns the following result as output for the augmented version of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1082.1">first step:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1083.1">
'Action: {
    "action": "Search",
    "action_input": "depth of the deepest section of the ocean in metric units"
}'</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1084.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1085.1">Search</span></strong><span class="koboSpan" id="kobo.1086.1"> agent’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.1087.1">action</span></strong><span class="koboSpan" id="kobo.1088.1"> represents a new intermediate step to be executed after the current one. </span><span class="koboSpan" id="kobo.1088.2">It will use the search tool to query Google with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1089.1">action_input</span></strong><span class="koboSpan" id="kobo.1090.1">. </span><span class="koboSpan" id="kobo.1090.2">In that sense, the chain is dynamic, as the output of one step can lead to additional steps added to the chain. </span><span class="koboSpan" id="kobo.1090.3">We add the result of each step to the input sequence of the future steps, and the LLM, via different prompt</span><a id="_idIndexMarker1401"/><span class="koboSpan" id="kobo.1091.1"> templates, ultimately determines the </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">next actions.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1093.1">This concludes our introduction to LangChain – a glimpse of what is possible </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">with LLMs.</span></span></p>
<h1 id="_idParaDest-177" lang="en-GB"><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.1095.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1096.1">In this chapter, we discussed a variety of topics. </span><span class="koboSpan" id="kobo.1096.2">We started with LLMs in the computer vision domain: ViT for image classification, DetR for object detection, and SD for text-to-image generation. </span><span class="koboSpan" id="kobo.1096.3">Next, we learned how to fine-tune an LLM with the Transformers library. </span><span class="koboSpan" id="kobo.1096.4">Finally, we used LangChain to implement a novel </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">LLM-driven application.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1098.1">In the next chapter, we’ll depart from our traditional topics and dive into the practical field </span><span class="No-Break"><span class="koboSpan" id="kobo.1099.1">of MLOps.</span></span></p>
</div>


<div class="Content" id="_idContainer1081">
<h1 id="_idParaDest-178" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.1.1">Part 4: </span><br/><span class="koboSpan" id="kobo.2.1">Developing </span><br/><span class="koboSpan" id="kobo.3.1">and Deploying Deep Neural Networks</span></h1>
</div>
<div id="_idContainer1082">
<p lang="en-GB"><span class="koboSpan" id="kobo.4.1">In this single-chapter part, we’ll discuss some techniques and tools that will help us develop and deploy neural </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">network models.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.6.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">following chapter:</span></span></p>
<ul>
<li lang="en-GB"><a href="B19627_10.xhtml#_idTextAnchor253"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 10</span></em></a><span class="koboSpan" id="kobo.9.1">, </span><em class="italic"><span class="koboSpan" id="kobo.10.1">Machine Learning Operations (MLOps)</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer1083">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer1084">
</div>
</div>
</body></html>