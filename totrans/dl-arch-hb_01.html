<html><head></head><body>
<div id="_idContainer022">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor015"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.2.1">Deep Learning Life Cycle</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will explore the intricacies of the deep learning life cycle. </span><span class="koboSpan" id="kobo.3.2">Sharing similar characteristics to the machine learning life cycle, the deep learning life cycle is a framework as much as it is a methodology that will allow a deep learning project idea to be insanely successful or to be completely scrapped when it is appropriate. </span><span class="koboSpan" id="kobo.3.3">We will grasp the reasons why the process is cyclical and understand some of the life cycle’s initial processes on a deeper level. </span><span class="koboSpan" id="kobo.3.4">Additionally, we will go through some high-level sneak peeks of the later processes of the life cycle that will be explored at a deeper level in </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">future chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Comprehensively, this chapter will help you do </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Understand the similarities and differences between the deep learning life cycle and its machine learning life </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">cycle counterpart</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Understand where domain knowledge fits in a deep </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">learning project</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Understand the few key steps in planning a deep learning project to make sure it can tangibly create </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">real-world value</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Grasp some deep learning model development details at a </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">high level</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Grasp the importance of model interpretation and the variety of deep learning interpretation techniques at a </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">high level</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Explore high-level concepts of model deployments and </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">their governance</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Learn to choose the necessary tools to carry out the processes in the deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">life cycle</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">We’ll cover this material in the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">following sections:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">Machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">life cycle</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">The construction strategy of a deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">life cycle</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">The data </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">preparation stage</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">model development</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Delivering </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">model insights</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.33.1">Managing risks</span></span></li>
</ul>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.34.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.35.1">This chapter includes some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">Python</span></strong><span class="koboSpan" id="kobo.37.1"> programming language. </span><span class="koboSpan" id="kobo.37.2">To complete it, you need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.40.1">matplotlib</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.41.1">seaborn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.42.1">tqdm</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.43.1">lingua</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.44.1">The code files are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1"><span class="No-Break"><span class="koboSpan" id="kobo.46.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.47.1">.</span></span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.48.1">Understanding the machine learning life cycle</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.49.1">Deep learning</span></strong><span class="koboSpan" id="kobo.50.1"> is a </span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.51.1">subset of the wider </span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.52.1">machine learning category. </span><span class="koboSpan" id="kobo.52.2">The main characteristic that sets it apart from other machine learning algorithms is the foundational building</span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.53.1"> block called </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">neural networks</span></strong><span class="koboSpan" id="kobo.55.1">. </span><span class="koboSpan" id="kobo.55.2">As deep learning has advanced tremendously since the early 2000s, it has made many previously unachievable feats possible through its machine learning counterparts. </span><span class="koboSpan" id="kobo.55.3">Specifically, deep learning has made breakthroughs in recognizing complex patterns that exist in complex and unstructured data such as text, images, videos, and audio. </span><span class="koboSpan" id="kobo.55.4">Some of the successful applications of deep learning today are face recognition with images, speech recognition from audio data, and language translation with </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">textual data.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">Machine learning, on the other hand, is a subset of the wider artificial intelligence category. </span><span class="koboSpan" id="kobo.57.2">Its algorithms, such as tree-based models and linear models, which are not considered to be deep learning models, still serve a wide range of use cases involving tabular data, which is the bulk of the data that’s stored by small and big organizations alike. </span><span class="koboSpan" id="kobo.57.3">This tabular data may exist in multiple structured databases and can span from 1 to 10 years’ worth of historical data that has the potential to be used for building predictive machine learning models. </span><span class="koboSpan" id="kobo.57.4">Some of the notable predictive applications for machine learning algorithms are fraud detection in the finance industry, product recommendations in e-commerce, and predictive maintenance in the manufacturing industry. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.58.1">Figure 1</span></em></span><em class="italic"><span class="koboSpan" id="kobo.59.1">.1</span></em><span class="koboSpan" id="kobo.60.1"> shows the relationships between deep learning, machine learning, and</span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.61.1"> artificial intelligence for a clearer visual distinction </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">between them:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<span class="koboSpan" id="kobo.63.1"><img alt="Figure 1.1 – Artificial intelligence relationships" src="image/B18187_01_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.64.1">Figure 1.1 – Artificial intelligence relationships</span></p>
<p><span class="koboSpan" id="kobo.65.1">Now that we know what deep</span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.66.1"> learning and machine learning are in a nutshell, we are ready for a glimpse of the machine learning life cycle, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.67.1">Figure 1</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.68.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<span class="koboSpan" id="kobo.70.1"><img alt="Figure 1.2 – Deep learning/machine learning life cycle" src="image/B18187_01_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.71.1">Figure 1.2 – Deep learning/machine learning life cycle</span></p>
<p><span class="koboSpan" id="kobo.72.1">As advanced and complex the deep learning algorithm is compared to other machine learning algorithms, the guiding methodologies that are needed to ensure success in both domains are unequivocally the same. </span><span class="koboSpan" id="kobo.72.2">The machine learning life cycle </span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.73.1">involves six stages that interact with each other in </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">different ways:</span></span></p>
<ol>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.75.1">Planning</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.76.1">Data Preparation</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.77.1">Model Development</span></strong></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Deliver </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.79.1">Model Insights</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.80.1">Model Deployment</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.81.1">Model Governance</span></strong></span></li>
</ol>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.82.1">Figure 1</span></em></span><em class="italic"><span class="koboSpan" id="kobo.83.1">.2</span></em><span class="koboSpan" id="kobo.84.1"> shows these six stages and the possible stage transitions depicted with arrows. </span><span class="koboSpan" id="kobo.84.2">Typically, a machine learning project will iterate between stages, depending on the business requirements. </span><span class="koboSpan" id="kobo.84.3">In a deep learning project, most of the innovative predictive use cases require manual data collection and data annotation, which is a process that lies in the realm of </span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.85.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.86.1">Data Preparation</span></strong><span class="koboSpan" id="kobo.87.1"> stage. </span><span class="koboSpan" id="kobo.87.2">As this process is generally time-consuming, especially when the data itself is not readily available, a go-to solution would be to start with an acceptable initial number of data and transition</span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.88.1"> into the </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">Model Development</span></strong><span class="koboSpan" id="kobo.90.1"> stage and, subsequently, to</span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.91.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">Deliver Model Insight</span></strong><span class="koboSpan" id="kobo.93.1"> stage to make sure results from the ideas </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">are sane.</span></span></p>
<p><span class="koboSpan" id="kobo.95.1">After the initial validation process, depending again on business requirements, practitioners would then decide to transition back into the </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">Data Preparation</span></strong><span class="koboSpan" id="kobo.97.1"> stage and continue to iterate through these stages cyclically in different data size milestones until results are satisfactory toward both the model development and business metrics. </span><span class="koboSpan" id="kobo.97.2">Once it gets approval from the necessary stakeholders, the project then goes into the </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">Model Deployment</span></strong><span class="koboSpan" id="kobo.99.1"> stage, where the built machine learning model will be served to allow its predictions to be consumed. </span><span class="koboSpan" id="kobo.99.2">The final stage is </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">Model Governance</span></strong><span class="koboSpan" id="kobo.101.1">, where practitioners carry out tasks that manage the risk, performance, and reliability of the deployed machine learning model. </span><span class="koboSpan" id="kobo.101.2">Model deployment and model governance both deserve more in-depth discussion and will be introduced in separate chapters closer to the end of this book. </span><span class="koboSpan" id="kobo.101.3">Whenever any of the key metrics fail to maintain themselves to a certain determined confidence level, the project will fall back into the </span><strong class="bold"><span class="koboSpan" id="kobo.102.1">Data Preparation</span></strong><span class="koboSpan" id="kobo.103.1"> stage of the cycle and repeat the same flow all </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">over again.</span></span></p>
<p><span class="koboSpan" id="kobo.105.1">The ideal machine learning </span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.106.1">project flows through the stages cyclically for as long as the business application needs it. </span><span class="koboSpan" id="kobo.106.2">However, machine learning projects are typically susceptible to a high probability of failure. </span><span class="koboSpan" id="kobo.106.3">According to a survey conducted by Dimensional Research and Alegion, covering around 300 machine learning practitioners from 20 different business industries, 78% of machine learning projects get held back or delayed at some point before deployment. </span><span class="koboSpan" id="kobo.106.4">Additionally, Gartner predicted that 85% of machine learning projects will fail (</span><a href="https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/"><span class="koboSpan" id="kobo.107.1">https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/</span></a><span class="koboSpan" id="kobo.108.1">). </span><span class="koboSpan" id="kobo.108.2">By expecting the unexpected, and anticipating failures before they happen, practitioners can likely circumvent potential failure factors early down the line in the planning stage. </span><span class="koboSpan" id="kobo.108.3">This also brings us to the trash icon bundled together in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.109.1">Figure 1</span></em></span><em class="italic"><span class="koboSpan" id="kobo.110.1">.2</span></em><span class="koboSpan" id="kobo.111.1">. </span><span class="koboSpan" id="kobo.111.2">Proper projects with a good plan typically get discarded only at the </span><strong class="bold"><span class="koboSpan" id="kobo.112.1">Deliver Model Insights</span></strong><span class="koboSpan" id="kobo.113.1"> stage, when it’s clear that the proposed model and project can’t deliver </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">satisfactory results.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">Now that we’ve covered an overview of the machine learning life cycle, let’s dive into each of the stages individually, broken down into sections, to help you discover the key tips and techniques that are needed the complete each stage successfully. </span><span class="koboSpan" id="kobo.115.2">These stages will be discussed in an abstract format and are not a concrete depiction of </span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.116.1">what you should ultimately be doing for your project since all projects are unique and strategies should always be evaluated on a </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">case-by-case basis.</span></span></p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.118.1">Strategizing the construction of a deep learning system</span></h1>
<p><span class="koboSpan" id="kobo.119.1">A deep learning model can only </span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.120.1">realize real-world value by being part of a system that performs some sort of operation. </span><span class="koboSpan" id="kobo.120.2">Bringing deep learning models from research papers to actual real-world usage is not an easy task. </span><span class="koboSpan" id="kobo.120.3">Thus, performing proper planning before conducting any project is a more reliable and structured way to achieve the desired goals. </span><span class="koboSpan" id="kobo.120.4">This section will discuss some considerations and strategies that will be beneficial when you start to plan your deep learning project </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">toward success.</span></span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.122.1">Starting the journey</span></h2>
<p><span class="koboSpan" id="kobo.123.1">Today, deep learning practitioners</span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.124.1"> tend to focus a lot on the algorithmic model-building part of the process. </span><span class="koboSpan" id="kobo.124.2">It takes a considerable amount of mental strength to not get hooked on the hype of </span><strong class="bold"><span class="koboSpan" id="kobo.125.1">state-of-the-art</span></strong><span class="koboSpan" id="kobo.126.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.127.1">SOTA</span></strong><span class="koboSpan" id="kobo.128.1">) research-focused </span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.129.1">techniques. </span><span class="koboSpan" id="kobo.129.2">With crazy techniques</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.130.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.131.1">pixtopix</span></strong><span class="koboSpan" id="kobo.132.1">, which is capable of generating high-resolution realistic color images from just sketches or image masks, and </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">natural language processing</span></strong><span class="koboSpan" id="kobo.134.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.135.1">NLP</span></strong><span class="koboSpan" id="kobo.136.1">) techniques</span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.137.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">GPT-3</span></strong><span class="koboSpan" id="kobo.139.1">, a 175-billion</span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.140.1"> parameters text generation model from OpenAI, and GPT-4, a multimodal text generation model that is a successor to GPT-3 and its sub-models, that are capable of generating practically anything you ask it to in a text format that ranges from text summarization to generating code, why </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">wouldn’t they?!</span></span></p>
<p><span class="koboSpan" id="kobo.142.1">Jokes aside, to become a true deep learning architect, we need to come to a consensus that any successful machine learning or deep learning project starts with the business </span><em class="italic"><span class="koboSpan" id="kobo.143.1">problem</span></em><span class="koboSpan" id="kobo.144.1"> and not from the shiny new research paper you just read online complete with a public GitHub repository. </span><span class="koboSpan" id="kobo.144.2">The planning stage often involves many business executives who are not savvy about the details of machine learning algorithms and often, the same set of people wouldn’t care about it at all. </span><span class="koboSpan" id="kobo.144.3">These algorithms are daunting for business-focused stakeholders to understand and, when added on top of the tough mental barriers of the adoption of artificial intelligence technologies itself, it doesn’t make the project any more likely to </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">be adopted.</span></span></p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.146.1">Evaluating deep learning’s worthiness</span></h2>
<p><span class="koboSpan" id="kobo.147.1">Deep learning shines the most in</span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.148.1"> handling unstructured data. </span><span class="koboSpan" id="kobo.148.2">This includes image data, text data, audio data, and video data. </span><span class="koboSpan" id="kobo.148.3">This is largely due to the model’s ability to automatically learn and extract complex, high-level features from the raw data. </span><span class="koboSpan" id="kobo.148.4">In the case of images and videos, deep learning models can capture spatial and temporal patterns, recognizing objects, scenes, and activities. </span><span class="koboSpan" id="kobo.148.5">With audio data, deep learning can understand the nuances of speech, noise, and various sound elements, making it possible to build applications such as speech recognition, voice assistants, and audio classification systems. </span><span class="koboSpan" id="kobo.148.6">For text data, deep learning models can capture the context, semantics, and syntax, enabling NLP tasks such as sentiment analysis, machine translation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">text summarization.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">This means that if this data exists and is utilized by your company in its business processes, there may be an opportunity to solve a problem with the help of deep learning. </span><span class="koboSpan" id="kobo.150.2">However, never overcomplicate problems just so you can solve them with deep learning. </span><span class="koboSpan" id="kobo.150.3">Equating this to something more relatable, you wouldn’t use a huge sledgehammer to get a nail into wood. </span><span class="koboSpan" id="kobo.150.4">It could work and you might get away with it, but you’d risk bending the nail or injuring yourself while </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">using it.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Once a problem has been identified, evaluate the business value of solving it. </span><span class="koboSpan" id="kobo.152.2">Not all problems are born the same and they can be ranked based on their business impact, value, complexity, risks, costs, and suitability for deep learning. </span><span class="koboSpan" id="kobo.152.3">Generally, you’d be looking for high impact, high value, low complexity, low risks, low cost, and high suitability to deep learning. </span><span class="koboSpan" id="kobo.152.4">Trade-offs between these metrics are expected but simply put, make sure the problem you’ve discovered is worth solving at all with deep learning. </span><span class="koboSpan" id="kobo.152.5">A general rule of thumb is to always resort to a simpler solution for a problem, even if it ends up abandoning the usage of deep learning technologies. </span><span class="koboSpan" id="kobo.152.6">Simple approaches tend to be more reliable, less costly, less prone to risks, and faster </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">to fruition.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">Consider a problem where a solution is needed to remove background scenes in a video feed and leave only humans or </span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.155.1">necessary objects untouched so that a more suitable background scene can be overlaid as a background instead. </span><span class="koboSpan" id="kobo.155.2">This is a common problem in the professional filmmaking industry in all film </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">genres today.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.157.1">Semantic segmentation</span></strong><span class="koboSpan" id="kobo.158.1">, which is </span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.159.1">the task of assigning a label to every pixel of an image in the width and height dimensions, is a method that is needed to solve such a problem. </span><span class="koboSpan" id="kobo.159.2">In this case, the task needs to assign labels that can help identify which pixels need to be removed. </span><span class="koboSpan" id="kobo.159.3">With the advent of many publicly available semantic segmentation datasets, deep learning has been able to advance considerably in the semantic segmentation field, allowing itself to achieve a very satisfactory fine-grained understanding of the world, enough so that it can be applied in the industry of autonomous driving and robot navigation most prominently. </span><span class="koboSpan" id="kobo.159.4">However, deep learning is not known to be 100% error-free and almost always has some error, even in the controlled evaluation dataset. </span><span class="koboSpan" id="kobo.159.5">In the case of human segmentation, for example, the model would likely result in the most errors in the fine hair areas. </span><span class="koboSpan" id="kobo.159.6">Most filmmakers aim for perfect depictions of their films and require that every single pixel gets removed appropriately without fail since a lot of money is spent on the time of the actors hired for the film. </span><span class="koboSpan" id="kobo.159.7">Additionally, a lot of time and money would be wasted in manually removing objects that could be otherwise simply removed if the scene had been shot with a green screen. </span><span class="koboSpan" id="kobo.159.8">This is an example of a case where we should not overcomplicate the problem. </span><span class="koboSpan" id="kobo.159.9">A green screen is all you need to solve the problem described: specifically, the rare chromakey green color. </span><span class="koboSpan" id="kobo.159.10">When green screens are prepped properly in the areas where the desired imagery will be overlaid digitally, image processing techniques alone can remove the pixels that are considered to be in the small light intensity range centered on the chromakey green color and achieve semantic segmentation effectively with a rule-based solution. </span><span class="koboSpan" id="kobo.159.11">The green screen is a simpler solution that is cost-effective, foolproof, and fast to </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">set up.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">That was a mouthful! </span><span class="koboSpan" id="kobo.161.2">Now, let’s go through a simpler problem. </span><span class="koboSpan" id="kobo.161.3">Consider a problem where we want to automatically and digitally identify when it rains. </span><span class="koboSpan" id="kobo.161.4">In this use case, it is important to understand the actual requirements and goals of identifying the rain: is it sufficient to detect rain exactly when it happens? </span><span class="koboSpan" id="kobo.161.5">Or do we need to identify whether rain will happen in the near future? </span><span class="koboSpan" id="kobo.161.6">What will we use the information of rain events for? </span><span class="koboSpan" id="kobo.161.7">These questions will guide whether deep learning is required or not. </span><span class="koboSpan" id="kobo.161.8">We, as humans, know that rain can be predicted by visual input by either looking at the presence of raindrops falling or looking at cloud conditions. </span><span class="koboSpan" id="kobo.161.9">However, if the use case is sufficient to detect rain when it happens, and the goal of detecting rain is to determine when to water the plants, a simpler approach would be to use an electronic sensor to detect the presence of water or humidity. </span><span class="koboSpan" id="kobo.161.10">Only when you want to estimate whether it will rain in the future, let’s say in 15 minutes, does deep learning make more sense to be applied as there are a lot of interactions between meteorological factors that can affect rainfall. </span><span class="koboSpan" id="kobo.161.11">Only by brainstorming each use case and analyzing all potential solutions, even outside of deep learning, can you make sure deep learning brings tangible business value compared to other solutions. </span><span class="koboSpan" id="kobo.161.12">Do not just apply deep learning because you </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">want to.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">At times, when value isn’t clear when you’re directly considering a use case, or when value is clear but you have no idea how to execute it, consider finding reference projects from companies in the same industry. </span><span class="koboSpan" id="kobo.163.2">Companies in the same industry have a high chance of wanting to optimize the same processes or solve the same pain points. </span><span class="koboSpan" id="kobo.163.3">Similar reference projects can serve as a guide to designing a deep learning system and can serve as proof that the use case being considered is worthy of the involvement of deep learning technologies. </span><span class="koboSpan" id="kobo.163.4">Of course, not everybody has access to details like this, but you’d be surprised what Google can tell you these days. </span><span class="koboSpan" id="kobo.163.5">Even if there isn’t a similar project being carried out for direct reference, you would likely be able to pivot upon the other machine learning project references that already have a track record of bringing value to the </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">same industry.</span></span></p>
<p><span class="koboSpan" id="kobo.165.1">Admittedly, rejecting deep learning at times would be a hard pill to swallow considering that most practitioners get paid to implement deep learning solutions. </span><span class="koboSpan" id="kobo.165.2">However, dismissing it earlier will allow you to focus your time on more valuable problems that would be more useful to solve with deep learning and prevent the risk of undermining the potential of deep learning in cases where simpler solutions can outperform deep learning. </span><span class="koboSpan" id="kobo.165.3">Criteria for deep learning worthiness should be evaluated on a case-by-case basis and as a practitioner, the best advice to follow is to simply practice common sense. </span><span class="koboSpan" id="kobo.165.4">Spend a good amount of </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.166.1">time going through the problem exploration and the worthiness evaluation process. </span><span class="koboSpan" id="kobo.166.2">The last thing you want is to spend a painstaking amount of time preparing data, building a deep learning model, and delivering very convincing model insights only to find out that the label you are trying to predict does not provide enough value for the business to </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">invest further.</span></span></p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.168.1">Defining success</span></h2>
<p><span class="koboSpan" id="kobo.169.1">Ever heard sentences</span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.170.1"> like “</span><em class="italic"><span class="koboSpan" id="kobo.171.1">My deep learning model just got 99% accuracy on my validation dataset!”</span></em><span class="koboSpan" id="kobo.172.1">? </span><span class="koboSpan" id="kobo.172.2">Data scientists often make the mistake of determining the success of a machine learning project just by using validation metrics they use to evaluate their machine learning models during the model development process. </span><span class="koboSpan" id="kobo.172.3">Model-building metrics such as accuracy, precision, or recall are important metrics to consider in a machine learning project but unless they add business values and connect to the business objectives in some way, they rarely mean anything. </span><span class="koboSpan" id="kobo.172.4">A project can achieve a good accuracy score but still fail to achieve the desired business goals. </span><span class="koboSpan" id="kobo.172.5">This can happen in cases when no proper success metrics have been defined early and subsequently cause a wrong label to be used in the data preparation and model development stages. </span><span class="koboSpan" id="kobo.172.6">Furthermore, even when the model metric positively impacts business processes directly, there is a chance that the achievement won’t be communicated effectively to business stakeholders and the worst case not considered to be successful when </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">reported as-is.</span></span></p>
<p><span class="koboSpan" id="kobo.174.1">Success metrics, when defined </span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.175.1">early, act as the machine learning project’s guardrails and ensure that the project goals are aligned with the business goals. </span><span class="koboSpan" id="kobo.175.2">One of the guardrails is that a success metric can help guide the choice of a proper label that can at inference time, tangibly improve the business processes or otherwise create value in the business. </span><span class="koboSpan" id="kobo.175.3">First, let’s make sure we are aligned with what a label means, which is a value that you want the machine learning model to predict. </span><span class="koboSpan" id="kobo.175.4">The purpose of a machine learning model is to assign these labels automatically given some form of input data, and thus during the data preparation and model development stages, a label needs to be chosen to serve that purpose. </span><span class="koboSpan" id="kobo.175.5">Choosing the wrong label can be catastrophic to a deep learning project as sometimes, when data is not readily available, it means the project has to start all over again from the data preparation stage. </span><span class="koboSpan" id="kobo.175.6">Labels should always be indirectly or directly attributed to the </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">success metric.</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">Success metrics, as the name suggests, can be plural, and range from time-based success definitions or milestones to the overall project success, and from intangible to tangible. </span><span class="koboSpan" id="kobo.177.2">It’s good practice to generally brainstorm and document all the possible success criteria from a low level to a high level. </span><span class="koboSpan" id="kobo.177.3">Another best practice is to make </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.178.1">sure to always define tangible success metrics alongside intangible metrics. </span><span class="koboSpan" id="kobo.178.2">Intangible metrics generate awareness, but tangible metrics make sure things are measurable and thus make them that much more attainable. </span><span class="koboSpan" id="kobo.178.3">A few examples of intangible and hard-to-measure metrics are </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.180.1">Increasing </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">customer satisfaction</span></span></li>
<li><span class="koboSpan" id="kobo.182.1">Increasing </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">employee performance</span></span></li>
<li><span class="koboSpan" id="kobo.184.1">Improving </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">shareholder outlook</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.186.1">Metrics are ways to measure something and are tied to goals to seal the deal. </span><span class="koboSpan" id="kobo.186.2">Goals themselves can be intangible, similar to the few examples listed previously, but so long as it is tied to tangible metrics, the project is off to a good start. </span><span class="koboSpan" id="kobo.186.3">When you have a clear goal, ask yourself in what way the goal can be proven to be achieved, demonstrated, or measured. </span><span class="koboSpan" id="kobo.186.4">A few examples of tangible success metrics for machine learning projects that could align with business goals are </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.188.1">Increase the time customers spend, which can be a proxy for </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">customer delight</span></span></li>
<li><span class="koboSpan" id="kobo.190.1">Increase company revenue, which can be a proxy for </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">employee performance</span></span></li>
<li><span class="koboSpan" id="kobo.192.1">Increase the </span><strong class="bold"><span class="koboSpan" id="kobo.193.1">click-through rate</span></strong><span class="koboSpan" id="kobo.194.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.195.1">CTR</span></strong><span class="koboSpan" id="kobo.196.1">), which </span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.197.1">can be a proxy for the effectiveness of targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">marketing campaigns</span></span></li>
<li><span class="koboSpan" id="kobo.199.1">Increase the </span><strong class="bold"><span class="koboSpan" id="kobo.200.1">customer lifetime value</span></strong><span class="koboSpan" id="kobo.201.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.202.1">CLTV</span></strong><span class="koboSpan" id="kobo.203.1">), which can</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.204.1"> be a proxy for long-term customer satisfaction </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">and loyalty</span></span></li>
<li><span class="koboSpan" id="kobo.206.1">Increase conversion rate, which can be a proxy for the success of promotional campaigns and website </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">user experience</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.208.1">This concept is not new nor limited to just machine learning projects – just about any single project carried out for a company as every single real-world project needs to be aligned with the business goal. </span><span class="koboSpan" id="kobo.208.2">Many foundational project management techniques can be applied similarly to machine learning projects, and spending time gaining some project management skills out of the machine learning field would be beneficial and transferable to machine learning projects. </span><span class="koboSpan" id="kobo.208.3">Additionally, as machine learning is considered to be a software-based technology, software project management methodologies </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">also apply.</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">A final concluding thought to take </span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.211.1">away is that machine learning systems are not about how advanced your machine learning models are, but instead about how humans and machine intelligence can work together to achieve a greater good and </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">create value.</span></span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.213.1">Planning resources</span></h2>
<p><span class="koboSpan" id="kobo.214.1">Deep learning often involves </span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.215.1">neural network architectures with a large set of parameters, otherwise called weights. </span><span class="koboSpan" id="kobo.215.2">These architecture’s sizes can go from holding a few parameters up to holding hundreds of billions of parameters. </span><span class="koboSpan" id="kobo.215.3">For example, an OpenAI GPT-3 text generation model holds 175 billion neural network parameters, which amounts to around 350 GB in computer storage size. </span><span class="koboSpan" id="kobo.215.4">This means that to run GPT-3, you need a machine </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.216.1">with a </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">random access memory</span></strong><span class="koboSpan" id="kobo.218.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.219.1">RAM</span></strong><span class="koboSpan" id="kobo.220.1">) size of at least </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">350 GB!</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">Deep learning model frameworks such as PyTorch and TensorFlow have been built to work with devices called </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">graphics processing units</span></strong><span class="koboSpan" id="kobo.224.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.225.1">GPUs</span></strong><span class="koboSpan" id="kobo.226.1">), which </span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.227.1">offer tremendous neural network model training and inference speedups. </span><span class="koboSpan" id="kobo.227.2">Off-the-shelf GPU devices commonly have a GPU RAM of 12 GB and are nowhere near the requirements needed to load a GPT-3 model in GPU mode. </span><span class="koboSpan" id="kobo.227.3">However, there are still methods to partition big models into multiple GPUs and run the model on GPUs. </span><span class="koboSpan" id="kobo.227.4">Additionally, some methods can allow for distributed GPU model training and inference to support larger data batch sizes at any one usage point. </span><span class="koboSpan" id="kobo.227.5">GPUs are not considered cheap devices and can cost anywhere from a few hundred bucks to hundreds of thousands from the most widely used GPU brand, Nvidia. </span><span class="koboSpan" id="kobo.227.6">With the rise of cryptocurrency technologies, the availability of GPUs is also reduced significantly due to people buying them immediately when they are in stock. </span><span class="koboSpan" id="kobo.227.7">All these emphasize the need to plan computing resources for training and inferencing deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">models beforehand.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">It is important to align your model development and deployment needs to your computing resource allocation early in the project. </span><span class="koboSpan" id="kobo.229.2">Start by gauging the range of sizes of deep learning architectures that are suitable for the task at hand either by browsing research papers or websites that provide a good summary of techniques, and setting aside computing resources for the model </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">development process.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.231.1">Tip</span></p>
<p class="callout"><a href="http://paperswithcode.com"><span class="koboSpan" id="kobo.232.1">paperswithcode.com</span></a><span class="koboSpan" id="kobo.233.1"> provides summaries of a wide variety of techniques grouped by a wide variety </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">of tasks!</span></span></p>
<p><span class="koboSpan" id="kobo.235.1">When computing resources are not readily available, make sure you always make purchase plans early, especially if it involves GPUs. </span><span class="koboSpan" id="kobo.235.2">But what if a physical machine is not desired? </span><span class="koboSpan" id="kobo.235.3">An alternative to using computing resources is to use paid cloud computing resource providers you can access online easily from anywhere in the world. </span><span class="koboSpan" id="kobo.235.4">During the model development stage, one of the benefits of having more GPUs with more RAM allocated is that it can allow you to train models faster by either using a larger data batch size during training or allowing the capability to train multiple models at any one time. </span><span class="koboSpan" id="kobo.235.5">It is generally fine to also use CPU-only deep learning model training, but the model training time would just inevitably be </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">much longer.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">The GPU and CPU-based computing resources that are required during training are often considered overkill to be used during inference time when they are deployed. </span><span class="koboSpan" id="kobo.237.2">Different applications have different deployment computing requirements and the decision on what resource specification to allocate can be gauged by asking yourself the following </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">three questions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.239.1">How often are the inference </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">requests made?</span></span><ul><li><span class="koboSpan" id="kobo.241.1">Many inference requests in a short period might signal the need to have more than one inference service up in multiple computing devices </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">in parallel</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.243.1">What is the average amount of samples that are requested for a prediction at any </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">one time?</span></span><ul><li><span class="koboSpan" id="kobo.245.1">Device RAM requirements should match batch </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">size expectations</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.247.1">How fast do you need </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">a reply?</span></span><ul><li><span class="koboSpan" id="kobo.249.1">GPUs are needed if it’s seconds or a faster response </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">time requirement</span></span></li><li><span class="koboSpan" id="kobo.251.1">CPUs can do the job if you don’t care about the </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">response time</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.253.1">Resource planning is not restricted to just computing resource planning – it also expands to human resource planning. </span><span class="koboSpan" id="kobo.253.2">Assumptions for the number of deep learning engineers and data scientists working together in a team would </span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.254.1">ultimately affect the choices of software libraries and tools used in the model development process. </span><span class="koboSpan" id="kobo.254.2">The analogy of choosing these tools will be introduced in </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">future sections.</span></span></p>
<p><span class="koboSpan" id="kobo.256.1">The next step is to prepare </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">your data.</span></span></p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.258.1">Preparing data</span></h1>
<p><span class="koboSpan" id="kobo.259.1">Data is to machine learning models as </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.260.1">is the fuel to your car, the electricity to your electronic devices, and the food for your body. </span><span class="koboSpan" id="kobo.260.2">A machine learning model works by trying to capture the relationships between the provided input and output data. </span><span class="koboSpan" id="kobo.260.3">Similar to how human brains work, a machine learning model will attempt to iterate through collected data examples and slowly build a memory of the patterns required to map the provided input data to the provided target output data. </span><span class="koboSpan" id="kobo.260.4">The data preparation stage consists of methods and processes required to prepare ready-to-use data to build a machine learning model that includes </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.262.1">Acquisition of raw input and targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">output data</span></span></li>
<li><span class="koboSpan" id="kobo.264.1">Exploratory data analysis of the </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">acquired data</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.266.1">Data pre-processing</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.267.1">We will discuss </span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.268.1">each of these topics in the </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">following subsections.</span></span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.270.1">Deep learning problem types</span></h2>
<p><span class="koboSpan" id="kobo.271.1">Deep learning can be broadly </span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.272.1">categorized into two problem types, namely </span><strong class="bold"><span class="koboSpan" id="kobo.273.1">supervised learning</span></strong><span class="koboSpan" id="kobo.274.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.275.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.276.1">. </span><span class="koboSpan" id="kobo.276.2">Both </span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.277.1">of these </span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.278.1">problem types involve building a deep learning model that is capable of making informed predictions as outputs, given well-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">data inputs.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">Supervised learning is a problem type where labels are involved that act as the source of truth to learn from. </span><span class="koboSpan" id="kobo.280.2">Labels</span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.281.1"> can </span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.282.1">exist in many forms and can be broken down into two problem types, namely </span><strong class="bold"><span class="koboSpan" id="kobo.283.1">classification</span></strong><span class="koboSpan" id="kobo.284.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.285.1">regression</span></strong><span class="koboSpan" id="kobo.286.1">. </span><span class="koboSpan" id="kobo.286.2">Classification is the process where a specific discrete class is predicted among other classes when given input data. </span><span class="koboSpan" id="kobo.286.3">Many more complex </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.287.1">problems derive from the base classification problem types, such</span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.288.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.289.1">instance segmentation</span></strong><span class="koboSpan" id="kobo.290.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.291.1">multilabel classification</span></strong><span class="koboSpan" id="kobo.292.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.293.1">object detection</span></strong><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">Regression, on </span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.295.1">the other hand, is the process where a continuous numerical value is predicted when given input data. </span><span class="koboSpan" id="kobo.295.2">Likewise, complex problem types can be derived from the base regression problem </span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.296.1">type, such</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.297.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.298.1">multi-regression</span></strong><span class="koboSpan" id="kobo.299.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.300.1">image bounding </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.301.1">box regression</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.303.1">Unsupervised learning, on the other hand, is a problem type where there aren’t any labels involved and the goals can vary widely. </span><span class="koboSpan" id="kobo.303.2">Anomaly detection, clustering, and feature representation learning are the most common problem types that belong to the unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">learning category.</span></span></p>
<p><span class="koboSpan" id="kobo.305.1">We will go through these two problem types separately for deep learning in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.306.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.307.1">, </span><em class="italic"><span class="koboSpan" id="kobo.308.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.309.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.310.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.311.1">, </span><em class="italic"><span class="koboSpan" id="kobo.312.1">Exploring Unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.313.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.315.1">Next, let’s learn about the things you should consider when </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">acquiring data.</span></span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.317.1">Acquiring data</span></h2>
<p><span class="koboSpan" id="kobo.318.1">Acquiring data in the </span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.319.1">context of </span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.320.1">deep learning usually involves unstructured data, which includes image data, video data, text data, and audio data. </span><span class="koboSpan" id="kobo.320.2">Sometimes, data can be readily available and stored through some business processes in a database but very often, it has to be collected manually from the environment from scratch. </span><span class="koboSpan" id="kobo.320.3">Additionally, very often, labels for this data are not readily available and require manual annotation work. </span><span class="koboSpan" id="kobo.320.4">Along with the capability of deep learning algorithms to process and digest highly complex data comes the need to feed it more data compared to its machine learning counterparts. </span><span class="koboSpan" id="kobo.320.5">The requirement to perform data collection and data annotation in high volumes is the main reason why deep learning is considered to have a high barrier of </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">entry today.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">Don’t rush into choosing an algorithm quickly in a machine learning project. </span><span class="koboSpan" id="kobo.322.2">Spend a quality amount of time formally defining the features that can be acquired to predict the target variable. </span><span class="koboSpan" id="kobo.322.3">Get help from domain experts during the process and brainstorm potential predictive features that relate to the target variable. </span><span class="koboSpan" id="kobo.322.4">In actual projects, it is common to spend a big portion of your time planning and acquiring the data while making sure the acquired data is fit for a machine learning model’s consumption and subsequently spending the rest of the time in model building, model deployment, and model governance. </span><span class="koboSpan" id="kobo.322.5">A lot of research has been done into handling bad-quality data during the model development stage but most of these techniques aren’t comprehensive and are limited in ways that they can cover up the inherent quality of the data. </span><span class="koboSpan" id="kobo.322.6">Displaying ignorance in quality assurance during the data acquisition stage and showing enthusiasm only in the data science portion of the workflow is a strong indicator that the project would be doomed to failure right from the </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">inception stage.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">Formulating a data acquisition strategy is a daunting task when you don’t know what it means to have good-quality data. </span><span class="koboSpan" id="kobo.324.2">Let’s go through a few pillars of data quality you should consider for your data in the context of actual </span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.325.1">business use </span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.326.1">cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">machine learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.328.1">Representativeness</span></strong><span class="koboSpan" id="kobo.329.1">: How representative is the data concerning the real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">data population?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.331.1">Consistency</span></strong><span class="koboSpan" id="kobo.332.1">: How consistent are the annotation methods? </span><span class="koboSpan" id="kobo.332.2">Does the same pattern match the same label or are there </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">some inconsistencies?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.334.1">Comprehensiveness</span></strong><span class="koboSpan" id="kobo.335.1">: Are all variations of a specific label covered in the </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">collected dataset?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.337.1">Uniqueness</span></strong><span class="koboSpan" id="kobo.338.1">: Does the data contain a lot of duplicated or </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">similar data?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">Fairness</span></strong><span class="koboSpan" id="kobo.341.1">: Is the collected data biased toward any specific labels or </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">data groups?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.343.1">Validity</span></strong><span class="koboSpan" id="kobo.344.1">: Does the data contain invalid fields? </span><span class="koboSpan" id="kobo.344.2">Do the data inputs match up with their labels? </span><span class="koboSpan" id="kobo.344.3">Is there </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">missing data?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.346.1">Let’s look at each of these </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">in detail.</span></span></p>
<h3><span class="koboSpan" id="kobo.348.1">Representativeness</span></h3>
<p><span class="koboSpan" id="kobo.349.1">Data should be collected in a</span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.350.1"> way that it mimics what data you will receive during model deployment as much as possible. </span><span class="koboSpan" id="kobo.350.2">Very often in research-based deep learning projects, researchers collect their data in a closed environment with controlled environmental variables. </span><span class="koboSpan" id="kobo.350.3">One of the reasons researchers prefer collecting data from a controlled environment is that they can build stabler machine learning models and generally try to prove a point. </span><span class="koboSpan" id="kobo.350.4">Eventually, when the research paper is published, you see amazing results that were applied using handpicked data to impress. </span><span class="koboSpan" id="kobo.350.5">These models, which are built on controlled data, fail miserably when you apply them to random uncontrolled real-world examples. </span><span class="koboSpan" id="kobo.350.6">Don’t get me wrong – it’s great to have these controlled datasets available to contribute toward a stabler machine learning model at times, but having uncontrolled real-world examples as a main part of the training and evaluation datasets is key to achieving a </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">generalizable model.</span></span></p>
<p><span class="koboSpan" id="kobo.352.1">Sometimes, the acquired training data has an expiry date and does not stay representative forever. </span><span class="koboSpan" id="kobo.352.2">This scenario is called data drift and will be discussed in more detail in the </span><em class="italic"><span class="koboSpan" id="kobo.353.1">Managing risk</span></em><span class="koboSpan" id="kobo.354.1"> section closer to the end of this chapter. </span><span class="koboSpan" id="kobo.354.2">The representativeness metric for data quality should also be evaluated based on the future expectations of </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.355.1">the data the model will receive </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">during deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.357.1">Consistency</span></h3>
<p><span class="koboSpan" id="kobo.358.1">Data labels that are not consistently </span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.359.1">annotated make it harder for machine learning models to learn from them. </span><span class="koboSpan" id="kobo.359.2">This happens when the domain ideologies and annotation strategies differ among multiple labelers and are just not defined properly. </span><span class="koboSpan" id="kobo.359.3">For example, “Regular” and “Normal” mean the same thing, but to the machine, it’s two completely different classes; so are “Normal” and “normal” with just a </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">capitalization difference!</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">Practice formalizing a proper strategy for label annotation during the planning stage before carrying out the actual annotation process. </span><span class="koboSpan" id="kobo.361.2">Cleaning the data for simple consistency errors is possible post-data annotation, but some consistency errors can be hard to detect and complex </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">to correct.</span></span></p>
<h3><span class="koboSpan" id="kobo.363.1">Comprehensiveness</span></h3>
<p><span class="koboSpan" id="kobo.364.1">Machine learning thrives in </span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.365.1">building a decisioning mechanism that is robust to multiple variations and views of any specific label. </span><span class="koboSpan" id="kobo.365.2">Being capable and accomplishing it are two different things. </span><span class="koboSpan" id="kobo.365.3">One of the prerequisites of decisioning robustness is that the data that’s used for training and evaluation itself has to be comprehensive enough to provide coverage for all possible variations of each provided label. </span><span class="koboSpan" id="kobo.365.4">How can comprehensiveness be judged? </span><span class="koboSpan" id="kobo.365.5">Well, that depends on the complexity of the labels and how varied they can present themselves naturally when the model is deployed. </span><span class="koboSpan" id="kobo.365.6">More complex labels naturally require more samples and less complex labels require </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">fewer samples.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">A good point to start with, in the context of deep learning, is to have at least 100 samples for each label and experiment with building a model and deriving model insights to see if there are enough samples for the model to generalize on unseen variations of the label. </span><span class="koboSpan" id="kobo.367.2">When the model doesn’t produce convincing results, that’s when you need to cycle back to the data preparation stage again to acquire more data variations of any specific label. </span><span class="koboSpan" id="kobo.367.3">The machine learning life cycle is inherently a cyclical process where you will experiment, explore, and verify while transitioning between stages to obtain the answers you need to solve your problems, so don’t be afraid to execute these different </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">stages cyclically.</span></span></p>
<h3><span class="koboSpan" id="kobo.369.1">Uniqueness</span></h3>
<p><span class="koboSpan" id="kobo.370.1">While having complete and</span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.371.1"> comprehensive data is beneficial to build a machine learning model that is robust to data variations, having duplicated versions of the same data variation in the acquired dataset risks creating a biased model. </span><span class="koboSpan" id="kobo.371.2">A biased model makes biased decisions that can be unethical and illegal and sometimes renders such decisions meaningless. </span><span class="koboSpan" id="kobo.371.3">Additionally, the amount of data acquired for any specific label is rendered meaningless when all of them are duplicated or very similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">each other.</span></span></p>
<p><span class="koboSpan" id="kobo.373.1">Machine learning models are generally trained on a subset of the acquired data and then evaluated on other subsets of the data to verify the model’s performance on unseen data. </span><span class="koboSpan" id="kobo.373.2">When the part of the dataset that is not unique gets placed in the evaluation partition of the acquired dataset by chance, the model risks reporting </span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.374.1">scores that are biased against the duplicated </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">data inputs.</span></span></p>
<h3><span class="koboSpan" id="kobo.376.1">Fairness</span></h3>
<p><span class="koboSpan" id="kobo.377.1">Does the acquired dataset</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.378.1"> represent minority groups properly? </span><span class="koboSpan" id="kobo.378.2">Is the dataset biased toward the majority groups in the population? </span><span class="koboSpan" id="kobo.378.3">There can be many reasons why a machine learning model turns out to be biased, but one of the main causes is data representation bias. </span><span class="koboSpan" id="kobo.378.4">Making sure the data is represented fairly and equitably is an ethical responsibility of all machine learning practitioners. </span><span class="koboSpan" id="kobo.378.5">There are a lot of types of bias, so this topic will have its own section and will be introduced along with methods of mitigating it in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.379.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.380.1">, </span><em class="italic"><span class="koboSpan" id="kobo.381.1">Exploring Bias </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.382.1">and Fairness</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.384.1">Validity</span></h3>
<p><span class="koboSpan" id="kobo.385.1">Are there outliers in the</span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.386.1"> dataset? </span><span class="koboSpan" id="kobo.386.2">Is there missing data in the dataset? </span><span class="koboSpan" id="kobo.386.3">Did you accidentally add a blank audio or image file to the properly collected and annotated dataset? </span><span class="koboSpan" id="kobo.386.4">Is the annotated label for the data input considered a valid label? </span><span class="koboSpan" id="kobo.386.5">These are some of the questions you should ask when considering the validity of </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">your dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.388.1">Invalid data is useless for machine learning models and some of these complicate the pre-processing required for them. </span><span class="koboSpan" id="kobo.388.2">The reasons for invalidity can range from simple human errors to complex domain knowledge mistakes. </span><span class="koboSpan" id="kobo.388.3">One of the methods to mitigate invalid data is to separate validated and unvalidated data. </span><span class="koboSpan" id="kobo.388.4">Include some form of automated or manual data validation process before a data sample gets included in the validated dataset category. </span><span class="koboSpan" id="kobo.388.5">Some of this validation logic can be derived from business processes or just common sense. </span><span class="koboSpan" id="kobo.388.6">For example, if we are taking age as input data, there are acceptable age ranges and there are age ranges that are just completely impossible, such as 1,000 years old. </span><span class="koboSpan" id="kobo.388.7">Having simple guardrails and verifying these values early when collecting them makes it possible to correct them then and there to get accurate and valid data. </span><span class="koboSpan" id="kobo.388.8">Otherwise, these data will likely be discarded when it comes to the model-building stage. </span><span class="koboSpan" id="kobo.388.9">Maintaining a structured framework to validate data ensures that the majority of the data stays relevant and usable by machine learning models and free from </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">simple mistakes.</span></span></p>
<p><span class="koboSpan" id="kobo.390.1">As for more complex invalidity, such as errors in the domain ideology, domain experts play a big part in making sure the data stays sane and logical. </span><span class="koboSpan" id="kobo.390.2">Always make sure you include domain experts when defining the data inputs and outputs in the discussion about how data should be collected and annotated for </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">model development.</span></span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.392.1">Making sense of data through exploratory data analysis (EDA)</span></h2>
<p><span class="koboSpan" id="kobo.393.1">After the acquisition of data, it is crucial to analyze the data to inspect its characteristics, patterns that exist, and the general quality </span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.394.1">of the data. </span><span class="koboSpan" id="kobo.394.2">Knowing the type of data you are dealing with allows you to plan a strategy for the subsequent model-building stage. </span><span class="koboSpan" id="kobo.394.3">Plot distribution graphs, calculate statistics, and perform univariate and multivariate analysis to understand the inherent relationships between the data that can help further ensure the validity of the data. </span><span class="koboSpan" id="kobo.394.4">The methods of analysis for different variable types are different and can require some form of domain knowledge beforehand. </span><span class="koboSpan" id="kobo.394.5">In the following subsections, we will be practically going through </span><strong class="bold"><span class="koboSpan" id="kobo.395.1">exploratory data analysis</span></strong><span class="koboSpan" id="kobo.396.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.397.1">EDA</span></strong><span class="koboSpan" id="kobo.398.1">) for text-based data to get a sense of the benefits of carrying out an </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">EDA task.</span></span></p>
<h3><span class="koboSpan" id="kobo.400.1">Practical text EDA</span></h3>
<p><span class="koboSpan" id="kobo.401.1">In this section, we will </span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.402.1">be manually exploring and analyzing a text-specific dataset using Python code, with the motive of building a deep learning model later in this book using the same dataset. </span><span class="koboSpan" id="kobo.402.2">The dataset we use will predict the categories of an item on an Indian e-commerce website based on its textual description. </span><span class="koboSpan" id="kobo.402.3">This use case will be useful to automatically group advertised items for user recommendation usage and can help increase purchasing volume on the </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">e-commerce website:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.404.1">Let’s start by defining the libraries that we will use in a notebook. </span><span class="koboSpan" id="kobo.404.2">We will be using </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">pandas</span></strong><span class="koboSpan" id="kobo.406.1"> for data manipulation and structuring, </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">matplotlib</span></strong><span class="koboSpan" id="kobo.408.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">seaborn</span></strong><span class="koboSpan" id="kobo.410.1"> for plotting graphs, </span><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">tqdm</span></strong><span class="koboSpan" id="kobo.412.1"> for visualizing iteration progress, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">lingua</span></strong><span class="koboSpan" id="kobo.414.1"> for text </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">language detection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.416.1">
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from lingua import Language, LanguageDetectorBuilder
tqdm.pandas()</span></pre></li> <li><span class="koboSpan" id="kobo.417.1">Next, let’s load the text dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">pandas</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.421.1">
dataset = pd.read_csv('ecommerceDataset.csv')</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">pandas</span></strong><span class="koboSpan" id="kobo.423.1"> has some convenient functions to visualize and describe the loaded dataset; let’s use them. </span><span class="koboSpan" id="kobo.423.2">Let’s start by visualizing three rows of the </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">raw data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.425.1">
dataset.head(3)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.426.1">This will display the following figure in </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">your notebook:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer011">
<span class="koboSpan" id="kobo.428.1"><img alt="Figure 1.3 – Visualizing the text dataset samples" src="image/B18187_01_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.429.1">Figure 1.3 – Visualizing the text dataset samples</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.430.1">Next, let’s describe the dataset by visualizing the data </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">column-based statistics:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.432.1">
dataset.describe()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.433.1">This will display the following figure in </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">your notebook:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer012">
<span class="koboSpan" id="kobo.435.1"><img alt="Figure 1.4 – Showing the statistics of the dataset" src="image/B18187_01_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.436.1">Figure 1.4 – Showing the statistics of the dataset</span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.437.1">With these</span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.438.1"> visualizations, it’s obvious that the description of the dataset aligns with what exists in the dataset, where the </span><strong class="bold"><span class="koboSpan" id="kobo.439.1">category</span></strong><span class="koboSpan" id="kobo.440.1"> column contains four unique class categories paired with the text description data column named </span><strong class="bold"><span class="koboSpan" id="kobo.441.1">description</span></strong><span class="koboSpan" id="kobo.442.1"> with evidence showing that they are strings. </span><span class="koboSpan" id="kobo.442.2">One important insight from the describing function is that there are duplicates in the text description. </span><span class="koboSpan" id="kobo.442.3">We can remove duplicates by taking the first example row among all the duplicates, but we also have to make sure that the duplicates have the same category, so let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">do that:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.444.1">
for i in tqdm(range(len(unique_description_information))):
     assert(
    len(
      dataset[
        dataset['description'] ==
        unique_description_information.keys()[i]
      ]['category'].unique()
    ) == 1
  )
dataset.drop_duplicates(subset=['description'], inplace=True)</span></pre></li> <li><span class="koboSpan" id="kobo.445.1">Let’s check the data types of </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">the columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.447.1">
dataset.dtypes</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.448.1">This will display the following figure in </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">your notebook:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer013">
<span class="koboSpan" id="kobo.450.1"><img alt="Figure 1.5 – Showing the data types of the dataset columns" src="image/B18187_01_005.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.451.1">Figure 1.5 – Showing the data types of the dataset columns</span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.452.1">When some samples aren’t inherently a string data type, such as empty data or maybe numbers data, pandas automatically use an </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">Object</span></strong><span class="koboSpan" id="kobo.454.1"> data type that categorizes the entire column as data types that are unknown to pandas. </span><span class="koboSpan" id="kobo.454.2">Let’s check for </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">empty values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.456.1">
dataset.isnull().sum()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.457.1">This gives us the </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">f</span></span><span class="No-Break"><a id="_idIndexMarker058"/></span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">ollowing output:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer014">
<span class="koboSpan" id="kobo.460.1"><img alt="Figure 1.6 – Checking empty values" src="image/B18187_01_006.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.461.1">Figure 1.6 – Checking empty values</span></p>
<ol>
<li value="8"><span class="koboSpan" id="kobo.462.1">It looks like the description column has one empty value, as expected. </span><span class="koboSpan" id="kobo.462.2">This might be rooted in a mistake when acquiring the data or it might truly be empty. </span><span class="koboSpan" id="kobo.462.3">Either way, let’s remove that row as we can’t do anything to recover it and convert the columns </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">into strings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.464.1">
dataset.dropna(inplace=True)
for column in ['category', 'description']:
    dataset[column] = dataset[column].astype("string")</span></pre></li> <li><span class="koboSpan" id="kobo.465.1">Earlier, we discovered four unique categories. </span><span class="koboSpan" id="kobo.465.2">Let’s make sure we have a decent amount of samples for each category by visualizing </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">its distribution:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.467.1">
sns.countplot(x="category", data=dataset)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.468.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">following figure:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer015">
<span class="koboSpan" id="kobo.470.1"><img alt="Figure 1.7 – A graph showing category distribution" src="image/B18187_01_007.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.471.1">Figure 1.7 – A graph showing category distribution</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.472.1">Each category has a good amount of data samples and doesn’t look like there are any </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">anomaly categories.</span></span></p>
<ol>
<li value="10"><span class="koboSpan" id="kobo.474.1">The goal here is to predict the category of the selling item through the item’s description on the Indian e-commerce website. </span><span class="koboSpan" id="kobo.474.2">From that context, we know that Indian citizens speak Hindi, so the dataset might not contain only English data. </span><span class="koboSpan" id="kobo.474.3">Let’s try to estimate and verify the available languages in the dataset</span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.475.1"> using an open sourced language detector tool called Lingua. </span><span class="koboSpan" id="kobo.475.2">Lingua uses both rule-based and machine learning model-based methods to detect more than 70 text languages that work great for short phrases, single words, and sentences. </span><span class="koboSpan" id="kobo.475.3">Because of that, Lingua has a better runtime and memory performance. </span><span class="koboSpan" id="kobo.475.4">Let’s start by initializing the language detector instance from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">lingua</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.477.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.478.1">
detector = LanguageDetectorBuilder.from_all_languages(
      ).with_preloaded_language_models().build()</span></pre></li> <li><span class="koboSpan" id="kobo.479.1">Now, we will randomly sample a small portion of the dataset to detect language as the detection algorithm takes time to complete. </span><span class="koboSpan" id="kobo.479.2">Using a 10% fraction of the data should allow us to adequately understand </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.481.1">
sampled_dataset = dataset.sample(frac=0.1, random_state=1234)
sampled_dataset['language'] = sampled_dataset[
    'description'
].progress_apply(lambda x: detector.detect_language_of(x))</span></pre></li> <li><span class="koboSpan" id="kobo.482.1">Now, let’s visualize the distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">the language:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.484.1">
sampled_dataset['language'].value_counts().plot(kind='bar')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.485.1">This will show the following </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">graph plot:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer016">
<span class="koboSpan" id="kobo.487.1"><img alt="Figure 1.8 – Text language distribution" src="image/B18187_01_008.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.488.1">Figure 1.8 – Text language distribution</span></p>
<ol>
<li value="13"><span class="koboSpan" id="kobo.489.1">Interestingly, Lingua detected some</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.490.1"> anomalous samples that aren’t English. </span><span class="koboSpan" id="kobo.490.2">The anomalous languages look like they might be mistakes made by Lingua. </span><span class="koboSpan" id="kobo.490.3">Hindi is also detected among them; this is more convincing than the other languages as the data is from an Indian e-commerce website. </span><span class="koboSpan" id="kobo.490.4">Let’s check these </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">samples out:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.492.1">
sampled_dataset[
    sampled_dataset['language'] == Language.HINDI
].description.iloc[0]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.493.1">This will show the </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">following text:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer017">
<span class="koboSpan" id="kobo.495.1"><img alt="Figure 1.9 – Visualizing Hindi text" src="image/B18187_01_009.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.496.1">Figure 1.9 – Visualizing Hindi text</span></p>
<ol>
<li value="14"><span class="koboSpan" id="kobo.497.1">It looks like there is a mix of Hindi and English here. </span><span class="koboSpan" id="kobo.497.2">How about another language, such </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">as French?</span></span><pre class="source-code"><span class="koboSpan" id="kobo.499.1">
sampled_dataset[
    sampled_dataset['language'] == Language.FRENCH
].description.iloc[0]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.500.1">This will show the </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">following text:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.502.1"><img alt="Figure 1.10 – Visualizing French text" src="image/B18187_01_010.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.503.1">Figure 1.10 – Visualizing French text</span></p>
<ol>
<li value="15"><span class="koboSpan" id="kobo.504.1">It looks like </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">potpourri</span></strong><span class="koboSpan" id="kobo.506.1"> was </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.507.1">the focused word here as this is a borrowed French word, but the text is still </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">generally English.</span></span></li>
<li><span class="koboSpan" id="kobo.509.1">Since the list of languages does not include languages that do not use space as a separator between logical word units, let’s attempt to gauge the distribution of words by using a space-based word separation. </span><span class="koboSpan" id="kobo.509.2">Word counts and character counts can affect the parameters of a deep learning neural network, so it will be useful to understand these values </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">during EDA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.511.1">
dataset['word_count'] = dataset['description'].apply(
    lambda x: len(x.split())
)
plt.figure(figsize=(15,4))
sns.histplot(data=dataset, x="word_count", bins=10)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.512.1">This will show the following </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">bar plot:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer019">
<span class="koboSpan" id="kobo.514.1"><img alt="Figure 1.11 – Word count distribution" src="image/B18187_01_011.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.515.1">Figure 1.11 – Word count distribution</span></p>
<p><span class="koboSpan" id="kobo.516.1">From the exploration and analysis of the text data, we can deduce a couple of reasons that will help set up the model type and structure we should use during the model </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">development stage:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.518.1">The labels are decently sampled with 5,000-11,000 worth of samples per label, making them suitable for deep </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">learning algorithms.</span></span></li>
<li><span class="koboSpan" id="kobo.520.1">The original data is not clean, has missing data, and duplicates but is fixable through manual processing. </span><span class="koboSpan" id="kobo.520.2">Using it as-is for model development would have the potential of creating a </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">biased model.</span></span></li>
<li><span class="koboSpan" id="kobo.522.1">The dataset has more than one language but mostly English text; this will allow us to make appropriate model choices during the model </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">development stage.</span></span></li>
<li><span class="koboSpan" id="kobo.524.1">An abundance of samples has fewer than 1,000 words, and some samples have 1,000-8,000 words. </span><span class="koboSpan" id="kobo.524.2">In some non-critical use cases, we can safely cap the number of words to around 1,000 words so that we can build a model with better memory and </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">runtime performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.526.1">The preceding practical example</span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.527.1"> should provide a simple experience of performing EDA that will be sufficient to understand the benefit and importance of running an in-depth EDA before going into the model development stage. </span><span class="koboSpan" id="kobo.527.2">Similar to the practical text EDA, we prepared a practical EDA sample workflow for other datasets that includes audio, image, and video datasets in our Packt GitHub repository that you should explore to get your </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">hands dirty.</span></span></p>
<p><span class="koboSpan" id="kobo.529.1">A major concept to grasp in this section is the importance of EDA and the level of curiosity you should display to uncover the truth about your data. </span><span class="koboSpan" id="kobo.529.2">Some methods are generalizable to other similar datasets, but treating any specific EDA workflow as a silver bullet blinds you to the increasing research people are contributing to this field. </span><span class="koboSpan" id="kobo.529.3">Ask questions about your data whenever you suspect something of it and attempt to uncover the answers yourself by doing manual or automated inspections however possible. </span><span class="koboSpan" id="kobo.529.4">Be creative in obtaining these answers and stay hungry in learning new ways you can figure out key information on </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">your data.</span></span></p>
<p><span class="koboSpan" id="kobo.531.1">In this section, we have </span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.532.1">methodologically and practically gone through EDA processes for different types of data. </span><span class="koboSpan" id="kobo.532.2">Next, we will explore what it takes to prepare the data for actual </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">model consumption.</span></span></p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.534.1">Data pre-processing</span></h2>
<p><span class="koboSpan" id="kobo.535.1">Data pre-processing </span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.536.1">involves data cleaning, data structuring, and data transforming so that a deep learning model will be capable of using the processed data for model training, evaluation, and inferencing during deployment. </span><span class="koboSpan" id="kobo.536.2">The processed data should not only be prepared just for the machine learning model to accept but should generally be processed in a way that optimizes the learning potential and increases the metric performance of the machine </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">learning model.</span></span></p>
<p><span class="koboSpan" id="kobo.538.1">Data cleaning is a process that </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.539.1">aims to increase the quality of the data acquired. </span><span class="koboSpan" id="kobo.539.2">An EDA process is a prerequisite to figuring out anything wrong with the dataset before some form of data cleaning can be done. </span><span class="koboSpan" id="kobo.539.3">Data cleaning and EDA are often executed iteratively until a satisfactory data quality level is achieved. </span><span class="koboSpan" id="kobo.539.4">Cleaning can be as simple as duplicate values removal, empty values removal, or removing values that don’t make logical sense, either in terms of common sense or through business logic. </span><span class="koboSpan" id="kobo.539.5">These are concepts that we explained earlier, where the same risks and issues </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">are applied.</span></span></p>
<p><span class="koboSpan" id="kobo.541.1">Data structuring, on the </span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.542.1">other hand, is a process that orchestrates the data ingest and loading process from the stored data that is cleaned and verified of its quality. </span><span class="koboSpan" id="kobo.542.2">This process determines how data should be loaded from a source or multiple of them and fed into the deep learning model. </span><span class="koboSpan" id="kobo.542.3">Sounds simple enough, right? </span><span class="koboSpan" id="kobo.542.4">This could be very simple if this is a small, single CSV dataset where there wouldn’t be any performance or memory issues. </span><span class="koboSpan" id="kobo.542.5">In reality, this could be very complex in cases where data might be partitioned and stored in different sources due to storage limitations. </span><span class="koboSpan" id="kobo.542.6">Here are some concrete factors you’d need to consider in </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">this process:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.544.1">Do you have enough RAM </span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.545.1">in your computer to process your desired batch size to supply data for your model? </span><span class="koboSpan" id="kobo.545.2">Make sure you also take your model size into account so that you won’t get memory overloads</span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.546.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.547.1">Out of Memory</span></strong><span class="koboSpan" id="kobo.548.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.549.1">OOM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">) errors!</span></span></li>
<li><span class="koboSpan" id="kobo.551.1">Is your data from different sources? </span><span class="koboSpan" id="kobo.551.2">Make sure you have permission to access these </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">data sources.</span></span></li>
<li><span class="koboSpan" id="kobo.553.1">Is the speed latency when accessing these sources acceptable? </span><span class="koboSpan" id="kobo.553.2">Consider moving this data to a better hardware resource that you can access with higher speeds, such</span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.554.1"> as a </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">solid-state drive</span></strong><span class="koboSpan" id="kobo.556.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.557.1">SSD</span></strong><span class="koboSpan" id="kobo.558.1">) instead of </span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.559.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.560.1">hard disk drive</span></strong><span class="koboSpan" id="kobo.561.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.562.1">HDD</span></strong><span class="koboSpan" id="kobo.563.1">), and from a remote network-accessible source to a direct local </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">hardware source.</span></span></li>
<li><span class="koboSpan" id="kobo.565.1">Do you even have enough local storage to store this data? </span><span class="koboSpan" id="kobo.565.2">Make sure you have enough storage to store this data, don’t overload the storage and risk performance slowdowns or worse, </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">computer breakdowns.</span></span></li>
<li><span class="koboSpan" id="kobo.567.1">Optimize the data loading and processing process so that it is fast. </span><span class="koboSpan" id="kobo.567.2">Store and cache outputs of data processes that are fixed so that you can save time that can be used to recompute </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">these outputs.</span></span></li>
<li><span class="koboSpan" id="kobo.569.1">Make sure the data structuring process is deterministic, even when there are processes that need randomness. </span><strong class="bold"><span class="koboSpan" id="kobo.570.1">Randomly deterministic</span></strong><span class="koboSpan" id="kobo.571.1"> is when the randomness can be reproduced in a repeat of the cycle. </span><span class="koboSpan" id="kobo.571.2">Determinism</span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.572.1"> helps make sure that the results that have been obtained can be reproduced and make sure model-building methods can be compared fairly </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">and reliably.</span></span></li>
<li><span class="koboSpan" id="kobo.574.1">Log data so that you can debug the process </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">when needed.</span></span></li>
<li><span class="koboSpan" id="kobo.576.1">Data partitioning methods. </span><span class="koboSpan" id="kobo.576.2">Make sure a proper cross-validation strategy is chosen that’s suitable for your dataset. </span><span class="koboSpan" id="kobo.576.3">If a time-based feature is included, consider whether you should construct a time-based partitioning </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.577.1">method where the training data consists of earlier time examples and the evaluation data is in the future. </span><span class="koboSpan" id="kobo.577.2">If not, a stratified partitioning method would be your </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">best bet.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.579.1">Different deep learning</span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.580.1"> frameworks, such</span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.581.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">PyTorch</span></strong><span class="koboSpan" id="kobo.583.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.584.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.585.1">, provide different </span><strong class="bold"><span class="koboSpan" id="kobo.586.1">application programming interfaces</span></strong><span class="koboSpan" id="kobo.587.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.588.1">APIs</span></strong><span class="koboSpan" id="kobo.589.1">) to implement </span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.590.1">the data structuring process. </span><span class="koboSpan" id="kobo.590.2">Some frameworks provide simpler interfaces that allow for easy setup pipelines while some frameworks provide complex interfaces that allow for a higher level of flexibility. </span><span class="koboSpan" id="kobo.590.3">Fortunately, many high-level libraries attempt to simplify the complex interfaces while maintaining flexibility, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">keras</span></strong><span class="koboSpan" id="kobo.592.1"> on top of TensorFlow, </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">Catalyst</span></strong><span class="koboSpan" id="kobo.594.1"> on top of PyTorch, </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">fast ai</span></strong><span class="koboSpan" id="kobo.596.1"> on top of PyTorch, </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">pytorch lightning</span></strong><span class="koboSpan" id="kobo.598.1"> on top of PyTorch, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">ignite</span></strong><span class="koboSpan" id="kobo.600.1"> on top </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">of PyTorch.</span></span></p>
<p><span class="koboSpan" id="kobo.602.1">Finally, data transformation</span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.603.1"> is a process </span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.604.1">that applies unique data variable-specific pre-processing to transform the raw cleaned data into more a representable, usable, and learnable format. </span><span class="koboSpan" id="kobo.604.2">An important factor to consider when attempting to execute the data structuring and transformation process is the type of deep learning model you intend to use. </span><span class="koboSpan" id="kobo.604.3">Any form of data transformation is often dependent on the deep learning architecture and dependent on the type of inputs it can accept. </span><span class="koboSpan" id="kobo.604.4">The most widely known and common deep learning model architectures are invented to tackle specific data types, such as convolutional neural networks for image data, transformer models for sequence-based data, and basic multilayer perceptrons for tabular data. </span><span class="koboSpan" id="kobo.604.5">However, deep learning models are considered to be flexible algorithms that can twist and bend to accept data of different forms and sizes, even in multimodal data conditions. </span><span class="koboSpan" id="kobo.604.6">Through collaboration with domain experts from the past few years, deep learning experts have been able to build creative forms of deep learning architectures that can handle multiple data modalities and even multiple unstructured data modalities that succeeded in learning cross-modality patterns. </span><span class="koboSpan" id="kobo.604.7">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">two examples:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.606.1">Robust Self-Supervised Audio-Visual Speech Recognition</span></em><span class="koboSpan" id="kobo.607.1">, by Meta AI (formerly </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">Facebook) (</span></span><a href="https://arxiv.org/pdf/2201.01763v2.pdf):"><span class="No-Break"><span class="koboSpan" id="kobo.609.1">https://arxiv.org/pdf/2201.01763v2.pdf</span></span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">):</span></span></a><ul><li><span class="koboSpan" id="kobo.611.1">This tackled the problem of speech recognition in the presence of multiple speeches by building a deep learning transformer-based model that can take in both audio and visual data </span><a id="_idIndexMarker078"/><span class="No-Break"><span class="koboSpan" id="kobo.612.1">called AV-HuBERT</span></span></li><li><span class="koboSpan" id="kobo.613.1">Visual data acted as supplementary data to help the deep learning model discern which speaker to </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">focus on.</span></span></li><li><span class="koboSpan" id="kobo.615.1">It achieved the latest state-of-the-art results on the LRS3-TED visual and audio lip </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">reading dataset</span></span></li></ul></li>
<li><em class="italic"><span class="koboSpan" id="kobo.617.1">Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework</span></em><span class="koboSpan" id="kobo.618.1">, by DAMO Academy and Alibaba </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">Group (</span></span><a href="https://arxiv.org/pdf/2202.03052v1.pdf):"><span class="No-Break"><span class="koboSpan" id="kobo.620.1">https://arxiv.org/pdf/2202.03052v1.pdf</span></span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">):</span></span></a><ul><li><span class="koboSpan" id="kobo.622.1">They built a model that took in text and image data and published a </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">pre-trained model</span></span></li><li><span class="koboSpan" id="kobo.624.1">At achieved state-of-the-art results on an image captioning task on the COCO </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">captions dataset</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.626.1">With that being said, data transformations are mainly differentiated into </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.627.1">two parts: </span><strong class="bold"><span class="koboSpan" id="kobo.628.1">feature engineering</span></strong><span class="koboSpan" id="kobo.629.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">data scaling</span></strong><span class="koboSpan" id="kobo.631.1">. </span><span class="koboSpan" id="kobo.631.2">Deep</span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.632.1"> learning is widely known for its feature engineering capabilities, which replace the need to manually craft custom features from raw data for learning. </span><span class="koboSpan" id="kobo.632.2">However, this doesn’t mean that it always makes sense to not perform any feature engineering. </span><span class="koboSpan" id="kobo.632.3">Many successful deep learning models have utilized engineered forms of features </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">as input.</span></span></p>
<p><span class="koboSpan" id="kobo.634.1">Now that we know what data</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.635.1"> pre-processing entails, let’s discuss and explore different data pre-processing techniques for unstructured data, both theoretically </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">and practically.</span></span></p>
<h3><span class="koboSpan" id="kobo.637.1">Text data pre-processing</span></h3>
<p><span class="koboSpan" id="kobo.638.1">Text data can be in </span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.639.1">different </span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.640.1">languages and exist in different domains, ranging from description data to informational documents and natural human text comments. </span><span class="koboSpan" id="kobo.640.2">Some of the most common text data pre-processing methods that are used for deep learning are </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">Stemming</span></strong><span class="koboSpan" id="kobo.643.1">: A process </span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.644.1">that removes the suffix of words in an attempt to reduce words into their base form. </span><span class="koboSpan" id="kobo.644.2">This promotes the cross-usage of the same features for different forms of the </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">same word.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.646.1">Lemmatization</span></strong><span class="koboSpan" id="kobo.647.1">: A process</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.648.1"> that reduces a word into its base form that produces real English words. </span><span class="koboSpan" id="kobo.648.2">Lemmatization has many of the same benefits as stemming but is considered better due to the linguistically valid word reduction outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">it produces.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.650.1">Text tokenization</span></strong><span class="koboSpan" id="kobo.651.1">, by </span><strong class="bold"><span class="koboSpan" id="kobo.652.1">Byte Pair Encoding</span></strong><span class="koboSpan" id="kobo.653.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.654.1">BPE</span></strong><span class="koboSpan" id="kobo.655.1">): Tokenization</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.656.1"> is a process that splits text into different parts</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.657.1"> that will be encoded and used by the deep learning models. </span><span class="koboSpan" id="kobo.657.2">BPE is a sub-word-based text-splitting algorithm that allows common words to be outputted as a single token but rare words get split into multiple tokens. </span><span class="koboSpan" id="kobo.657.3">These split tokens can reuse representations from matching sub-words. </span><span class="koboSpan" id="kobo.657.4">This is to reduce the vocabulary that can exist at any one time, reduce the amount of out-of-vocabulary tokens, and allow token representations to be learned </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">more efficiently.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.659.1">One uncommon pre-processing method that will be useful to build more generalizable text deep learning models is text data augmentation. </span><span class="koboSpan" id="kobo.659.2">Text data augmentation can be done in a </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">few ways:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.661.1">Replacing verbs with their synonyms</span></strong><span class="koboSpan" id="kobo.662.1">: This can be done by using the set of synonym dictionaries from the NLTK library’s WordNet English lexical database. </span><span class="koboSpan" id="kobo.662.2">The obtained augmented text will maintain the same meaning with verb </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">synonym replacement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.664.1">Back translation</span></strong><span class="koboSpan" id="kobo.665.1">: This involves translating text into another language and back to the original language using translation services such as Google or using open sourced translation models. </span><span class="koboSpan" id="kobo.665.2">The obtained back-translated</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.666.1"> text will </span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.667.1">be in a slightly </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">different form.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.669.1">Audio data pre-processing</span></h3>
<p><span class="koboSpan" id="kobo.670.1">Audio data is essentially</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.671.1"> sequence-based data and, in</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.672.1"> some cases, multiple sequences exist. </span><span class="koboSpan" id="kobo.672.2">One of the most commonly used pre-processing methods for audio is raw audio data transformed into different forms of spectrograms</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.673.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.674.1">Short-Time Fourier Transform</span></strong><span class="koboSpan" id="kobo.675.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.676.1">STFT</span></strong><span class="koboSpan" id="kobo.677.1">), which is a process that converts audio from the time domain into the frequency domain. </span><span class="koboSpan" id="kobo.677.2">A spectrogram audio conversion allows audio data to be broken down and represented in a range of frequencies instead of a single waveform representation that is a combination of the signals from all audio frequencies. </span><span class="koboSpan" id="kobo.677.3">These spectrograms are two-dimensional data and thus can be treated as an image and fed into convolutional neural networks. </span><span class="koboSpan" id="kobo.677.4">Data scaling methods such as log scaling and log-mel scaling are also commonly applied to these spectrograms to further emphasize </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">frequency characteristics.</span></span></p>
<h3><span class="koboSpan" id="kobo.679.1">Image data pre-processing</span></h3>
<p><span class="koboSpan" id="kobo.680.1">Image data augmentation </span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.681.1">is a type</span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.682.1"> of image-based feature engineering technique that is capable of increasing the comprehensiveness potential of the original data. </span><span class="koboSpan" id="kobo.682.2">A best practice for applying this technique is to structure the data pipeline to apply image augmentations randomly during the training process by batch instead of providing a fixed augmented set of data for the deep learning model. </span><span class="koboSpan" id="kobo.682.3">Choosing the type of image augmentation requires some understanding of the business requirements of the use case. </span><span class="koboSpan" id="kobo.682.4">Here are some examples where it doesn’t make sense to apply </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">certain augmentations:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.684.1">When the orientation of the image affects the validity of the target label, orientation modification types of augmentation such as rotation and image flipping wouldn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">be suitable</span></span></li>
<li><span class="koboSpan" id="kobo.686.1">When the color of the image affects the validity of the target label, color modification types of augmentation such as grayscale, channel shuffle, hue saturation shift, and RGB shift </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">aren’t suitable</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.688.1">After excluding obvious augmentations that won’t be suitable, a common but effective method to figure out the best set of augmentations list is iterative experiments and </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">model comparisons.</span></span></p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.690.1">Developing deep learning models</span></h1>
<p><span class="koboSpan" id="kobo.691.1">Let’s start with a short recap of </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.692.1">what deep learning is. </span><span class="koboSpan" id="kobo.692.2">Deep learning’s core foundational building block is a neural network. </span><span class="koboSpan" id="kobo.692.3">A neural network is an algorithm that was made to simulate the human brain. </span><span class="koboSpan" id="kobo.692.4">Its building blocks are called neurons, which mimic the billions of neurons the human brain contains. </span><span class="koboSpan" id="kobo.692.5">Neurons, in the context of neural networks, are objects that store simple information called weights and biases. </span><span class="koboSpan" id="kobo.692.6">Think of these as the memory of </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">the algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.694.1">Deep learning architectures are essentially neural network architectures that have three or more neural network layers. </span><span class="koboSpan" id="kobo.694.2">Neural network layers can be categorized into three high-level groups – the input layer, the hidden layer, and the output layer. </span><span class="koboSpan" id="kobo.694.3">The input layer is the simplest layer group and whose functionality is to pass the input data to subsequent layers. </span><span class="koboSpan" id="kobo.694.4">This layer group does not contain biases and can be considered passive neurons, but the group still contains weights in its connections to neurons from subsequent layers. </span><span class="koboSpan" id="kobo.694.5">The hidden layer comprises neurons that contain biases and weights in their connections to neurons from subsequent layers. </span><span class="koboSpan" id="kobo.694.6">Finally, the output layer comprises neurons that relate to the number of classes and problem types and contains bias. </span><span class="koboSpan" id="kobo.694.7">A best practice when counting neural network layers is to exclude the input layer when doing so. </span><span class="koboSpan" id="kobo.694.8">So, a neural network with one input layer, one hidden layer, and one output layer is considered to be a two-layer neural network. </span><span class="koboSpan" id="kobo.694.9">The following figure shows a basic neural network, called </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.695.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.696.1">multilayer perceptron</span></strong><span class="koboSpan" id="kobo.697.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.698.1">MLP</span></strong><span class="koboSpan" id="kobo.699.1">), with a single input layer, a single hidden layer, and a single </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">output layer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.701.1"><img alt="Figure 1.12 – A simple deep learning architecture, also called an MLP" src="image/B18187_01_012.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.702.1">Figure 1.12 – A simple deep learning architecture, also called an MLP</span></p>
<p><span class="koboSpan" id="kobo.703.1">Being a subset of the wider </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.704.1">machine learning category, deep learning models are capable of learning patterns from the data through a loss function and an optimizer algorithm that optimizes the loss function. </span><span class="koboSpan" id="kobo.704.2">A loss function defines the error made by the model so that its memory (weights and biases) can be updated to perform better in the next iteration. </span><span class="koboSpan" id="kobo.704.3">An optimizer algorithm is an algorithm that decides the strategy to update the weights given the </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">loss value.</span></span></p>
<p><span class="koboSpan" id="kobo.706.1">With this short recap, let’s dive into a summary of the common deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">model families.</span></span></p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.708.1">Deep learning model families</span></h2>
<p><span class="koboSpan" id="kobo.709.1">These layers can come in many</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.710.1"> forms as researchers have been able to invent new layer definitions to tackle new problem types and almost always comes with a non-linear activation function that allows the model to capture non-linear relationships between the data. </span><span class="koboSpan" id="kobo.710.2">Along with the variation of layers come many different deep learning architecture families that are meant for different problem types. </span><span class="koboSpan" id="kobo.710.3">A few of the most common and widely used deep learning models are </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.712.1">MLP</span></strong><span class="koboSpan" id="kobo.713.1"> for tabular </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.714.1">data types. </span><span class="koboSpan" id="kobo.714.2">This will be explored in </span><a href="B18187_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.715.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.716.1">, </span><em class="italic"><span class="koboSpan" id="kobo.717.1">Designing Deep </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.718.1">Learning Architectures</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.720.1">Convolutional neural network</span></strong><span class="koboSpan" id="kobo.721.1"> for </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.722.1">image data types. </span><span class="koboSpan" id="kobo.722.2">This will be explored in </span><a href="B18187_03.xhtml#_idTextAnchor051"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.723.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.724.1">, </span><em class="italic"><span class="koboSpan" id="kobo.725.1">Understanding Convolutional </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.726.1">Neural Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.728.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.729.1"> for </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.730.1">anomaly detection, data compression, data denoising, and feature representation learning. </span><span class="koboSpan" id="kobo.730.2">This will be explored in </span><a href="B18187_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.731.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.732.1">, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.733.1">Understanding Autoencoders</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.735.1">Gated recurrent unit</span></strong><span class="koboSpan" id="kobo.736.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.737.1">GRU</span></strong><span class="koboSpan" id="kobo.738.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.740.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.741.1">LSTM</span></strong><span class="koboSpan" id="kobo.742.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.743.1">Transformers</span></strong><span class="koboSpan" id="kobo.744.1"> for sequence </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.745.1">data </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.746.1">types. </span><span class="koboSpan" id="kobo.746.2">These</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.747.1"> will be explored in </span><a href="B18187_04.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.748.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.749.1">, </span><em class="italic"><span class="koboSpan" id="kobo.750.1">Understanding Recurrent Neural Networks</span></em><span class="koboSpan" id="kobo.751.1">, and </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.752.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.753.1">, </span><em class="italic"><span class="koboSpan" id="kobo.754.1">Understanding Neural Network </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.755.1">Transformers</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">, respectively.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.757.1">These architectures will be the focus of </span><em class="italic"><span class="koboSpan" id="kobo.758.1">Chapters 2 to 6</span></em><span class="koboSpan" id="kobo.759.1">, where we will discuss their methodology and go through some practical evaluation. </span><span class="koboSpan" id="kobo.759.2">Next, let’s discover the problem types we can tackle in </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">deep learning.</span></span></p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.761.1">The model development strategy</span></h2>
<p><span class="koboSpan" id="kobo.762.1">Today, deep learning models </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.763.1">are easy to invent and create due to the advent of deep learning frameworks such as PyTorch and TensorFlow, along with their high-level library wrappers. </span><span class="koboSpan" id="kobo.763.2">Which framework you should choose at this point is a matter of preference regarding their interfaces as both frameworks are matured with years of improvement work done. </span><span class="koboSpan" id="kobo.763.3">Only when there is a pressing need for a very custom function to tackle a unique problem type will you need to choose the framework that can execute what you need. </span><span class="koboSpan" id="kobo.763.4">Once you’ve chosen your deep learning framework, the deep model creation, training, and evaluation process is pretty much covered </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">all around.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">However, model management functions do not come out of the box from these frameworks. </span><span class="koboSpan" id="kobo.765.2">Model management is an area of technology that allows teams, businesses, and deep learning practitioners to reliably, quickly, and effectively build models, evaluate models, deliver model insights, deploy models to production, and govern models. </span><span class="koboSpan" id="kobo.765.3">Model management can sometimes be referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.766.1">machine learning operations</span></strong><span class="koboSpan" id="kobo.767.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.768.1">MLOps</span></strong><span class="koboSpan" id="kobo.769.1">). </span><span class="koboSpan" id="kobo.769.2">You </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.770.1">might still be wondering why you’d need such functionalities, especially if you’ve been building some deep learning models off Kaggle, a platform that hosts data and machine learning problems as competitions. </span><span class="koboSpan" id="kobo.770.2">So, here are some factors that drive the need to utilize </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">these functionalities:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.772.1">It is cumbersome to compare </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">models manually:</span></span><ul><li><span class="koboSpan" id="kobo.774.1">Manually typing performance data in an Excel sheet to keep track of model performance is slow </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">and unreliable</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.776.1">Model artifacts are hard to keep </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">track of:</span></span><ul><li><span class="koboSpan" id="kobo.778.1">A model has many artifacts, such as its trained weights, performance graphs, feature importance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">prediction explanations</span></span></li><li><span class="koboSpan" id="kobo.780.1">It is also cumbersome to compare </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">model artifacts</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.782.1">Model versioning is needed to make sure model-building experiments are </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">not repeated:</span></span><ul><li><span class="koboSpan" id="kobo.784.1">Overriding the top-performing model with the most reliable model insights is the last thing you want </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">to experience</span></span></li><li><span class="koboSpan" id="kobo.786.1">Versioning should depend on the data partitioning method, model settings, and software </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">library versions</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.788.1">It is not straightforward to deploy and </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">govern models</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.790.1">Depending on the size of the team involved in the project and how often components need to be reused, different software </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.791.1">and libraries would fit the bill. </span><span class="koboSpan" id="kobo.791.2">These software and libraries are split into paid and free (usually open sourced) categories. </span><strong class="bold"><span class="koboSpan" id="kobo.792.1">Metaflow</span></strong><span class="koboSpan" id="kobo.793.1">, an </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.794.1">open sourced software, is suitable for bigger data science teams where there are many chances of components needing to be reused across other projects and </span><strong class="bold"><span class="koboSpan" id="kobo.795.1">MLFlow</span></strong><span class="koboSpan" id="kobo.796.1"> (open sourced software) would be more suitable for small or single-person teams. </span><span class="koboSpan" id="kobo.796.2">Other notable model management tools are </span><strong class="bold"><span class="koboSpan" id="kobo.797.1">Comet</span></strong><span class="koboSpan" id="kobo.798.1"> (paid), </span><strong class="bold"><span class="koboSpan" id="kobo.799.1">Weights &amp; Biases</span></strong><span class="koboSpan" id="kobo.800.1"> (paid), </span><strong class="bold"><span class="koboSpan" id="kobo.801.1">Neptune </span></strong><span class="koboSpan" id="kobo.802.1">(paid), and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.803.1">Algorithmia</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.804.1"> (paid).</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">With that, we have provided a brief overview of deep learning model development methodology and strategy; we will dive deeper into model development topics in the next few chapters. </span><span class="koboSpan" id="kobo.805.2">But before that, let’s continue with an overview of the topic of delivering </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">model insights.</span></span></p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.807.1">Delivering model insights</span></h1>
<p><span class="koboSpan" id="kobo.808.1">Model metric</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.809.1"> performance, when used exclusively for model comparisons and the model choosing process, is often not the most effective way to reliably obtain the true best model. </span><span class="koboSpan" id="kobo.809.2">When people care about the decisions that can potentially be made by the machine learning model, they typically require more information and insights to eventually put their trust in the ability of the model to make decisions. </span><span class="koboSpan" id="kobo.809.3">Ultimately, when models are not trusted, they don’t get deployed. </span><span class="koboSpan" id="kobo.809.4">However, trust doesn’t just depend on insights of the model. </span><span class="koboSpan" id="kobo.809.5">Building trust in a model involves ensuring accurate, reliable, and unbiased predictions that align with domain expertise and business objectives, while providing stakeholders with insights into the model’s performance metrics, decision-making logic, and rationale behind its predictions. </span><span class="koboSpan" id="kobo.809.6">Addressing potential biases and demonstrating fairness are crucial for gaining confidence in the model’s dependability. </span><span class="koboSpan" id="kobo.809.7">This ongoing trust-building process extends beyond initial deployment, as the model must consistently exhibit sound decision-making, justify predictions, and maintain unbiased performance. </span><span class="koboSpan" id="kobo.809.8">By fostering trust, the model becomes a valuable and reliable tool for real-world applications, leading to increased adoption and utilization across various domains </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">and industries.</span></span></p>
<p><span class="koboSpan" id="kobo.811.1">Deliver model insights that matter to the business. </span><span class="koboSpan" id="kobo.811.2">Other than delivering model insights with the obvious goal of ensuring model trust and eliminating trust issues, actual performance metrics are equally important. </span><span class="koboSpan" id="kobo.811.3">Make sure you translate model metrics into layman’s business metrics whenever possible to effectively communicate the potential positive impact that the model can bring to the business. </span><span class="koboSpan" id="kobo.811.4">Success metrics, which are defined earlier in the planning phase, should be reported with actual values at </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">this stage.</span></span></p>
<p><span class="koboSpan" id="kobo.813.1">The process of inducing trust in a model doesn’t stop after the model gets deployed. </span><span class="koboSpan" id="kobo.813.2">Similar to how humans are required to explain their decisions in life, machine learning models (if expected to replace humans to automate the decisioning process) are also required to do so. </span><span class="koboSpan" id="kobo.813.3">This process is called prediction explanations. </span><span class="koboSpan" id="kobo.813.4">In some cases, model decisions are expected to be used as a reference where there is a human in the loop that acts as a domain expert to verify decisions before the decisions are carried out. </span><span class="koboSpan" id="kobo.813.5">Prediction explanations are almost always a necessity in these conditions where the users of the model are interested in why the model made its predictions instead of using the </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">predictions directly.</span></span></p>
<p><span class="koboSpan" id="kobo.815.1">Model insights also allow you to improve a model’s performance. </span><span class="koboSpan" id="kobo.815.2">Remember that the machine learning life cycle is naturally an iterative process. </span><span class="koboSpan" id="kobo.815.3">Some concrete examples of where this condition could happen are </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.817.1">You realize that the model is biased against a particular group and either go back to the data acquisition stage to acquire more data from the less represented group or change to a modeling technique that is robust </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">to bias</span></span></li>
<li><span class="koboSpan" id="kobo.819.1">You realize that the model performs badly in one class and go back to the model development stage to use a different deep learning model loss function that can focus on the badly </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">performing class</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.821.1">Deep learning models are known to be a black box model. </span><span class="koboSpan" id="kobo.821.2">However, in reality, today, there have been many published research papers on deep learning explanation methods that have allowed deep learning to break the boundaries of a black box. </span><span class="koboSpan" id="kobo.821.3">We will dive into the different ways we can interpret and provide insights for deep learning models in </span><em class="italic"><span class="koboSpan" id="kobo.822.1">Part 2</span></em><span class="koboSpan" id="kobo.823.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.825.1">Now that we have more context of the processes involved in the deep learning life cycle, in the next section, we will discuss </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.826.1">risks that can exist throughout the life cycle of the project that you need to </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">worry about.</span></span></p>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.828.1">Managing risks</span></h1>
<p><span class="koboSpan" id="kobo.829.1">Deep learning systems </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.830.1">are exposed to a multitude of risks starting early, from inception to system adoption. </span><span class="koboSpan" id="kobo.830.2">Usually, most people who are assigned to work on a deep learning project are responsible only for a specific stage of the machine learning life cycle, such as model development or data preparation. </span><span class="koboSpan" id="kobo.830.3">This practice can be detrimental when the work of one stage in the life cycle generates problems at a later stage, which often happens in cases where the team members involved have little sense of the bigger picture in play. </span><span class="koboSpan" id="kobo.830.4">Risks involved in a deep learning system generally involve interactions between stages in the machine learning life cycle and follow the concept of </span><em class="italic"><span class="koboSpan" id="kobo.831.1">Garbage in, garbage out</span></em><span class="koboSpan" id="kobo.832.1">. </span><span class="koboSpan" id="kobo.832.2">Making sure everyone working on building the system has a sense of accountability for what eventually gets outputted from the entirety of the system instead of just the individual stages is one of the foundational keys to managing risks in a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">learning system.</span></span></p>
<p><span class="koboSpan" id="kobo.834.1">But what are the risks? </span><span class="koboSpan" id="kobo.834.2">Let’s start with something that can be handled way before anything tangible is made – something that happens when a use case is evaluated </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">for worthiness.</span></span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.836.1">Ethical and regulatory risks</span></h2>
<p><span class="koboSpan" id="kobo.837.1">Deep learning can be</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.838.1"> applied in practically</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.839.1"> any industry but some of the hardest industries to get deep learning adopted in are the highly regulated industries, such as the medical and financial industries. </span><span class="koboSpan" id="kobo.839.2">The regulations that are imposed in these industries ultimately determine what a deep learning system can or cannot do in the industry. </span><span class="koboSpan" id="kobo.839.3">Regulations are mostly introduced by governments and most commonly involve ethical and legal considerations. </span><span class="koboSpan" id="kobo.839.4">In these highly regulated industries, it is common to experience audits being conducted monthly, weekly, or even daily to make sure the companies are compliant with the regulations imposed. </span><span class="koboSpan" id="kobo.839.5">One of the main reasons certain industries are regulated more aggressively is that the repercussions of any actions in these industries bear a heavy cost to the well-being of the people or the country. </span><span class="koboSpan" id="kobo.839.6">Deep learning systems need to be built in a way that they will be compliant with these regulations to avoid the risk of facing decommission, the risk of not getting adopted at all, and, worst of all, the risk of getting a huge fine by </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">regulatory officials.</span></span></p>
<p><span class="koboSpan" id="kobo.841.1">At times, deep learning models can perform way better than their human counterparts but, on the other side, no deep learning model is perfect. </span><span class="koboSpan" id="kobo.841.2">Humans make mistakes, everybody knows that for a fact, but another reality we need to realize is that machines undoubtedly also make mistakes. </span><span class="koboSpan" id="kobo.841.3">The highest risk is when humans trust the machines so much that they give 100% trust in them to make decisions! </span><span class="koboSpan" id="kobo.841.4">So, how can we account for these mistakes and who will be responsible </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">for them?</span></span></p>
<p><span class="koboSpan" id="kobo.843.1">Let’s see what we humans do in situations without machines. </span><span class="koboSpan" id="kobo.843.2">When the stakes are high, important decisions always go through a hierarchy of approvals before a decision can be final. </span><span class="koboSpan" id="kobo.843.3">These hierarchies of approvals signal the need to make important decisions accountable and reliable. </span><span class="koboSpan" id="kobo.843.4">The more approvals we get for a decision, the more we can say that we are confident in making that important decision. </span><span class="koboSpan" id="kobo.843.5">Some examples of important decisions that commonly require a hierarchy of approvals include the decision to hire an employee in a company, the decision on insurance premiums to charge, or the decision of whether to invest money into a certain business. </span><span class="koboSpan" id="kobo.843.6">With that context in mind, deep learning systems need to have similar workflows for obtaining approvals when a deep learning model makes a certain predictive decision in high-stakes use cases. </span><span class="koboSpan" id="kobo.843.7">These workflows can include any form of insight, and an explanation of the predictions can be used to make it easier for domain experts to judge the validity of the decisions. </span><span class="koboSpan" id="kobo.843.8">Adding a human touch makes the deep learning system many times more ethical and trustable enough to be part of the high-stake </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">decision workflow.</span></span></p>
<p><span class="koboSpan" id="kobo.845.1">Let’s take a medical industry use case – for instance, a use case to predict lung disease through X-ray scans. </span><span class="koboSpan" id="kobo.845.2">From an ethical standpoint, it is not right that a deep learning model can have the complete power to strip a person’s hope of life by predicting extremely harmful diseases such as end-stage lung cancer. </span><span class="koboSpan" id="kobo.845.3">If the predicted extreme disease is misdiagnosed, patients may have grieved unnecessarily or spent an unnecessary amount of money on expensive tests to verify the claims. </span><span class="koboSpan" id="kobo.845.4">Having an approval workflow in the deep learning </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.846.1">system allows </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.847.1">doctors to use these results as an assistive method would solve the ethical concerns of using an automated </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">decisioning system.</span></span></p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.849.1">Business context mismatch</span></h2>
<p><span class="koboSpan" id="kobo.850.1">Reiterating the previous </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.851.1">point in the </span><em class="italic"><span class="koboSpan" id="kobo.852.1">Defining success</span></em><span class="koboSpan" id="kobo.853.1"> section, aligning the desired deep learning input data and target label choice to the</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.854.1"> business context and how the target predictions are consumed makes deep learning systems adoptable. </span><span class="koboSpan" id="kobo.854.2">The risk involved here is when the business value is either not properly defined or not matched properly by the deep learning system. </span><span class="koboSpan" id="kobo.854.3">Even when the target is appropriate for the business context, how these predictions are consumed holds the key to deriving value. </span><span class="koboSpan" id="kobo.854.4">Failing to match the business context in any way simply risks the rejection of </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">the systems.</span></span></p>
<p><span class="koboSpan" id="kobo.856.1">Understanding the business context involves understanding who the targeted user groups are and who they are not. </span><span class="koboSpan" id="kobo.856.2">A key step in this process is documenting and building user group personas. </span><span class="koboSpan" id="kobo.856.3">User personas hold information about their workflows, history, statistics, and concerns that should provide the context needed to build a system that aligns with the specific needs of potential users. </span><span class="koboSpan" id="kobo.856.4">Don’t be afraid to conduct market research and make proper validation of the targeted user needs in the process of building these user personas. </span><span class="koboSpan" id="kobo.856.5">After all, it takes a lot of effort to build a deep learning system and it would not be great to waste time building a system that nobody wants </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">to use.</span></span></p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.858.1">Data collection and annotation risks</span></h2>
<p><span class="koboSpan" id="kobo.859.1">Your machine learning </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.860.1">model </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.861.1">will only be as good as the quality of your data. </span><span class="koboSpan" id="kobo.861.2">Deep learning requires a substantially larger quantity of data compared to machine learning. </span><span class="koboSpan" id="kobo.861.3">Additionally, very often for new deep learning use cases, data is not available off the shelf and must be manually collected and meticulously annotated. </span><span class="koboSpan" id="kobo.861.4">Making sure that the data is collected and annotated in a way that ensures quality is upheld is a very </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">hard job.</span></span></p>
<p><span class="koboSpan" id="kobo.863.1">The strategies and methods that are used to collect and annotate data for deep learning vary across use cases. </span><span class="koboSpan" id="kobo.863.2">Sometimes, the data collection and annotation process can happen at the same time. </span><span class="koboSpan" id="kobo.863.3">This can happen either when there are natural labels or when a label is prespecified before data </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">is collected:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.865.1">Natural labels</span></strong><span class="koboSpan" id="kobo.866.1"> are labels that usually </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.867.1">come naturally after some time passes. </span><span class="koboSpan" id="kobo.867.2">For example, this can be the gender of a baby through an ultrasound image; another use case is the price of a house as a label with the range of images of the property as the </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">input data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.869.1">Prespecified labels</span></strong><span class="koboSpan" id="kobo.870.1"> are labels</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.871.1"> where the input data is collected with predetermined labels. </span><span class="koboSpan" id="kobo.871.2">For example, this can be the gender label of some speech data that was collected in a controlled environment just for building a machine learning model or the age of a person in a survey before taking a facial photo shot to build a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">learning model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.873.1">These two types of labels are relatively safe from risks since the labels are likely to </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">be accurate.</span></span></p>
<p><span class="koboSpan" id="kobo.875.1">A final method of data collection and data annotation that poses a substantial amount of risk is when the labels are post-annotated after the data is collected. </span><strong class="bold"><span class="koboSpan" id="kobo.876.1">Post annotation</span></strong><span class="koboSpan" id="kobo.877.1"> requires some form of domain knowledge about the characteristics of the</span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.878.1"> desired label and does not always result in a 100% truth value due to human evaluation errors. </span><span class="koboSpan" id="kobo.878.2">Unconscious labeling errors happen when the labeler simply makes an error by accident. </span><span class="koboSpan" id="kobo.878.3">Conscious labeling errors, on the other hand, happen when the labeler actively decides to make a wrong label decision with intent. </span><span class="koboSpan" id="kobo.878.4">This intent can be from a strong or loosely rooted belief tied to a certain pattern in the data, or just a plain error made purposely for some reason. </span><span class="koboSpan" id="kobo.878.5">The risk being presented here is the potential of the labels being mistakenly annotated. </span><span class="koboSpan" id="kobo.878.6">Fortunately, there are a few strategies that can be executed to eliminate these risks; these will be introduced in the </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">following paragraphs.</span></span></p>
<p><span class="koboSpan" id="kobo.880.1">Unconscious labeling errors are hard to circumvent as humans are susceptible to a certain amount of error. </span><span class="koboSpan" id="kobo.880.2">However, different people generally have varying levels of focus and selective choice of the labelers could be a viable option for some. </span><span class="koboSpan" id="kobo.880.3">If the labelers are hired with financial remuneration just to annotate the labels for </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.881.1">your data, another</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.882.1"> viable option is to periodically provide data that is already labeled in secret in between providing data that needs labeling for the labelers to label the data. </span><span class="koboSpan" id="kobo.882.2">This strategy allows you to evaluate the performance of individual labelers and provide compensation according to the accuracy of their annotation work (yes, we can use metrics that are used to evaluate machine learning models too). </span><span class="koboSpan" id="kobo.882.3">As a positive side effect, labelers also would be incentivized to perform better in their </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">annotation work.</span></span></p>
<p><span class="koboSpan" id="kobo.884.1">Conscious labeling errors are the most dangerous risk here due to their potential to affect the quality of the entire dataset. </span><span class="koboSpan" id="kobo.884.2">When a wrong consensus on the pattern of data associated with any target label is used for the entire labeling process, the legitimacy of the labels won’t be discovered until the later stages of the machine learning life cycle. </span><span class="koboSpan" id="kobo.884.3">Machine learning models are only equipped with techniques to learn the patterns required to map the input data to the provided target labels and will likely be able to perform very well, even in conditions where the labels are incorrect. </span><span class="koboSpan" id="kobo.884.4">So long as there is a pattern that the labeler enforced during the labeling process, no matter right or wrong, machine learning models will do their best to learn the patterns needed to perform the exact input-to-output mapping. </span><span class="koboSpan" id="kobo.884.5">So, how do we mitigate this risk? </span><span class="koboSpan" id="kobo.884.6">Making sure ideologies are defined properly with the help of domain experts plays an important part in the mitigation process, but this strategy by itself holds varying levels of effectiveness, depending on the number of domain experts involved, the expertise level of the domain experts themselves, the number of labelers, and the varying degrees of anomalies or quality of the </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">collected data.</span></span></p>
<p><span class="koboSpan" id="kobo.886.1">Even domain experts can be wrong at times. </span><span class="koboSpan" id="kobo.886.2">Let’s take doctors as an example – how many times have you heard about a doctor giving wrong prescriptions and wrong medical assessments? </span><span class="koboSpan" id="kobo.886.3">How about the times when you misheard someone’s speech and had to guess what the other person just said? </span><span class="koboSpan" id="kobo.886.4">In the last example, the domain expert is you, and the domain is language speech comprehension. </span><span class="koboSpan" id="kobo.886.5">Additionally, when the number of labelers exceeds one person and forms a labeling team, one of the most prominent risks that can happen is a mismatch between different preferred domain ideologies to label the data. </span><span class="koboSpan" id="kobo.886.6">Sometimes, this happens because of the different variations of a certain label that can exist in a digital format or the existence of confusing patterns that can deter the analytical capabilities of the labeler. </span><span class="koboSpan" id="kobo.886.7">Sometimes, this can also happen due to the inherent bias the labeler or the domain expert has toward a certain label or input data. </span><span class="koboSpan" id="kobo.886.8">Subsequently, bias in the data creates bias in the model and creates ethical issues that demote trust in the decisions of a machine learning model. </span><span class="koboSpan" id="kobo.886.9">When there is a lack of trust in a machine learning model, the project will fail to be adopted and will lose its business value. </span><span class="koboSpan" id="kobo.886.10">The topic of bias, fairness, and trust will be discussed more extensively in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.887.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.888.1">, </span><em class="italic"><span class="koboSpan" id="kobo.889.1">Exploring Bias and Fairness</span></em><span class="koboSpan" id="kobo.890.1">, which will elaborate on its origins and ways to </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">mitigate it.</span></span></p>
<p><span class="koboSpan" id="kobo.892.1">Data with wrong labels that still have correct labels are </span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.893.1">called noisy labels. </span><span class="koboSpan" id="kobo.893.2">Fortunately, there are methods in deep learning that can help you work your way around noisy labels, such as weakly supervised learning. </span><span class="koboSpan" id="kobo.893.3">However, remember that it’s always a good idea to fix the issue at the source, which is when the data is collected and annotated, rather than after. </span><span class="koboSpan" id="kobo.893.4">Let’s dive into another strategy we can use to make the labeling process less risky. </span><span class="koboSpan" id="kobo.893.5">The data labeling process for deep learning projects usually involves using a software tool that allows the specific desired labels to be annotated. </span><span class="koboSpan" id="kobo.893.6">Software tools make annotation work faster and easier, and using a labeling collaboration software tool can make annotation work more error-proof. </span><span class="koboSpan" id="kobo.893.7">A good collaboration-based labeling tool will allow labelers to align their findings of the data with </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.894.1">each other in </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.895.1">some way, which will promote a common ideology when labeling. </span><span class="koboSpan" id="kobo.895.2">For instance, automatically alerting the entire labeler team when an easily misinterpreted pattern is identified can help prevent more data from being mislabeled and make sure all the previously related data </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">gets rereviewed.</span></span></p>
<p><span class="koboSpan" id="kobo.897.1">As a final point here, not as a risk, unlabeled data presents huge untapped potential for machine learning. </span><span class="koboSpan" id="kobo.897.2">Although it lacks specific labels to achieve a particular goal, there are inherent relationships between the data that can be learned. </span><span class="koboSpan" id="kobo.897.3">In </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.898.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.899.1">, </span><em class="italic"><span class="koboSpan" id="kobo.900.1">Exploring Unsupervised Deep Learning</span></em><span class="koboSpan" id="kobo.901.1">, we will explore how to use unsupervised learning to leverage this data as a foundation for subsequent supervised learning, which forms the workflow more widely known as </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">semi-supervised learning.</span></span></p>
<p><span class="koboSpan" id="kobo.903.1">Next, we will go through the next risk related to security in the consumption of data for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">learning models.</span></span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.905.1">Data security risk</span></h2>
<p><span class="koboSpan" id="kobo.906.1">Security, in the </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.907.1">context </span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.908.1">of machine learning, relates to preventing the unauthorized usage of data, protecting the privacy of data, and preventing events or attacks that are unwanted relating to the usage of the data. </span><span class="koboSpan" id="kobo.908.2">The risk here is when data security is compromised and can result in failure to comply with regulatory standards, the downfall of the model’s performance, or the corruption and destruction of business processes. </span><span class="koboSpan" id="kobo.908.3">In this section, we will go through four types of data security risk, namely sensitive data handling, data licensing, software licensing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.909.1">adversarial attacks.</span></span></p>
<h3><span class="koboSpan" id="kobo.910.1">Sensitive data handling</span></h3>
<p><span class="koboSpan" id="kobo.911.1">Some data is more </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.912.1">sensitive </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.913.1">than others and can be linked to regulatory risks. </span><span class="koboSpan" id="kobo.913.2">Sensitive data gives rise to data privacy regulations that govern the usage of personal data in different jurisdictions. </span><span class="koboSpan" id="kobo.913.3">Specifics of regulations vary but they generally revolve around lawfully protecting the rights of the usage of personal data and requiring consent for any actions taken on these types of data along with terms required when using such data. </span><span class="koboSpan" id="kobo.913.4">Examples of such regulations are the </span><strong class="bold"><span class="koboSpan" id="kobo.914.1">General Data Protection Regulation</span></strong><span class="koboSpan" id="kobo.915.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.916.1">GDPR</span></strong><span class="koboSpan" id="kobo.917.1">), which </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.918.1">covers the European </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.919.1">Union, the </span><strong class="bold"><span class="koboSpan" id="kobo.920.1">Personal Data Protection Act</span></strong><span class="koboSpan" id="kobo.921.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.922.1">PDPA</span></strong><span class="koboSpan" id="kobo.923.1">), which covers</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.924.1"> Singapore, the </span><strong class="bold"><span class="koboSpan" id="kobo.925.1">California Consumer Privacy Act</span></strong><span class="koboSpan" id="kobo.926.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.927.1">CCPA</span></strong><span class="koboSpan" id="kobo.928.1">), which covers only the state of California in the United States, and</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.929.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.930.1">Consumer Data Protection Act</span></strong><span class="koboSpan" id="kobo.931.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.932.1">CDPA</span></strong><span class="koboSpan" id="kobo.933.1">), which covers the state of Virginia in the United States. </span><span class="koboSpan" id="kobo.933.2">This means that you can’t just collect data that is categorized as personal, annotate it, build a model, and deploy it without adhering to these regulations as doing so would be considered a crime in some of these jurisdictions. </span><span class="koboSpan" id="kobo.933.3">Other than requesting consent, one of the common methods that’s used to mitigate this risk is to anonymize data so that the data can’t be identified by any single person. </span><span class="koboSpan" id="kobo.933.4">However, anonymization has to be done reliably so that the key general information is maintained while reliably removing any possibility of reconstructing the identity of a person. </span><span class="koboSpan" id="kobo.933.5">Making sure sensitive and personal data is handled properly goes a long way in building trust in the decisions of a machine learning model. </span><span class="koboSpan" id="kobo.933.6">Always practice extreme caution in handling sensitive and personal data to ensure the longevity of your machine </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">learning project.</span></span></p>
<h4><span class="koboSpan" id="kobo.935.1">Data and software licensing</span></h4>
<p><span class="koboSpan" id="kobo.936.1">Particularly for deep learning</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.937.1"> projects, a lot of data is required to build a good quality model. </span><span class="koboSpan" id="kobo.937.2">The availability of publicly available datasets helps shorten the time and complexity of a project by partially removing the cost and time needed to collect and label data. </span><span class="koboSpan" id="kobo.937.3">However, most datasets, like software, have licenses associated with them. </span><span class="koboSpan" id="kobo.937.4">These licenses govern how the data associated with the license can be used. </span><span class="koboSpan" id="kobo.937.5">The most important criterion of data license about machine learning models for business problems is whether the data allows for commercial usage or not. </span><span class="koboSpan" id="kobo.937.6">As most business use cases are considered to be commercial use cases due to profits derived from them, datasets with a license that prevents commercial usage cannot be used. </span><span class="koboSpan" id="kobo.937.7">Examples of data licenses with terms that prevent commercial usage are all derivatives of </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.938.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.939.1">Creative Commons Attribution-NonCommercial</span></strong><span class="koboSpan" id="kobo.940.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.941.1">CC-BY-NC</span></strong><span class="koboSpan" id="kobo.942.1">) license. </span><span class="koboSpan" id="kobo.942.2">Similarly, open sourced software also poses a risk to deep learning projects. </span><span class="koboSpan" id="kobo.942.3">Always make sure you triple-check the licenses before using any publicly available data or software for your project. </span><span class="koboSpan" id="kobo.942.4">Using data or code that has terms that prevent commercial usage in your commercial business project puts your project at risk of being fined or sued for </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">license infringement.</span></span></p>
<h4><span class="koboSpan" id="kobo.944.1">Adversarial attacks</span></h4>
<p><span class="koboSpan" id="kobo.945.1">When an application of machine learning is widely known, it exposes itself to targeted attacks meant to maliciously derail and manipulate the decisions of a model. </span><span class="koboSpan" id="kobo.945.2">This brings us to a type of attack that can affect a deployed deep learning</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.946.1"> model, called </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.947.1">an </span><strong class="bold"><span class="koboSpan" id="kobo.948.1">adversarial attack</span></strong><span class="koboSpan" id="kobo.949.1">. </span><span class="koboSpan" id="kobo.949.2">Adversarial attacks are a type of attack that involves manipulating the data input in a certain way to affect the predictions from the machine learning model. </span><span class="koboSpan" id="kobo.949.3">The most common adversarial attacks are caused by adversarial data examples that are modified from the actual input data in a way that they appear to maintain their legitimacy as input data but are capable of skewing the model’s prediction. </span><span class="koboSpan" id="kobo.949.4">The level of risk of such an attack varies across different use cases and depends on how much a user can interact with the system that eventually passes the input data to the machine learning model. </span><span class="koboSpan" id="kobo.949.5">One of the most widely known adversarial examples for a deep learning model is an optimized image that looks like random color noise and, when used to perturb pixel values of another image by overlaying its own pixel values, generates an adversarial example. </span><span class="koboSpan" id="kobo.949.6">The original image that’s obtained after the perturbation looks as though it’s untouched visually to a human but is capable of producing erroneous misclassification. </span><span class="koboSpan" id="kobo.949.7">The following figure shows an example of this perturbation as part of a tutorial from </span><a href="B18187_14.xhtml#_idTextAnchor206"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.950.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.951.1">, </span><em class="italic"><span class="koboSpan" id="kobo.952.1">Analyzing Adversarial Performance</span></em><span class="koboSpan" id="kobo.953.1">. </span><span class="koboSpan" id="kobo.953.2">This figure depicts the result of a neural network called ResNet50, trained to classify facial identity, that, when given a facial image, correctly predicted the right identity. </span><span class="koboSpan" id="kobo.953.3">However, when added together with another noise image array that was strategically and automatically generated just with access to the predicted class probabilities, the model mistakenly predicts the identity of the facial image, even when the combined image looks visually the same as the </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">original image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.955.1"><img alt="Figure 1.13 – Example of a potential image-based adversarial attack" src="image/B18187_01_013.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.956.1">Figure 1.13 – Example of a potential image-based adversarial attack</span></p>
<p><span class="koboSpan" id="kobo.957.1">Theoretically, adversarial attacks can be made using any type of data and are not limited to images. </span><span class="koboSpan" id="kobo.957.2">As the stakes of machine learning use cases grow higher, it’s more likely that attackers are willing to invest in a research</span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.958.1"> team that produces novel</span><a id="_idIndexMarker141"/> <span class="No-Break"><span class="koboSpan" id="kobo.959.1">adversarial examples.</span></span></p>
<p><span class="koboSpan" id="kobo.960.1">Some of the adversarial examples and the methods that generate them rely on access to the deep learning model itself so that a targeted adversarial example can be made. </span><span class="koboSpan" id="kobo.960.2">This means that it is not remotely possible for a potential attacker to be able to succeed in confusing a model created by someone else unless they have access to the model. </span><span class="koboSpan" id="kobo.960.3">However, many businesses utilize publicly available pre-trained models as-is and apply transfer learning methods to reduce the amount of work needed to satisfy a business use case. </span><span class="koboSpan" id="kobo.960.4">Any publicly available pre-trained models will also be available to the attackers, allowing them to build targeted adversarial examples for individual models. </span><span class="koboSpan" id="kobo.960.5">Examples of such pre-trained models include all the publicly available ImageNet pre-trained convolutional neural network models </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">and weights.</span></span></p>
<p><span class="koboSpan" id="kobo.962.1">So, how do we attempt to mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">this risk?</span></span></p>
<p><span class="koboSpan" id="kobo.964.1">One of the methods we can use to mitigate the risk of an adversarial attack is to train deep learning models with examples from the known set of methods from public research. </span><span class="koboSpan" id="kobo.964.2">By training with the known adversarial examples, the model will learn how to ignore the adversarial information through the learning process and be unfazed by such examples during validation and inference time. </span><span class="koboSpan" id="kobo.964.3">Evaluating different variations of the adversarial examples also helps set expectations on when the model </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">will fail.</span></span></p>
<p><span class="koboSpan" id="kobo.966.1">In this section, we discussed an in-depth take on the security issues when dealing with and consuming data for machine learning purposes. </span><span class="koboSpan" id="kobo.966.2">In </span><a href="B18187_14.xhtml#_idTextAnchor206"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.967.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.968.1">, </span><em class="italic"><span class="koboSpan" id="kobo.969.1">Analyzing Adversarial Performance</span></em><span class="koboSpan" id="kobo.970.1">, we will go through a more in-depth </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.971.1">practical evaluation of adversarial attack technologies for different data modalities and how to mitigate them in the context of deep learning. </span><span class="koboSpan" id="kobo.971.2">Next, we will</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.972.1"> dive into another category of risk that is at the core of the model </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">development </span></span><span class="No-Break"><a id="_idIndexMarker144"/></span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">process.</span></span></p>
<h3><span class="koboSpan" id="kobo.975.1">Overfitting and underfitting</span></h3>
<p><span class="koboSpan" id="kobo.976.1">During the model development process, which is the process of training, validating, and testing a machine learning model, one of the most foundational risks to handle is overfitting a model and underfitting </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">a model.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.978.1">Overfitting</span></strong><span class="koboSpan" id="kobo.979.1"> is an event where a machine learning</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.980.1"> model becomes so biased toward</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.981.1"> the provided training dataset examples and learned patterns that it can only exclusively distinguish examples that belong in the training dataset while failing to distinguish any examples in the validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">testing dataset.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.983.1">Underfitting</span></strong><span class="koboSpan" id="kobo.984.1">, on the other hand, is an event where a machine learning model fails to capture any patterns</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.985.1"> of the provided training, validation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">testing</span></span><span class="No-Break"><a id="_idIndexMarker148"/></span><span class="No-Break"><span class="koboSpan" id="kobo.987.1"> dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.988.1">Learning a generalizable pattern to output mapping capability is the key to building a valuable and usable machine learning model. </span><span class="koboSpan" id="kobo.988.2">However, there is no silver bullet to achieving a nicely fitted model, and very often, it requires iterating between the data preparation, model development, and deliver model </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">insights stages.</span></span></p>
<p><span class="koboSpan" id="kobo.990.1">Here are some tips to prevent overfitting</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.991.1"> and ensure generalization in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">deep learning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.993.1">Augment your data in expectation of the eventual </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">deployment conditions</span></span></li>
<li><span class="koboSpan" id="kobo.995.1">Collect enough data to cover every single variation possible of your </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">targeted label</span></span></li>
<li><span class="koboSpan" id="kobo.997.1">Collaborate with domain experts and understand key indicators </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">and patterns</span></span></li>
<li><span class="koboSpan" id="kobo.999.1">Use cross-validation methods to ensure the model gets evaluated fairly on </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">unseen data</span></span></li>
<li><span class="koboSpan" id="kobo.1001.1">Use simpler neural network architectures </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">whenever possible</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1003.1">Here are some tips to </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">prevent</span></span><span class="No-Break"><a id="_idIndexMarker150"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1"> underfitting:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1006.1">Evaluate a variety of </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">different models</span></span></li>
<li><span class="koboSpan" id="kobo.1008.1">Collaborate with domain experts and understand key indicators </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">and patterns</span></span></li>
<li><span class="koboSpan" id="kobo.1010.1">Make sure the data is clean and has as low an error </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">as possible</span></span></li>
<li><span class="koboSpan" id="kobo.1012.1">Make sure there is </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">enough data</span></span></li>
<li><span class="koboSpan" id="kobo.1014.1">Start with a small set of data inputs when building your model and build your way up to more complex data to ensure models can fit appropriately on </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">your data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1016.1">In this section, we have discussed issues while building a model. </span><span class="koboSpan" id="kobo.1016.2">Next, we will be discussing a type of risk that will affect the built model after it has </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">ben trained.</span></span></p>
<h3><span class="koboSpan" id="kobo.1018.1">Model consistency risk</span></h3>
<p><span class="koboSpan" id="kobo.1019.1">One of the major traits of the machine</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.1020.1"> learning process is that it is cyclical. </span><span class="koboSpan" id="kobo.1020.2">Models</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.1021.1"> get retrained constantly in hopes of finding better settings. </span><span class="koboSpan" id="kobo.1021.2">These settings can be a different data partitioning method, a different data transformation method, a different model, or the same model with different model settings. </span><span class="koboSpan" id="kobo.1021.3">Often, the models that are built need to be compared against each other fairly and equitably. </span><span class="koboSpan" id="kobo.1021.4">Model consistency is the one feature that ensures that a fair comparison can be made between different model versions and performance metrics. </span><span class="koboSpan" id="kobo.1021.5">Every single process before a model is obtained is required to be consistent so that when anybody tries to execute the same processes with the same settings, the same model should be obtainable and reproducible. </span><span class="koboSpan" id="kobo.1021.6">We should reiterate that even when some processes require randomness when building the model, it needs to be randomly deterministic. </span><span class="koboSpan" id="kobo.1021.7">This is needed so that the only difference in the setup is the targeted settings and </span><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">nothing else.</span></span></p>
<p><span class="koboSpan" id="kobo.1023.1">Model consistency doesn’t stop with just the reproducibility of a model – it extends to the consistency of the predictions of a model. </span><span class="koboSpan" id="kobo.1023.2">The predictions should be the same when predicted using a different batch size setting, and the same data input should always produce the same predictions. </span><span class="koboSpan" id="kobo.1023.3">Inconsistencies in model predictions are a major red flag that signals that anything the model produces is not representative of what you will get during deployment and any derived model insights would </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">be misinformation.</span></span></p>
<p><span class="koboSpan" id="kobo.1025.1">To combat model consistency risk, always make sure your code produces consistent results by seeding the random number generator whenever possible. </span><span class="koboSpan" id="kobo.1025.2">Always validate model consistency either manually</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.1026.1"> or automatically in the model development stage after you build </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">the model.</span></span></p>
<h3><span class="koboSpan" id="kobo.1028.1">Model degradation risk</span></h3>
<p><span class="koboSpan" id="kobo.1029.1">When you have built</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.1030.1"> a model, verified it, and demonstrated its impact </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.1031.1">on the business, you then take it into the model deployment stage and position it for use. </span><span class="koboSpan" id="kobo.1031.2">One mistake is to think that this is the end of the deep learning project and that you can take your hands off and just let the model do its job. </span><span class="koboSpan" id="kobo.1031.3">Sadly, most machine learning models degrade, depending on the level of generalization your model achieved during the model development stage on the data available in the wild. </span><span class="koboSpan" id="kobo.1031.4">A common scenario that happens when a model gets deployed is that, initially, the model’s performance and the characteristics of the data received during deployment stay the same, and change over time. </span><span class="koboSpan" id="kobo.1031.5">Time has the potential to change the conditions of the environment and anything in the world. </span><span class="koboSpan" id="kobo.1031.6">Machines grow old, seasons change, and people change, and expecting that conditions and variables surrounding the machine learning model will change can allow you to make sure models stay relevant and impactful to </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">the business.</span></span></p>
<p><span class="koboSpan" id="kobo.1033.1">How a model can degrade can be categorized into three categories, namely </span><strong class="bold"><span class="koboSpan" id="kobo.1034.1">data drift</span></strong><span class="koboSpan" id="kobo.1035.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1036.1">concept drift</span></strong><span class="koboSpan" id="kobo.1037.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1038.1">model drift</span></strong><span class="koboSpan" id="kobo.1039.1">. </span><span class="koboSpan" id="kobo.1039.2">Conceptually, drift can be associated with a boat slowly drifting away from its ideal position. </span><span class="koboSpan" id="kobo.1039.3">In the case of machine learning projects, instead of a boat drifting away, it’s the data or model drifting away from its original perceived behavior or pattern. </span><span class="koboSpan" id="kobo.1039.4">Let’s briefly go through these types </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">of drift.</span></span></p>
<p><span class="koboSpan" id="kobo.1041.1">Data drift is a form of degradation</span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.1042.1"> that is associated with the input data the model needs to provide a prediction value. </span><span class="koboSpan" id="kobo.1042.2">When a deployed model experiences data drift, it means that the received that’s data during deployment doesn’t belong to the inherent distribution of the data that was trained and validated by the machine learning model. </span><span class="koboSpan" id="kobo.1042.3">If there were ways to validate</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.1043.1"> the model on the new data supplied during deployment, data</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.1044.1"> drift would potentially cause a shift in the original expected metric performance obtained during model validation. </span><span class="koboSpan" id="kobo.1044.2">An example of data drift in the context of a deep learning model can be a use case requiring the prediction of human actions outdoors. </span><span class="koboSpan" id="kobo.1044.3">In this use case, data drift would be that the original data that was collected consisted of humans in summer clothing during the summer season, and due to winter, the people are wearing winter </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">clothing instead.</span></span></p>
<p><span class="koboSpan" id="kobo.1046.1">Concept drift</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.1047.1"> is a form of degradation that is associated with the change in how the input data interacts with the target output data. </span><span class="koboSpan" id="kobo.1047.2">In the planning stage, domain experts and machine learning practitioners collaborate to define input variables that can affect the targeted output data. </span><span class="koboSpan" id="kobo.1047.3">This defined input and output setup will subsequently be used for building a machine learning model. </span><span class="koboSpan" id="kobo.1047.4">Sometimes, however, not all of the context that can affect the targeted output data is included as input variables due to either the availability of that data or just the lack of domain knowledge. </span><span class="koboSpan" id="kobo.1047.5">This introduces a dependence on the conditions of the missing context associated with the data collected for machine learning. </span><span class="koboSpan" id="kobo.1047.6">When the conditions of the missing context drift away from the base values that exist in the training and validation data, concept drift occurs, rendering the original concept irrelevant or shifted. </span><span class="koboSpan" id="kobo.1047.7">In simpler terms, this means that the same input doesn’t map to the same output from the training data anymore. </span><span class="koboSpan" id="kobo.1047.8">In the context of deep learning, we can take a sentiment classification use case that is based on textual data. </span><span class="koboSpan" id="kobo.1047.9">Let’s say that a comment or speech can be a negative, neutral, or positive sentiment based on both the jurisdiction and the text itself. </span><span class="koboSpan" id="kobo.1047.10">This means that in some jurisdictions, people have different thresholds on what is considered negative, neutral, or positive and grade things differently. </span><span class="koboSpan" id="kobo.1047.11">Training the model with only textual data wouldn’t allow itself to generalize across jurisdictions accurately and thus would face concept drift any time it gets deployed in </span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">another jurisdiction.</span></span></p>
<p><span class="koboSpan" id="kobo.1049.1">Lastly, model drift</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.1050.1"> is a form of degradation</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.1051.1"> associated with operational metrics and easily measurable metrics. </span><span class="koboSpan" id="kobo.1051.2">Some of the factors of degradation that can be categorized into model drift are model latency, model throughput, and model error rates. </span><span class="koboSpan" id="kobo.1051.3">Model</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.1052.1"> drift metrics are generally easy to measure and track compared to the other two types </span><span class="No-Break"><span class="koboSpan" id="kobo.1053.1">of drift.</span></span></p>
<p><span class="koboSpan" id="kobo.1054.1">One of the best workflows for mitigating these risks is by tracking the metrics under all of these three types of drift and having a clear</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.1055.1"> path for a new machine learning model to be built, which is a process I call </span><strong class="bold"><span class="koboSpan" id="kobo.1056.1">drift reset</span></strong><span class="koboSpan" id="kobo.1057.1">. </span><span class="koboSpan" id="kobo.1057.2">Now that we’ve covered a brief overview of model degradation, in </span><a href="B18187_16.xhtml#_idTextAnchor238"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1058.1">Chapter 16</span></em></span></a><span class="koboSpan" id="kobo.1059.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1060.1">Governing Deep Learning Models</span></em><span class="koboSpan" id="kobo.1061.1">, and </span><a href="B18187_17.xhtml#_idTextAnchor247"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1062.1">Chapter 17</span></em></span></a><span class="koboSpan" id="kobo.1063.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1064.1">Managing Drift Effectively in a Dynamic Environment</span></em><span class="koboSpan" id="kobo.1065.1">, we will go more in-depth on these risks and discuss</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.1066.1"> practically how to mitigate this risk in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">deep learning.</span></span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.1068.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1069.1">In this chapter, you learned what it takes to complete a deep learning project successfully in a repeatable and consistent manner at a somewhat mid to high-level view. </span><span class="koboSpan" id="kobo.1069.2">The topics we discussed in this chapter were structured to be more comprehensive at the earlier stages of the deep learning life, which covers the planning and data </span><span class="No-Break"><span class="koboSpan" id="kobo.1070.1">preparation stages.</span></span></p>
<p><span class="koboSpan" id="kobo.1071.1">In the following chapters, we will explore the mid to final stages of the life cycle more comprehensively. </span><span class="koboSpan" id="kobo.1071.2">This involves everything after the data preparation stage, which includes model development, model insights, model deployment, and, finally, </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">model governance.</span></span></p>
<p><span class="koboSpan" id="kobo.1073.1">In the next chapter, we will start to explore common and widely used deep learning architectures </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">more extensively.</span></span></p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.1075.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.1076.1">Bowen Shi, Wei-Ning Hsu, Abdelrahman Mohamed. </span><em class="italic"><span class="koboSpan" id="kobo.1077.1">Robust Self-Supervised Audio-Visual Speech Recognition</span></em><span class="koboSpan" id="kobo.1078.1">, 2022. </span><span class="koboSpan" id="kobo.1078.2">Toyota Technological Institute at Chicago, Meta </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">AI: </span></span><a href="https://arxiv.org/pdf/2201.01763v2.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">https://arxiv.org/pdf/2201.01763v2.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.1082.1">Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang. </span><em class="italic"><span class="koboSpan" id="kobo.1083.1">Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework</span></em><span class="koboSpan" id="kobo.1084.1">. </span><span class="koboSpan" id="kobo.1084.2">DAMO Academy, Alibaba </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">Group: </span></span><a href="https://arxiv.org/pdf/2202.03052v1.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">https://arxiv.org/pdf/2202.03052v1.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">.</span></span></li>
</ul>
</div>
</body></html>