<html><head></head><body>
		<div id="_idContainer049">
			<h1 id="_idParaDest-88" class="chapter-number"><a id="_idTextAnchor107"/>5</h1>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor108"/>Natural Language Data –  Finding and Preparing Data</h1>
			<p><a id="_idTextAnchor109"/><a id="_idTextAnchor110"/>This chapter will teach you how to identify and prepare data for processing with natural language understanding techniques. It will discuss data from databases, the web, and different kinds of documents, as well as privacy and ethics considerations. The Wizard of Oz technique will be covered briefly. If you don’t have access to your own data, or if you wish to compare your results to those of other researchers, this chapter will also discuss generally available and frequently used corpora. It will then go on to discuss preprocessing steps such as stemming <span class="No-Break">and lemmatization.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Sources of data <span class="No-Break">and annotation</span></li>
				<li>Ensuring privacy and observing <span class="No-Break">ethical considerations</span></li>
				<li>Generally <span class="No-Break">available corpora</span></li>
				<li><span class="No-Break">Preprocessing data</span></li>
				<li>Application-specific types <span class="No-Break">of preprocessing</span></li>
				<li>Choosing among <span class="No-Break">preprocessing techniques</span></li>
			</ul>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor111"/>Finding sources of data and annotating it</h1>
			<p>Data is <a id="_idIndexMarker265"/>where <a id="_idIndexMarker266"/>all <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) projects <a id="_idIndexMarker267"/>start. Data can be in the form of written texts or transcribed speech. The purpose of data is to teach an NLP system what it should do when it’s given similar data in the future. Specific collections of data are also called <em class="italic">corpora</em> or <em class="italic">datasets</em>, and we will often use these terms interchangeably. Very recently, large pretrained models have been developed that greatly reduce the need for data in many applications. However, these pretrained models, which will be discussed in detail in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, do not in most cases eliminate the need for <span class="No-Break">application-specific data.</span></p>
			<p>Written language data can be of any length, ranging from very short texts such as tweets to multi-page documents or even books. Written language can be interactive, such as a record of a chatbot session between a user and a system, or it can be non-interactive, such as a newspaper article or blog. Similarly, spoken language can be long or short. Like written language, it can be interactive, such as a transcription of a conversation between two people, or non-interactive, such as broadcast news. What all NLP data has in common is that it is language, and it consists of words, in one or more human languages, that are used by people to communicate with each other. The goal of every NLP project is to take that data, process it by using specific algorithms, and gain information about what the author or authors of the data had in mind when they created <span class="No-Break">the data.</span></p>
			<p>One of the<a id="_idIndexMarker268"/> first <a id="_idIndexMarker269"/>steps in any NLP project is finding the right data. For that, you’ll want to consider your goals in doing <span class="No-Break">the project.</span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor112"/>Finding data for your own application</h2>
			<p>If you have a<a id="_idIndexMarker270"/> specific, practical application in mind that you’d like to build, it’s often easy to know what kind of data you need. For example, to build an enterprise assistant (one of the types of interactive applications shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.3</em>), you’ll need examples of conversations between users and either human agents or interactive systems of the kind you would like to build. These examples could already be available in the form of existing call <span class="No-Break">center records.</span></p>
			<p>The examples we will<a id="_idIndexMarker271"/> consider in the following subsections are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Conversations in <span class="No-Break">call centers</span></li>
				<li><span class="No-Break">Chat logs</span></li>
				<li><span class="No-Break">Databases</span></li>
				<li>Message boards and <span class="No-Break">customer reviews</span></li>
			</ul>
			<p><span class="No-Break">Let’s begin!</span></p>
			<h3>Conversations in call centers</h3>
			<p>If you’re planning <a id="_idIndexMarker272"/>to build a voice assistant <a id="_idIndexMarker273"/>that performs customer support, in many cases, the goal is to offload some work from an existing call center. In that case, there will frequently be many transcripts of previous calls between human agents and customers that can serve as data for training an application. The customers’ questions will be examples of what the application will need to understand, and the agents’ responses will be examples of how the system should respond. There will need to be an annotation process that assigns an overall intent, or customer goal, to each customer utterance. Most of the time, the annotation process will also need to label entities within the utterance. Before annotation of the intents and entities begins, there should be an initial design step where the intents and entities <span class="No-Break">are determined.</span></p>
			<p>Once the data has been annotated, it can be used as training data for applications. The training process will vary significantly depending on the technologies used. We will cover the use of the data in training in detail in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> and <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<h3>Chat logs</h3>
			<p>If you have a <a id="_idIndexMarker274"/>website<a id="_idIndexMarker275"/> that includes a chat window, the questions typed by customers can serve as training data, just like transcripts from call center conversations. The only difference in the data is that call center data is based on speech rather than typing. Otherwise, the annotation, design, and training processes will be very similar. The data itself will be a bit different since typed inputs tend to be shorter than spoken inputs and will contain <span class="No-Break">spelling errors.</span></p>
			<h3>Databases</h3>
			<p>Databases can<a id="_idIndexMarker276"/> often <a id="_idIndexMarker277"/>be a good source of enterprise data. Databases often contain free text fields, where a user can enter any information they like. Free text fields are used for information such as narrative summaries of incident reports, and they often contain rich information that’s not captured elsewhere in the other database fields. NLP can be extremely valuable for learning from this rich information because the contents of free text fields can be classified and analyzed using NLP techniques. This analysis can provide additional insight into the<a id="_idIndexMarker278"/> topic<a id="_idIndexMarker279"/> of <span class="No-Break">the database.</span></p>
			<h3>Message boards and customer reviews</h3>
			<p>Like free <a id="_idIndexMarker280"/>text <a id="_idIndexMarker281"/>fields, message <a id="_idIndexMarker282"/>boards and customer support<a id="_idIndexMarker283"/> forums contain unformatted inputs from customers. Customer support message boards and customer product reviews can be valuable sources of data about product failures as well as customer attitudes about products. Although this information can be reviewed by human analysts, human analysis is time-consuming and expensive. In many cases, this information is abundant and can be used as the basis of very useful <span class="No-Break">NLP applications.</span></p>
			<p>So far, we’ve talked about application-specific data and how you can find and analyze it for the purposes of the application. On the other hand, sometimes you will be interested in analyzing data as part of a research project. The next section discusses where you can obtain data for a <span class="No-Break">research project.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor113"/>Finding data for a research project</h2>
			<p>If your goal is to<a id="_idIndexMarker284"/> contribute to the science of NLP, or if you just want to be able to compare your algorithms to other researchers’ work, the kind of data you need will be quite different from the data discussed previously. Rather than finding data (possibly proprietary data inside your enterprise) that no one’s ever used before, you will want to use data that’s freely available to other researchers. Ideally, this data will already be public, but if not, it’s important for you to make it available to others in the NLP research community so that they can replicate your work. Scientific conferences or journals will nearly always require that any newly collected data be made available before a paper can be presented or published. We will now discuss several ways of collecting <span class="No-Break">new data.</span></p>
			<h3>Collecting data</h3>
			<p>Although there are <a id="_idIndexMarker285"/>many sources of pre-existing data, sometimes you will not find exactly what you need from currently available resources. Perhaps you want to work on a very specific technical topic, or perhaps you’re interested in a rapidly changing topic such as COVID-19. You may need data that’s specific to a particular local area or seasonal data that’s only applicable to certain times of the year. For all of these reasons, it may be necessary to collect data specifically for <span class="No-Break">your project.</span></p>
			<p>There are several good ways to collect data under these circumstances, including <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>), crowdsourcing data, and Wizard of Oz. We will <a id="_idIndexMarker286"/>review these in the <span class="No-Break">following sections.</span></p>
			<h4>APIs</h4>
			<p>Some social <a id="_idIndexMarker287"/>media services have feeds that can be accessed from APIs. Twitter, for example, has an API that developers can use to access Twitter <span class="No-Break">services (</span><a href="https://developer.twitter.com/en/docs/twitter-api"><span class="No-Break">https://developer.twitter.com/en/docs/twitter-api</span></a><span class="No-Break">).</span></p>
			<h4>Crowdsourcing data</h4>
			<p>Some data can<a id="_idIndexMarker288"/> be generated by human workers using platforms such as Amazon’s Mechanical Turk (<a href="https://www.mturk.com/">https://www.mturk.com/</a>). The data to be generated has to be clearly described for crowdworkers, along with any parameters or constraints on the data to be generated. Data that is easy for the average person to understand, as opposed to technical or specialized scientific data, is especially suitable for crowdsourcing. Crowdsourcing can be an effective way to obtain data; however, for this data to be useful, some precautions have to <span class="No-Break">be taken:</span></p>
			<ul>
				<li>It’s important to make sure that the crowdworkers have adequate instructions so that the data they create is sufficiently similar to the real data that the system will encounter during deployment. Crowdworker data has to be monitored to make sure that the crowdworkers are properly <span class="No-Break">following instructions.</span></li>
				<li>Crowdworkers have to have the right knowledge in order to create data that’s appropriate for specialized applications. For example, if the crowdworkers are to generate medical reports, they have to have <span class="No-Break">medical backgrounds.</span></li>
				<li>Crowdworkers have to have sufficient knowledge of the language that they’re generating data for in order to ensure that the data is representative of the real data. It is not necessary for the data to be perfectly grammatical – language encountered during deployment is not, especially speech data. Insisting on perfectly <a id="_idIndexMarker289"/>grammatical data can lead to stilted, <span class="No-Break">unrealistic data.</span></li>
			</ul>
			<h4>Wizard of Oz</h4>
			<p>The <strong class="bold">Wizard of Oz</strong> (<strong class="bold">WoZ</strong>) method <a id="_idIndexMarker290"/>of collecting <a id="_idIndexMarker291"/>data is based on setting up a computer-human interaction situation where a system appears to be processing the user’s inputs, but in fact, the processing is done by a human behind the scenes. The <em class="italic">Wizard of Oz</em> reference is from the line in the movie where the wizard says, “<em class="italic">Pay no attention to the man behind the curtain</em>,” who is actually controlling a projection of what is supposed to be the wizard. The idea behind the technique is that if the user believes that a system is doing the processing, the user’s behavior will represent how they would behave with an actual system. While the WoZ method can provide very high-quality data, it is expensive, since the setup must be carefully arranged so that subjects in the experiment are unaware that they’re interacting with an automated system. You can find details about the WoZ paradigm at <a href="https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment">https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment</a>, and more information about conducting a WoZ experiment <span class="No-Break">here: </span><a href="https://www.answerlab.com/insights/wizard-of-oz-testing"><span class="No-Break">https://www.answerlab.com/insights/wizard-of-oz-testing</span></a><span class="No-Break">.</span></p>
			<p>In data collection, we are not only interested in the language itself but also in additional information that describes the data, or <em class="italic">metadata</em>. The next section will describe metadata in general, and then continue discussing an extremely important type of <span class="No-Break">metadata, </span><span class="No-Break"><em class="italic">annotation</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor114"/>Metadata</h2>
			<p>Datasets often include <a id="_idIndexMarker292"/>metadata. Metadata refers to information about the data rather than the data itself. Almost any information that data providers think might be useful in further processing can be included as metadata. Some of the most common types of metadata include the human language of the data, the speaker and time and place of speech for spoken data, and the author of written data. If spoken data is the result of a speech recognition process, the speech recognizer’s confidence is usually included as metadata. The next section covers annotation, which is probably the most important type <span class="No-Break">of metadata.</span></p>
			<h3>Annotation</h3>
			<p>One of the most <a id="_idIndexMarker293"/>important types of metadata is the intended NLP result for a text, or <span class="No-Break">the annotation.</span></p>
			<p>For the most part, newly collected data will need to be annotated, unless the experiment to be done involves unsupervised learning (more on unsupervised learning in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>). Annotation is the process of associating an input with the NLP result that the trained system is intended to produce. The system <em class="italic">learns</em> how to analyze data by processing the annotated examples, and then applies that learning to new, <span class="No-Break">unannotated examples.</span></p>
			<p>Since annotation is actually the <em class="italic">supervision</em> in supervised learning, data used in unsupervised learning experiments does not need to be annotated with the intended <span class="No-Break">NLP result.</span></p>
			<p>There are several software tools that can be used to annotate NLP text data. For example, the <strong class="bold">General Architecture for Text Engineering</strong> (<strong class="bold">GATE</strong>) (<a href="https://gate.ac.uk/">https://gate.ac.uk/</a>) includes <a id="_idIndexMarker294"/>a well-tested user interface that enables annotators to assign meanings to documents and parts <span class="No-Break">of documents.</span></p>
			<p>In the following sections, we will “learn about the” transcription of speech data and the question of <span class="No-Break">inter-annotator agreement.</span></p>
			<h4>Transcription</h4>
			<p>Transcription, or <a id="_idIndexMarker295"/>converting the speech contained in audio files to its written form, is a necessary annotation step for speech data. If the speech does not contain significant amounts of noise, commercial <strong class="bold">automatic speech recognition</strong> (<strong class="bold">ASR</strong>) systems <a id="_idIndexMarker296"/>such as Nuance Dragon (<a href="https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html">https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html</a>) can provide a fairly accurate first pass at transcribing audio files. If you do use commercial ASR for transcription, the results should still be reviewed by a researcher in order to catch and correct any errors made by the ASR system. On the other hand, if the speech is very noisy or quiet, or if it contains speech from several people talking over each other, commercial ASR systems will probably make too many errors for the automatic transcription results to be useful. In that situation, manual transcription software such as TranscriberAG (<a href="http://transag.sourceforge.net/">http://transag.sourceforge.net/</a>) can be used. You should keep in mind that manual transcription of noisy or otherwise problematic speech is likely to be a slow process because the transcriber has to first understand the speech in order to <span class="No-Break">transcribe it.</span></p>
			<h4>Inter-annotator agreement</h4>
			<p>Annotators will <a id="_idIndexMarker297"/>not always agree on the correct annotation for any given data item. There can be general differences in opinion about the correct annotation, especially if the annotation instructions are unclear. In addition, annotators might not be thinking carefully about what the annotations should be, or the data that the annotators are being asked to annotate might be inherently subjective. For these reasons, annotation of the same data is often done by several annotators, especially if the data is subjective. Annotating an emotion associated with text or speech is a good example of data that has a lot of potential for disagreement. The degree of agreement among annotators is<a id="_idIndexMarker298"/> called <strong class="bold">inter-annotator agreement</strong> and is measured by what is known as the<strong class="bold"> kappa statistic</strong>. The kappa<a id="_idIndexMarker299"/> statistic is preferable to just computing a percentage agreement because it takes into account the possibility that the annotators might agree <span class="No-Break">by chance.</span></p>
			<p><strong class="bold">Natural language toolkit</strong> (<strong class="bold">NLTK</strong>) includes<a id="_idIndexMarker300"/> a package, <strong class="source-inline">nltk.metrics.agreement</strong>, that can be used to calculate <span class="No-Break">inter-annotator agreement.</span></p>
			<p>So far, we have discussed the process of obtaining your own data, but there are also many pre-existing datasets available that have already been annotated and are freely available. We’ll discuss pre-existing data in the next section. When we start working directly with data in later chapters, we will be using <span class="No-Break">pre-existing datasets.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor115"/>Generally available corpora</h2>
			<p>The easiest <a id="_idIndexMarker301"/>way to get data is to use pre-existing <a id="_idIndexMarker302"/>corpora that cover the kind of problem that you’re trying to solve. With pre-existing corpora, you will not need to collect the data, and you will probably not need to annotate it unless the pre-existing annotations are not suitable for your problem. Any privacy questions will have been addressed before the dataset was published. An added advantage to using pre-existing corpora is that other researchers have probably published papers describing their work on the corpus, which you can compare to your <span class="No-Break">own work.</span></p>
			<p>Fortunately, there are many standard preexisting datasets that you can download and work with, covering almost any NLP problem. Some are free and others are available for <span class="No-Break">a fee.</span></p>
			<p>Preexisting datasets are available from a number of organizations, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Linguistic Data Consortium (<a href="https://www.ldc.upenn.edu/">https://www.ldc.upenn.edu/</a>): Provides a wide variety of text <a id="_idIndexMarker303"/>and speech data in many languages and also manages <span class="No-Break">donated data.</span></li>
				<li>Hugging Face (<a href="https://huggingface.co/">https://huggingface.co/</a>): Provides datasets in many languages, as well as<a id="_idIndexMarker304"/> NLP models. Some of the popular datasets available from Hugging Face include movie reviews, product reviews, and Twitter <span class="No-Break">emotion categories.</span></li>
				<li>Kaggle (<a href="https://www.kaggle.com/">https://www.kaggle.com/</a>): Provides <a id="_idIndexMarker305"/>many datasets, including <span class="No-Break">user-contributed datasets.</span></li>
				<li><strong class="bold">European language resources association</strong> (<strong class="bold">ELRA</strong>) (<a href="http://www.elra.info/en/">http://www.elra.info/en/</a>): A European <a id="_idIndexMarker306"/>organization that provides <span class="No-Break">multilingual data.</span></li>
				<li>Data distributed with NLP libraries such as NLTK and spaCy: NLP libraries include dozens<a id="_idIndexMarker307"/> of <a id="_idIndexMarker308"/>corpora of all sizes and languages. Much of the data is annotated in support of many different types <span class="No-Break">of applications.</span></li>
				<li>Government data: Governments collect vast amounts of data, including text data, which is often publicly available and can be used <span class="No-Break">for research.</span></li>
				<li>Librispeech <a href="https://www.openslr.org/12">https://www.openslr.org/12</a>: A large dataset of read speech, based on audiobooks<a id="_idIndexMarker309"/> read for people with visual impairments. Primarily used for <span class="No-Break">speech projects.</span></li>
			</ul>
			<p>Up to this <a id="_idIndexMarker310"/>point, we have covered the topics of obtaining data and adding metadata, including annotation. Before data is used in NLP applications, we also need to ensure that it is used ethically. Ethical considerations are the topic of the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor116"/>Ensuring privacy and observing ethical considerations</h1>
			<p>Language data, especially data internal to an enterprise, may contain sensitive information. Examples that come to mind right away are medical and financial data. When an application deals with these kinds of topics, it is very likely to contain sensitive information about health or finances. Information can become even more sensitive if it is associated with a specific person. This is <a id="_idIndexMarker311"/>called <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>), which is defined by the United States Department of Labor <span class="No-Break">as follows:</span></p>
			<p>“<em class="italic">Any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means</em>” (<a href="https://www.dol.gov/general/ppii">https://www.dol.gov/general/ppii</a>). This is a broad and complex issue, a full treatment of which is out of the scope of this book. However, it’s worth discussing a few important points specific to NLP applications that should be considered if you need to deal with any kind of <span class="No-Break">sensitive data.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor117"/>Ensuring the privacy of training data</h2>
			<p>In the case of generally <a id="_idIndexMarker312"/>available corpora, the data has typically been prepared so that sensitive information has been removed. If you are dealing with data you have obtained yourself, on the other hand, you will have to consider how to deal with sensitive information that you may encounter in the <span class="No-Break">raw data.</span></p>
			<p>One common strategy is to replace sensitive data with placeholders, such as <strong class="source-inline">&lt;NAME&gt;</strong>, <strong class="source-inline">&lt;LOCATION&gt;</strong>, and <strong class="source-inline">&lt;PHONENUMBER&gt;</strong>. This allows the training process to learn how to process the natural language without exposing any sensitive data. This should not affect the ability of the trained system to process natural language because it would be rare that an application would classify utterances differently depending on specific names or locations. If the classification depends on more specific information than just, for example, <strong class="source-inline">&lt;LOCATION&gt;</strong>, a more specific placeholder can be used, such as a city or country name. It is also actually helpful to use placeholders in training because it reduces the chances of overfitting the trained model on the specific names in the <span class="No-Break">training data.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor118"/>Ensuring the privacy of runtime data</h2>
			<p>At runtime, when <a id="_idIndexMarker313"/>an application is deployed, incoming data will naturally contain sensitive data that the users enter in order to accomplish their goals. Any precautions that are normally taken to secure data entered with forms (non-NLP data such as social security numbers or credit card numbers) will of course apply to data entered with natural language text and speech. In some cases, the handling of this data will be subject to regulations and legislation, which you will need to be aware of <span class="No-Break">and follow.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor119"/>Treating human subjects ethically</h2>
			<p>Natural language and <a id="_idIndexMarker314"/>speech data can be collected in<a id="_idIndexMarker315"/> the course of an experiment, such as a WoZ study, when human users or subjects provide speech and text for research. Universities and other research institutions have committees that review the planned procedures for any experiments with human subjects to ensure that the subjects are treated ethically. For example, subjects must provide informed consent to the experiment, they must not be harmed, their anonymity must be protected, deceptive practices must be avoided, and they must be able to withdraw from the study at any time. If you are collecting data in an experimental context, make sure to find out about the rules at your institution regarding approval by your human subjects committee or the equivalent. You may need to allocate extra lead time for the approval process to <span class="No-Break">go through.</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor120"/>Treating crowdworkers ethically</h2>
			<p>If crowdworkers<a id="_idIndexMarker316"/> such as Amazon Mechanical Turk workers are <a id="_idIndexMarker317"/>used to create or annotate data, it is important to remember to treat them fairly – most importantly, to pay them fairly and on time – but also to make sure they have the right tools to do their jobs and to listen respectfully to any concerns or questions that they might have about <span class="No-Break">their tasks.</span></p>
			<p>Now that we have identified a source of data and discussed ethical issues, let’s move on to what we can do to get the data ready to be used in an application, or preprocessing. We will first cover general topics in preprocessing in the next section, and then discuss preprocessing techniques for <span class="No-Break">specific applications.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor121"/>Preprocessing data</h1>
			<p>Once the data is <a id="_idIndexMarker318"/>available, it usually needs to be cleaned or preprocessed before the actual natural language <span class="No-Break">processing begins.</span></p>
			<p>There are two major goals in preprocessing data. The first goal is to remove items that can’t be processed by the system – these might include items such as emojis, HTML markup, spelling errors, foreign words, or some Unicode characters such as <em class="italic">smart quotes</em>. There are a number of existing Python libraries that can help with this, and we’ll be showing how to use them in the next section, <em class="italic">Removing non-text</em>. The second goal is addressed in the section called <em class="italic">Regularizing text</em>. We regularize text so that differences among words in the text that are not relevant to the application’s goal can be ignored. For example, in some applications, we might want to ignore the differences between uppercase <span class="No-Break">and lowercase.</span></p>
			<p>There are many possible preprocessing tasks that can be helpful in preparing natural language data. Some are almost universally done, such as tokenization, while others are only done in particular types of applications. We will be discussing both types of preprocessing tasks in <span class="No-Break">this section.</span></p>
			<p>Different applications will need different kinds of preprocessing. Other than the most common preprocessing tasks, such as tokenization, exactly what kinds of preprocessing need to be done has to be carefully considered. A useful preprocessing step for one kind of application can completely remove essential information that’s needed in another kind of application. Consequently, for each preprocessing step, it’s important to think through its purpose and how it will contribute to the effectiveness of the application. In particular, you should use preprocessing steps thoughtfully if you will be using <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) in <a id="_idIndexMarker319"/>your NLP application. Since they are trained on normal (unregularized) text, regularizing input text to LLMs will make the input text less similar to the training text. This usually causes problems with machine learning models such <span class="No-Break">as LLMs.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor122"/>Removing non-text</h2>
			<p>Many natural<a id="_idIndexMarker320"/> language components are only able to process textual characters, but documents can also contain characters that are not text. Depending on the purpose of the application, you can either remove them from the text completely or replace them with equivalent characters that the system is able <span class="No-Break">to process.</span></p>
			<p>Note that the idea of <em class="italic">non-text</em> is not an all-or-none concept. What is considered <em class="italic">non-text</em> is to some extent application-specific. While the standard characters in the ASCII character set (<a href="https://www.ascii-code.com/">https://www.ascii-code.com/</a>) are clear examples of text, other characters can sometimes be considered non-text. For example, your application’s <em class="italic">non-text</em> could include such items as currency symbols, math symbols, or texts written in scripts other than the main script of the rest of the text (such as a Chinese word in an otherwise <span class="No-Break">English document).</span></p>
			<p>In the next two sections, we will look at removing or replacing two common examples of non-text: emojis and<a id="_idIndexMarker321"/> smart quotes. These examples should provide a general framework for removing other types <span class="No-Break">of non-text.</span></p>
			<h3>Removing emojis</h3>
			<p>One of the most common<a id="_idIndexMarker322"/> types of non-text is emojis. Social media posts are very likely to contain emojis, but they are a very interesting kind of text for NLP processing. If the natural language tools being used in your application don’t support emojis, the emojis can either be removed or replaced with their text equivalents. In most applications, it is likely that you will want to remove or replace emojis, but it is also possible in some cases that your NLP application will be able to interpret them directly. In that case, you won’t want to <span class="No-Break">remove them.</span></p>
			<p>One way to replace emojis is to use regular expressions that search for the Unicode (see <a href="https://home.unicode.org/">https://home.unicode.org/</a> for more information about Unicode) representations of emojis in the text. Still, the set of emojis is constantly expanding, so it is difficult to write a regular expression that covers all possibilities. Another approach is to use a Python library that directly accesses the Unicode data from <a href="https://unicode.org">unicode.org</a>, which defines standard <span class="No-Break">emojis (</span><a href="https://home.unicode.org/"><span class="No-Break">https://home.unicode.org/</span></a><span class="No-Break">).</span></p>
			<p>One package that can be used to remove or replace emojis is <span class="No-Break"><strong class="source-inline">demoji</strong></span><span class="No-Break"> (</span><a href="https://pypi.org/project/demoji/"><span class="No-Break">https://pypi.org/project/demoji/</span></a><span class="No-Break">).</span></p>
			<p>Using <strong class="source-inline">demoji</strong> is just a matter of installing it and running over text that may contain <span class="No-Break">undesired emojis:</span></p>
			<ol>
				<li>First, <span class="No-Break">install </span><span class="No-Break"><strong class="source-inline">demoji</strong></span><span class="No-Break">:</span><pre class="source-code">
$ pip install demoji</pre></li>
				<li>Then, given a text that contains emojis, run to replace the emojis with descriptions using the <span class="No-Break">following code:</span><pre class="source-code">
demoji.replace_with_desc(text)</pre></li>
			</ol>
			<p>Or, if you want to remove the emojis completely or replace them with a specific alternative of your choosing, you can run the following code:</p>
			<pre class="source-code">
demoji.replace(text,replacement_text)</pre>
			<p>For example, <em class="italic">Figure 5.1</em> shows a text that includes an emoji of a birthday cake and shows how this can be replaced with the description <strong class="source-inline">:birthday cake:</strong>, or simply removed:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B19005_05_01.jpg" alt="Figure 5.1 – Replacing or removing emojis"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Replacing or removing emojis</p>
			<p>Even if the emoji doesn’t <a id="_idIndexMarker323"/>cause problems with running the software, leaving the emoji in place means that any meaning associated with the emoji will be ignored, because the NLP software will not understand the meaning of the emoji. If the emoji is replaced with a description, some of its meaning (for example, that the emoji represents a birthday cake) can be taken <span class="No-Break">into account.</span></p>
			<h3>Removing smart quotes</h3>
			<p>Word processing programs <a id="_idIndexMarker324"/>sometimes automatically change typed quotation marks into <em class="italic">smart quotes</em>, or <em class="italic">curly quotes</em>, which look better than straight quotes, but which other software may not be prepared for. Smart quotes can cause problems with some NLP software that is not expecting smart quotes. If your text contains smart quotes, they can easily be replaced with the normal Python string replacement method, as in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
text = "here is a string with "smart" quotes"
text = text.replace(""", "\"").replace(""","\"")
print(text)
here is a string with "smart" quotes</pre>
			<p>Note that the replacement straight quotes in the <strong class="source-inline">replace</strong> method need to be escaped with a backslash, like any use of <span class="No-Break">literal quotes.</span></p>
			<p>The previous two sections covered removing non-text items such as emojis and smart quotes. We will <a id="_idIndexMarker325"/>now talk about some techniques for regularizing text or modifying it to make it <span class="No-Break">more uniform.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor123"/>Regularizing text</h2>
			<p>In this section, we <a id="_idIndexMarker326"/>will be covering the most important techniques for regularizing text. We will talk about the goals of each technique and how to apply it <span class="No-Break">in Python.</span></p>
			<h3>Tokenization</h3>
			<p>Nearly all NLP software <a id="_idIndexMarker327"/>operates on the level of words, so the text needs to be broken into words for processing to work. In many languages, the primary way of separating text into words is by whitespaces, but there are many special cases where this heuristic doesn’t work. In <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>, you can see the code for splitting on whitespace and the code for tokenization <span class="No-Break">using NLTK:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B19005_05_02.jpg" alt="Figure 5.2 – Python code for tokenization by splitting on whitespace and using NLTK’s tokenization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Python code for tokenization by splitting on whitespace and using NLTK’s tokenization</p>
			<p>Running the<a id="_idIndexMarker328"/> code in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> results in the tokenizations shown in <span class="No-Break"><em class="italic">Table 5.1</em></span><span class="No-Break">:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Issue</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Result from splitting </strong><span class="No-Break"><strong class="bold">on whitespace</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">NLTK result</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Walk here.</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Punctuation should not be included in <span class="No-Break">the token</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['</strong><span class="No-Break"><strong class="source-inline">Walk', 'here.']</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['Walk', '</strong><span class="No-Break"><strong class="source-inline">here', '.']</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">Walk   </strong><span class="No-Break"><strong class="source-inline">here.</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Extra whitespace should not count as <span class="No-Break">a token</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['Walk', '', '</strong><span class="No-Break"><strong class="source-inline">here.']</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['Walk', '</strong><span class="No-Break"><strong class="source-inline">here', '.']</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">Don't </strong><span class="No-Break"><strong class="source-inline">walk here.</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The contraction “don’t” should count as <span class="No-Break">two tokens</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">["Don't", '</strong><span class="No-Break"><strong class="source-inline">walk', 'here.']</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['Do', "n't", 'walk', '</strong><span class="No-Break"><strong class="source-inline">here', '.']</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">$</strong><span class="No-Break"><strong class="source-inline">100</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The “$” should be a <span class="No-Break">separate token</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['$</strong><span class="No-Break"><strong class="source-inline">100']</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">['$', '</strong><span class="No-Break"><strong class="source-inline">100']</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 5.1 – Tokenization results by splitting on whitespace and using NLTK’s tokenization</p>
			<p>Looking at <em class="italic">Table 5.1</em>, we can see that the simple heuristic of splitting the text on whitespace results in errors in <span class="No-Break">some cases:</span></p>
			<ul>
				<li>In the first row of <em class="italic">Table 5.1</em>, we can compare the two approaches when punctuation occurs at the end of a token, but there is no whitespace between the token and <a id="_idIndexMarker329"/>punctuation. This means that just separating tokens on whitespace results in incorrectly including the punctuation in the token. This means that <strong class="source-inline">walk</strong>, <strong class="source-inline">walk,</strong>, <strong class="source-inline">walk.</strong>, <strong class="source-inline">walk?</strong>, <strong class="source-inline">walk;</strong>, <strong class="source-inline">walk:</strong>, and <strong class="source-inline">walk!</strong> will all be considered to be different words. If all of those words appear to be different, any generalizations found in training based on one version of the word won’t apply to the <span class="No-Break">other versions.</span></li>
				<li>In the second row, we can see that if split on whitespace, two whitespaces in a row will result in an extra blank token. The result of this will be to throw off any algorithms that take into account the fact that two words are next to <span class="No-Break">each other.</span></li>
				<li>Contractions also cause problems when splitting on whitespace is used for tokenization. Contractions of two words won’t be separated into their components, which means <a id="_idIndexMarker330"/>that the <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) algorithms won’t be able to take into account that <em class="italic">do not</em> and <em class="italic">don’t</em> have the <span class="No-Break">same meaning.</span></li>
				<li>Finally, when presented with words with monetary amounts or other measurements, the algorithms won’t be able to take into account that <em class="italic">$100</em> and <em class="italic">100 dollars</em> have the <span class="No-Break">same meaning.</span></li>
			</ul>
			<p>It is tempting to try to write regular expressions to take care of these exceptions to the generalization that most words are surrounded by whitespace. However, in practice, it is very hard to capture all of the cases. As we attempt to cover more cases with regular expressions, the regular expressions will become more complex and difficult to maintain. For that reason, it is preferable to use a library such as NLTK’s, which has been developed over many years and has been thoroughly tested. You can try different texts with this code to see <a id="_idIndexMarker331"/>what kinds of results you get with different <span class="No-Break">tokenization approaches.</span></p>
			<p>As <em class="italic">Table 5.1</em> shows, either way, the result will be a list of strings, which is a convenient form for <span class="No-Break">further processing.</span></p>
			<h3>Lower casing</h3>
			<p>In languages that use<a id="_idIndexMarker332"/> uppercase and lowercase in their writing systems, most documents contain words with upper and lowercase letters. As in the case of tokenization, having the same word written in slightly different formats means that data from one format won’t apply to data in the other formats. For example, <em class="italic">Walk</em>, <em class="italic">walk</em>, and <em class="italic">WALK</em> will all count as different words. In order to make them all count as the same word, the text is normally all in lowercase. This can be done by looping over a list of word tokens and applying the <strong class="source-inline">lower()</strong> Python function, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B19005_05_03.jpg" alt="Figure 5.3 – Converting text to all lowercase"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Converting text to all lowercase</p>
			<p>Converting all words to lowercase has some drawbacks, however. Case differences are sometimes important for meaning. The biggest example of this is that it will make it hard for NLP software to tell the difference between proper names and ordinary words, which differ in case. This can<a id="_idIndexMarker333"/> cause errors in <strong class="bold">part of speech</strong> (<strong class="bold">POS</strong>) tagging <a id="_idIndexMarker334"/>or <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) if the tagger or named entity recognizer is trained on data that includes case differences. Similarly, words can sometimes be written in all caps for emphasis. This may indicate something about the sentiment expressed in the sentence – perhaps the writer is excited or angry – and this information could be helpful in sentiment analysis. For these reasons, the position of each preprocessing step in a pipeline should be considered to make sure no information is removed before it’s needed. This will<a id="_idIndexMarker335"/> be discussed in more detail in the <em class="italic">Text preprocessing pipeline</em> section later in <span class="No-Break">this chapter.</span></p>
			<h3>Stemming</h3>
			<p>Words in many <a id="_idIndexMarker336"/>languages appear in different forms depending on how they’re used in a sentence. For example, English nouns have different forms depending on whether they’re singular or plural, and English verbs have different forms depending on their tense. English has only a few variations, but other languages sometimes have many more. For example, Spanish has dozens of verb forms that indicate past, present, or future tenses or whether the subject of the verb is first, second, or third person, or singular or plural. Technically, in linguistics, these different forms are referred<a id="_idIndexMarker337"/> to as <strong class="bold">inflectional morphology</strong>. The endings themselves are <a id="_idIndexMarker338"/>referred to as <span class="No-Break"><strong class="bold">inflectional morphemes</strong></span><span class="No-Break">.</span></p>
			<p>Of course, in speech or text that is directed toward another person, these different forms are very important in conveying the speaker’s meaning. However, if the goal of our NLP application is to classify documents into different categories, paying attention to different forms of a word is likely not to be necessary. Just as with punctuation, different forms of words can cause them to be treated as completely separate words by NLU processors, despite the fact that the words are very similar <span class="No-Break">in meaning.</span></p>
			<p><strong class="bold">Stemming</strong> and <strong class="bold">lemmatization</strong> are<a id="_idIndexMarker339"/> two similar methods of regularizing these <a id="_idIndexMarker340"/>different forms. Stemming is the simpler approach, so we will look at that first. Stemming basically means removing specific letters that end some words and that are frequently, but not always, inflectional morphemes – for example, the <em class="italic">s</em> at the end of <em class="italic">walks</em>, or the <em class="italic">ed</em> at the end of <em class="italic">walked</em>. Stemming algorithms don’t have any knowledge of the actual words in a language; they’re just guessing what may or may not be an ending. For that reason, they make a lot of mistakes. They can make mistakes by either removing too many letters or not enough letters, resulting in two words being collapsed that are actually different words, or not collapsing words that should be treated as the <span class="No-Break">same word.</span></p>
			<p><strong class="source-inline">PorterStemmer</strong> is a widely used stemming tool and is built into NLTK. It can be used as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B19005_05_04.jpg" alt="Figure 5.4 – Results from stemming with PorterStemmer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Results from stemming with PorterStemmer</p>
			<p>Note that the<a id="_idIndexMarker341"/> results include a number of mistakes. While <strong class="source-inline">walked</strong> becomes <strong class="source-inline">walk</strong>, and <strong class="source-inline">going</strong> becomes <strong class="source-inline">go</strong>, which are good results, the other changes that the stemmer made <span class="No-Break">are errors:</span></p>
			<ul>
				<li><strong class="source-inline">exercise</strong> <img src="image/01.png" alt=""/> <span class="No-Break"><strong class="source-inline">exercis</strong></span></li>
				<li><strong class="source-inline">every</strong> <img src="image/01.png" alt=""/> <span class="No-Break"><strong class="source-inline">everi</strong></span></li>
				<li><strong class="source-inline">evening</strong> <img src="image/01.png" alt=""/> <span class="No-Break"><strong class="source-inline">even</strong></span></li>
				<li><strong class="source-inline">this</strong> <img src="image/01.png" alt=""/> <span class="No-Break"><strong class="source-inline">thi</strong></span></li>
			</ul>
			<p>The Porter stemmer also only works for English because its stemming algorithm includes heuristics, such as removing the <em class="italic">s</em> at the end of words, that only apply to English. NLTK also includes a multilingual stemmer, called the Snowball stemmer, that can be used with more languages, including Arabic, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, <span class="No-Break">and Swedish.</span></p>
			<p>However, since these<a id="_idIndexMarker342"/> stemmers don’t have any specific knowledge of the words of the languages they’re applied to, they can make mistakes, as we have seen. A similar but more accurate approach actually makes use of a dictionary, so that it doesn’t make errors like the ones listed previously. This approach is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">lemmatization</strong></span><span class="No-Break">.</span></p>
			<h3>Lemmatizing and part of speech tagging</h3>
			<p>Lemmatization, like<a id="_idIndexMarker343"/> stemming, has <a id="_idIndexMarker344"/>the goal of reducing variation in the words that occur in the text. However, lemmatization actually replaces each word with its root word (found by looking the word up in a computational dictionary) rather than simply removing what looks like suffixes. However, identifying the root word often depends on the part of speech, and lemmatization can be inaccurate if it doesn’t know the word’s part of speech. The part of speech can be identified <a id="_idIndexMarker345"/>through <strong class="bold">part of speech tagging</strong>, covered in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, which assigns the most probable part of speech to each word in a text. For that reason, lemmatization and part of speech tagging are often <span class="No-Break">performed together.</span></p>
			<p>For the dictionary in this example, we’ll use WordNet, developed at Princeton University (<a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a>), an important source of information about words and their parts of speech. The original WordNet was developed for English, but WordNets for other languages have also been developed. We briefly mentioned WordNet in the <em class="italic">Semantic analysis</em> section of <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, because WordNet contains semantic information as well as information about parts <span class="No-Break">of speech.</span></p>
			<p>In this example, we’ll just use the part of speech information, not the semantic information. <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5</em> shows importing the WordNet lemmatizer, the tokenizer, and the part of speech tagger. We then have to align the names of the parts of speech between WordNet and the part of speech tagger, because the part of speech tagger and WordNet don’t use the same names for the parts of speech. We then go through the text, lemmatizing <span class="No-Break">each word:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B19005_05_05.jpg" alt="Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve walked every evening this week”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve walked every evening this week”</p>
			<p>As the<a id="_idIndexMarker346"/> lemmatized <a id="_idIndexMarker347"/>result shows, many of the words in the input text have been replaced by their lemmas – <strong class="source-inline">going</strong> is replaced with <strong class="source-inline">go</strong>, <strong class="source-inline">is</strong> is replaced with <strong class="source-inline">be</strong>, and <strong class="source-inline">walked</strong> is replaced <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">walk</strong></span><span class="No-Break">.</span></p>
			<p>Note that <strong class="source-inline">evening</strong> hasn’t been replaced by <em class="italic">even</em>, as it was in the stemming example. If <em class="italic">evening</em> had been the present participle of the verb <em class="italic">even</em>, it would have been replaced by <strong class="source-inline">even</strong>, but <em class="italic">even</em> isn’t the root word for <em class="italic">evening</em> here. In this case, <em class="italic">evening</em> just refers to<a id="_idIndexMarker348"/> the <a id="_idIndexMarker349"/>time <span class="No-Break">of day.</span></p>
			<h3>Stopword removal</h3>
			<p><strong class="bold">Stopwords</strong> are<a id="_idIndexMarker350"/> extremely<a id="_idIndexMarker351"/> common words that are not helpful in distinguishing documents and so they are often removed in classification applications. However, if the application involves any kind of detailed sentence analysis, these common words are needed so that the system can figure out what the analysis <span class="No-Break">should be.</span></p>
			<p>Normally, stopwords include words such as pronouns, prepositions, articles, and conjunctions. Which words should be considered to be stopwords for a particular language is a matter of judgment. For example, spaCy has many more English stopwords (326) than NLTK (179). These specific stopwords were chosen by the spaCy and NLTK developers because they thought those stopwords would be useful in practice. You can use whichever one you find <span class="No-Break">more convenient.</span></p>
			<p>Let’s take a look at the stopwords provided by <span class="No-Break">each system.</span></p>
			<p>First, to run the NLTK and spaCy systems, you may need to do some preliminary setup. If you are working in a command-line or terminal environment, you can ensure that NLTK and spaCy are available by entering the following commands on the <span class="No-Break">command line:</span></p>
			<ol>
				<li><strong class="source-inline">pip install -U pip </strong><span class="No-Break"><strong class="source-inline">setuptools wheel</strong></span></li>
				<li><strong class="source-inline">pip install -</strong><span class="No-Break"><strong class="source-inline">U spacy</strong></span></li>
				<li><strong class="source-inline">python -m spacy </strong><span class="No-Break"><strong class="source-inline">download en_core_web_sm</strong></span></li>
			</ol>
			<p>On the other hand, if you’re working in the (recommended) Jupyter Notebook environment covered in <a href="B19005_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, you can enter the same commands in a Jupyter code cell but precede each command <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">!</strong></span><span class="No-Break">.</span></p>
			<p>Once you’ve confirmed that your NLTK and spaCy environments are set up, you can look at the NLTK stopwords by running the code in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B19005_05_06.jpg" alt="Figure 5.6 – Viewing the first few stopwords for NLTK"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Viewing the first few stopwords for NLTK</p>
			<p>Note that <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em> just shows the first few stopwords. You can see them all by running the code in <span class="No-Break">your environment.</span></p>
			<p>To see the<a id="_idIndexMarker352"/> spaCy stopwords, run the code in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.7</em>, which also just shows the first <span class="No-Break">few stopwords:</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B19005_05_07.jpg" alt="Figure 5.7 – Viewing the first few stopwords for spaCy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Viewing the first few stopwords for spaCy</p>
			<p>Comparing the stopwords provided by both packages, we can see that the two sets have a lot in common, but <a id="_idIndexMarker353"/>there are differences as well. In the end, the stopwords you use are up to you. Both sets work well <span class="No-Break">in practice.</span></p>
			<h3>Removing punctuation</h3>
			<p>Removing punctuation <a id="_idIndexMarker354"/>can also be useful since punctuation, like stopwords, appears in most documents and therefore doesn’t help distinguish <span class="No-Break">document categories.</span></p>
			<p>Punctuation can be removed by defining a string of punctuation symbols and removing items in that string with regular expressions, or by removing every non-alphanumeric word in the text. The latter approach is more robust because it’s easy to overlook an uncommon <span class="No-Break">punctuation symbol.</span></p>
			<p>The code to remove punctuation can be seen in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B19005_05_08.jpg" alt="Figure 5.8 – Removing punctuation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Removing punctuation</p>
			<p>The original text is the value of the <strong class="source-inline">text_to_remove_punct</strong> variable, in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.8</em>, which contains <a id="_idIndexMarker355"/>several punctuation marks – specifically, an exclamation mark, a comma, and a period. The result is the value of the <strong class="source-inline">tokens_no_punct</strong> variable, shown in the <span class="No-Break">last line.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor124"/>Spelling correction</h2>
			<p>Correcting misspelled<a id="_idIndexMarker356"/> words is another way of removing noise and regularizing text input. Misspelled words are much less likely to have occurred in training data than correctly spelled words, so they will be harder to recognize when a new text is being processed. In addition, any training that includes the correctly spelled version of the word will not recognize or be able to make use of data including the incorrectly spelled word. This means that it’s worth considering adding a spelling correction preprocessing step to the NLP pipeline. However, we don’t want to automatically apply spelling correction in every project. Some of the reasons not to use spelling correction are <span class="No-Break">the following:</span></p>
			<ul>
				<li>Some types of text will naturally be full of spelling mistakes – for example, social media posts. Because spelling mistakes will occur in texts that need to be processed by the application, it might be a good idea not to try to correct spelling mistakes in either the training or <span class="No-Break">runtime data.</span></li>
				<li>Spelling error correction doesn’t always do the right thing, and a spelling correction that results in the wrong word won’t be helpful. It will just introduce errors into <span class="No-Break">the processing.</span></li>
				<li>Some types of texts include many proper names or foreign words that aren’t known to the spell checker, which will try to correct them. Again, this will just <span class="No-Break">introduce errors.</span></li>
			</ul>
			<p>If you do choose to use spelling corrections, there are many spell checkers available in Python. One recommended spell checker is <strong class="source-inline">pyspellchecker</strong>, which can be installed <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
$ pip install pyspellchecker</pre>
			<p>The spell-checking code and<a id="_idIndexMarker357"/> the result can be seen in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B19005_05_09.jpg" alt="Figure 5.9 – Spell-checking with pyspellchecker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Spell-checking with pyspellchecker</p>
			<p>You can see from <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.9</em> that it’s easy to make mistakes with spell-checking. <strong class="source-inline">pyspellchecker</strong> correctly changes <strong class="source-inline">agains</strong> to <strong class="source-inline">against</strong>, but it also made a mistake correcting <strong class="source-inline">Ms.</strong> to <strong class="source-inline">is</strong> and it didn’t know anything about the name <strong class="source-inline">Ramalingam</strong>, so it didn’t have a correction <span class="No-Break">for that.</span></p>
			<p>Keep in mind that input generated by <strong class="bold">ASR</strong> will not contain spelling errors, because ASR can only output words in their dictionaries, which are all correctly spelled. Of course, ASR output can contain mistakes, but those mistakes will just be the result of substituting a wrong word for the word that was actually spoken, and they can’t be corrected by <span class="No-Break">spelling correction.</span></p>
			<p>Also note that stemming and lemmatization can result in tokens that aren’t real words, which you don’t want to be corrected. If spelling correction is used in a pipeline, make sure that it occurs before stemming <span class="No-Break">and lemmatization.</span></p>
			<h3>Expanding contractions</h3>
			<p>Another way to increase the<a id="_idIndexMarker358"/> uniformity of data is to expand contractions – that is, words such as <em class="italic">don’t</em> would be expanded to their full form, <em class="italic">do not</em>. This will allow the system to recognize an occurrence of <em class="italic">do</em> and an occurrence of <em class="italic">not</em> when it <span class="No-Break">finds </span><span class="No-Break"><em class="italic">don’t</em></span><span class="No-Break">.</span></p>
			<p>So far, we have reviewed many generic preprocessing techniques. Next, let’s move on to more specific techniques that are only applicable to certain types <span class="No-Break">of applications.</span></p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor125"/>Application-specific types of preprocessing</h1>
			<p>The preprocessing<a id="_idIndexMarker359"/> topics we have covered in the previous sections are generally applicable to many types of text in many applications. Additional preprocessing steps can also be used in specific applications, and we will cover these in the <span class="No-Break">next sections.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor126"/>Substituting class labels for words and numbers</h2>
			<p>Sometimes data <a id="_idIndexMarker360"/>includes specific words or tokens that have equivalent semantics. For example, a text corpus might include the names of US states, but for the purposes of the application, we only care that <em class="italic">some</em> state was mentioned – we don’t care which one. In that case, we can substitute a <em class="italic">class token</em> for the specific state name. Consider the interaction in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B19005_05_10New.jpg" alt="Figure 5.10 – Class token substitution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Class token substitution</p>
			<p>If we substitute the class token, <strong class="source-inline">&lt;state_name&gt;</strong>, for <strong class="source-inline">Texas</strong>, all of the other state names will be easier to recognize, because instead of having to learn 50 states, the system will only have to learn about the general <span class="No-Break">class, </span><span class="No-Break"><strong class="source-inline">&lt;state_name&gt;</strong></span><span class="No-Break">.</span></p>
			<p>Another reason to use class tokens is if the texts contain alphanumeric tokens such as dates, phone numbers, or social security numbers, especially if there are too many to enumerate. Class tokens such as <strong class="source-inline">&lt;social_security_number&gt;</strong> can be substituted for the<a id="_idIndexMarker361"/> actual numbers. This has the added benefit of masking, or <em class="italic">redacting</em>, sensitive information such as specific social <span class="No-Break">security numbers.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor127"/>Redaction</h2>
			<p>As we discussed in <a id="_idIndexMarker362"/>the <em class="italic">Ensuring privacy and observing ethical considerations</em> section, data can contain sensitive information such as people’s names, health information, social security numbers, or telephone numbers. This information should be redacted before the data is used <span class="No-Break">in training.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor128"/>Domain-specific stopwords</h2>
			<p>Both NLTK and spaCy have<a id="_idIndexMarker363"/> the<a id="_idIndexMarker364"/> capability to add and remove stopwords from their lists. For example, if your application has some very common domain-specific words that aren’t on the built-in stopword list, you can add these words to the application-specific stopword list. Conversely, if some words that are normally stopwords are actually meaningful in the application, these words can be removed from <span class="No-Break">the stopwords.</span></p>
			<p>A good example is the word <em class="italic">not</em>, which is a stopword for both NLTK and spaCy. In many document classification applications, it’s fine to consider <em class="italic">not</em> as a stopword; however, in applications such as sentiment analysis, <em class="italic">not</em> and other related words (for example, <em class="italic">nothing</em> or <em class="italic">none</em>) can be important clues for a negative sentiment. Removing them can cause errors if sentences such as <em class="italic">I do not like this product</em> becomes <em class="italic">I do like this product</em>. In that<a id="_idIndexMarker365"/> case, you should<a id="_idIndexMarker366"/> remove <em class="italic">not</em> and other negative words from the list of stopwords you <span class="No-Break">are using.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor129"/>Remove HTML markup</h2>
			<p>If the <a id="_idIndexMarker367"/>application<a id="_idIndexMarker368"/> is based on web pages, they will contain HTML formatting tags that aren’t useful in NLP. The Beautiful Soup library (<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a>) can perform the task of removing HTML tags. While Beautiful Soup has many functions for working with HTML documents, for our purposes, the most useful function is <strong class="source-inline">get_text()</strong>, which extracts the text from an <span class="No-Break">HTML document.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor130"/>Data imbalance</h2>
			<p>Text classification, where<a id="_idIndexMarker369"/> the task is to assign each document to one of a set of classes, is one of the most common types of NLP applications. In every real-life classification dataset, some of the classes will have more examples than others. This <a id="_idIndexMarker370"/>problem is called <strong class="bold">data imbalance</strong>. If the data is severely imbalanced, this will cause problems with machine learning algorithms. Two common techniques for addressing data imbalance<a id="_idIndexMarker371"/> are <strong class="bold">oversampling</strong> and <strong class="bold">undersampling</strong>. Oversampling<a id="_idIndexMarker372"/> means that some of the items in the less frequent classes are duplicated, and undersampling means that some of the items in the more common classes are removed. Both approaches can be used at the same time – frequent classes can be undersampled while infrequent classes can be oversampled. We will be discussing this topic in detail in <a href="B19005_14.xhtml#_idTextAnchor248"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor131"/>Using text preprocessing pipelines</h2>
			<p>Getting your <a id="_idIndexMarker373"/>data ready<a id="_idIndexMarker374"/> for NLP often involves multiple steps, and each step takes the output of the previous step and adds new information, or in general, gets the data further prepared for NLP. A sequence of preprocessing steps like this is<a id="_idIndexMarker375"/> called a <strong class="bold">pipeline</strong>. For example, an NLP pipeline could include tokenization followed by lemmatization and then stopword removal. By adding and removing steps in a pipeline, you can easily experiment with different preprocessing steps and see whether they make a difference in <span class="No-Break">the results.</span></p>
			<p>Pipelines can be used to prepare both training data to be used in learning a model, as well as test data to be used at runtime or during testing. In general, if a preprocessing step is always going to be needed (for example, tokenization), it’s worth considering using it on the training data once and then saving the resulting data as a dataset. This will save time if<a id="_idIndexMarker376"/> you’re running experiments with different configurations of preprocessing steps to find the <span class="No-Break">best configuration.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor132"/>Choosing among preprocessing techniques</h1>
			<p><em class="italic">Table 5.2</em> is a summary <a id="_idIndexMarker377"/>of the preprocessing techniques described in this chapter, along with their advantages and disadvantages. It is important for every<a id="_idIndexMarker378"/> project to consider which techniques will lead to <span class="No-Break">improved results:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/Table_01.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Figure">Table 5.2 – Advantages and disadvantages of preprocessing techniques</p>
			<p>Many techniques, such as spelling correction, have the potential to introduce errors because the technology is not perfect. This is particularly true for less well-studied languages, for which the relevant algorithms can be less mature than those of <span class="No-Break">better-studied languages.</span></p>
			<p>It is worth starting with an initial test with only the most necessary techniques (such as tokenization) and<a id="_idIndexMarker379"/> introducing additional techniques only if the results of the initial test are not good enough. Sometimes, the errors introduced by preprocessing can cause the overall results to get worse. It is important to keep evaluating results during the investigation to make sure that results aren’t getting worse. Evaluation will be discussed in detail in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor133"/>Summary</h1>
			<p>In this chapter, we covered how to find and use natural language data, including finding data for a specific application as well as using generally <span class="No-Break">available corpora.</span></p>
			<p>We discussed a wide variety of techniques for preparing data for NLP, including annotation, which provides the foundation for supervised learning. We also discussed common preprocessing steps that remove noise and decrease variation in the data and allow machine learning algorithms to focus on the most informative differences among different categories of texts. Another important set of topics covered in this chapter had to do with privacy and ethics – how to ensure the privacy of information included in text data and how to ensure that crowdsourcing workers who are generating data or who are annotating data are <span class="No-Break">treated fairly.</span></p>
			<p>The next chapter will discuss exploratory techniques for getting an overall picture of a dataset, such as summary statistics (word frequencies, category frequencies, and so on). It will also discuss visualization tools (such as matplotlib) that can provide the kinds of insights that can be best obtained by looking at graphical representations of text data. Finally, it will discuss the kinds of decisions that can be made based on visualization and <span class="No-Break">statistical results.</span></p>
		</div>
	</body></html>