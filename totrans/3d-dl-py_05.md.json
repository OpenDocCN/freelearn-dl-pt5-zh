["```py\n    import os\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    import matplotlib.pyplot as plt\n    from skimage import img_as_ubyte\n    from pytorch3d.io import load_obj\n    from pytorch3d.structures import Meshes\n    from pytorch3d.renderer import (\n    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n    )\n    ```", "```py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n    else:\n        device = torch.device(\"cpu\")\n        print(\"WARNING: CPU only, this will be slow!\")\n    ```", "```py\n    output_dir = './result_teapot'\n    ```", "```py\n    verts, faces_idx, _ = load_obj(\"./data/teapot.obj\")\n    faces = faces_idx.verts_idx\n    verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n    textures = TexturesVertex(verts_features=verts_rgb.to(device))\n    teapot_mesh = Meshes(\n        verts=[verts.to(device)],\n        faces=[faces.to(device)],\n        textures=textures\n    )\n    ```", "```py\n    cameras = FoVPerspectiveCameras(device=device)\n    ```", "```py\nblend_params = BlendParams(sigma=1e-4, gamma=1e-4)\nraster_settings = RasterizationSettings(\n    image_size=256,\n    blur_radius=np.log(1\\. / 1e-4 - 1.) * blend_params.sigma,\n    faces_per_pixel=100,\n)\nsilhouette_renderer = MeshRenderer(\n    rasterizer=MeshRasterizer(\n        cameras=cameras,\n        raster_settings=raster_settings\n    ),\n    shader=SoftSilhouetteShader(blend_params=blend_params)\n)\n```", "```py\n    raster_settings = RasterizationSettings(\n        image_size=256,\n        blur_radius=0.0,\n        faces_per_pixel=1,\n    )\n    lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n    phong_renderer = MeshRenderer(\n        rasterizer=MeshRasterizer(\n            cameras=cameras,\n            raster_settings=raster_settings\n        ),\n        shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n    )\n    ```", "```py\n    distance = 3\n    elevation = 50.0\n    azimuth = 0.0\n    R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n    ```", "```py\n    silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)\n    image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n    silhouette = silhouette.cpu().numpy()\n    image_ref = image_ref.cpu().numpy()\n    plt.figure(figsize=(10, 10))\n    plt.imshow(silhouette.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n    plt.grid(False)\n    plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))\n    plt.close()\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image_ref.squeeze())\n    plt.grid(False)\n    plt.savefig(os.path.join(output_dir, 'target_rgb.png'))\n    plt.close()\n    ```", "```py\nclass Model(nn.Module):\n    def __init__(self, meshes, renderer, image_ref):\n        super().__init__()\n        self.meshes = meshes\n        self.device = meshes.device\n        self.renderer = renderer\n        image_ref = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))\n        self.register_buffer('image_ref', image_ref)\n        self.camera_position = nn.Parameter(\n            torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))\n    def forward(self):\n        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n        image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n        loss = torch.sum((image[..., 3] - self.image_ref) ** 2)\n        return loss, image\n```", "```py\n    model = Model(meshes=teapot_mesh, renderer=silhouette_renderer, image_ref=image_ref).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n    _, image_init = model()\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])\n    plt.grid(False)\n    plt.title(\"Starting Silhouette\")\n    plt.savefig(os.path.join(output_dir, 'starting_silhouette.png'))\n    plt.close()\n    ```", "```py\n    for i in range(0, 200):\n        if i%10 == 0:\n            print('i = ', i)\n        optimizer.zero_grad()\n        loss, _ = model()\n        loss.backward()\n        optimizer.step()\n        if loss.item() < 500:\n            break\n        R = look_at_rotation(model.camera_position[None, :], device=model.device)\n        T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n        image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n        image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n        image = img_as_ubyte(image)\n        plt.figure()\n        plt.imshow(image[..., :3])\n        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n        plt.axis(\"off\")\n        plt.savefig(os.path.join(output_dir, 'fitting_' + str(i) + '.png'))\n        plt.close()\n    ```", "```py\n    import os\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    import matplotlib.pyplot as plt\n    from skimage import img_as_ubyte\n    from pytorch3d.io import load_objs_as_meshes\n    from pytorch3d.renderer import (\n    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n    SoftSilhouetteShader, HardPhongShader, PointLights,\n    SoftPhongShader\n    )\n    ```", "```py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    ```", "```py\n    output_dir = './result_cow'\n    ```", "```py\n    obj_filename = \"./data/cow_mesh/cow.obj\"\n    cow_mesh = load_objs_as_meshes([obj_filename], device=device)\n    ```", "```py\n    cameras = FoVPerspectiveCameras(device=device)\n    lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n    ```", "```py\n    blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n    raster_settings = RasterizationSettings(\n        image_size=256,\n        blur_radius=np.log(1\\. / 1e-4 - 1.) * blend_params.sigma,\n        faces_per_pixel=100,\n    )\n    renderer_silhouette = MeshRenderer(\n        rasterizer=MeshRasterizer(\n            cameras=cameras,\n            raster_settings=raster_settings\n        ),\n        shader=SoftSilhouetteShader(blend_params=blend_params)\n    )\n    ```", "```py\n    sigma = 1e-4\n    raster_settings_soft = RasterizationSettings(\n        image_size=256,\n        blur_radius=np.log(1\\. / 1e-4 - 1.)*sigma,\n        faces_per_pixel=50,\n    )\n    renderer_textured = MeshRenderer(\n        rasterizer=MeshRasterizer(\n            cameras=cameras,\n            raster_settings=raster_settings_soft\n        ),\n        shader=SoftPhongShader(device=device,\n            cameras=cameras,\n            lights=lights)\n    )\n    ```", "```py\n    raster_settings = RasterizationSettings(\n        image_size=256,\n        blur_radius=0.0,\n        faces_per_pixel=1,\n    )\n    phong_renderer = MeshRenderer(\n        rasterizer=MeshRasterizer(\n            cameras=cameras,\n            raster_settings=raster_settings\n        ),\n        shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n    )\n    ```", "```py\n    distance = 3\n    elevation = 50.0\n    azimuth = 0.0\n    R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n    ```", "```py\n    silhouette = renderer_silhouette(meshes_world=cow_mesh, R=R, T=T)\n    image_ref = phong_renderer(meshes_world=cow_mesh, R=R, T=T)\n    silhouette = silhouette.cpu().numpy()\n    image_ref = image_ref.cpu().numpy()\n    plt.figure(figsize=(10, 10))\n    plt.imshow(silhouette.squeeze()[..., 3])\n    plt.grid(False)\n    plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))\n    plt.close()\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image_ref.squeeze())\n    plt.grid(False)\n    plt.savefig(os.path.join(output_dir, 'target_rgb.png'))\n    plt.close()\n    ```", "```py\n    class Model(nn.Module):\n        def __init__(self, meshes, renderer_silhouette, renderer_textured, image_ref, weight_silhouette, weight_texture):\n            super().__init__()\n            self.meshes = meshes\n            self.device = meshes.device\n            self.renderer_silhouette = renderer_silhouette\n            self.renderer_textured = renderer_textured\n            self.weight_silhouette = weight_silhouette\n            self.weight_texture = weight_texture\n            image_ref_silhouette = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))\n            self.register_buffer('image_ref_silhouette', image_ref_silhouette)\n            image_ref_textured = torch.from_numpy((image_ref[..., :3]).astype(np.float32))\n            self.register_buffer('image_ref_textured', image_ref_textured)\n            self.camera_position = nn.Parameter(\n                torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))\n        def forward(self):\n            # Render the image using the updated camera position. Based on the new position of the\n            # camera we calculate the rotation and translation matrices\n            R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n            T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n            image_silhouette = self.renderer_silhouette(meshes_world=self.meshes.clone(), R=R, T=T)\n            image_textured = self.renderer_textured(meshes_world=self.meshes.clone(), R=R, T=T)\n            loss_silhouette = torch.sum((image_silhouette[..., 3] - self.image_ref_silhouette) ** 2)\n            loss_texture = torch.sum((image_textured[..., :3] - self.image_ref_textured) ** 2)\n            loss = self.weight_silhouette * loss_silhouette + self.weight_texture * loss_texture\n            return loss, image_silhouette, image_textured\n    ```", "```py\n    model = Model(meshes=cow_mesh, renderer_silhouette=renderer_silhouette, renderer_textured = renderer_textured,\n                  image_ref=image_ref, weight_silhouette=1.0, weight_texture=0.1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n    ```", "```py\n    for i in range(0, 200):\n        if i%10 == 0:\n            print('i = ', i)\n        optimizer.zero_grad()\n        loss, image_silhouette, image_textured = model()\n        loss.backward()\n        optimizer.step()\n        plt.figure()\n        plt.imshow(image_silhouette[..., 3].detach().squeeze().cpu().numpy())\n        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n        plt.axis(\"off\")\n        plt.savefig(os.path.join(output_dir, 'soft_silhouette_' + str(i) + '.png'))\n        plt.close()\n        plt.figure()\n        plt.imshow(image_textured.detach().squeeze().cpu().numpy())\n        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n        plt.axis(\"off\")\n        plt.savefig(os.path.join(output_dir, 'soft_texture_' + str(i) + '.png'))\n        plt.close()\n        R = look_at_rotation(model.camera_position[None, :], device=model.device)\n        T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n        image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n        plt.figure()\n        plt.imshow(image[..., 3].detach().squeeze().cpu().numpy())\n        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n        plt.axis(\"off\")\n        plt.savefig(os.path.join(output_dir, 'hard_silhouette_' + str(i) + '.png'))\n        plt.close()\n        image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n        image = img_as_ubyte(image)\n        plt.figure()\n        plt.imshow(image[..., :3])\n        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n        plt.axis(\"off\")\n        plt.savefig(os.path.join(output_dir, 'hard_texture_' + str(i) + '.png'))\n        plt.close()\n        if loss.item() < 800:\n            break\n    ```"]