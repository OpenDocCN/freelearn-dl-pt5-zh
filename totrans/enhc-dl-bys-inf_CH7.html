<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch012.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-7-practical-considerations-for-bayesian-deep-learning" class="level1 chapterHead" data-number="12">
<h1 class="chapterHead" data-number="12"><span class="titlemark">Chapter 7</span><br/>
<span id="x1-1130007"></span>Practical Considerations for Bayesian Deep Learning</h1>
<p>Over the last two chapters, <a href="CH5.xhtml#x1-600005"><em>Chapter 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches for Bayesian</em> <em>Deep Learning</em></a> and <a href="CH6.xhtml#x1-820006"><em>Chapter 6</em></a>, <a href="CH6.xhtml#x1-820006"><em>Using the Standard Toolbox for Bayesian</em> <em>Deep Learning</em></a>, we’ve been introduced to a range of methods that facilitate Bayesian inference with neural networks. <a href="CH5.xhtml#x1-600005"><em>Chapter 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches</em> <em>for Bayesian Deep Learning</em></a> introduced specially crafted Bayesian neural network approximations, while <a href="CH6.xhtml#x1-820006"><em>Chapter 6</em></a>, <a href="CH6.xhtml#x1-820006"><em>Using the Standard Toolbox for</em> <em>Bayesian Deep Learning</em></a> showed how we can use the standard toolbox of machine learning to add uncertainty estimates to our models. These families of methods come with their own advantages and disadvantages. In this chapter, we will explore some of these differences in practical scenarios in order to help you understand how to select the best method for the task at hand.</p>
<p>We will also look at different sources of uncertainty, which can improve your understanding of the data or help you choose a different exception path based on the source of uncertainty. For example, if a model is uncertain because the input data is inherently noisy, you might want to send the data to a human for review. However, if a model is uncertain because the input data has not been seen before, it might be helpful to add this data to your model so that it can reduce its uncertainty on this type of data. Bayesian deep learning techniques can help you to distinguish between these sources of uncertainty. These topics will be covered in the following sections:</p>
<ul>
<li><p>Balancing uncertainty quality and computational considerations</p></li>
<li><p>BDL and sources of uncertainty</p></li>
</ul>
<p><span id="x1-113001r178"></span></p>
<section id="technical-requirements-5" class="level2 sectionHead" data-number="12.1">
<h2 class="sectionHead" data-number="12.1" id="sigil_toc_id_77"><span class="titlemark">7.1 </span> <span id="x1-1140001"></span>Technical requirements</h2>
<p>To complete the practical tasks in this chapter, you will need a Python 3.8 environment with the SciPy and scikit-learn stack and the following additional Python packages installed:</p>
<ul>
<li><p>TensorFlow 2.0</p></li>
<li><p>TensorFlow Probability</p></li>
</ul>
<p>All of the code for this book can be found on the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-114001r180"></span></p>
</section>
<section id="balancing-uncertainty-quality-and-computational-considerations" class="level2 sectionHead" data-number="12.2">
<h2 class="sectionHead" data-number="12.2" id="sigil_toc_id_78"><span class="titlemark">7.2 </span> <span id="x1-1150002"></span>Balancing uncertainty quality and computational considerations</h2>
<p>While Bayesian methods have many benefits, there are also trade-offs to consider in terms of memory and computational overheads. These considerations play a critical role in selecting the most appropriate methods to use within real-world applications.</p>
<p>In this section, we’ll examine the trade-offs <span id="dx1-115001"></span>between different methods in terms of performance and uncertainty quality, and we’ll learn how we can use TensorFlow’s profiling tools to measure the computational costs associated with different models. <span id="x1-115002r177"></span></p>
<section id="setting-up-our-experiments" class="level3 subsectionHead" data-number="12.2.1">
<h3 class="subsectionHead" data-number="12.2.1" id="sigil_toc_id_79"><span class="titlemark">7.2.1 </span> <span id="x1-1160001"></span>Setting up our experiments</h3>
<p>To evaluate the performance of different models, we’ll need a few different datasets. One of these is the California Housing dataset, which is <span id="dx1-116001"></span>conveniently provided by scikit-learn. The others we’ll use are commonly used in papers comparing uncertainty models: the Wine Quality dataset and the Concrete Comdivssive Strength dataset. Let’s take a look at a breakdown of these datasets:</p>
<ul>
<li><p><strong>California Housing</strong>: This dataset comprises a number of features for different regions in California derived from the 1990 California census. The dependent variable is house value, which is provided as the median value for each block of houses. In older papers, you’ll see the Boston Housing dataset used; the California Housing dataset is now favored due to ethical issues around the Boston Housing dataset.</p></li>
<li><p><strong>Wine Quality</strong>: The Wine Quality dataset comprises features pertaining to the chemical composition of a variety of different wines. The value we’re trying to predict is the subjective quality of the wine.</p></li>
<li><p><strong>Concrete Comdivssive Strength</strong>: The Concrete Comdivssive Strength dataset’s features describe the ingredients used for mixing concrete, and each data point is a different concrete mixture. The dependent variable is the concrete’s comdivssive strength.</p></li>
</ul>
<p>The following experiments will use code from the book’s GitHub repository ( <a href="https://github.com/PacktPublishing/Bayesian-Deep-Learning" class="url"><span class="No-Break">https://github.com/PacktPublishing/Bayesian-Deep-Learning</span></a>), which we’ve seen in various forms in the previous chapters. The example assumes that we’re running the code from within this repository.</p>
<section id="importing-our-dependencies" class="level4 likesubsubsectionHead" data-number="12.2.1.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.2.1.1"><span id="x1-1170001"></span>Importing our dependencies</h4>
<p>As usual, we’ll start by <span id="dx1-117001"></span>importing our dependencies:</p>
<pre id="fancyvrb100" class="fancyvrb"><span id="x1-117020r1"></span> 
<code><span id="textcolor2585"><span>import</span></span><span> </span><span id="textcolor2586"><span>tensorflow</span></span><span> </span><span id="textcolor2587"><span>as</span></span><span> </span><span id="textcolor2588"><span>tf</span></span> <span id="x1-117022r2"></span> </code>
<code><span id="textcolor2589"><span>import</span></span><span> </span><span id="textcolor2590"><span>numpy</span></span><span> </span><span id="textcolor2591"><span>as</span></span><span> </span><span id="textcolor2592"><span>np</span></span> <span id="x1-117024r3"></span> </code>
<code><span id="textcolor2593"><span>import</span></span><span> </span><span id="textcolor2594"><span>matplotlib.pyplot</span></span><span> </span><span id="textcolor2595"><span>as</span></span><span> </span><span id="textcolor2596"><span>plt</span></span> <span id="x1-117026r4"></span> </code>
<code><span id="textcolor2597"><span>import</span></span><span> </span><span id="textcolor2598"><span>tensorflow_probability</span></span><span> </span><span id="textcolor2599"><span>as</span></span><span> </span><span id="textcolor2600"><span>tfp</span></span> <span id="x1-117028r5"></span> </code>
<code><span id="textcolor2601"><span>from</span></span><span> </span><span id="textcolor2602"><span>sklearn.metrics</span></span><span> </span><span id="textcolor2603"><span>import</span></span><span> accuracy_score,</span><span> mean_squared_error</span> <span id="x1-117030r6"></span> </code>
<code><span id="textcolor2604"><span>from</span></span><span> </span><span id="textcolor2605"><span>sklearn.datasets</span></span><span> </span><span id="textcolor2606"><span>import</span></span><span> fetch_california_housing,</span><span> load_diabetes</span> <span id="x1-117032r7"></span> </code>
<code><span id="textcolor2607"><span>from</span></span><span> </span><span id="textcolor2608"><span>sklearn.model_selection</span></span><span> </span><span id="textcolor2609"><span>import</span></span><span> train_test_split</span> <span id="x1-117034r8"></span> </code>
<code><span id="textcolor2610"><span>import</span></span><span> </span><span id="textcolor2611"><span>seaborn</span></span><span> </span><span id="textcolor2612"><span>as</span></span><span> </span><span id="textcolor2613"><span>sns</span></span> <span id="x1-117036r9"></span> </code>
<code><span id="textcolor2614"><span>import</span></span><span> </span><span id="textcolor2615"><span>pandas</span></span><span> </span><span id="textcolor2616"><span>as</span></span><span> </span><span id="textcolor2617"><span>pd</span></span> <span id="x1-117038r10"></span> </code>
<code><span id="textcolor2618"><span>import</span></span><span> </span><span id="textcolor2619"><span>os</span></span> <span id="x1-117040r11"></span> </code>
<code><span id="x1-117042r12"></span></code>
<code><span id="textcolor2620"><span>from</span></span><span> </span><span id="textcolor2621"><span>bayes_by_backprop</span></span><span> </span><span id="textcolor2622"><span>import</span></span><span> BBBRegressor</span> <span id="x1-117044r13"></span> </code>
<code><span id="textcolor2623"><span>from</span></span><span> </span><span id="textcolor2624"><span>pbp</span></span><span> </span><span id="textcolor2625"><span>import</span></span><span> PBP</span> <span id="x1-117046r14"></span> </code>
<code><span id="textcolor2626"><span>from</span></span><span> </span><span id="textcolor2627"><span>mc_dropout</span></span><span> </span><span id="textcolor2628"><span>import</span></span><span> MCDropout</span> <span id="x1-117048r15"></span> </code>
<code><span id="textcolor2629"><span>from</span></span><span> </span><span id="textcolor2630"><span>ensemble</span></span><span> </span><span id="textcolor2631"><span>import</span></span><span> Ensemble</span> <span id="x1-117050r16"></span> </code>
<code><span id="textcolor2632"><span>from</span></span><span> </span><span id="textcolor2633"><span>bdl_ablation_data</span></span><span> </span><span id="textcolor2634"><span>import</span></span><span> load_wine_quality,</span><span> load_concrete</span> <span id="x1-117052r17"></span> </code>
<code><span id="textcolor2635"><span>from</span></span><span> </span><span id="textcolor2636"><span>bdl_metrics</span></span><span> </span><span id="textcolor2637"><span>import</span></span><span> likelihood</span></code></pre>
<p>Here, we can see that we’re using a number of model classes defined in the repository. While these classes each support different architectures of models, they’ll be using the default structure, which is defined in <code>constants.py</code>. This structure comprises a single densely connected hidden layer of 64 units, and a single densely connected output layer. The BBB and PBP equivalents will be used and are defined as their default architectures in their respective classes.</p>
</section>
<section id="preparing-our-data-and-models" class="level4 likesubsubsectionHead" data-number="12.2.1.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.2.1.2"><span id="x1-1180001"></span>preparing our data and models</h4>
<p>Now we need to prepare our data and <span id="dx1-118001"></span>models to run our experiments. Firstly, we’ll set up a dictionary that we can iterate over to access data from different datasets:</p>
<pre id="fancyvrb101" class="fancyvrb"><span id="x1-118009r1"></span> 
<code><span>datasets</span><span> </span><span id="textcolor2639"><span>=</span></span><span> {</span> <span id="x1-118011r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2640"><span>"california_housing"</span></span><span>:</span><span> fetch_california_housing(return_X_y</span><span id="textcolor2641"><span>=</span></span><span id="textcolor2642"><span>True</span></span><span>,</span><span> as_frame</span><span id="textcolor2643"><span>=</span></span><span id="textcolor2644"><span>True</span></span><span>),</span> <span id="x1-118013r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2645"><span>"diabetes"</span></span><span>:</span><span> load_diabetes(return_X_y</span><span id="textcolor2646"><span>=</span></span><span id="textcolor2647"><span>True</span></span><span>,</span><span> as_frame</span><span id="textcolor2648"><span>=</span></span><span id="textcolor2649"><span>True</span></span><span>),</span> <span id="x1-118015r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2650"><span>"wine_quality"</span></span><span>:</span><span> load_wine_quality(),</span> <span id="x1-118017r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2651"><span>"concrete"</span></span><span>:</span><span> load_concrete(),</span> <span id="x1-118019r6"></span> </code>
<code><span>}</span></code></pre>
<p>Next, we’ll create another dictionary to allow us to iterate over our different BDL models:</p>
<pre id="fancyvrb102" class="fancyvrb"><span id="x1-118027r1"></span> 
<code><span>models</span><span> </span><span id="textcolor2652"><span>=</span></span><span> {</span> <span id="x1-118029r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2653"><span>"BBB"</span></span><span>:</span><span> BBBRegressor,</span> <span id="x1-118031r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2654"><span>"PBP"</span></span><span>:</span><span> PBP,</span> <span id="x1-118033r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2655"><span>"MCDropout"</span></span><span>:</span><span> MCDropout,</span> <span id="x1-118035r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2656"><span>"Ensemble"</span></span><span>:</span><span> Ensemble,</span> <span id="x1-118037r6"></span> </code>
<code><span>}</span></code></pre>
<p>Finally, we’ll create a dictionary to hold our results:</p>
<pre id="fancyvrb103" class="fancyvrb"><span id="x1-118045r1"></span> 
<code><span>results</span><span> </span><span id="textcolor2657"><span>=</span></span><span> {</span> <span id="x1-118047r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2658"><span>"LL"</span></span><span>:</span><span> [],</span> <span id="x1-118049r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2659"><span>"MSE"</span></span><span>:</span><span> [],</span> <span id="x1-118051r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2660"><span>"Method"</span></span><span>:</span><span> [],</span> <span id="x1-118053r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2661"><span>"Dataset"</span></span><span>:</span><span> [],</span> <span id="x1-118055r6"></span> </code>
<code><span>}</span></code></pre>
<p>Here, we see that we’ll be recording two results: the log-likelihood, and the mean-squared error. We’re using these metrics as we’re looking at regression problems, but for classification problems you may opt to use F-score or accuracy in place of the mean-squared error, and expected calibration error in place of (or as well as) log-likelihood. We’ll also be storing the <span id="dx1-118056"></span>model type in the <code>Method</code> field, and the dataset in the <code>Dataset</code> field.</p>
</section>
<section id="running-our-experiments" class="level4 likesubsubsectionHead" data-number="12.2.1.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.2.1.3"><span id="x1-1190001"></span>Running our experiments</h4>
<p>Now we’re ready to run our experiments. However, we’re <span id="dx1-119001"></span>not only interested in the model performance, but also the computational considerations of our various models. As such, we’ll see calls to <code>tf.profiler</code> in the following code. First, however, we’ll set a few parameters:</p>
<pre id="fancyvrb104" class="fancyvrb"><span id="x1-119008r1"></span> 
<code><span id="textcolor2663"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Parameters</span></span> <span id="x1-119010r2"></span> </code>
<code><span>epochs</span><span> </span><span id="textcolor2664"><span>=</span></span><span> </span><span id="textcolor2665"><span>10</span></span> <span id="x1-119012r3"></span> </code>
<code><span>batch_size</span><span> </span><span id="textcolor2666"><span>=</span></span><span> </span><span id="textcolor2667"><span>16</span></span> <span id="x1-119014r4"></span> </code>
<code><span>logdir_base</span><span> </span><span id="textcolor2668"><span>=</span></span><span> </span><span id="textcolor2669"><span>"profiling"</span></span></code></pre>
<p>Here, we’re setting the number of epochs each model will train for, as well as the batch size each model will use. We’re also setting <code>logdir_base</code>, the location that all of our profiling logs will be written to.</p>
<p>Now we’re ready to drop in our experiment code. We’ll start by iterating over the datasets:</p>
<pre id="fancyvrb105" class="fancyvrb"><span id="x1-119021r1"></span> 
<code><span id="textcolor2670"><span>for</span></span><span> dataset_key</span><span> </span><span id="textcolor2671"><span>in</span></span><span> datasets</span><span id="textcolor2672"><span>.</span></span><span>keys():</span> <span id="x1-119023r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> X,</span><span> y</span><span> </span><span id="textcolor2673"><span>=</span></span><span> datasets[dataset_key]</span> <span id="x1-119025r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> X_train,</span><span> X_test,</span><span> y_train,</span><span> y_test</span><span> </span><span id="textcolor2674"><span>=</span></span><span> train_test_split(X,</span><span> y,</span><span> test_size</span><span id="textcolor2675"><span>=</span></span><span id="textcolor2676"><span>0.33</span></span><span>)</span> <span id="x1-119027r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2677"><span>...</span></span></code></pre>
<p>Here, we see that for each dataset we’re splitting the data, using <img src="../media/file149.jpg" class="frac" data-align="middle" alt="2 3"/> of the data for training and <img src="../media/file150.jpg" class="frac" data-align="middle" alt="1 3"/> for testing.</p>
<p>Next, we iterate over the models:</p>
<pre id="fancyvrb106" class="fancyvrb"><span id="x1-119035r1"></span> 
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2678"><span>...</span></span> <span id="x1-119037r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2679"><span>for</span></span><span> model_key</span><span> </span><span id="textcolor2680"><span>in</span></span><span> models</span><span id="textcolor2681"><span>.</span></span><span>keys():</span> <span id="x1-119039r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> logdir</span><span> </span><span id="textcolor2682"><span>=</span></span><span> os</span><span id="textcolor2683"><span>.</span></span><span>path</span><span id="textcolor2684"><span>.</span></span><span>join(logdir_base,</span><span> model_key</span><span> </span><span id="textcolor2685"><span>+</span></span><span> </span><span id="textcolor2686"><span>"_train"</span></span><span>)</span> <span id="x1-119041r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> os</span><span id="textcolor2687"><span>.</span></span><span>makedirs(logdir,</span><span> exist_ok</span><span id="textcolor2688"><span>=</span></span><span id="textcolor2689"><span>True</span></span><span>)</span> <span id="x1-119043r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2690"><span>.</span></span><span>profiler</span><span id="textcolor2691"><span>.</span></span><span>experimental</span><span id="textcolor2692"><span>.</span></span><span>start(logdir)</span> <span id="x1-119045r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2693"><span>...</span></span></code></pre>
<p>For each model, we instantiate a new log directory to log the training information. We then instantiate the model and run <code>model.fit()</code>:</p>
<pre id="fancyvrb107" class="fancyvrb"><span id="x1-119051r1"></span> 
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2695"><span>...</span></span> <span id="x1-119053r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> model</span><span> </span><span id="textcolor2696"><span>=</span></span><span> models[model_key]()</span> <span id="x1-119055r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> model</span><span id="textcolor2697"><span>.</span></span><span>fit(X_train,</span><span> y_train,</span><span> batch_size</span><span id="textcolor2698"><span>=</span></span><span>batch_size,</span><span> n_epochs</span><span id="textcolor2699"><span>=</span></span><span>epochs)</span></code></pre>
<p>Once the model fits, we stop the profiler and <span id="dx1-119056"></span>create a new directory to log the prediction information, after which we start the profiler again:</p>
<pre id="fancyvrb108" class="fancyvrb"><span id="x1-119064r1"></span> 
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2700"><span>...</span></span> <span id="x1-119066r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2701"><span>.</span></span><span>profiler</span><span id="textcolor2702"><span>.</span></span><span>experimental</span><span id="textcolor2703"><span>.</span></span><span>stop()</span> <span id="x1-119068r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> logdir</span><span> </span><span id="textcolor2704"><span>=</span></span><span> os</span><span id="textcolor2705"><span>.</span></span><span>path</span><span id="textcolor2706"><span>.</span></span><span>join(logdir_base,</span><span> model_key</span><span> </span><span id="textcolor2707"><span>+</span></span><span> </span><span id="textcolor2708"><span>"_predict"</span></span><span>)</span> <span id="x1-119070r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> os</span><span id="textcolor2709"><span>.</span></span><span>makedirs(logdir,</span><span> exist_ok</span><span id="textcolor2710"><span>=</span></span><span id="textcolor2711"><span>True</span></span><span>)</span> <span id="x1-119072r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2712"><span>.</span></span><span>profiler</span><span id="textcolor2713"><span>.</span></span><span>experimental</span><span id="textcolor2714"><span>.</span></span><span>start(logdir)</span> <span id="x1-119074r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2715"><span>...</span></span></code></pre>
<p>With the profiler running, we run predict, after which we again stop the profiler. With our predictions in hand, we can compute our mean squared error and log-likelihood, and store these to our <code>results</code> dictionary. Finally, we run <code>tf.keras.backend.clear_session()</code> to clear our TensorFlow graph after each experiment within our <code>model</code> loop:</p>
<pre id="fancyvrb109" class="fancyvrb"><span id="x1-119095r1"></span> 
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2719"><span>...</span></span> <span id="x1-119097r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_divd,</span><span> y_var</span><span> </span><span id="textcolor2720"><span>=</span></span><span> model</span><span id="textcolor2721"><span>.</span></span><span>predict(X_test)</span> <span id="x1-119099r3"></span> </code>
<code><span id="x1-119101r4"></span></code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2722"><span>.</span></span><span>profiler</span><span id="textcolor2723"><span>.</span></span><span>experimental</span><span id="textcolor2724"><span>.</span></span><span>stop()</span> <span id="x1-119103r5"></span> </code>
<code><span id="x1-119105r6"></span></code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_divd</span><span> </span><span id="textcolor2725"><span>=</span></span><span> y_divd</span><span id="textcolor2726"><span>.</span></span><span>reshape(</span><span id="textcolor2727"><span>-</span></span><span id="textcolor2728"><span>1</span></span><span>)</span> <span id="x1-119107r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_var</span><span> </span><span id="textcolor2729"><span>=</span></span><span> y_var</span><span id="textcolor2730"><span>.</span></span><span>reshape(</span><span id="textcolor2731"><span>-</span></span><span id="textcolor2732"><span>1</span></span><span>)</span> <span id="x1-119109r8"></span> </code>
<code><span id="x1-119111r9"></span></code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> mse</span><span> </span><span id="textcolor2733"><span>=</span></span><span> mean_squared_error(y_test,</span><span> y_divd)</span> <span id="x1-119113r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ll</span><span> </span><span id="textcolor2734"><span>=</span></span><span> likelihood(y_test,</span><span> y_divd,</span><span> y_var)</span> <span id="x1-119115r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> results[</span><span id="textcolor2735"><span>"MSE"</span></span><span>]</span><span id="textcolor2736"><span>.</span></span><span>append(mse)</span> <span id="x1-119117r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> results[</span><span id="textcolor2737"><span>"LL"</span></span><span>]</span><span id="textcolor2738"><span>.</span></span><span>append(ll)</span> <span id="x1-119119r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> results[</span><span id="textcolor2739"><span>"Method"</span></span><span>]</span><span id="textcolor2740"><span>.</span></span><span>append(model_key)</span> <span id="x1-119121r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> results[</span><span id="textcolor2741"><span>"Dataset"</span></span><span>]</span><span id="textcolor2742"><span>.</span></span><span>append(dataset_key)</span> <span id="x1-119123r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2743"><span>.</span></span><span>keras</span><span id="textcolor2744"><span>.</span></span><span>backend</span><span id="textcolor2745"><span>.</span></span><span>clear_session()</span> <span id="x1-119125r16"></span> </code>
<code><span id="textcolor2746"><span>...</span></span></code></pre>
<p>Once we’ve got results for all <span id="dx1-119126"></span>models and all datasets, we convert our results dictionary into a pandas DataFrame:</p>
<pre id="fancyvrb110" class="fancyvrb"><span id="x1-119130r1"></span> 
<code><span id="textcolor2747"><span>...</span></span> <span id="x1-119132r2"></span> </code>
<code><span>results</span><span> </span><span id="textcolor2748"><span>=</span></span><span> pd</span><span id="textcolor2749"><span>.</span></span><span>DataFrame(results)</span></code></pre>
<p>Now we’re ready to analyze our data! <span id="x1-119133r182"></span></p>
</section>
</section>
<section id="analyzing-model-performance" class="level3 subsectionHead" data-number="12.2.2">
<h3 class="subsectionHead" data-number="12.2.2" id="sigil_toc_id_80"><span class="titlemark">7.2.2 </span> <span id="x1-1200002"></span>Analyzing model performance</h3>
<p>With the data obtained <span id="dx1-120001"></span>from our experiments, we can plot this to see which models performed best on which datasets. To do so, we’ll use the following plotting code:</p>
<pre id="fancyvrb111" class="fancyvrb"><span id="x1-120026r1"></span> 
<code><span>results[</span><span id="textcolor2750"><span>'NLL'</span></span><span>]</span><span> </span><span id="textcolor2751"><span>=</span></span><span> </span><span id="textcolor2752"><span>-</span></span><span id="textcolor2753"><span>1</span></span><span id="textcolor2754"><span>*</span></span><span>results[</span><span id="textcolor2755"><span>'LL'</span></span><span>]</span> <span id="x1-120028r2"></span> </code>
<code><span id="x1-120030r3"></span></code>
<code><span>i</span><span> </span><span id="textcolor2756"><span>=</span></span><span> </span><span id="textcolor2757"><span>1</span></span> <span id="x1-120032r4"></span> </code>
<code><span id="textcolor2758"><span>for</span></span><span> dataset</span><span> </span><span id="textcolor2759"><span>in</span></span><span> datasets</span><span id="textcolor2760"><span>.</span></span><span>keys():</span> <span id="x1-120034r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2761"><span>for</span></span><span> metric</span><span> </span><span id="textcolor2762"><span>in</span></span><span> [</span><span id="textcolor2763"><span>"NLL"</span></span><span>,</span><span> </span><span id="textcolor2764"><span>"MSE"</span></span><span>]:</span> <span id="x1-120036r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> df_plot</span><span> </span><span id="textcolor2765"><span>=</span></span><span> results[(results[</span><span id="textcolor2766"><span>'Dataset'</span></span><span>]</span><span id="textcolor2767"><span>==</span></span><span>dataset)]</span> <span id="x1-120038r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> df_plot</span><span> </span><span id="textcolor2768"><span>=</span></span><span> groupedvalues</span><span> </span><span id="textcolor2769"><span>=</span></span><span> df_plot</span><span id="textcolor2770"><span>.</span></span><span>groupby(</span><span id="textcolor2771"><span>'Method'</span></span><span>)</span><span id="textcolor2772"><span>.</span></span><span>sum()</span><span id="textcolor2773"><span>.</span></span><span>reset_index()</span> <span id="x1-120040r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2774"><span>.</span></span><span>subplot(</span><span id="textcolor2775"><span>3</span></span><span>,</span><span id="textcolor2776"><span>2</span></span><span>,i)</span> <span id="x1-120042r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ax</span><span> </span><span id="textcolor2777"><span>=</span></span><span> sns</span><span id="textcolor2778"><span>.</span></span><span>barplot(data</span><span id="textcolor2779"><span>=</span></span><span>df_plot,</span><span> x</span><span id="textcolor2780"><span>=</span></span><span id="textcolor2781"><span>"Method"</span></span><span>,</span><span> y</span><span id="textcolor2782"><span>=</span></span><span>metric)</span> <span id="x1-120044r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2783"><span>for</span></span><span> index,</span><span> row</span><span> </span><span id="textcolor2784"><span>in</span></span><span> groupedvalues</span><span id="textcolor2785"><span>.</span></span><span>iterrows():</span> <span id="x1-120046r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2786"><span>if</span></span><span> metric</span><span> </span><span id="textcolor2787"><span>==</span></span><span> </span><span id="textcolor2788"><span>"NLL"</span></span><span>:</span> <span id="x1-120048r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ax</span><span id="textcolor2789"><span>.</span></span><span>text(row</span><span id="textcolor2790"><span>.</span></span><span>name,</span><span> </span><span id="textcolor2791"><span>0</span></span><span>,</span><span> </span><span id="textcolor2792"><span>round</span></span><span>(row</span><span id="textcolor2793"><span>.</span></span><span>NLL,</span><span> </span><span id="textcolor2794"><span>2</span></span><span>),</span> <span id="x1-120050r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> color</span><span id="textcolor2795"><span>=</span></span><span id="textcolor2796"><span>'white'</span></span><span>,</span><span> ha</span><span id="textcolor2797"><span>=</span></span><span id="textcolor2798"><span>'center'</span></span><span>)</span> <span id="x1-120052r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2799"><span>else</span></span><span>:</span> <span id="x1-120054r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ax</span><span id="textcolor2800"><span>.</span></span><span>text(row</span><span id="textcolor2801"><span>.</span></span><span>name,</span><span> </span><span id="textcolor2802"><span>0</span></span><span>,</span><span> </span><span id="textcolor2803"><span>round</span></span><span>(row</span><span id="textcolor2804"><span>.</span></span><span>MSE,</span><span> </span><span id="textcolor2805"><span>2</span></span><span>),</span> <span id="x1-120056r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> color</span><span id="textcolor2806"><span>=</span></span><span id="textcolor2807"><span>'white'</span></span><span>,</span><span> ha</span><span id="textcolor2808"><span>=</span></span><span id="textcolor2809"><span>'center'</span></span><span>)</span> <span id="x1-120058r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2810"><span>.</span></span><span>title(dataset)</span> <span id="x1-120060r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2811"><span>if</span></span><span> metric</span><span> </span><span id="textcolor2812"><span>==</span></span><span> </span><span id="textcolor2813"><span>"NLL"</span></span><span> </span><span id="textcolor2814"><span>and</span></span><span> dataset</span><span> </span><span id="textcolor2815"><span>==</span></span><span> </span><span id="textcolor2816"><span>"california_housing"</span></span><span>:</span> <span id="x1-120062r19"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2817"><span>.</span></span><span>ylim(</span><span id="textcolor2818"><span>0</span></span><span>,</span><span> </span><span id="textcolor2819"><span>100</span></span><span>)</span> <span id="x1-120064r20"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> i</span><span id="textcolor2820"><span>+=</span></span><span id="textcolor2821"><span>1</span></span> <span id="x1-120066r21"></span> </code>
<code><span>fig</span><span> </span><span id="textcolor2822"><span>=</span></span><span> plt</span><span id="textcolor2823"><span>.</span></span><span>gcf()</span> <span id="x1-120068r22"></span> </code>
<code><span>fig</span><span id="textcolor2824"><span>.</span></span><span>set_size_inches(</span><span id="textcolor2825"><span>10</span></span><span>,</span><span> </span><span id="textcolor2826"><span>8</span></span><span>)</span> <span id="x1-120070r23"></span> </code>
<code><span>plt</span><span id="textcolor2827"><span>.</span></span><span>tight_layout()</span></code></pre>
<p>Note that initially we add a <code>'NLL'</code> field to our pandas DataFrame. This gives us the negative log-likelihood. This makes things a little less confusing when looking at the plots, as lower is better for both mean squared error and negative log-likelihood.</p>
<p>The code iterates over the datasets and metrics, and <span id="dx1-120072"></span>creates some nice bar plots with the help of the Seaborn plotting library. In addition to this, we use calls to <code>ax.text()</code> to overlay the metric values on the bar plots, allowing us to see the values clearly.</p>
<p>Also notice how, for the California Housing data, we’re capping our <em>y</em> values at 100 for our negative log-likelihood. This is because, for this dataset, our negative log-likelihood value is <em>incredibly</em> high - making it difficult to view this in context with the other values. This is another reason why we’re overlaying the metric values, to allow us to compare them easily as one of the values exceeds the limit of the plot.</p>
<div class="IMG---Figure">
<img src="../media/file151.png" alt="PIC"/> <span id="x1-120074r1"></span> <span id="x1-120075"></span></div>
<p class="IMG---Caption">Figure 7.1: Bar plot of results from LL and MSE experiments 
</p>
<p>It’s worth noting that, for fair comparison, we’ve used the equivalent architecture across all models, used the same batch size, and trained for the <span id="dx1-120076"></span>same number of epochs.</p>
<p>As we see here, there’s no single best method: each model performs differently depending on the data, and a model with low mean squared error isn’t guaranteed to also have a low negative log-likelihood score. Generally speaking, MC dropout exhibits the worst mean squared error scores; however, it also produces the best negative log-likelihood observed during our experiments for the Wine Quality dataset, for which it achieves a negative log-likelihood of 2.9. This is due to the fact that, while it generally performs worse in terms of error, its uncertainties are very high. As such, because it is more uncertain in regions for which it is wrong, it produces a more favorable negative log-likelihood score. We can see this if we plot the errors against the uncertainties:</p>
<div class="IMG---Figure">
<img src="../media/file152.png" alt="PIC"/> <span id="x1-120077r2"></span> <span id="x1-120078"></span></div>
<p class="IMG---Caption">Figure 7.2: Scatter plot of errors vs uncertainty estimates 
</p>
<p>In <em>Figure</em> <a href="#x1-120077r2"><em>7.2</em></a>, we see the BBB, PBP, and ensemble results in the plot on the left, while MC dropout’s results are in the plot on the right. The reason for this is that MC dropout’s uncertainty estimates are two orders of magnitude higher than the uncertainty estimates produced by the other methods, thus they can’t be clearly represented on the same axes. These very high-magnitude uncertainties are also the reason behind its comparatively low negative log-likelihood score. This is quite a surprising example for MC dropout, as it is typically <em>over-confident</em>, whereas in this case, it’s clearly <em>under-confident</em>.</p>
<p>While MC dropout’s under-confidence may lead to better likelihood scores, these metrics need to be considered in context; we typically want a good trade-off between likelihood and error. As such, PBP is probably the best choice in the case of the Wine Quality data as it has the lowest error, but it also has a reasonable likelihood; its negative log-likelihood is not so low as to be suspicious, but also low enough to know that the uncertainty <span id="dx1-120079"></span>estimates will be reasonably consistent and principled.</p>
<p>In the case of the other datasets, the choices are a little more straightforward: BBB is the clear winner for California Housing, and PBP again proves to be the sensible choice on balance in the case of Concrete Comdivssive Strength. This is all with the important caveat that none of these networks have been specifically optimized for these datasets: this is merely an illustrative example.</p>
<p>Crucially, it’s going to come down to the specific application and how much robust uncertainty estimates matter. For example, in a safety-critical scenario, you’ll want to go with a method with the most robust uncertainty estimates, and so you may favor under-confidence over a lower error because you want to make sure that you’re only going with the model when you’re very confident in its outcome. In these cases, you may well go for an under-confident but high likelihood (low negative likelihood) method such as MC dropout on the Wine Quality dataset.</p>
<p>In other cases, perhaps uncertainty doesn’t matter at all, in which case you may just go for a standard neural network. But in most mission-critical or safety-critical applications, you’re going to want to strike a balance and take advantage of the additional information provided by model uncertainty estimates while still achieving a low error score. However, practically, these performance metrics aren’t the only thing we need to consider when developing machine learning systems. We also care about the practical implications. In the next section, we’ll see how the computational requirements of these models stack up against each other. <span id="x1-120080r186"></span></p>
</section>
<section id="computational-considerations-of-bayesian-deep-learning-models" class="level3 subsectionHead" data-number="12.2.3">
<h3 class="subsectionHead" data-number="12.2.3" id="sigil_toc_id_81"><span class="titlemark">7.2.3 </span> <span id="x1-1210003"></span>Computational considerations of Bayesian deep learning models</h3>
<p>For every real-world application of machine learning, there are considerations beyond performance: we also need to understand the practical <span id="dx1-121001"></span>limitations of the compute infrastructure. They are usually governed by a few things, but existing infrastructure and cost tend to come up time and time again.</p>
<p>Existing infrastructure is often important because unless it’s a totally new project, it’s a case of working out how a machine learning model can be integrated, and this means either finding or requesting additional computational resources on a hardware or software stack. It will come as no surprise that cost is a significant factor: every project has a budget, and the expense that the machine learning component of the solution brings to the table needs to be balanced against the advantages that it provides. The budget will often dictate what machine learning solutions are feasible based on the cost of the computational resources required to train, deploy, and run them at inference time.</p>
<p>To get some insight into how the methods here compare in terms of their computational requirements, we’ll look at the output of the TensorFlow profiler that we included in our experiment code. To do this, we simply need to run TensorBoard from the command line, pointing to the logging directory for the particular model we’re interested in:</p>
<pre id="fancyvrb112" class="fancyvrb"><span id="x1-121004r1"></span> 
<code><span>tensorboard</span><span> --logdir</span><span> profiling/BBB_train/</span></code></pre>
<p>This will start a TensorBoard instance (typically at <span class="No-Break">http://localhost:6006/</span>). Copy the URL into your browser, and you’ll be greeted with the TensorBoard GUI. TensorBoard provides you with a suite of tools for understanding the performance of your TensorFlow models, from the execution time right down to the memory allocation of different processes. You can scroll through the available tools via the <strong>Tools</strong> selection box in the top left corner of the screen:</p>
<div class="IMG---Figure">
<img src="../media/tensorboard_tool_select.JPG" class="graphics" alt="PIC"/> <span id="x1-121005r3"></span> <span id="x1-121006"></span></div>
<p class="IMG---Caption">Figure 7.3: Tool selection box in the TensorBoard GUI 
</p>
<p>To get a very detailed picture of what’s going on, take a look at the Trace Viewer:</p>
<div class="IMG---Figure">
<img src="../media/file154.png" alt="PIC"/> <span id="x1-121007r4"></span> <span id="x1-121008"></span></div>
<p class="IMG---Caption">Figure 7.4: Trace Viewer in the TensorBoard GUI 
</p>
<p>Here, we get an overall picture of the time taken to run our model’s functions, as well as a detailed picture of which processes are running under the hood, and <span id="dx1-121009"></span>how long each of these processes take to run. We can even dig deeper by double-clicking on a block and looking at its statistics. For example, we can double-click on the <strong>train</strong> block:</p>
<div class="IMG---Figure">
<img src="../media/file155.png" alt="PIC"/> <span id="x1-121010r5"></span> <span id="x1-121011"></span></div>
<p class="IMG---Caption">Figure 7.5: Trace Viewer from the TensorBoard GUI, highlighting the train block 
</p>
<p>This brings up some information at the bottom of our screen. This allows us to closely examine the run time of this process. If we click on <strong>Duration</strong>, we get a detailed breakdown of the run duration statistics for the process:</p>
<div class="IMG---Figure">
<img src="../media/tensorboard_profiling_trace_train_stats.JPG" class="graphics" alt="PIC"/> <span id="x1-121012r6"></span> <span id="x1-121013"></span></div>
<p class="IMG---Caption">Figure 7.6: Examining the statistics of a block in the TensorBoard Trace Viewer 
</p>
<p>Here, we see that the process was run 10 times (once per epoch) and that the average duration is 144,527,053 ns (nanoseconds). Let’s use our profiler results for the Concrete Comdivssion Strength dataset and collect the runtime and memory allocation information using TensorBoard. If we do this for each of our models’ training runs, we obtain the following information:</p>
<div class="IMG---Figure">
<div class="tabular">
<table id="TBL-1" class="tabular">
<tbody>
<tr id="TBL-1-1-" class="odd" style="vertical-align:baseline;">
<td id="TBL-1-1-1" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-1-1-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-1-1-3" class="td11" style="text-align: left; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:left;">
<strong>Profiling data for model training</strong>
</div>
</td>
<td id="TBL-1-1-4" class="td11" style="text-align: left; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:left;">

</div>
</td>
</tr>
<tr class="even hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-1-2-" class="odd" style="vertical-align:baseline;">
<td id="TBL-1-2-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>Model</strong></td>
<td id="TBL-1-2-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-1-2-3" class="td11" style="text-align: center; white-space: nowrap;"><strong>Peak memory usage (MiB)</strong></td>
<td id="TBL-1-2-4" class="td11" style="text-align: center; white-space: nowrap;"><strong>Duration (ms)</strong></td>
</tr>
<tr class="even hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-1-3-" class="odd" style="vertical-align:baseline;">
<td id="TBL-1-3-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>BBB</strong></td>
<td id="TBL-1-3-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-1-3-3" class="td11" style="text-align: center; white-space: nowrap;">0.09</td>
<td id="TBL-1-3-4" class="td11" style="text-align: center; white-space: nowrap;">4270</td>
</tr>
<tr id="TBL-1-4-" class="even" style="vertical-align:baseline;">
<td id="TBL-1-4-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>PBP</strong></td>
<td id="TBL-1-4-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-1-4-3" class="td11" style="text-align: center; white-space: nowrap;">0.253</td>
<td id="TBL-1-4-4" class="td11" style="text-align: center; white-space: nowrap;">10754</td>
</tr>
<tr id="TBL-1-5-" class="odd" style="vertical-align:baseline;">
<td id="TBL-1-5-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>MCDropout</strong></td>
<td id="TBL-1-5-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-1-5-3" class="td11" style="text-align: center; white-space: nowrap;">0.126</td>
<td id="TBL-1-5-4" class="td11" style="text-align: center; white-space: nowrap;">2198</td>
</tr>
<tr id="TBL-1-6-" class="even" style="vertical-align:baseline;">
<td id="TBL-1-6-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>Ensemble</strong></td>
<td id="TBL-1-6-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-1-6-3" class="td11" style="text-align: center; white-space: nowrap;">0.215</td>
<td id="TBL-1-6-4" class="td11" style="text-align: center; white-space: nowrap;">20630</td>
</tr>
<tr class="odd hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-1-7-" class="even" style="vertical-align:baseline;">
<td id="TBL-1-7-1" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<span id="x1-121014r7"></span> <span id="x1-121015"></span></div>
<p class="IMG---Caption">Figure 7.7: Table of profiling data for model training for Concrete Comdivssive Strength dataset 
</p>
<p>Here, we see that MC dropout is the fastest model to train for this dataset, taking half as long as BBB. We also see that the ensemble takes by far the <span id="dx1-121016"></span>longest to train, nearly 10 times as long as MC dropout, despite the fact that the ensemble comprises only 5 models. In terms of memory usage, we see that the ensemble again performs poorly, but that PBP is the most memory hungry of the models, while BBB has the lowest peak memory usage.</p>
<p>But it’s not just training that counts. We also need to factor in the computational cost of inference. Looking at our profiling data for our models’ predict functions, we see the following:</p>
<div class="IMG---Figure">
<div class="tabular">
<table id="TBL-2" class="tabular">
<tbody>
<tr id="TBL-2-1-" class="odd" style="vertical-align:baseline;">
<td id="TBL-2-1-1" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-2-1-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-2-1-3" class="td11" style="text-align: left; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:left;">
<strong>Profiling data for model predictions</strong>
</div>
</td>
<td id="TBL-2-1-4" class="td11" style="text-align: left; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:left;">

</div>
</td>
</tr>
<tr class="even hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-2-2-" class="odd" style="vertical-align:baseline;">
<td id="TBL-2-2-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>Model</strong></td>
<td id="TBL-2-2-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-2-2-3" class="td11" style="text-align: center; white-space: nowrap;"><strong>Peak memory usage (MiB)</strong></td>
<td id="TBL-2-2-4" class="td11" style="text-align: center; white-space: nowrap;"><strong>Duration (ms)</strong></td>
</tr>
<tr class="even hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-2-3-" class="odd" style="vertical-align:baseline;">
<td id="TBL-2-3-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>BBB</strong></td>
<td id="TBL-2-3-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-2-3-3" class="td11" style="text-align: center; white-space: nowrap;">0.116</td>
<td id="TBL-2-3-4" class="td11" style="text-align: center; white-space: nowrap;">849</td>
</tr>
<tr id="TBL-2-4-" class="even" style="vertical-align:baseline;">
<td id="TBL-2-4-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>PBP</strong></td>
<td id="TBL-2-4-2" class="td11" style="text-align: center; white-space: nowrap;"><div class="multicolumn" style="white-space:nowrap; text-align:center;">

</div>
</td>
<td id="TBL-2-4-3" class="td11" style="text-align: center; white-space: nowrap;">1.27</td>
<td id="TBL-2-4-4" class="td11" style="text-align: center; white-space: nowrap;">176</td>
</tr>
<tr id="TBL-2-5-" class="odd" style="vertical-align:baseline;">
<td id="TBL-2-5-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>MCDropout</strong></td>
<td id="TBL-2-5-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-2-5-3" class="td11" style="text-align: center; white-space: nowrap;">0.548</td>
<td id="TBL-2-5-4" class="td11" style="text-align: center; white-space: nowrap;">23</td>
</tr>
<tr id="TBL-2-6-" class="even" style="vertical-align:baseline;">
<td id="TBL-2-6-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>Ensemble</strong></td>
<td id="TBL-2-6-2" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td id="TBL-2-6-3" class="td11" style="text-align: center; white-space: nowrap;">0.389</td>
<td id="TBL-2-6-4" class="td11" style="text-align: center; white-space: nowrap;">17</td>
</tr>
<tr class="odd hline">
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
<td style="text-align: left;"><hr/>
</td>
</tr>
<tr id="TBL-2-7-" class="even" style="vertical-align:baseline;">
<td id="TBL-2-7-1" class="td11" style="text-align: left; white-space: nowrap;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<span id="x1-121017r8"></span> <span id="x1-121018"></span></div>
<p class="IMG---Caption">Figure 7.8: Table of profiling data for model prediction for Concrete Comdivssive Strength dataset 
</p>
<p>Interestingly, here we see that the ensemble is in the lead when it comes to model inference speed, and it also comes in second when we look at peak memory usage for predictions. In contrast, PBP has by far the highest peak memory usage, while BBB takes the longest to run inference.</p>
<p>There are various factors contributing to the results we see here. Firstly, it’s important to note that none of these models are properly optimized for computational performance. For example, we could significantly cut down the training duration for our ensemble by training all the ensemble members in parallel, which we don’t do here. Similarly, because PBP used a lot of high-level code in its implementation (unlike the other methods, which are all built on nicely optimized TensorFlow or TensorFlow Probability code), its performance suffers as a result.</p>
<p>Most crucially, we need to ensure that, when <span id="dx1-121019"></span>selecting the right model for the job, we consider the computational implications as well as the typical performance metrics. So, with all that in mind, how do we choose the right model? <span id="x1-121020r189"></span></p>
</section>
<section id="choosing-the-right-model" class="level3 subsectionHead" data-number="12.2.4">
<h3 class="subsectionHead" data-number="12.2.4" id="sigil_toc_id_82"><span class="titlemark">7.2.4 </span> <span id="x1-1220004"></span>Choosing the right model</h3>
<p>With our performance metrics and profiling <span id="dx1-122001"></span>information in hand, we have all the data we need to choose the right model for the task. But model selection isn’t easy; we see here that all our models have advantages and disadvantages.</p>
<p>If we start with performance metrics, then we see that BBB has the lowest mean squared error, but it also has a very high negative log-likelihood. So, the best choice on the basis of performance metrics alone is PBP: it has the lowest negative log-likelihood score, and its mean squared error is nowhere near as poor as MC dropout’s error, making PBP the best choice, on balance.</p>
<p>However, if we look at the computational implications in <em>Figure</em> <a href="#x1-121014r7"><em>7.7</em></a> and <a href="#x1-121017r8"><em>7.8</em></a>, we see that PBP is one of the worst choices both in terms of memory usage and execution time. The best choice here, on balance, would be MC dropout: its prediction time is only a little slower than the prediction time of the ensemble, and it has the shortest training duration.</p>
<p>At the end of the day, it’s entirely down to the application: perhaps inference doesn’t need to be run in real time, so we can go with our PBP implementation. Or perhaps inference time and low error are our key considerations, in which case the ensemble is a good choice. As we see here, metrics and computational overheads need to be considered in context, and, as with any class of machine learning models, there’s no single best choice for all applications. It’s all down to choosing the right tool for the job.</p>
<p>In this section, we’ve introduced tools for comprehensively understanding model performance and demonstrated how important it is to consider a range of factors when selecting a model. Fundamentally, performance analysis and profiling are as important for helping us to make the right practical choices as they are for helping us to uncover opportunities for further improvements. We may not have time for further optimizing our code and so may need to be pragmatic and go with the best computationally optimized method we have to hand. Alternatively, the business case may dictate that we need the model with the best performance, which may justify investing time to optimize our code and reduce the computational overheads of a given method. In the next section, we’ll take a look at another important practical consideration of working with BDL methods as we learn how we can use these methods to better understand sources of uncertainty. <span id="x1-122002r181"></span></p>
</section>
</section>
<section id="bdl-and-sources-of-uncertainty" class="level2 sectionHead" data-number="12.3">
<h2 class="sectionHead" data-number="12.3" id="sigil_toc_id_83"><span class="titlemark">7.3 </span> <span id="x1-1230003"></span>BDL and sources of uncertainty</h2>
<p>In this case study, we will look at how we can <span id="dx1-123001"></span>model aleatoric and epistemic uncertainty in a regression problem when we are trying to predict a continuous outcome variable. We will use a real-life dataset of diamonds that contains the physical attributes of more than 50,000 diamonds as well as their prices. In particular, we will look at the relationship between the weight of a diamond (measured as its <strong>carat</strong>) and the price paid for the diamond.</p>
<section id="step-1-setting-up-the-environment" class="level4 likesubsubsectionHead" data-number="12.3.0.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.3.0.1"><span id="x1-1240003"></span>Step 1: Setting up the environment</h4>
<p>To set up the environment, we import several packages. We import <code>tensorflow</code> and <code>tensorflow_probability</code> for building and training vanilla and probabilistic neural networks, <code>tensorflow_datasets</code> for importing the diamonds data set, <code>numpy</code> for performing calculations and operations on numerical arrays (such as calculating the mean), <code>pandas</code> for handling DataFrames, and <code>matplotlib</code> for plotting:</p>
<pre id="fancyvrb113" class="fancyvrb"><span id="x1-124014r1"></span> 
<code><span id="textcolor2830"><span>import</span></span><span> </span><span id="textcolor2831"><span>matplotlib.pyplot</span></span><span> </span><span id="textcolor2832"><span>as</span></span><span> </span><span id="textcolor2833"><span>plt</span></span> <span id="x1-124016r2"></span> </code>
<code><span id="textcolor2834"><span>import</span></span><span> </span><span id="textcolor2835"><span>numpy</span></span><span> </span><span id="textcolor2836"><span>as</span></span><span> </span><span id="textcolor2837"><span>np</span></span> <span id="x1-124018r3"></span> </code>
<code><span id="textcolor2838"><span>import</span></span><span> </span><span id="textcolor2839"><span>pandas</span></span><span> </span><span id="textcolor2840"><span>as</span></span><span> </span><span id="textcolor2841"><span>pd</span></span> <span id="x1-124020r4"></span> </code>
<code><span id="textcolor2842"><span>import</span></span><span> </span><span id="textcolor2843"><span>tensorflow</span></span><span> </span><span id="textcolor2844"><span>as</span></span><span> </span><span id="textcolor2845"><span>tf</span></span> <span id="x1-124022r5"></span> </code>
<code><span id="textcolor2846"><span>import</span></span><span> </span><span id="textcolor2847"><span>tensorflow_probability</span></span><span> </span><span id="textcolor2848"><span>as</span></span><span> </span><span id="textcolor2849"><span>tfp</span></span> <span id="x1-124024r6"></span> </code>
<code><span id="textcolor2850"><span>import</span></span><span> </span><span id="textcolor2851"><span>tensorflow_datasets</span></span><span> </span><span id="textcolor2852"><span>as</span></span><span> </span><span id="textcolor2853"><span>tfds</span></span></code></pre>
<p>First, we load the diamonds dataset using the <code>load</code> function provided by <code>tensorflow_datasets</code>. We load the dataset as a <code>pandas</code> DataFrame, which is convenient for preparing the data for training and inference.</p>
<pre id="fancyvrb114" class="fancyvrb"><span id="x1-124031r1"></span> 
<code><span>ds</span><span> </span><span id="textcolor2854"><span>=</span></span><span> tfds</span><span id="textcolor2855"><span>.</span></span><span>load(</span><span id="textcolor2856"><span>'diamonds'</span></span><span>,</span><span> split</span><span id="textcolor2857"><span>=</span></span><span id="textcolor2858"><span>'train'</span></span><span>)</span> <span id="x1-124033r2"></span> </code>
<code><span>df</span><span> </span><span id="textcolor2859"><span>=</span></span><span> tfds</span><span id="textcolor2860"><span>.</span></span><span>as_dataframe(ds)</span></code></pre>
<p>The dataset contains many different attributes of diamonds, but here we will focus on carat and price by selecting the respective columns from the DataFrame:</p>
<pre id="fancyvrb115" class="fancyvrb"><span id="x1-124036r1"></span> 
<code><span>df</span><span> </span><span id="textcolor2861"><span>=</span></span><span> df[[</span><span id="textcolor2862"><span>"features/carat"</span></span><span>,</span><span> </span><span id="textcolor2863"><span>"price"</span></span><span>]]</span></code></pre>
<p>We then divide the dataset into train and test splits. We use 80% of the data for training and 20% for testing:</p>
<pre id="fancyvrb116" class="fancyvrb"><span id="x1-124040r1"></span> 
<code><span>train_df</span><span> </span><span id="textcolor2864"><span>=</span></span><span> df</span><span id="textcolor2865"><span>.</span></span><span>sample(frac</span><span id="textcolor2866"><span>=</span></span><span id="textcolor2867"><span>0.8</span></span><span>,</span><span> random_state</span><span id="textcolor2868"><span>=</span></span><span id="textcolor2869"><span>0</span></span><span>)</span> <span id="x1-124042r2"></span> </code>
<code><span>test_df</span><span> </span><span id="textcolor2870"><span>=</span></span><span> df</span><span id="textcolor2871"><span>.</span></span><span>drop(train_df</span><span id="textcolor2872"><span>.</span></span><span>index)</span></code></pre>
<p>For further processing, we convert the train and test DataFrames to NumPy arrays:</p>
<pre id="fancyvrb117" class="fancyvrb"><span id="x1-124048r1"></span> 
<code><span>carat</span><span> </span><span id="textcolor2873"><span>=</span></span><span> np</span><span id="textcolor2874"><span>.</span></span><span>array(train_df[</span><span id="textcolor2875"><span>'features/carat'</span></span><span>])</span> <span id="x1-124050r2"></span> </code>
<code><span>price</span><span> </span><span id="textcolor2876"><span>=</span></span><span> np</span><span id="textcolor2877"><span>.</span></span><span>array(train_df[</span><span id="textcolor2878"><span>'price'</span></span><span>])</span> <span id="x1-124052r3"></span> </code>
<code><span>carat_test</span><span> </span><span id="textcolor2879"><span>=</span></span><span> np</span><span id="textcolor2880"><span>.</span></span><span>array(test_df[</span><span id="textcolor2881"><span>'features/carat'</span></span><span>])</span> <span id="x1-124054r4"></span> </code>
<code><span>price_test</span><span> </span><span id="textcolor2882"><span>=</span></span><span> np</span><span id="textcolor2883"><span>.</span></span><span>array(test_df[</span><span id="textcolor2884"><span>'price'</span></span><span>])</span></code></pre>
<p>We also save the number of training samples into a variable because we will need it later during model training:</p>
<pre id="fancyvrb118" class="fancyvrb"><span id="x1-124057r1"></span> 
<code><span>NUM_TRAIN_SAMPLES</span><span> </span><span id="textcolor2885"><span>=</span></span><span> carat</span><span id="textcolor2886"><span>.</span></span><span>shape[</span><span id="textcolor2887"><span>0</span></span><span>]</span></code></pre>
<p>Finally, we define a plotting function. This function will come in handy during the rest of the case study. It allows us to plot the data points as well as the fitted <span id="dx1-124058"></span>model predictions and their standard deviations:</p>
<pre id="fancyvrb119" class="fancyvrb"><span id="x1-124095r1"></span> 
<code><span id="textcolor2888"><span>def</span></span><span> </span><span id="textcolor2889"><span>plot_scatter</span></span><span>(x_data,</span><span> y_data,</span><span> x_hat</span><span id="textcolor2890"><span>=</span></span><span id="textcolor2891"><span>None</span></span><span>,</span><span> y_hats</span><span id="textcolor2892"><span>=</span></span><span id="textcolor2893"><span>None</span></span><span>,</span><span> plot_std</span><span id="textcolor2894"><span>=</span></span><span id="textcolor2895"><span>False</span></span><span>):</span> <span id="x1-124097r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2896"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Plot</span><span class="cmitt-10x-x-109"> the</span><span class="cmitt-10x-x-109"> data</span><span class="cmitt-10x-x-109"> as</span><span class="cmitt-10x-x-109"> scatter</span><span class="cmitt-10x-x-109"> points</span></span> <span id="x1-124099r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2897"><span>.</span></span><span>scatter(x_data,</span><span> y_data,</span><span> color</span><span id="textcolor2898"><span>=</span></span><span id="textcolor2899"><span>"k"</span></span><span>,</span><span> label</span><span id="textcolor2900"><span>=</span></span><span id="textcolor2901"><span>"Data"</span></span><span>)</span> <span id="x1-124101r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2902"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Plot</span><span class="cmitt-10x-x-109"> x</span><span class="cmitt-10x-x-109"> and</span><span class="cmitt-10x-x-109"> y</span><span class="cmitt-10x-x-109"> values</span><span class="cmitt-10x-x-109"> predicted</span><span class="cmitt-10x-x-109"> by</span><span class="cmitt-10x-x-109"> the</span><span class="cmitt-10x-x-109"> model,</span><span class="cmitt-10x-x-109"> if</span><span class="cmitt-10x-x-109"> provided</span></span> <span id="x1-124103r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2903"><span>if</span></span><span> x_hat</span><span> </span><span id="textcolor2904"><span>is</span></span><span> </span><span id="textcolor2905"><span>not</span></span><span> </span><span id="textcolor2906"><span>None</span></span><span> </span><span id="textcolor2907"><span>and</span></span><span> y_hats</span><span> </span><span id="textcolor2908"><span>is</span></span><span> </span><span id="textcolor2909"><span>not</span></span><span> </span><span id="textcolor2910"><span>None</span></span><span>:</span> <span id="x1-124105r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2911"><span>if</span></span><span> </span><span id="textcolor2912"><span>not</span></span><span> </span><span id="textcolor2913"><span>isinstance</span></span><span>(y_hats,</span><span> </span><span id="textcolor2914"><span>list</span></span><span>):</span> <span id="x1-124107r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_hats</span><span> </span><span id="textcolor2915"><span>=</span></span><span> [y_hats]</span> <span id="x1-124109r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2916"><span>for</span></span><span> ind,</span><span> y_hat</span><span> </span><span id="textcolor2917"><span>in</span></span><span> </span><span id="textcolor2918"><span>enumerate</span></span><span>(y_hats):</span> <span id="x1-124111r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2919"><span>.</span></span><span>plot(</span> <span id="x1-124113r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> x_hat,</span> <span id="x1-124115r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_hat</span><span id="textcolor2920"><span>.</span></span><span>mean(),</span> <span id="x1-124117r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> color</span><span id="textcolor2921"><span>=</span></span><span id="textcolor2922"><span>"#e41a1c"</span></span><span>,</span> <span id="x1-124119r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> label</span><span id="textcolor2923"><span>=</span></span><span id="textcolor2924"><span>"prediction"</span></span><span> </span><span id="textcolor2925"><span>if</span></span><span> ind</span><span> </span><span id="textcolor2926"><span>==</span></span><span> </span><span id="textcolor2927"><span>0</span></span><span> </span><span id="textcolor2928"><span>else</span></span><span> </span><span id="textcolor2929"><span>None</span></span><span>,</span> <span id="x1-124121r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-124123r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2930"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Plot</span><span class="cmitt-10x-x-109"> standard</span><span class="cmitt-10x-x-109"> deviation,</span><span class="cmitt-10x-x-109"> if</span><span class="cmitt-10x-x-109"> requested</span></span> <span id="x1-124125r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2931"><span>if</span></span><span> plot_std:</span> <span id="x1-124127r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor2932"><span>for</span></span><span> ind,</span><span> y_hat</span><span> </span><span id="textcolor2933"><span>in</span></span><span> </span><span id="textcolor2934"><span>enumerate</span></span><span>(y_hats):</span> <span id="x1-124129r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2935"><span>.</span></span><span>plot(</span> <span id="x1-124131r19"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> x_hat,</span> <span id="x1-124133r20"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_hat</span><span id="textcolor2936"><span>.</span></span><span>mean()</span><span> </span><span id="textcolor2937"><span>+</span></span><span> </span><span id="textcolor2938"><span>2</span></span><span> </span><span id="textcolor2939"><span>*</span></span><span> y_hat</span><span id="textcolor2940"><span>.</span></span><span>stddev(),</span> <span id="x1-124135r21"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> color</span><span id="textcolor2941"><span>=</span></span><span id="textcolor2942"><span>"#e41a1c"</span></span><span>,</span> <span id="x1-124137r22"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> linestyle</span><span id="textcolor2943"><span>=</span></span><span id="textcolor2944"><span>"dashed"</span></span><span>,</span> <span id="x1-124139r23"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> label</span><span id="textcolor2945"><span>=</span></span><span id="textcolor2946"><span>"prediction</span><span> +</span><span> stddev"</span></span><span> </span><span id="textcolor2947"><span>if</span></span><span> ind</span><span> </span><span id="textcolor2948"><span>==</span></span><span> </span><span id="textcolor2949"><span>0</span></span><span> </span><span id="textcolor2950"><span>else</span></span><span> </span><span id="textcolor2951"><span>None</span></span><span>,</span> <span id="x1-124141r24"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-124143r25"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2952"><span>.</span></span><span>plot(</span> <span id="x1-124145r26"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> x_hat,</span> <span id="x1-124147r27"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> y_hat</span><span id="textcolor2953"><span>.</span></span><span>mean()</span><span> </span><span id="textcolor2954"><span>-</span></span><span> </span><span id="textcolor2955"><span>2</span></span><span> </span><span id="textcolor2956"><span>*</span></span><span> y_hat</span><span id="textcolor2957"><span>.</span></span><span>stddev(),</span> <span id="x1-124149r28"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> color</span><span id="textcolor2958"><span>=</span></span><span id="textcolor2959"><span>"#e41a1c"</span></span><span>,</span> <span id="x1-124151r29"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> linestyle</span><span id="textcolor2960"><span>=</span></span><span id="textcolor2961"><span>"dashed"</span></span><span>,</span> <span id="x1-124153r30"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> label</span><span id="textcolor2962"><span>=</span></span><span id="textcolor2963"><span>"prediction</span><span> -</span><span> stddev"</span></span><span> </span><span id="textcolor2964"><span>if</span></span><span> ind</span><span> </span><span id="textcolor2965"><span>==</span></span><span> </span><span id="textcolor2966"><span>0</span></span><span> </span><span id="textcolor2967"><span>else</span></span><span> </span><span id="textcolor2968"><span>None</span></span><span>,</span> <span id="x1-124155r31"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-124157r32"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2969"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Plot</span><span class="cmitt-10x-x-109"> x-</span><span class="cmitt-10x-x-109"> and</span><span class="cmitt-10x-x-109"> y-axis</span><span class="cmitt-10x-x-109"> labels</span><span class="cmitt-10x-x-109"> as</span><span class="cmitt-10x-x-109"> well</span><span class="cmitt-10x-x-109"> as</span><span class="cmitt-10x-x-109"> a</span><span class="cmitt-10x-x-109"> legend</span></span> <span id="x1-124159r33"></span> </code>
<code><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2970"><span>.</span></span><span>xlabel(</span><span id="textcolor2971"><span>"carat"</span></span><span>)</span> <span id="x1-124161r34"></span> </code>
<code><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2972"><span>.</span></span><span>ylabel(</span><span id="textcolor2973"><span>"price"</span></span><span>)</span> <span id="x1-124163r35"></span> </code>
<code><span> </span><span> </span><span> </span><span> plt</span><span id="textcolor2974"><span>.</span></span><span>legend()</span></code></pre>
<p>Using this function, we can have a first look at the training <span id="dx1-124164"></span>data by running the following:</p>
<pre id="fancyvrb120" class="fancyvrb"><span id="x1-124167r1"></span> 
<code><span>plot_scatter(carat,</span><span> price)</span></code></pre>
<p>The training data distribution is shown in <em>Figure</em> <a href="#x1-124168r9"><em>7.9</em></a>. We observe that the relationship between carat and diamond price is non-linear, with prices increasing more rapidly at higher carat.</p>
<div class="IMG---Figure">
<img src="../media/file157.png" alt="PIC"/> <span id="x1-124168r9"></span> <span id="x1-124169"></span></div>
<p class="IMG---Caption">Figure 7.9: Relationship between the carat of a diamond and its price 
</p>
</section>
<section id="step-2-fitting-a-model-without-uncertainty" class="level4 likesubsubsectionHead" data-number="12.3.0.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.3.0.2"><span id="x1-1250003"></span>Step 2: Fitting a model without uncertainty</h4>
<p>Having completed the setup, we are ready to fit <span id="dx1-125001"></span>some regression models to the data. We start by fitting a neural network model without quantifying the uncertainty in the predictions. This allows us to establish a baseline and to introduce some tools (in the form of functions) that will be useful for all of the models in this case study.</p>
<p>It is recommended that you normalize the input features to a neural network model. In this example, that means normalizing the weight of the diamonds in carats. Normalizing input features will make the model converge faster during training. <code>tensorflow.keras</code> provides a convenient normalization function that allows us to do just that. We can use it as follows:</p>
<pre id="fancyvrb121" class="fancyvrb"><span id="x1-125006r1"></span> 
<code><span>normalizer</span><span> </span><span id="textcolor2976"><span>=</span></span><span> tf</span><span id="textcolor2977"><span>.</span></span><span>keras</span><span id="textcolor2978"><span>.</span></span><span>layers</span><span id="textcolor2979"><span>.</span></span><span>Normalization(input_shape</span><span id="textcolor2980"><span>=</span></span><span>(</span><span id="textcolor2981"><span>1</span></span><span>,),</span><span> axis</span><span id="textcolor2982"><span>=</span></span><span id="textcolor2983"><span>None</span></span><span>)</span> <span id="x1-125008r2"></span> </code>
<code><span>normalizer</span><span id="textcolor2984"><span>.</span></span><span>adapt(carat)</span></code></pre>
<p>We will also need a loss function, ideally one that can be used for all the models in this case study. A regression model can be posed as <em>P</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x,w</em>), the probability distribution of labels <em>y</em> given the inputs <em>x</em> and model parameters <em>w</em>. We can fit such a model to the data by minimizing the negative log-likelihood loss <span class="cmsy-10x-x-109">−</span><em>logP</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x</em>). In <code>Python</code> code, this can be written as a function that takes as input the true outcome value <code>y_true</code> and the predicted outcome distribution <code>y_divd</code> and returns the negative log-likelihood of the outcome value under the predicted outcome distribution, which is implemented in the <code>log_prob()</code> method provided by the <code>distributions</code> module in <code>tensorflow_probability</code>:</p>
<pre id="fancyvrb122" class="fancyvrb"><span id="x1-125018r1"></span> 
<code><span id="textcolor2985"><span>def</span></span><span> </span><span id="textcolor2986"><span>negloglik</span></span><span>(y_true,</span><span> y_divd):</span> <span id="x1-125020r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor2987"><span>return</span></span><span> </span><span id="textcolor2988"><span>-</span></span><span>y_divd</span><span id="textcolor2989"><span>.</span></span><span>log_prob(y_true)</span></code></pre>
<p>Equipped with these tools, let’s build our first model. We use the normalizer function that we just defined to normalize the model inputs. We then stack two dense layers on top. The first dense layer consists of 32 nodes.<span id="dx1-125021"></span> This allows us to <span id="dx1-125022"></span>model the non-linearity observed in the data. The second dense layer consists of one node in order to reduce the model prediction to a single value. Importantly, we do not use the output produced by this second dense layer as the model output. Instead, we use the dense layer output to parameterize the mean of a normal distribution, which means that we are modeling the ground truth labels using a normal distribution. We also set the variance of the normal distribution to 1. Parameterizing the mean of a distribution while setting the variance to a fixed value implies that we are modeling the overall trend of the data without yet quantifying uncertainty in the model’s predictions:</p>
<pre id="fancyvrb123" class="fancyvrb"><span id="x1-125034r1"></span> 
<code><span>model</span><span> </span><span id="textcolor2990"><span>=</span></span><span> tf</span><span id="textcolor2991"><span>.</span></span><span>keras</span><span id="textcolor2992"><span>.</span></span><span>Sequential(</span> <span id="x1-125036r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> [</span> <span id="x1-125038r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> normalizer,</span> <span id="x1-125040r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2993"><span>.</span></span><span>keras</span><span id="textcolor2994"><span>.</span></span><span>layers</span><span id="textcolor2995"><span>.</span></span><span>Dense(</span><span id="textcolor2996"><span>32</span></span><span>,</span><span> activation</span><span id="textcolor2997"><span>=</span></span><span id="textcolor2998"><span>"relu"</span></span><span>),</span> <span id="x1-125042r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor2999"><span>.</span></span><span>keras</span><span id="textcolor3000"><span>.</span></span><span>layers</span><span id="textcolor3001"><span>.</span></span><span>Dense(</span><span id="textcolor3002"><span>1</span></span><span>),</span> <span id="x1-125044r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3003"><span>.</span></span><span>layers</span><span id="textcolor3004"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-125046r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3005"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3006"><span>.</span></span><span>distributions</span><span id="textcolor3007"><span>.</span></span><span>Normal(loc</span><span id="textcolor3008"><span>=</span></span><span>t,</span><span> scale</span><span id="textcolor3009"><span>=</span></span><span id="textcolor3010"><span>1</span></span><span>)</span> <span id="x1-125048r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-125050r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-125052r10"></span> </code>
<code><span>)</span></code></pre>
<p>As we’ve seen in previous case studies, to train the model we use the <code>compile()</code> and <code>fit()</code> functions. During compilation of the model, we specify the <code>Adam</code> optimizer and the previously defined loss function. For the <code>fit</code> function, we specify that we want to train the model for 100 epochs on the carat and price data:</p>
<pre id="fancyvrb124" class="fancyvrb"><span id="x1-125062r1"></span> 
<code><span id="textcolor3012"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Compile</span></span> <span id="x1-125064r2"></span> </code>
<code><span>model</span><span id="textcolor3013"><span>.</span></span><span>compile(optimizer</span><span id="textcolor3014"><span>=</span></span><span>tf</span><span id="textcolor3015"><span>.</span></span><span>optimizers</span><span id="textcolor3016"><span>.</span></span><span>Adam(learning_rate</span><span id="textcolor3017"><span>=</span></span><span id="textcolor3018"><span>0.01</span></span><span>),</span><span> loss</span><span id="textcolor3019"><span>=</span></span><span>negloglik)</span> <span id="x1-125066r3"></span> </code>
<code><span id="textcolor3020"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Fit</span></span> <span id="x1-125068r4"></span> </code>
<code><span>model</span><span id="textcolor3021"><span>.</span></span><span>fit(carat,</span><span> price,</span><span> epochs</span><span id="textcolor3022"><span>=</span></span><span id="textcolor3023"><span>100</span></span><span>,</span><span> verbose</span><span id="textcolor3024"><span>=</span></span><span id="textcolor3025"><span>0</span></span><span>)</span></code></pre>
<p>We can then obtain the model’s predictions on the hold-out test <span id="dx1-125069"></span>data and visualize everything using our <code>plot_scatter()</code> function:</p>
<pre id="fancyvrb125" class="fancyvrb"><span id="x1-125078r1"></span> 
<code><span id="textcolor3026"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Define</span><span class="cmitt-10x-x-109"> range</span><span class="cmitt-10x-x-109"> for</span><span class="cmitt-10x-x-109"> model</span><span class="cmitt-10x-x-109"> input</span></span> <span id="x1-125080r2"></span> </code>
<code><span>carat_hat</span><span> </span><span id="textcolor3027"><span>=</span></span><span> tf</span><span id="textcolor3028"><span>.</span></span><span>linspace(carat_test</span><span id="textcolor3029"><span>.</span></span><span>min(),</span><span> carat_test</span><span id="textcolor3030"><span>.</span></span><span>max(),</span><span> </span><span id="textcolor3031"><span>100</span></span><span>)</span> <span id="x1-125082r3"></span> </code>
<code><span id="textcolor3032"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Obtain</span><span class="cmitt-10x-x-109"> model's</span><span class="cmitt-10x-x-109"> price</span><span class="cmitt-10x-x-109"> predictions</span><span class="cmitt-10x-x-109"> on</span><span class="cmitt-10x-x-109"> test</span><span class="cmitt-10x-x-109"> data</span></span> <span id="x1-125084r4"></span> </code>
<code><span>price_hat</span><span> </span><span id="textcolor3033"><span>=</span></span><span> model(carat_hat)</span> <span id="x1-125086r5"></span> </code>
<code><span id="textcolor3034"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Plot</span><span class="cmitt-10x-x-109"> test</span><span class="cmitt-10x-x-109"> data</span><span class="cmitt-10x-x-109"> and</span><span class="cmitt-10x-x-109"> model</span><span class="cmitt-10x-x-109"> predictions</span></span> <span id="x1-125088r6"></span> </code>
<code><span>plot_scatter(carat_test,</span><span> price_test,</span><span> carat_hat,</span><span> price_hat)</span></code></pre>
<p>This produces the following chart:</p>
<div class="IMG---Figure">
<img src="../media/prediction_model_no_uncertainty.png" alt="PIC"/> <span id="x1-125089r10"></span> <span id="x1-125090"></span></div>
<p class="IMG---Caption">Figure 7.10: Predictions without uncertainty on diamond test data 
</p>
<p>We can see in <em>Figure</em> <a href="#x1-125089r10"><em>7.10</em></a> that the model captures the non-linear trend of the data. As a diamond’s weight increases, the model predicts prices to increase more rapidly as we add more and more weight.</p>
<p>However, there is another obvious trend in the data that the model does not capture. We can observe that as weight increases, there is more and more variability in the price. At low weight, we only observe a little scatter around the fitted line, but the scatter increases at higher weight. We can consider this variability as inherent to the problem. That is, even if we had a lot more training data, we would still not be able to predict price, especially at high weight, perfectly. This <span id="dx1-125091"></span>sort of variability is aleatoric uncertainty, which we first encountered in <a href="CH4.xhtml#x1-490004"><em>Chapter 4</em></a>, <a href="CH4.xhtml#x1-490004"><em>Introducing Bayesian Deep Learning</em></a>, and will have a closer look at in the next subsection.</p>
</section>
<section id="step-3-fitting-a-model-with-aleatoric-uncertainty" class="level4 likesubsubsectionHead" data-number="12.3.0.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.3.0.3"><span id="x1-1260003"></span>Step 3: Fitting a model with aleatoric uncertainty</h4>
<p>We can account for aleatoric uncertainty in our model by <span id="dx1-126001"></span>predicting the standard deviation of the normal distribution in addition to predicting its mean. As before, we build a model with a normalizer layer and two dense layers. However, this time the second <span id="dx1-126002"></span>dense layer will output two values instead of one. The first output value will again be used to parameterize the mean of a normal distribution. But the second output value will parameterize the variance of the normal distribution, which allows us to quantify the aleatoric uncertainty in the model’s predictions:</p>
<pre id="fancyvrb126" class="fancyvrb"><span id="x1-126016r1"></span> 
<code><span>model_aleatoric</span><span> </span><span id="textcolor3035"><span>=</span></span><span> tf</span><span id="textcolor3036"><span>.</span></span><span>keras</span><span id="textcolor3037"><span>.</span></span><span>Sequential(</span> <span id="x1-126018r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> [</span> <span id="x1-126020r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> normalizer,</span> <span id="x1-126022r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor3038"><span>.</span></span><span>keras</span><span id="textcolor3039"><span>.</span></span><span>layers</span><span id="textcolor3040"><span>.</span></span><span>Dense(</span><span id="textcolor3041"><span>32</span></span><span>,</span><span> activation</span><span id="textcolor3042"><span>=</span></span><span id="textcolor3043"><span>"relu"</span></span><span>),</span> <span id="x1-126024r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor3044"><span>.</span></span><span>keras</span><span id="textcolor3045"><span>.</span></span><span>layers</span><span id="textcolor3046"><span>.</span></span><span>Dense(</span><span id="textcolor3047"><span>2</span></span><span>),</span> <span id="x1-126026r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3048"><span>.</span></span><span>layers</span><span id="textcolor3049"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-126028r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3050"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3051"><span>.</span></span><span>distributions</span><span id="textcolor3052"><span>.</span></span><span>Normal(</span> <span id="x1-126030r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> loc</span><span id="textcolor3053"><span>=</span></span><span>t[</span><span id="textcolor3054"><span>...</span></span><span>,</span><span> :</span><span id="textcolor3055"><span>1</span></span><span>],</span><span> scale</span><span id="textcolor3056"><span>=</span></span><span id="textcolor3057"><span>1e-3</span></span><span> </span><span id="textcolor3058"><span>+</span></span><span> tf</span><span id="textcolor3059"><span>.</span></span><span>math</span><span id="textcolor3060"><span>.</span></span><span>softplus(</span><span id="textcolor3061"><span>0.05</span></span><span> </span><span id="textcolor3062"><span>*</span></span><span> t[</span><span id="textcolor3063"><span>...</span></span><span>,</span><span> </span><span id="textcolor3064"><span>1</span></span><span>:])</span> <span id="x1-126032r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-126034r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-126036r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-126038r12"></span> </code>
<code><span>)</span></code></pre>
<p>We again compile and fit the model on the weight and price data:</p>
<pre id="fancyvrb127" class="fancyvrb"><span id="x1-126046r1"></span> 
<code><span id="textcolor3065"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Compile</span></span> <span id="x1-126048r2"></span> </code>
<code><span>model_aleatoric</span><span id="textcolor3066"><span>.</span></span><span>compile(</span> <span id="x1-126050r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> optimizer</span><span id="textcolor3067"><span>=</span></span><span>tf</span><span id="textcolor3068"><span>.</span></span><span>optimizers</span><span id="textcolor3069"><span>.</span></span><span>Adam(learning_rate</span><span id="textcolor3070"><span>=</span></span><span id="textcolor3071"><span>0.05</span></span><span>),</span><span> loss</span><span id="textcolor3072"><span>=</span></span><span>negloglik</span> <span id="x1-126052r4"></span> </code>
<code><span>)</span> <span id="x1-126054r5"></span> </code>
<code><span id="textcolor3073"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Fit</span></span> <span id="x1-126056r6"></span> </code>
<code><span>model_aleatoric</span><span id="textcolor3074"><span>.</span></span><span>fit(carat,</span><span> price,</span><span> epochs</span><span id="textcolor3075"><span>=</span></span><span id="textcolor3076"><span>100</span></span><span>,</span><span> verbose</span><span id="textcolor3077"><span>=</span></span><span id="textcolor3078"><span>0</span></span><span>)</span></code></pre>
<p>Now, we can obtain and visualize predictions on the test data. Note that this time, we pass <code>plot_std=True</code> in order to also plot the standard deviation of the predicted output distribution:</p>
<pre id="fancyvrb128" class="fancyvrb"><span id="x1-126064r1"></span> 
<code><span>carat_hat</span><span> </span><span id="textcolor3081"><span>=</span></span><span> tf</span><span id="textcolor3082"><span>.</span></span><span>linspace(carat_test</span><span id="textcolor3083"><span>.</span></span><span>min(),</span><span> carat_test</span><span id="textcolor3084"><span>.</span></span><span>max(),</span><span> </span><span id="textcolor3085"><span>100</span></span><span>)</span> <span id="x1-126066r2"></span> </code>
<code><span>price_hat</span><span> </span><span id="textcolor3086"><span>=</span></span><span> model_aleatoric(carat_hat)</span> <span id="x1-126068r3"></span> </code>
<code><span>plot_scatter(</span> <span id="x1-126070r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> carat_test,</span><span> price_test,</span><span> carat_hat,</span><span> price_hat,</span><span> plot_std</span><span id="textcolor3087"><span>=</span></span><span id="textcolor3088"><span>True</span></span><span>,</span> <span id="x1-126072r5"></span> </code>
<code><span>)</span></code></pre>
<p>We have now trained a model that represents the variation inherent to the data. The dashed error bars in <em>Figure</em> <a href="#x1-126075r11"><em>7.11</em></a> show the predicted variability of price as a <span id="dx1-126073"></span>function of weight. We can <span id="dx1-126074"></span>observe that the model is indeed less certain about what price to predict at weight above 1 carat, reflecting the larger scatter in the data that we observe at the higher weight range.</p>
<div class="IMG---Figure">
<img src="../media/prediction_model_aleatoric_uncertainty.png" alt="PIC"/> <span id="x1-126075r11"></span> <span id="x1-126076"></span></div>
<p class="IMG---Caption">Figure 7.11: Predictions with aleatoric uncertainty on diamond test data 
</p>
</section>
<section id="step-4-fitting-a-model-with-epistemic-uncertainty" class="level4 likesubsubsectionHead" data-number="12.3.0.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.3.0.4"><span id="x1-1270003"></span>Step 4: Fitting a model with epistemic uncertainty</h4>
<p>In addition to aleatoric uncertainty, we also deal with <span id="dx1-127001"></span>epistemic uncertainty – the uncertainty that stems not from the data, but from our model. Looking back at <em>Figure</em> <a href="#x1-126075r11"><em>7.11</em></a>, for example, the solid line, which represents the mean of our model prediction, appears to <span id="dx1-127002"></span>capture the trend of the data reasonably well. But given that training data is limited, we cannot be 100% certain that we found the true mean of the underlying data distribution. Maybe the true mean is actually a little bit greater or a little less than what we estimated it to be. In this section, we look at how we can model such uncertainty, and we will also see that epistemic uncertainty can be reduced by observing more data.</p>
<p>The trick to modeling epistemic uncertainty is, once again, to represent the weights in our neural network by a distribution rather than a point estimate. We can achieve this by replacing the dense layers that we used previously with DenseVariational layers from <code>tensorflow_probability</code>. Under the hood, this will implement BBB, which we first learned about in <a href="CH5.xhtml#x1-600005"><em>Chapter 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches for Bayesian Deep Learning</em></a>. In brief, when using BBB, we learn the posterior distribution over the weights of our network using the principle of variational learning. In order to do so, we need to define both prior and posterior distribution functions.</p>
<p>Note that the code example for BBB presented in <a href="CH5.xhtml#x1-600005"><em>Chapter 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled</em> <em>Approaches for Bayesian Deep Learning</em></a> made use of predefined <code>tensorflow_probability</code> modules for 2D convolution and dense layers with the reparameterization trick, which implicitly defined prior and posterior functions for us. In this example, we will define the prior and posterior functions for the dense layer ourselves.</p>
<p>We start by defining the prior over the dense layer’s weights (both the kernel and the bias terms). The prior distribution models the uncertainty in the weights before we observe any data. It can be defined using a multivariate normal distribution that has a trainable mean and a variance that is fixed at 1:</p>
<pre id="fancyvrb129" class="fancyvrb"><span id="x1-127019r1"></span> 
<code><span id="textcolor3089"><span>def</span></span><span> </span><span id="textcolor3090"><span>prior</span></span><span>(kernel_size,</span><span> bias_size</span><span id="textcolor3091"><span>=</span></span><span id="textcolor3092"><span>0</span></span><span>,</span><span> dtype</span><span id="textcolor3093"><span>=</span></span><span id="textcolor3094"><span>None</span></span><span>):</span> <span id="x1-127021r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> n</span><span> </span><span id="textcolor3095"><span>=</span></span><span> kernel_size</span><span> </span><span id="textcolor3096"><span>+</span></span><span> bias_size</span> <span id="x1-127023r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3097"><span>return</span></span><span> tf</span><span id="textcolor3098"><span>.</span></span><span>keras</span><span id="textcolor3099"><span>.</span></span><span>Sequential(</span> <span id="x1-127025r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> [</span> <span id="x1-127027r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3100"><span>.</span></span><span>layers</span><span id="textcolor3101"><span>.</span></span><span>VariableLayer(n,</span><span> dtype</span><span id="textcolor3102"><span>=</span></span><span>dtype),</span> <span id="x1-127029r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3103"><span>.</span></span><span>layers</span><span id="textcolor3104"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-127031r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3105"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3106"><span>.</span></span><span>distributions</span><span id="textcolor3107"><span>.</span></span><span>Independent(</span> <span id="x1-127033r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3108"><span>.</span></span><span>distributions</span><span id="textcolor3109"><span>.</span></span><span>Normal(loc</span><span id="textcolor3110"><span>=</span></span><span>t,</span><span> scale</span><span id="textcolor3111"><span>=</span></span><span id="textcolor3112"><span>1</span></span><span>),</span> <span id="x1-127035r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> reinterpreted_batch_ndims</span><span id="textcolor3113"><span>=</span></span><span id="textcolor3114"><span>1</span></span><span>,</span> <span id="x1-127037r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-127039r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127041r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-127043r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> )</span></code></pre>
<p>We also define the variational posterior. The <span id="dx1-127044"></span>variational posterior is an <span id="dx1-127045"></span>approximation to the distribution of the dense layer’s weights after we have observed the training data. We again use a multivariate normal distribution:</p>
<pre id="fancyvrb130" class="fancyvrb"><span id="x1-127064r1"></span> 
<code><span id="textcolor3115"><span>def</span></span><span> </span><span id="textcolor3116"><span>posterior</span></span><span>(kernel_size,</span><span> bias_size</span><span id="textcolor3117"><span>=</span></span><span id="textcolor3118"><span>0</span></span><span>,</span><span> dtype</span><span id="textcolor3119"><span>=</span></span><span id="textcolor3120"><span>None</span></span><span>):</span> <span id="x1-127066r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> n</span><span> </span><span id="textcolor3121"><span>=</span></span><span> kernel_size</span><span> </span><span id="textcolor3122"><span>+</span></span><span> bias_size</span> <span id="x1-127068r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> c</span><span> </span><span id="textcolor3123"><span>=</span></span><span> np</span><span id="textcolor3124"><span>.</span></span><span>log(np</span><span id="textcolor3125"><span>.</span></span><span>expm1(</span><span id="textcolor3126"><span>1.0</span></span><span>))</span> <span id="x1-127070r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3127"><span>return</span></span><span> tf</span><span id="textcolor3128"><span>.</span></span><span>keras</span><span id="textcolor3129"><span>.</span></span><span>Sequential(</span> <span id="x1-127072r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> [</span> <span id="x1-127074r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3130"><span>.</span></span><span>layers</span><span id="textcolor3131"><span>.</span></span><span>VariableLayer(</span><span id="textcolor3132"><span>2</span></span><span> </span><span id="textcolor3133"><span>*</span></span><span> n,</span><span> dtype</span><span id="textcolor3134"><span>=</span></span><span>dtype),</span> <span id="x1-127076r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3135"><span>.</span></span><span>layers</span><span id="textcolor3136"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-127078r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3137"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3138"><span>.</span></span><span>distributions</span><span id="textcolor3139"><span>.</span></span><span>Independent(</span> <span id="x1-127080r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3140"><span>.</span></span><span>distributions</span><span id="textcolor3141"><span>.</span></span><span>Normal(</span> <span id="x1-127082r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> loc</span><span id="textcolor3142"><span>=</span></span><span>t[</span><span id="textcolor3143"><span>...</span></span><span>,</span><span> :n],</span> <span id="x1-127084r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> scale</span><span id="textcolor3144"><span>=</span></span><span id="textcolor3145"><span>1e-5</span></span><span> </span><span id="textcolor3146"><span>+</span></span><span> tf</span><span id="textcolor3147"><span>.</span></span><span>nn</span><span id="textcolor3148"><span>.</span></span><span>softplus(c</span><span> </span><span id="textcolor3149"><span>+</span></span><span> t[</span><span id="textcolor3150"><span>...</span></span><span>,</span><span> n:]),</span> <span id="x1-127086r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127088r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> reinterpreted_batch_ndims</span><span id="textcolor3151"><span>=</span></span><span id="textcolor3152"><span>1</span></span><span>,</span> <span id="x1-127090r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-127092r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127094r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-127096r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> )</span></code></pre>
<p>Equipped with these prior and posterior functions, we can define our model. As before, we use the normalizer layer to normalize our inputs and then stack two dense layers on top of each other. But this time, the dense layers will represent their parameters as distributions rather than point estimates. We achieve this by using the DenseVariational layers <span id="dx1-127097"></span>from <code>tensorflow_probability</code> together with our prior and posterior functions. The final output layer is a normal distribution <span id="dx1-127099"></span>with its variance set to 1 and with its mean parameterized by the output of the preceding DenseVariational layer:</p>
<pre id="fancyvrb131" class="fancyvrb"><span id="x1-127124r1"></span> 
<code><span id="textcolor3153"><span>def</span></span><span> </span><span id="textcolor3154"><span>build_epistemic_model</span></span><span>():</span> <span id="x1-127126r2"></span> </code>
<code><span> </span><span> model</span><span> </span><span id="textcolor3155"><span>=</span></span><span> tf</span><span id="textcolor3156"><span>.</span></span><span>keras</span><span id="textcolor3157"><span>.</span></span><span>Sequential(</span> <span id="x1-127128r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> [</span> <span id="x1-127130r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> normalizer,</span> <span id="x1-127132r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3158"><span>.</span></span><span>layers</span><span id="textcolor3159"><span>.</span></span><span>DenseVariational(</span> <span id="x1-127134r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3160"><span>32</span></span><span>,</span> <span id="x1-127136r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_prior_fn</span><span id="textcolor3161"><span>=</span></span><span>prior,</span> <span id="x1-127138r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_posterior_fn</span><span id="textcolor3162"><span>=</span></span><span>posterior,</span> <span id="x1-127140r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kl_weight</span><span id="textcolor3163"><span>=</span></span><span id="textcolor3164"><span>1</span></span><span> </span><span id="textcolor3165"><span>/</span></span><span> NUM_TRAIN_SAMPLES,</span> <span id="x1-127142r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> activation</span><span id="textcolor3166"><span>=</span></span><span id="textcolor3167"><span>"relu"</span></span><span>,</span> <span id="x1-127144r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127146r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3168"><span>.</span></span><span>layers</span><span id="textcolor3169"><span>.</span></span><span>DenseVariational(</span> <span id="x1-127148r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3170"><span>1</span></span><span>,</span> <span id="x1-127150r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_prior_fn</span><span id="textcolor3171"><span>=</span></span><span>prior,</span> <span id="x1-127152r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_posterior_fn</span><span id="textcolor3172"><span>=</span></span><span>posterior,</span> <span id="x1-127154r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kl_weight</span><span id="textcolor3173"><span>=</span></span><span id="textcolor3174"><span>1</span></span><span> </span><span id="textcolor3175"><span>/</span></span><span> NUM_TRAIN_SAMPLES,</span> <span id="x1-127156r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127158r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3176"><span>.</span></span><span>layers</span><span id="textcolor3177"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-127160r19"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3178"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3179"><span>.</span></span><span>distributions</span><span id="textcolor3180"><span>.</span></span><span>Normal(loc</span><span id="textcolor3181"><span>=</span></span><span>t,</span><span> scale</span><span id="textcolor3182"><span>=</span></span><span id="textcolor3183"><span>1</span></span><span>)</span> <span id="x1-127162r20"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-127164r21"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-127166r22"></span> </code>
<code><span> </span><span> )</span> <span id="x1-127168r23"></span> </code>
<code><span> </span><span> </span><span id="textcolor3184"><span>return</span></span><span> model</span></code></pre>
<p>To observe the effect of the amount of available training data on epistemic uncertainty estimates, we first fit our model on a small subset of data before fitting it on all available training data. We take the first 500 samples from the training dataset:</p>
<pre id="fancyvrb132" class="fancyvrb"><span id="x1-127172r1"></span> 
<code><span>carat_subset</span><span> </span><span id="textcolor3185"><span>=</span></span><span> carat[:</span><span id="textcolor3186"><span>500</span></span><span>]</span> <span id="x1-127174r2"></span> </code>
<code><span>price_subset</span><span> </span><span id="textcolor3187"><span>=</span></span><span> price[:</span><span id="textcolor3188"><span>500</span></span><span>]</span></code></pre>
<p>We build, compile, and fit the model as before:</p>
<pre id="fancyvrb133" class="fancyvrb"><span id="x1-127184r1"></span> 
<code><span id="textcolor3189"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Build</span></span> <span id="x1-127186r2"></span> </code>
<code><span>model_epistemic</span><span> </span><span id="textcolor3190"><span>=</span></span><span> build_epistemic_model()</span> <span id="x1-127188r3"></span> </code>
<code><span id="textcolor3191"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Compile</span></span> <span id="x1-127190r4"></span> </code>
<code><span>model_epistemic</span><span id="textcolor3192"><span>.</span></span><span>compile(</span> <span id="x1-127192r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> optimizer</span><span id="textcolor3193"><span>=</span></span><span>tf</span><span id="textcolor3194"><span>.</span></span><span>optimizers</span><span id="textcolor3195"><span>.</span></span><span>Adam(learning_rate</span><span id="textcolor3196"><span>=</span></span><span id="textcolor3197"><span>0.01</span></span><span>),</span><span> loss</span><span id="textcolor3198"><span>=</span></span><span>negloglik</span> <span id="x1-127194r6"></span> </code>
<code><span>)</span> <span id="x1-127196r7"></span> </code>
<code><span id="textcolor3199"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Fit</span></span> <span id="x1-127198r8"></span> </code>
<code><span>model_epistemic</span><span id="textcolor3200"><span>.</span></span><span>fit(carat_subset,</span><span> price_subset,</span><span> epochs</span><span id="textcolor3201"><span>=</span></span><span id="textcolor3202"><span>100</span></span><span>,</span><span> verbose</span><span id="textcolor3203"><span>=</span></span><span id="textcolor3204"><span>0</span></span><span>)</span></code></pre>
<p>We then obtain and plot predictions on the test data. Note that here, we sample from the posterior distribution 10 times, which allows us to observe how much the predicted mean varies with every sample iteration. If the predicted mean varies a lot, this <span id="dx1-127199"></span>means that epistemic uncertainty is estimated to be large, while if the mean varies only very little, there is only little <span id="dx1-127200"></span>estimated epistemic uncertainty:</p>
<pre id="fancyvrb134" class="fancyvrb"><span id="x1-127207r1"></span> 
<code><span>carat_hat</span><span> </span><span id="textcolor3205"><span>=</span></span><span> tf</span><span id="textcolor3206"><span>.</span></span><span>linspace(carat_test</span><span id="textcolor3207"><span>.</span></span><span>min(),</span><span> carat_test</span><span id="textcolor3208"><span>.</span></span><span>max(),</span><span> </span><span id="textcolor3209"><span>100</span></span><span>)</span> <span id="x1-127209r2"></span> </code>
<code><span>price_hats</span><span> </span><span id="textcolor3210"><span>=</span></span><span> [model_epistemic(carat_hat)</span><span> </span><span id="textcolor3211"><span>for</span></span><span> _</span><span> </span><span id="textcolor3212"><span>in</span></span><span> </span><span id="textcolor3213"><span>range</span></span><span>(</span><span id="textcolor3214"><span>10</span></span><span>)]</span> <span id="x1-127211r3"></span> </code>
<code><span>plot_scatter(</span> <span id="x1-127213r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> carat_test,</span><span> price_test,</span><span> carat_hat,</span><span> price_hats,</span> <span id="x1-127215r5"></span> </code>
<code><span>)</span></code></pre>
<p>In <em>Figure</em> <a href="#x1-127216r12"><em>7.12</em></a>, we can observe that the predicted mean varies over the 10 different samples. Interestingly, variation (and thus epistemic uncertainty) seems to be lower at lower weights and increases as weight increases.</p>
<div class="IMG---Figure">
<img src="../media/prediction_model_high_epistemic_uncertainty.png" alt="PIC"/> <span id="x1-127216r12"></span> <span id="x1-127217"></span></div>
<p class="IMG---Caption">Figure 7.12: Predictions with high epistemic uncertainty on diamond test data 
</p>
<p>In order to <span id="dx1-127218"></span>verify that epistemic uncertainty can be <span id="dx1-127219"></span>reduced by training on more data, we train our model on the full training dataset:</p>
<pre id="fancyvrb135" class="fancyvrb"><span id="x1-127229r1"></span> 
<code><span id="textcolor3215"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Build</span></span> <span id="x1-127231r2"></span> </code>
<code><span>model_epistemic_full</span><span> </span><span id="textcolor3216"><span>=</span></span><span> build_epistemic_model()</span> <span id="x1-127233r3"></span> </code>
<code><span id="textcolor3217"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Compile</span></span> <span id="x1-127235r4"></span> </code>
<code><span>model_epistemic_full</span><span id="textcolor3218"><span>.</span></span><span>compile(</span> <span id="x1-127237r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> optimizer</span><span id="textcolor3219"><span>=</span></span><span>tf</span><span id="textcolor3220"><span>.</span></span><span>optimizers</span><span id="textcolor3221"><span>.</span></span><span>Adam(learning_rate</span><span id="textcolor3222"><span>=</span></span><span id="textcolor3223"><span>0.01</span></span><span>),</span><span> loss</span><span id="textcolor3224"><span>=</span></span><span>negloglik</span> <span id="x1-127239r6"></span> </code>
<code><span>)</span> <span id="x1-127241r7"></span> </code>
<code><span id="textcolor3225"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Fit</span></span> <span id="x1-127243r8"></span> </code>
<code><span>model_epistemic_full</span><span id="textcolor3226"><span>.</span></span><span>fit(carat,</span><span> price,</span><span> epochs</span><span id="textcolor3227"><span>=</span></span><span id="textcolor3228"><span>100</span></span><span>,</span><span> verbose</span><span id="textcolor3229"><span>=</span></span><span id="textcolor3230"><span>0</span></span><span>)</span></code></pre>
<p>And then plot the predictions for the full data model:</p>
<pre id="fancyvrb136" class="fancyvrb"><span id="x1-127250r1"></span> 
<code><span>carat_hat</span><span> </span><span id="textcolor3231"><span>=</span></span><span> tf</span><span id="textcolor3232"><span>.</span></span><span>linspace(carat_test</span><span id="textcolor3233"><span>.</span></span><span>min(),</span><span> carat_test</span><span id="textcolor3234"><span>.</span></span><span>max(),</span><span> </span><span id="textcolor3235"><span>100</span></span><span>)</span> <span id="x1-127252r2"></span> </code>
<code><span>price_hats</span><span> </span><span id="textcolor3236"><span>=</span></span><span> [model_epistemic_full(carat_hat)</span><span> </span><span id="textcolor3237"><span>for</span></span><span> _</span><span> </span><span id="textcolor3238"><span>in</span></span><span> </span><span id="textcolor3239"><span>range</span></span><span>(</span><span id="textcolor3240"><span>10</span></span><span>)]</span> <span id="x1-127254r3"></span> </code>
<code><span>plot_scatter(</span> <span id="x1-127256r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> carat_test,</span><span> price_test,</span><span> carat_hat,</span><span> price_hats,</span> <span id="x1-127258r5"></span> </code>
<code><span>)</span></code></pre>
<p>As expected, we see in <em>Figure</em> <a href="#x1-127259r13"><em>7.13</em></a> that epistemic uncertainty is much lower now and the predicted mean varies very little over the 10 samples (to the point where it is hard to see any difference between the 10 red curves):</p>
<div class="IMG---Figure">
<img src="../media/prediction_model_low_epistemic_uncertainty.png" alt="PIC"/> <span id="x1-127259r13"></span> <span id="x1-127260"></span></div>
<p class="IMG---Caption">Figure 7.13: Predictions with low epistemic uncertainty on diamond test data 
</p>
</section>
<section id="step-5-fitting-a-model-with-aleatoric-and-epistemic-uncertainty" class="level4 likesubsubsectionHead" data-number="12.3.0.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="12.3.0.5"><span id="x1-1280003"></span>Step 5: Fitting a model with aleatoric and epistemic uncertainty</h4>
<p>As a final exercise, we can put all the building <span id="dx1-128001"></span>blocks together and build a neural network that models both aleatoric and epistemic uncertainty. We can achieve this by using two DenseVariational layers (which will allow us to model epistemic uncertainty) and then stacking a normal distribution layer on top whose mean and variance are parameterized by the outputs of the second DenseVariational layer (which will allow us to model aleatoric uncertainty):</p>
<pre id="fancyvrb137" class="fancyvrb"><span id="x1-128027r1"></span> 
<code><span id="textcolor3241"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> Build</span><span class="cmitt-10x-x-109"> model.</span></span> <span id="x1-128029r2"></span> </code>
<code><span>model_epistemic_aleatoric</span><span> </span><span id="textcolor3242"><span>=</span></span><span> tf</span><span id="textcolor3243"><span>.</span></span><span>keras</span><span id="textcolor3244"><span>.</span></span><span>Sequential(</span> <span id="x1-128031r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> [</span> <span id="x1-128033r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> normalizer,</span> <span id="x1-128035r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3245"><span>.</span></span><span>layers</span><span id="textcolor3246"><span>.</span></span><span>DenseVariational(</span> <span id="x1-128037r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3247"><span>32</span></span><span>,</span> <span id="x1-128039r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_prior_fn</span><span id="textcolor3248"><span>=</span></span><span>prior,</span> <span id="x1-128041r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_posterior_fn</span><span id="textcolor3249"><span>=</span></span><span>posterior,</span> <span id="x1-128043r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kl_weight</span><span id="textcolor3250"><span>=</span></span><span id="textcolor3251"><span>1</span></span><span> </span><span id="textcolor3252"><span>/</span></span><span> NUM_TRAIN_SAMPLES,</span> <span id="x1-128045r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> activation</span><span id="textcolor3253"><span>=</span></span><span id="textcolor3254"><span>"relu"</span></span><span>,</span> <span id="x1-128047r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-128049r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3255"><span>.</span></span><span>layers</span><span id="textcolor3256"><span>.</span></span><span>DenseVariational(</span> <span id="x1-128051r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3257"><span>1</span></span><span> </span><span id="textcolor3258"><span>+</span></span><span> </span><span id="textcolor3259"><span>1</span></span><span>,</span> <span id="x1-128053r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_prior_fn</span><span id="textcolor3260"><span>=</span></span><span>prior,</span> <span id="x1-128055r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> make_posterior_fn</span><span id="textcolor3261"><span>=</span></span><span>posterior,</span> <span id="x1-128057r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kl_weight</span><span id="textcolor3262"><span>=</span></span><span id="textcolor3263"><span>1</span></span><span> </span><span id="textcolor3264"><span>/</span></span><span> NUM_TRAIN_SAMPLES,</span> <span id="x1-128059r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-128061r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3265"><span>.</span></span><span>layers</span><span id="textcolor3266"><span>.</span></span><span>DistributionLambda(</span> <span id="x1-128063r19"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3267"><span>lambda</span></span><span> t:</span><span> tfp</span><span id="textcolor3268"><span>.</span></span><span>distributions</span><span id="textcolor3269"><span>.</span></span><span>Normal(</span> <span id="x1-128065r20"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> loc</span><span id="textcolor3270"><span>=</span></span><span>t[</span><span id="textcolor3271"><span>...</span></span><span>,</span><span> :</span><span id="textcolor3272"><span>1</span></span><span>],</span><span> scale</span><span id="textcolor3273"><span>=</span></span><span id="textcolor3274"><span>1e-3</span></span><span> </span><span id="textcolor3275"><span>+</span></span><span> tf</span><span id="textcolor3276"><span>.</span></span><span>math</span><span id="textcolor3277"><span>.</span></span><span>softplus(</span><span id="textcolor3278"><span>0.05</span></span><span> </span><span id="textcolor3279"><span>*</span></span><span> t[</span><span id="textcolor3280"><span>...</span></span><span>,</span><span> </span><span id="textcolor3281"><span>1</span></span><span>:])</span> <span id="x1-128067r21"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> )</span> <span id="x1-128069r22"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-128071r23"></span> </code>
<code><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-128073r24"></span> </code>
<code><span>)</span></code></pre>
<p>We can build and train this model following the same procedure as before. We can then again perform inference on the test data for 10 times, which yields the predictions shown in <em>Figure</em> <a href="#x1-128075r14"><em>7.14</em></a>. Every of the 10 inferences now yields a predicted mean and <span id="dx1-128074"></span>standard deviation. The standard deviation represents the estimated aleatoric uncertainty for every inference, and the variation observed across the different inferences represents the epistemic uncertainty.</p>
<div class="IMG---Figure">
<img src="../media/prediction_model_aleatoric_and_epistemic_uncertainty.png" alt="PIC"/> <span id="x1-128075r14"></span> <span id="x1-128076"></span></div>
<p class="IMG---Caption">Figure 7.14: Predictions with both epistemic and aleatoric uncertainty on diamond test data 
</p>
<p><span id="x1-128077r196"></span></p>
</section>
<section id="sources-of-uncertainty-image-classification-case-study" class="level3 subsectionHead" data-number="12.3.1">
<h3 class="subsectionHead" data-number="12.3.1" id="sigil_toc_id_84"><span class="titlemark">7.3.1 </span> <span id="x1-1290001"></span>Sources of uncertainty: Image classification case study</h3>
<p>In the previous case study, we saw how we can model <span id="dx1-129001"></span>aleatoric and epistemic uncertainty in a regression problem. In this section, we’ll look at the MNIST digits dataset one more time to model aleatoric and epistemic uncertainty. We will also explore how aleatoric uncertainty can be difficult to reduce, whereas epistemic uncertainty can be reduced with more data.</p>
<p>Let’s start with our data. To make our example more insightful, we will not just use the standard MNIST dataset but also use a variant of MNIST named AmbiguousMNIST. This dataset contains generated images that are, unsurprisingly, inherently ambiguous. Let’s first load the data and then explore the AmbiguousMNIST dataset. We’ll start with the necessary imports:</p>
<pre id="fancyvrb138" class="fancyvrb"><span id="x1-129012r1"></span> 
<code><span id="textcolor3282"><span>import</span></span><span> </span><span id="textcolor3283"><span>tensorflow</span></span><span> </span><span id="textcolor3284"><span>as</span></span><span> </span><span id="textcolor3285"><span>tf</span></span> <span id="x1-129014r2"></span> </code>
<code><span id="textcolor3286"><span>import</span></span><span> </span><span id="textcolor3287"><span>tensorflow_probability</span></span><span> </span><span id="textcolor3288"><span>as</span></span><span> </span><span id="textcolor3289"><span>tfp</span></span> <span id="x1-129016r3"></span> </code>
<code><span id="textcolor3290"><span>import</span></span><span> </span><span id="textcolor3291"><span>matplotlib.pyplot</span></span><span> </span><span id="textcolor3292"><span>as</span></span><span> </span><span id="textcolor3293"><span>plt</span></span> <span id="x1-129018r4"></span> </code>
<code><span id="textcolor3294"><span>import</span></span><span> </span><span id="textcolor3295"><span>numpy</span></span><span> </span><span id="textcolor3296"><span>as</span></span><span> </span><span id="textcolor3297"><span>np</span></span> <span id="x1-129020r5"></span> </code>
<code><span id="textcolor3298"><span>from</span></span><span> </span><span id="textcolor3299"><span>sklearn.utils</span></span><span> </span><span id="textcolor3300"><span>import</span></span><span> shuffle</span> <span id="x1-129022r6"></span> </code>
<code><span id="textcolor3301"><span>from</span></span><span> </span><span id="textcolor3302"><span>sklearn.metrics</span></span><span> </span><span id="textcolor3303"><span>import</span></span><span> roc_auc_score</span> <span id="x1-129024r7"></span> </code>
<code><span id="textcolor3304"><span>import</span></span><span> </span><span id="textcolor3305"><span>ddu_dirty_mnist</span></span> <span id="x1-129026r8"></span> </code>
<code><span id="textcolor3306"><span>from</span></span><span> </span><span id="textcolor3307"><span>scipy.stats</span></span><span> </span><span id="textcolor3308"><span>import</span></span><span> entropy</span> <span id="x1-129028r9"></span> </code>
<code><span>tfd</span><span> </span><span id="textcolor3309"><span>=</span></span><span> tfp</span><span id="textcolor3310"><span>.</span></span><span>distributions</span></code></pre>
<p>We can download the AmbiguousMNIST dataset with the <code>ddu_dirty_mnist</code> library:</p>
<pre id="fancyvrb139" class="fancyvrb"><span id="x1-129045r1"></span> 
<code><span>dirty_mnist_train</span><span> </span><span id="textcolor3311"><span>=</span></span><span> ddu_dirty_mnist</span><span id="textcolor3312"><span>.</span></span><span>DirtyMNIST(</span> <span id="x1-129047r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3313"><span>"."</span></span><span>,</span> <span id="x1-129049r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> train</span><span id="textcolor3314"><span>=</span></span><span id="textcolor3315"><span>True</span></span><span>,</span> <span id="x1-129051r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> download</span><span id="textcolor3316"><span>=</span></span><span id="textcolor3317"><span>True</span></span><span>,</span> <span id="x1-129053r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> normalize</span><span id="textcolor3318"><span>=</span></span><span id="textcolor3319"><span>False</span></span><span>,</span> <span id="x1-129055r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> noise_stddev</span><span id="textcolor3320"><span>=</span></span><span id="textcolor3321"><span>0</span></span> <span id="x1-129057r7"></span> </code>
<code><span>)</span> <span id="x1-129059r8"></span> </code>
<code><span id="x1-129061r9"></span></code>
<code><span id="textcolor3322"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> regular</span><span class="cmitt-10x-x-109"> MNIST</span></span> <span id="x1-129063r10"></span> </code>
<code><span>train_imgs</span><span> </span><span id="textcolor3323"><span>=</span></span><span> dirty_mnist_train</span><span id="textcolor3324"><span>.</span></span><span>datasets[</span><span id="textcolor3325"><span>0</span></span><span>]</span><span id="textcolor3326"><span>.</span></span><span>data</span><span id="textcolor3327"><span>.</span></span><span>numpy()</span> <span id="x1-129065r11"></span> </code>
<code><span>train_labels</span><span> </span><span id="textcolor3328"><span>=</span></span><span> dirty_mnist_train</span><span id="textcolor3329"><span>.</span></span><span>datasets[</span><span id="textcolor3330"><span>0</span></span><span>]</span><span id="textcolor3331"><span>.</span></span><span>targets</span><span id="textcolor3332"><span>.</span></span><span>numpy()</span> <span id="x1-129067r12"></span> </code>
<code><span id="textcolor3333"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> AmbiguousMNIST</span></span> <span id="x1-129069r13"></span> </code>
<code><span>train_imgs_amb</span><span> </span><span id="textcolor3334"><span>=</span></span><span> dirty_mnist_train</span><span id="textcolor3335"><span>.</span></span><span>datasets[</span><span id="textcolor3336"><span>1</span></span><span>]</span><span id="textcolor3337"><span>.</span></span><span>data</span><span id="textcolor3338"><span>.</span></span><span>numpy()</span> <span id="x1-129071r14"></span> </code>
<code><span>train_labels_amb</span><span> </span><span id="textcolor3339"><span>=</span></span><span> dirty_mnist_train</span><span id="textcolor3340"><span>.</span></span><span>datasets[</span><span id="textcolor3341"><span>1</span></span><span>]</span><span id="textcolor3342"><span>.</span></span><span>targets</span><span id="textcolor3343"><span>.</span></span><span>numpy()</span></code></pre>
<p>We then concatenate and shuffle the images and labels so that <span id="dx1-129072"></span>we have a good mix of both datasets during training. We also fix the shape of the dataset so that it fits the setup of our model:</p>
<pre id="fancyvrb140" class="fancyvrb"><span id="x1-129080r1"></span> 
<code><span>train_imgs,</span><span> train_labels</span><span> </span><span id="textcolor3344"><span>=</span></span><span> shuffle(</span> <span id="x1-129082r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> np</span><span id="textcolor3345"><span>.</span></span><span>concatenate([train_imgs,</span><span> train_imgs_amb]),</span> <span id="x1-129084r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> np</span><span id="textcolor3346"><span>.</span></span><span>concatenate([train_labels,</span><span> train_labels_amb])</span> <span id="x1-129086r4"></span> </code>
<code><span>)</span> <span id="x1-129088r5"></span> </code>
<code><span>train_imgs</span><span> </span><span id="textcolor3347"><span>=</span></span><span> np</span><span id="textcolor3348"><span>.</span></span><span>expand_dims(train_imgs[:,</span><span> </span><span id="textcolor3349"><span>0</span></span><span>,</span><span> :,</span><span> :],</span><span> </span><span id="textcolor3350"><span>-</span></span><span id="textcolor3351"><span>1</span></span><span>)</span> <span id="x1-129090r6"></span> </code>
<code><span>train_labels</span><span> </span><span id="textcolor3352"><span>=</span></span><span> tf</span><span id="textcolor3353"><span>.</span></span><span>one_hot(train_labels,</span><span> </span><span id="textcolor3354"><span>10</span></span><span>)</span></code></pre>
<p><em>Figure</em> <a href="#x1-129091r15"><em>7.15</em></a> gives an example of the AmbiguousMNIST images. We can see that the images are in between classes: a 4 can also be interpreted as an 9, a 0 can be interpreted as a 6, and vice versa. This means that our model will most likely struggle to classify at least a portion of these images correctly as they are inherently noisy.</p>
<div class="IMG---Figure">
<img src="../media/file158.png" alt="PIC"/> <span id="x1-129091r15"></span> <span id="x1-129092"></span></div>
<p class="IMG---Caption">Figure 7.15: Examples of images from the AmbiguousMNIST dataset 
</p>
<p>Now that we have our train dataset, let’s <span id="dx1-129093"></span>load our test dataset as well. We will just use the standard MNIST test dataset:</p>
<pre id="fancyvrb141" class="fancyvrb"><span id="x1-129099r1"></span> 
<code><span>(test_imgs,</span><span> test_labels)</span><span> </span><span id="textcolor3355"><span>=</span></span><span> tf</span><span id="textcolor3356"><span>.</span></span><span>keras</span><span id="textcolor3357"><span>.</span></span><span>datasets</span><span id="textcolor3358"><span>.</span></span><span>mnist</span><span id="textcolor3359"><span>.</span></span><span>load_data()[</span><span id="textcolor3360"><span>1</span></span><span>]</span> <span id="x1-129101r2"></span> </code>
<code><span>test_imgs</span><span> </span><span id="textcolor3361"><span>=</span></span><span> test_imgs</span><span> </span><span id="textcolor3362"><span>/</span></span><span> </span><span id="textcolor3363"><span>255.</span></span> <span id="x1-129103r3"></span> </code>
<code><span>test_imgs</span><span> </span><span id="textcolor3364"><span>=</span></span><span> np</span><span id="textcolor3365"><span>.</span></span><span>expand_dims(test_imgs,</span><span> </span><span id="textcolor3366"><span>-</span></span><span id="textcolor3367"><span>1</span></span><span>)</span> <span id="x1-129105r4"></span> </code>
<code><span>test_labels</span><span> </span><span id="textcolor3368"><span>=</span></span><span> tf</span><span id="textcolor3369"><span>.</span></span><span>one_hot(test_labels,</span><span> </span><span id="textcolor3370"><span>10</span></span><span>)</span></code></pre>
<p>We can now start to define our model. In this example, we use a small Bayesian neural net with <strong>Flipout</strong> layers. These layers sample from the kernel and bias posteriors during the forward pass and thus add stochasticity to our model. We can use this later on when we want to compute uncertainty values:</p>
<pre id="fancyvrb142" class="fancyvrb"><span id="x1-129129r1"></span> 
<code><span>kl_divergence_function</span><span> </span><span id="textcolor3371"><span>=</span></span><span> </span><span id="textcolor3372"><span>lambda</span></span><span> q,</span><span> p,</span><span> _:</span><span> tfd</span><span id="textcolor3373"><span>.</span></span><span>kl_divergence(q,</span><span> p)</span><span> </span><span id="textcolor3374"><span>/</span></span><span> tf</span><span id="textcolor3375"><span>.</span></span><span>cast(</span> <span id="x1-129131r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3376"><span>60000</span></span><span>,</span><span> dtype</span><span id="textcolor3377"><span>=</span></span><span>tf</span><span id="textcolor3378"><span>.</span></span><span>float32</span> <span id="x1-129133r3"></span> </code>
<code><span>)</span> <span id="x1-129135r4"></span> </code>
<code><span id="x1-129137r5"></span></code>
<code><span>model</span><span> </span><span id="textcolor3379"><span>=</span></span><span> tf</span><span id="textcolor3380"><span>.</span></span><span>keras</span><span id="textcolor3381"><span>.</span></span><span>models</span><span id="textcolor3382"><span>.</span></span><span>Sequential(</span> <span id="x1-129139r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> [</span> <span id="x1-129141r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3383"><span>*</span></span><span>block(</span><span id="textcolor3384"><span>5</span></span><span>),</span> <span id="x1-129143r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3385"><span>*</span></span><span>block(</span><span id="textcolor3386"><span>16</span></span><span>),</span> <span id="x1-129145r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3387"><span>*</span></span><span>block(</span><span id="textcolor3388"><span>120</span></span><span>,</span><span> max_pool</span><span id="textcolor3389"><span>=</span></span><span id="textcolor3390"><span>False</span></span><span>),</span> <span id="x1-129147r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor3391"><span>.</span></span><span>keras</span><span id="textcolor3392"><span>.</span></span><span>layers</span><span id="textcolor3393"><span>.</span></span><span>Flatten(),</span> <span id="x1-129149r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3394"><span>.</span></span><span>layers</span><span id="textcolor3395"><span>.</span></span><span>DenseFlipout(</span> <span id="x1-129151r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3396"><span>84</span></span><span>,</span> <span id="x1-129153r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kernel_divergence_fn</span><span id="textcolor3397"><span>=</span></span><span>kl_divergence_function,</span> <span id="x1-129155r14"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> activation</span><span id="textcolor3398"><span>=</span></span><span>tf</span><span id="textcolor3399"><span>.</span></span><span>nn</span><span id="textcolor3400"><span>.</span></span><span>relu,</span> <span id="x1-129157r15"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-129159r16"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> tfp</span><span id="textcolor3401"><span>.</span></span><span>layers</span><span id="textcolor3402"><span>.</span></span><span>DenseFlipout(</span> <span id="x1-129161r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3403"><span>10</span></span><span>,</span> <span id="x1-129163r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kernel_divergence_fn</span><span id="textcolor3404"><span>=</span></span><span>kl_divergence_function,</span> <span id="x1-129165r19"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> activation</span><span id="textcolor3405"><span>=</span></span><span>tf</span><span id="textcolor3406"><span>.</span></span><span>nn</span><span id="textcolor3407"><span>.</span></span><span>softmax,</span> <span id="x1-129167r20"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ),</span> <span id="x1-129169r21"></span> </code>
<code><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-129171r22"></span> </code>
<code><span>)</span></code></pre>
<p>We define a block as follows:</p>
<pre id="fancyvrb143" class="fancyvrb"><span id="x1-129186r1"></span> 
<code><span id="textcolor3408"><span>def</span></span><span> </span><span id="textcolor3409"><span>block</span></span><span>(filters:</span><span> </span><span id="textcolor3410"><span>int</span></span><span>,</span><span> max_pool:</span><span> </span><span id="textcolor3411"><span>bool</span></span><span> </span><span id="textcolor3412"><span>=</span></span><span> </span><span id="textcolor3413"><span>True</span></span><span>):</span> <span id="x1-129188r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> conv_layer</span><span> </span><span id="textcolor3414"><span>=</span></span><span> </span><span> tfp</span><span id="textcolor3415"><span>.</span></span><span>layers</span><span id="textcolor3416"><span>.</span></span><span>Convolution2DFlipout(</span> <span id="x1-129190r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> filters,</span> <span id="x1-129192r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kernel_size</span><span id="textcolor3417"><span>=</span></span><span id="textcolor3418"><span>5</span></span><span>,</span> <span id="x1-129194r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> padding</span><span id="textcolor3419"><span>=</span></span><span id="textcolor3420"><span>"same"</span></span><span>,</span> <span id="x1-129196r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> kernel_divergence_fn</span><span id="textcolor3421"><span>=</span></span><span>kl_divergence_function,</span> <span id="x1-129198r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> activation</span><span id="textcolor3422"><span>=</span></span><span>tf</span><span id="textcolor3423"><span>.</span></span><span>nn</span><span id="textcolor3424"><span>.</span></span><span>relu)</span> <span id="x1-129200r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3425"><span>if</span></span><span> </span><span id="textcolor3426"><span>not</span></span><span> max_pool:</span> <span id="x1-129202r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor3427"><span>return</span></span><span> (conv_layer,)</span> <span id="x1-129204r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> max_pool</span><span> </span><span id="textcolor3428"><span>=</span></span><span> tf</span><span id="textcolor3429"><span>.</span></span><span>keras</span><span id="textcolor3430"><span>.</span></span><span>layers</span><span id="textcolor3431"><span>.</span></span><span>MaxPooling2D(</span> <span id="x1-129206r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> pool_size</span><span id="textcolor3432"><span>=</span></span><span>[</span><span id="textcolor3433"><span>2</span></span><span>,</span><span> </span><span id="textcolor3434"><span>2</span></span><span>],</span><span> strides</span><span id="textcolor3435"><span>=</span></span><span>[</span><span id="textcolor3436"><span>2</span></span><span>,</span><span> </span><span id="textcolor3437"><span>2</span></span><span>],</span><span> padding</span><span id="textcolor3438"><span>=</span></span><span id="textcolor3439"><span>"same"</span></span> <span id="x1-129208r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> )</span> <span id="x1-129210r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3440"><span>return</span></span><span> conv_layer,</span><span> max_pool</span></code></pre>
<p>We compile our model and <span id="dx1-129211"></span>can start training:</p>
<pre id="fancyvrb144" class="fancyvrb"><span id="x1-129225r1"></span> 
<code><span>model</span><span id="textcolor3441"><span>.</span></span><span>compile(</span> <span id="x1-129227r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> tf</span><span id="textcolor3442"><span>.</span></span><span>keras</span><span id="textcolor3443"><span>.</span></span><span>optimizers</span><span id="textcolor3444"><span>.</span></span><span>Adam(),</span> <span id="x1-129229r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> loss</span><span id="textcolor3445"><span>=</span></span><span id="textcolor3446"><span>"categorical_crossentropy"</span></span><span>,</span> <span id="x1-129231r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> metrics</span><span id="textcolor3447"><span>=</span></span><span>[</span><span id="textcolor3448"><span>"accuracy"</span></span><span>],</span> <span id="x1-129233r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> experimental_run_tf_function</span><span id="textcolor3449"><span>=</span></span><span id="textcolor3450"><span>False</span></span><span>,</span> <span id="x1-129235r6"></span> </code>
<code><span>)</span> <span id="x1-129237r7"></span> </code>
<code><span>model</span><span id="textcolor3451"><span>.</span></span><span>fit(</span> <span id="x1-129239r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> x</span><span id="textcolor3452"><span>=</span></span><span>train_imgs,</span> <span id="x1-129241r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> y</span><span id="textcolor3453"><span>=</span></span><span>train_labels,</span> <span id="x1-129243r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> validation_data</span><span id="textcolor3454"><span>=</span></span><span>(test_imgs,</span><span> test_labels),</span> <span id="x1-129245r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> epochs</span><span id="textcolor3455"><span>=</span></span><span id="textcolor3456"><span>50</span></span> <span id="x1-129247r12"></span> </code>
<code><span>)</span></code></pre>
<p>We are now interested in separating images via epistemic uncertainty and aleatoric uncertainty. Epistemic uncertainty should separate our in-distribution images from out-of-distribution images, as these images can be seen as <span id="dx1-129248"></span>unknown unknowns: our model has never seen these images before, and should therefore assign high epistemic uncertainty (or <em>knowledge uncertainty</em>) to them. Although our model was trained on the AmbiguousMNIST dataset, the model should still have high aleatoric uncertainty when it would see images from this dataset at test time: training with these images does not reduce aleatoric uncertainty (or <em>data uncertainty</em>) as the images are inherently ambiguous.</p>
<p>We use the FashionMNIST dataset as the out-of-distribution dataset. We use the AmbiguousMNIST test set as our ambiguous dataset for testing:</p>
<pre id="fancyvrb145" class="fancyvrb"><span id="x1-129262r1"></span> 
<code><span>(_,</span><span> _),</span><span> (ood_imgs,</span><span> _)</span><span> </span><span id="textcolor3457"><span>=</span></span><span> tf</span><span id="textcolor3458"><span>.</span></span><span>keras</span><span id="textcolor3459"><span>.</span></span><span>datasets</span><span id="textcolor3460"><span>.</span></span><span>fashion_mnist</span><span id="textcolor3461"><span>.</span></span><span>load_data()</span> <span id="x1-129264r2"></span> </code>
<code><span>ood_imgs</span><span> </span><span id="textcolor3462"><span>=</span></span><span> np</span><span id="textcolor3463"><span>.</span></span><span>expand_dims(ood_imgs</span><span> </span><span id="textcolor3464"><span>/</span></span><span> </span><span id="textcolor3465"><span>255.</span></span><span>,</span><span> </span><span id="textcolor3466"><span>-</span></span><span id="textcolor3467"><span>1</span></span><span>)</span> <span id="x1-129266r3"></span> </code>
<code><span id="x1-129268r4"></span></code>
<code><span>ambiguous_mnist_test</span><span> </span><span id="textcolor3468"><span>=</span></span><span> ddu_dirty_mnist</span><span id="textcolor3469"><span>.</span></span><span>AmbiguousMNIST(</span> <span id="x1-129270r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3470"><span>"."</span></span><span>,</span> <span id="x1-129272r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> train</span><span id="textcolor3471"><span>=</span></span><span id="textcolor3472"><span>False</span></span><span>,</span> <span id="x1-129274r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> download</span><span id="textcolor3473"><span>=</span></span><span id="textcolor3474"><span>True</span></span><span>,</span> <span id="x1-129276r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> normalize</span><span id="textcolor3475"><span>=</span></span><span id="textcolor3476"><span>False</span></span><span>,</span> <span id="x1-129278r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> noise_stddev</span><span id="textcolor3477"><span>=</span></span><span id="textcolor3478"><span>0</span></span> <span id="x1-129280r10"></span> </code>
<code><span>)</span> <span id="x1-129282r11"></span> </code>
<code><span>amb_imgs</span><span> </span><span id="textcolor3479"><span>=</span></span><span> ambiguous_mnist_test</span><span id="textcolor3480"><span>.</span></span><span>data</span><span id="textcolor3481"><span>.</span></span><span>numpy()</span><span id="textcolor3482"><span>.</span></span><span>reshape(</span><span id="textcolor3483"><span>60000</span></span><span>,</span><span> </span><span id="textcolor3484"><span>28</span></span><span>,</span><span> </span><span id="textcolor3485"><span>28</span></span><span>,</span><span> </span><span id="textcolor3486"><span>1</span></span><span>)[:</span><span id="textcolor3487"><span>10000</span></span><span>]</span> <span id="x1-129284r12"></span> </code>
<code><span>amb_labels</span><span> </span><span id="textcolor3488"><span>=</span></span><span> tf</span><span id="textcolor3489"><span>.</span></span><span>one_hot(ambiguous_mnist_test</span><span id="textcolor3490"><span>.</span></span><span>targets</span><span id="textcolor3491"><span>.</span></span><span>numpy(),</span><span> </span><span id="textcolor3492"><span>10</span></span><span>)</span><span id="textcolor3493"><span>.</span></span><span>numpy()</span></code></pre>
<p>Let’s use the stochasticity of our model to create a variety of model predictions. We iterate over our test images fifty times:</p>
<pre id="fancyvrb146" class="fancyvrb"><span id="x1-129297r1"></span> 
<code><span>divds_id</span><span> </span><span id="textcolor3494"><span>=</span></span><span> []</span> <span id="x1-129299r2"></span> </code>
<code><span>divds_ood</span><span> </span><span id="textcolor3495"><span>=</span></span><span> []</span> <span id="x1-129301r3"></span> </code>
<code><span>divds_amb</span><span> </span><span id="textcolor3496"><span>=</span></span><span> []</span> <span id="x1-129303r4"></span> </code>
<code><span id="textcolor3497"><span>for</span></span><span> _</span><span> </span><span id="textcolor3498"><span>in</span></span><span> </span><span id="textcolor3499"><span>range</span></span><span>(</span><span id="textcolor3500"><span>50</span></span><span>):</span> <span id="x1-129305r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> divds_id</span><span id="textcolor3501"><span>.</span></span><span>append(model(test_imgs))</span> <span id="x1-129307r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> divds_ood</span><span id="textcolor3502"><span>.</span></span><span>append(model(ood_imgs))</span> <span id="x1-129309r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> divds_amb</span><span id="textcolor3503"><span>.</span></span><span>append(model(amb_imgs))</span> <span id="x1-129311r8"></span> </code>
<code><span id="textcolor3504"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> format</span><span class="cmitt-10x-x-109"> data</span><span class="cmitt-10x-x-109"> such</span><span class="cmitt-10x-x-109"> that</span><span class="cmitt-10x-x-109"> we</span><span class="cmitt-10x-x-109"> have</span><span class="cmitt-10x-x-109"> it</span><span class="cmitt-10x-x-109"> in</span><span class="cmitt-10x-x-109"> shape</span><span class="cmitt-10x-x-109"> n_images,</span><span class="cmitt-10x-x-109"> n_predictions,</span><span class="cmitt-10x-x-109"> n_classes</span></span> <span id="x1-129313r9"></span> </code>
<code><span>divds_id</span><span> </span><span id="textcolor3505"><span>=</span></span><span> np</span><span id="textcolor3506"><span>.</span></span><span>moveaxis(np</span><span id="textcolor3507"><span>.</span></span><span>stack(divds_id),</span><span> </span><span id="textcolor3508"><span>0</span></span><span>,</span><span> </span><span id="textcolor3509"><span>1</span></span><span>)</span> <span id="x1-129315r10"></span> </code>
<code><span>divds_ood</span><span> </span><span id="textcolor3510"><span>=</span></span><span> np</span><span id="textcolor3511"><span>.</span></span><span>moveaxis(np</span><span id="textcolor3512"><span>.</span></span><span>stack(divds_ood),</span><span> </span><span id="textcolor3513"><span>0</span></span><span>,</span><span> </span><span id="textcolor3514"><span>1</span></span><span>)</span> <span id="x1-129317r11"></span> </code>
<code><span>divds_amb</span><span> </span><span id="textcolor3515"><span>=</span></span><span> np</span><span id="textcolor3516"><span>.</span></span><span>moveaxis(np</span><span id="textcolor3517"><span>.</span></span><span>stack(divds_amb),</span><span> </span><span id="textcolor3518"><span>0</span></span><span>,</span><span> </span><span id="textcolor3519"><span>1</span></span><span>)</span></code></pre>
<p>We can then define some <span id="dx1-129318"></span>functions to compute the different kinds of uncertainty:</p>
<pre id="fancyvrb147" class="fancyvrb"><span id="x1-129328r1"></span> 
<code><span id="textcolor3520"><span>def</span></span><span> </span><span id="textcolor3521"><span>total_uncertainty</span></span><span>(divds:</span><span> np</span><span id="textcolor3522"><span>.</span></span><span>ndarray)</span><span> </span><span id="textcolor3523"><span>-</span><em>&gt;</em></span><span> np</span><span id="textcolor3524"><span>.</span></span><span>ndarray:</span> <span id="x1-129330r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3525"><span>return</span></span><span> entropy(np</span><span id="textcolor3526"><span>.</span></span><span>mean(divds,</span><span> axis</span><span id="textcolor3527"><span>=</span></span><span id="textcolor3528"><span>1</span></span><span>),</span><span> axis</span><span id="textcolor3529"><span>=-</span></span><span id="textcolor3530"><span>1</span></span><span>)</span> <span id="x1-129332r3"></span> </code>
<code><span id="x1-129334r4"></span></code>
<code><span id="textcolor3531"><span>def</span></span><span> </span><span id="textcolor3532"><span>data_uncertainty</span></span><span>(divds:</span><span> np</span><span id="textcolor3533"><span>.</span></span><span>ndarray)</span><span> </span><span id="textcolor3534"><span>-</span><em>&gt;</em></span><span> np</span><span id="textcolor3535"><span>.</span></span><span>ndarray:</span> <span id="x1-129336r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3536"><span>return</span></span><span> np</span><span id="textcolor3537"><span>.</span></span><span>mean(entropy(divds,</span><span> axis</span><span id="textcolor3538"><span>=</span></span><span id="textcolor3539"><span>2</span></span><span>),</span><span> axis</span><span id="textcolor3540"><span>=-</span></span><span id="textcolor3541"><span>1</span></span><span>)</span> <span id="x1-129338r6"></span> </code>
<code><span id="x1-129340r7"></span></code>
<code><span id="textcolor3542"><span>def</span></span><span> </span><span id="textcolor3543"><span>knowledge_uncertainty</span></span><span>(divds:</span><span> np</span><span id="textcolor3544"><span>.</span></span><span>ndarray)</span><span> </span><span id="textcolor3545"><span>-</span><em>&gt;</em></span><span> np</span><span id="textcolor3546"><span>.</span></span><span>ndarray:</span> <span id="x1-129342r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3547"><span>return</span></span><span> total_uncertainty(divds)</span><span> </span><span id="textcolor3548"><span>-</span></span><span> data_uncertainty(divds)</span></code></pre>
<p>Finally, we can see how well our model can distinguish between in-distribution, ambiguous, and out-of-distribution images. Let’s plot a histogram of the different distributions according to the different uncertainty methods:</p>
<pre id="fancyvrb148" class="fancyvrb"><span id="x1-129355r1"></span> 
<code><span>labels</span><span> </span><span id="textcolor3549"><span>=</span></span><span> [</span><span id="textcolor3550"><span>"In-distribution"</span></span><span>,</span><span> </span><span id="textcolor3551"><span>"Out-of-distribution"</span></span><span>,</span><span> </span><span id="textcolor3552"><span>"Ambiguous"</span></span><span>]</span> <span id="x1-129357r2"></span> </code>
<code><span>uncertainty_functions</span><span> </span><span id="textcolor3553"><span>=</span></span><span> [total_uncertainty,</span><span> data_uncertainty,</span><span> knowledge_uncertainty]</span> <span id="x1-129359r3"></span> </code>
<code><span>fig,</span><span> axes</span><span> </span><span id="textcolor3554"><span>=</span></span><span> plt</span><span id="textcolor3555"><span>.</span></span><span>subplots(</span><span id="textcolor3556"><span>1</span></span><span>,</span><span> </span><span id="textcolor3557"><span>3</span></span><span>,</span><span> figsize</span><span id="textcolor3558"><span>=</span></span><span>(</span><span id="textcolor3559"><span>20</span></span><span>,</span><span id="textcolor3560"><span>5</span></span><span>))</span> <span id="x1-129361r4"></span> </code>
<code><span id="textcolor3561"><span>for</span></span><span> ax,</span><span> uncertainty</span><span> </span><span id="textcolor3562"><span>in</span></span><span> </span><span id="textcolor3563"><span>zip</span></span><span>(axes,</span><span> uncertainty_functions):</span> <span id="x1-129363r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3564"><span>for</span></span><span> scores,</span><span> label</span><span> </span><span id="textcolor3565"><span>in</span></span><span> </span><span id="textcolor3566"><span>zip</span></span><span>([divds_id,</span><span> divds_ood,</span><span> divds_amb],</span><span> labels):</span> <span id="x1-129365r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> ax</span><span id="textcolor3567"><span>.</span></span><span>hist(uncertainty(scores),</span><span> bins</span><span id="textcolor3568"><span>=</span></span><span id="textcolor3569"><span>20</span></span><span>,</span><span> label</span><span id="textcolor3570"><span>=</span></span><span>label,</span><span> alpha</span><span id="textcolor3571"><span>=</span></span><span id="textcolor3572"><span>.8</span></span><span>)</span> <span id="x1-129367r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> ax</span><span id="textcolor3573"><span>.</span></span><span>title</span><span id="textcolor3574"><span>.</span></span><span>set_text(uncertainty</span><span id="textcolor3575"><span>.</span></span><span id="textcolor3576"><span>__name__</span></span><span id="textcolor3577"><span>.</span></span><span>replace(</span><span id="textcolor3578"><span>"_"</span></span><span>,</span><span> </span><span id="textcolor3579"><span>"</span><span> "</span></span><span>)</span><span id="textcolor3580"><span>.</span></span><span>capitalize())</span> <span id="x1-129369r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> ax</span><span id="textcolor3581"><span>.</span></span><span>legend(loc</span><span id="textcolor3582"><span>=</span></span><span id="textcolor3583"><span>"upper</span><span> right"</span></span><span>)</span> <span id="x1-129371r9"></span> </code>
<code><span>plt</span><span id="textcolor3584"><span>.</span></span><span>legend()</span> <span id="x1-129373r10"></span> </code>
<code><span>plt</span><span id="textcolor3585"><span>.</span></span><span>savefig(</span><span id="textcolor3586"><span>"uncertainty_types.png"</span></span><span>,</span><span> dpi</span><span id="textcolor3587"><span>=</span></span><span id="textcolor3588"><span>300</span></span><span>)</span> <span id="x1-129375r11"></span> </code>
<code><span>plt</span><span id="textcolor3589"><span>.</span></span><span>show()</span></code></pre>
<p>This produces the following output:</p>
<div class="IMG---Figure">
<img src="../media/file159.png" alt="PIC"/> <span id="x1-129376r16"></span> <span id="x1-129377"></span></div>
<p class="IMG---Caption">Figure 7.16: The different types of uncertainty on MNIST 
</p>
<p>What can we observe?</p>
<ul>
<li><p>Total and data uncertainty are <span id="dx1-129378"></span>relatively good at distinguishing in-distribution data from out-of-distribution and ambiguous data.</p></li>
<li><p>However, data and total uncertainty are not able to separate ambiguous data from out-of-distribution data. To do that, we need knowledge uncertainty. We can see that knowledge uncertainty clearly separates ambiguous data from out-of-distribution data.</p></li>
<li><p>We trained on ambiguous samples as well, but that doesn’t reduce the uncertainty of the ambiguous test samples to uncertainty levels similar to the original in-distribution data. This shows that data uncertainty cannot easily be reduced. The data is inherently ambiguous, no matter how much ambiguous data the model sees.</p></li>
</ul>
<p>We can confirm these observations by looking at the AUROC for the different combinations of distributions.</p>
<p>We can first compute the AUROC score to compute the ability of our model to separate in-distribution and ambiguous images from out-of-distribution images:</p>
<pre id="fancyvrb149" class="fancyvrb"><span id="x1-129396r1"></span> 
<code><span id="textcolor3590"><span>def</span></span><span> </span><span id="textcolor3591"><span>auc_id_and_amb_vs_ood</span></span><span>(uncertainty):</span> <span id="x1-129398r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> scores_id</span><span> </span><span id="textcolor3592"><span>=</span></span><span> uncertainty(divds_id)</span> <span id="x1-129400r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> scores_ood</span><span> </span><span id="textcolor3593"><span>=</span></span><span> uncertainty(divds_ood)</span> <span id="x1-129402r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> scores_amb</span><span> </span><span id="textcolor3594"><span>=</span></span><span> uncertainty(divds_amb)</span> <span id="x1-129404r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> scores_id</span><span> </span><span id="textcolor3595"><span>=</span></span><span> np</span><span id="textcolor3596"><span>.</span></span><span>concatenate([scores_id,</span><span> scores_amb])</span> <span id="x1-129406r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> labels</span><span> </span><span id="textcolor3597"><span>=</span></span><span> np</span><span id="textcolor3598"><span>.</span></span><span>concatenate([np</span><span id="textcolor3599"><span>.</span></span><span>zeros_like(scores_id),</span><span> np</span><span id="textcolor3600"><span>.</span></span><span>ones_like(scores_ood)])</span> <span id="x1-129408r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3601"><span>return</span></span><span> roc_auc_score(labels,</span><span> np</span><span id="textcolor3602"><span>.</span></span><span>concatenate([scores_id,</span><span> scores_ood]))</span> <span id="x1-129410r8"></span> </code>
<code><span id="x1-129412r9"></span></code>
<code><span id="x1-129414r10"></span></code>
<code><span id="textcolor3603"><span>print</span></span><span>(</span><span id="textcolor3604"><span>f</span></span><span id="textcolor3605"><span>"</span></span><span id="textcolor3606"><span>{</span></span><span>auc_id_and_amb_vs_ood(total_uncertainty)</span><span id="textcolor3607"><span>=:</span></span><span id="textcolor3608"><span>.2%</span></span><span id="textcolor3609"><span>}</span></span><span id="textcolor3610"><span>"</span></span><span>)</span> <span id="x1-129416r11"></span> </code>
<code><span id="textcolor3611"><span>print</span></span><span>(</span><span id="textcolor3612"><span>f</span></span><span id="textcolor3613"><span>"</span></span><span id="textcolor3614"><span>{</span></span><span>auc_id_and_amb_vs_ood(knowledge_uncertainty)</span><span id="textcolor3615"><span>=:</span></span><span id="textcolor3616"><span>.2%</span></span><span id="textcolor3617"><span>}</span></span><span id="textcolor3618"><span>"</span></span><span>)</span> <span id="x1-129418r12"></span> </code>
<code><span id="textcolor3619"><span>print</span></span><span>(</span><span id="textcolor3620"><span>f</span></span><span id="textcolor3621"><span>"</span></span><span id="textcolor3622"><span>{</span></span><span>auc_id_and_amb_vs_ood(data_uncertainty)</span><span id="textcolor3623"><span>=:</span></span><span id="textcolor3624"><span>.2%</span></span><span id="textcolor3625"><span>}</span></span><span id="textcolor3626"><span>"</span></span><span>)</span> <span id="x1-129420r13"></span> </code>
<code><span id="textcolor3627"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> output:</span></span> <span id="x1-129422r14"></span> </code>
<code><span id="textcolor3628"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_and_amb_vs_ood(total_uncertainty)=91.81%</span></span> <span id="x1-129424r15"></span> </code>
<code><span id="textcolor3629"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_and_amb_vs_ood(knowledge_uncertainty)=98.87%</span></span> <span id="x1-129426r16"></span> </code>
<code><span id="textcolor3630"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_and_amb_vs_ood(data_uncertainty)=84.29%</span></span></code></pre>
<p>We see a confirmation of what we saw in our histograms: knowledge uncertainty is far better than the other two types of uncertainty at separating in-distribution and ambiguous data from out-of-distribution data.</p>
<pre id="fancyvrb150" class="fancyvrb"><span id="x1-129441r1"></span> 
<code><span id="textcolor3631"><span>def</span></span><span> </span><span id="textcolor3632"><span>auc_id_vs_amb</span></span><span>(uncertainty):</span> <span id="x1-129443r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> scores_id,</span><span> scores_amb</span><span> </span><span id="textcolor3633"><span>=</span></span><span> uncertainty(divds_id),</span><span> uncertainty(divds_amb)</span> <span id="x1-129445r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> labels</span><span> </span><span id="textcolor3634"><span>=</span></span><span> np</span><span id="textcolor3635"><span>.</span></span><span>concatenate([np</span><span id="textcolor3636"><span>.</span></span><span>zeros_like(scores_id),</span><span> np</span><span id="textcolor3637"><span>.</span></span><span>ones_like(scores_amb)])</span> <span id="x1-129447r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor3638"><span>return</span></span><span> roc_auc_score(labels,</span><span> np</span><span id="textcolor3639"><span>.</span></span><span>concatenate([scores_id,</span><span> scores_amb]))</span> <span id="x1-129449r5"></span> </code>
<code><span id="x1-129451r6"></span></code>
<code><span id="x1-129453r7"></span></code>
<code><span id="textcolor3640"><span>print</span></span><span>(</span><span id="textcolor3641"><span>f</span></span><span id="textcolor3642"><span>"</span></span><span id="textcolor3643"><span>{</span></span><span>auc_id_vs_amb(total_uncertainty)</span><span id="textcolor3644"><span>=:</span></span><span id="textcolor3645"><span>.2%</span></span><span id="textcolor3646"><span>}</span></span><span id="textcolor3647"><span>"</span></span><span>)</span> <span id="x1-129455r8"></span> </code>
<code><span id="textcolor3648"><span>print</span></span><span>(</span><span id="textcolor3649"><span>f</span></span><span id="textcolor3650"><span>"</span></span><span id="textcolor3651"><span>{</span></span><span>auc_id_vs_amb(knowledge_uncertainty)</span><span id="textcolor3652"><span>=:</span></span><span id="textcolor3653"><span>.2%</span></span><span id="textcolor3654"><span>}</span></span><span id="textcolor3655"><span>"</span></span><span>)</span> <span id="x1-129457r9"></span> </code>
<code><span id="textcolor3656"><span>print</span></span><span>(</span><span id="textcolor3657"><span>f</span></span><span id="textcolor3658"><span>"</span></span><span id="textcolor3659"><span>{</span></span><span>auc_id_vs_amb(data_uncertainty)</span><span id="textcolor3660"><span>=:</span></span><span id="textcolor3661"><span>.2%</span></span><span id="textcolor3662"><span>}</span></span><span id="textcolor3663"><span>"</span></span><span>)</span> <span id="x1-129459r10"></span> </code>
<code><span id="textcolor3664"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> output:</span></span> <span id="x1-129461r11"></span> </code>
<code><span id="textcolor3665"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_vs_amb(total_uncertainty)=94.71%</span></span> <span id="x1-129463r12"></span> </code>
<code><span id="textcolor3666"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_vs_amb(knowledge_uncertainty)=87.06%</span></span> <span id="x1-129465r13"></span> </code>
<code><span id="textcolor3667"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> auc_id_vs_amb(data_uncertainty)=95.21%</span></span></code></pre>
<p>We can see that both total uncertainty and <span id="dx1-129466"></span>data uncertainty are able to separate in-distribution from ambiguous data pretty well. Using data uncertainty gives us a small improvement over using total uncertainty. Knowledge uncertainty, however, is not able to distinguish between in-distribution data and ambiguous data. <span id="x1-129467r197"></span></p>
</section>
</section>
<section id="summary-6" class="level2 sectionHead" data-number="12.4">
<h2 class="sectionHead" data-number="12.4" id="sigil_toc_id_85"><span class="titlemark">7.4 </span> <span id="x1-1300004"></span>Summary</h2>
<p>In this chapter, we’ve taken a look at a number of practical considerations of using Bayesian deep learning: exploring trade-offs in model performance and learning how we can use Bayesian neural network methods to better understand the effects of different uncertainty sources on our data.</p>
<p>In the next chapter, we’ll dig further into applying BDL through a variety of case studies, demonstrating the benefits of these methods in a range of practical settings. <span id="x1-130001r212"></span></p>
</section>
<section id="further-reading-4" class="level2 sectionHead" data-number="12.5">
<h2 class="sectionHead" data-number="12.5" id="sigil_toc_id_86"><span class="titlemark">7.5 </span> <span id="x1-1310005"></span>Further reading</h2>
<ul>
<li><p><em>Practical Considerations for Probabilistic Backpropagation</em>, Matt Benatan <em>et al.</em>: In this paper, the authors explore methods to get the most out of PBP, demonstrating how different early stopping approaches can be used to improve training, exploring the tradeoffs associated with mini-batching, and more</p></li>
<li><p><em>Modeling aleatoric and epistemic uncertainty using TensorFlow and</em> <em>TensorFlow Probability</em>, Alexander Molak: In this Jupyter notebook, the author shows how to model aleatoric and epistemic uncertainty on regression toy data</p></li>
<li><p><em>Weight Uncertainty in Neural Networks</em>, Charles Blundell <em>et al.</em>: In this paper, the authors introduce BBB, which we use in the regression case study and is one of the key pieces of BDL literature</p></li>
<li><p><em>Deep Deterministic Uncertainty: A Simple Baseline</em>, Jishnu Mukhoti <em>et al.</em>: In this work, the authors describe several experiments related to the different types of uncertainty and introduce the <em>AmbiguousMNIST</em> dataset that we used in the last case study</p></li>
<li><p><em>Uncertainty Estimation in Deep Learning with application to Spoken</em> <em>Language Assessment</em>, Andrey Malinin: This thesis highlights the different sources of uncertainty with intuitive examples</p></li>
</ul>
<p><span id="x1-131001r179"></span></p>
</section>
</section>
</body>
</html>