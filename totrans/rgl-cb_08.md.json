["```py\nkaggle datasets download -d crowdflower/twitter-airline-sentiment --unzip\n```", "```py\npip install pandas numpy scikit-learn matplotlib torch transformers\n```", "```py\n        import torch import torch.nn as nn\n        ```", "```py\n        import torch.optim as optim from torch.utils.data\n        ```", "```py\n        import DataLoader, Dataset from sklearn.model_selection\n        ```", "```py\n        import train_test_split from sklearn.preprocessing\n        ```", "```py\n        import LabelEncoder from transformers\n        ```", "```py\n        import AutoTokenizer\n        ```", "```py\n        import pandas as pd\n        ```", "```py\n        import matplotlib.pyplot as plt\n        ```", "```py\n    # Load data\n    ```", "```py\n    data = pd.read_csv('Tweets.csv')\n    ```", "```py\n    data[['airline_sentiment', 'text']].head()\n    ```", "```py\n# Split data into train and test sets\ntrain_data, test_data = train_test_split(data,\n    test_size=0.2, random_state=0)\n```", "```py\n    # Define dataset class class TextClassificationDataset(Dataset):\n    ```", "```py\n    def __init__(self, data, max_length):\n    ```", "```py\n        self.data = data\n    ```", "```py\n        self.tokenizer = AutoTokenizer.from_pretrained(\n    ```", "```py\n            'bert-base-uncased')\n    ```", "```py\n        self.tokens = self.tokenizer(\n    ```", "```py\n            data['text'].to_list(), padding=True,\n    ```", "```py\n            truncation=True, max_length=max_length,\n    ```", "```py\n            return_tensors='pt')['input_ids']\n    ```", "```py\n        le = LabelEncoder()\n    ```", "```py\n        self.labels = torch.tensor(le.fit_transform(\n    ```", "```py\n            data['airline_sentiment']))\n    ```", "```py\n    def __len__(self):\n    ```", "```py\n        return len(self.data)\n    ```", "```py\n    def __getitem__(self, index):\n    ```", "```py\n        return self.tokens[index], self.labels[index]\n    ```", "```py\nbatch_size = 64 max_length = 24\n# Initialize datasets and dataloaders\ntrain_dataset = TextClassificationDataset(train_data,\n    max_length)\ntest_dataset = TextClassificationDataset(test_data,\n    max_length)\ntrain_dataloader = DataLoader(train_dataset,\n    batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset,\n    batch_size=batch_size, shuffle=True)\n```", "```py\n# Define RNN model\nclass RNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim,\n        hidden_size, output_size, num_layers=3):\n            super(RNNClassifier, self).__init__()\n            self.num_layers = num_layers\n            self.hidden_size = hidden_size\n            self.embedding = nn.Embedding(\n                num_embeddings=vocab_size,\n                embedding_dim=embedding_dim)\n            self.rnn = nn.RNN(\n                input_size=embedding_dim,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                nonlinearity='relu',\n                batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        zero_hidden = torch.zeros(self.num_layers,\n            batch_size, self.hidden_size)\n        embedded = self.embedding(inputs)\n        output, hidden = self.rnn(embedded, zero_hidden)\n        output = torch.softmax(self.fc(output[:, -1]),\n            dim=1)\n        return output\n```", "```py\nvocab_size = train_dataset.tokenizer.vocab_size\nembedding_dim = 64\nhidden_dim = 64\noutput_size = 3\nmodel = RNNClassifier(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_size=hidden_dim,\n    output_size=output_size, )\nrandom_data = torch.randint(0, vocab_size,\n    size=(batch_size, max_length))\nresult = model(random_data)\nprint('Resulting output tensor:', result.shape) print('Sum of the output tensor:', result.sum())\n```", "```py\nResulting output tensor: torch.Size([64, 3]) Sum of the output tensor: tensor(64.0000, grad_fn=<SumBackward0>)\n```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n```", "```py\ndef epoch_step_tweet(model, dataloader,\n    training_set: bool):\n        running_loss = 0\n        correct = 0.\n    for i, data in enumerate(dataloader, 0):\n        # Get the inputs: data is a list of [inputs, labels]\n        inputs, labels = data\n        if training_set:\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n        # Forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels) .long()\n        if training_set:\n            loss.backward()\n            optimizer.step()\n        # Add correct predictions for this batch\n        correct += (outputs.argmax(\n            dim=1) == labels).float().sum()\n        # Compute loss for this batch\n        running_loss += loss.item()\n    return running_loss, correct\n```", "```py\ndef train_tweet_classification(model,\n    train_dataloader, test_dataloader, criterion,\n    epochs: int = 20):\n        # Train the model\n        train_losses = []\n        test_losses = []\n        train_accuracy = []\n        test_accuracy = []\n    for epoch in range(20):\n        running_train_loss = 0.\n        correct = 0.\n        model.train()\n        running_train_loss,\n        correct = epoch_step_tweet(model,\n            dataloader=train_dataloader,\n            training_set=True)\n        # Compute and store loss and accuracy for this epoch\n        train_epoch_loss = running_train_loss / len(\n            train_dataloader)\n        train_losses.append(train_epoch_loss)\n        train_epoch_accuracy = correct / len(\n            train_dataset)\n        train_accuracy.append(train_epoch_accuracy)\n        ## Evaluate the model on the test set\n        running_test_loss = 0.\n        correct = 0.\n        model.eval()\n        with torch.no_grad():\n            running_test_loss,\n            correct = epoch_step_tweet(model,\n                dataloader=test_dataloader,\n                training_set=False)\n            test_epoch_loss = running_test_loss / len(\n                test_dataloader)\n            test_losses.append(test_epoch_loss)\n            test_epoch_accuracy = correct / len(\n            test_dataset)\n            test_accuracy.append(test_epoch_accuracy)\n        # Print stats\n        print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\\\n    \\t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')\n    return train_losses, test_losses, train_accuracy,\n    test_accuracy\n```", "```py\ntrain_losses, test_losses, train_accuracy, test_accuracy = train_tweet_classification(model,\n    train_dataloader, test_dataloader, criterion,\n    epochs=20)\n```", "```py\n[epoch 20] Training: loss=0.727 accuracy=0.824 |  Test: loss=0.810 accuracy=0.738\n```", "```py\nplt.plot(train_losses, label='train')\nplt.plot(test_losses, label='test')\nplt.xlabel('epoch') plt.ylabel('loss (CE)')\nplt.legend() plt.show()\n```", "```py\nplt.plot(train_accuracy, label='train')\nplt.plot(test_accuracy, label='test')\nplt.xlabel('epoch') plt.ylabel('Accuracy')\nplt.legend() plt.show()\n```", "```py\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n```", "```py\ntokenizer.tokenize(\"Let's use regularization in ML. Regularization should help to improve model robustness\")\n```", "```py\n['let',  \"'\",  's',  'use',  'regular',  '##ization',  'in',\n  'ml',  '.',  'regular',  '##ization',  'should',  'help',\n  'to',  'improve',  'model',  'robust',  '##ness']\n```", "```py\ntokenizer(\"Let's use regularization in ML. Regularization should help to improve model robustness\")['input_ids']\n```", "```py\n[101,  2292,  1005,  1055,  2224,  3180,  3989,  1999,  19875,\n  1012,  3180,  3989,  2323,  2393,  2000,  5335,  2944,  15873,\n  2791,  102]\n```", "```py\nkaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip\n```", "```py\npip install pandas numpy scikit-learn matplotlib torch transformers\n```", "```py\n        import torch\n        ```", "```py\n        import torch.nn as nn\n        ```", "```py\n        import torch.optim as optim from torch.utils.data\n        ```", "```py\n        import DataLoader,Dataset from sklearn.model_selection\n        ```", "```py\n        import train_test_split from sklearn.preprocessing\n        ```", "```py\n        import LabelEncoder from transformers\n        ```", "```py\n        import AutoTokenizer\n        ```", "```py\n        import pandas as pd\n        ```", "```py\n        import numpy as np\n        ```", "```py\n        import matplotlib.pyplot as plt\n        ```", "```py\n    # Load data\n    ```", "```py\n    data = pd.read_csv('IMDB Dataset.csv')\n    ```", "```py\n    data.head()\n    ```", "```py\n                                   review    sentiment 0  One of the other reviewers has mentioned that ...       positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...     positive\n3  Basically there's a family where a little boy ...       negative\n4  Petter Mattei's \"Love in the Time of Money\" is...     positive\n```", "```py\n    # Split data into train and test sets\n    ```", "```py\n    Train_data, test_data = train_test_split(data,\n    ```", "```py\n        test_size=0.2, random_state=0)\n    ```", "```py\n        # Define dataset class\n        ```", "```py\n        class TextClassificationDataset(Dataset):\n        ```", "```py\n            def __init__(self, data, max_length):\n        ```", "```py\n                self.data = data\n        ```", "```py\n                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n        ```", "```py\n                self.tokens = self.tokenizer(\n        ```", "```py\n                    data['review'].to_list(), padding=True,\n        ```", "```py\n                    truncation=True, max_length=max_length,\n        ```", "```py\n                    return_tensors='pt')['input_ids']\n        ```", "```py\n                le = LabelEncoder()\n        ```", "```py\n                self.labels = torch.tensor(le.fit_transform(\n        ```", "```py\n                    data['sentiment']).astype(np.float32))\n        ```", "```py\n            def __len__(self):\n        ```", "```py\n                return len(self.data)\n        ```", "```py\n            def __getitem__(self, index):\n        ```", "```py\n                return self.tokens[index],self.labels[index]\n        ```", "```py\n    batch_size = 64\n    ```", "```py\n    max_words = 64\n    ```", "```py\n    # Initialize datasets and dataloaders\n    ```", "```py\n    train_dataset = TextClassificationDataset(train_data,\n    ```", "```py\n        max_words)\n    ```", "```py\n    test_dataset = TextClassificationDataset(test_data,\n    ```", "```py\n        max_words)\n    ```", "```py\n    train_dataloader = DataLoader(train_dataset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    test_dataloader = DataLoader(test_dataset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n        # Define GRU model\n        ```", "```py\n        class GRUClassifier(nn.Module):\n        ```", "```py\n            def __init__(self, vocab_size, embedding_dim,\n        ```", "```py\n                hidden_size, output_size, num_layers=3):\n        ```", "```py\n                    super(GRUClassifier, self).__init__()\n        ```", "```py\n                    self.num_layers = num_layers\n        ```", "```py\n                    self.hidden_size = hidden_size\n        ```", "```py\n                    self.embedding = nn.Embedding(\n        ```", "```py\n                        num_embeddings=vocab_size,\n        ```", "```py\n                        embedding_dim=embedding_dim)\n        ```", "```py\n                    self.gru = nn.GRU(\n        ```", "```py\n                        input_size=embedding_dim,\n        ```", "```py\n                        hidden_size=hidden_size,\n        ```", "```py\n                        num_layers=num_layers,\n        ```", "```py\n                        batch_first=True)\n        ```", "```py\n                    self.fc = nn.Linear(hidden_size,\n        ```", "```py\n                        output_size)\n        ```", "```py\n            def forward(self, inputs):\n        ```", "```py\n                batch_size = inputs.size(0)\n        ```", "```py\n                zero_hidden = torch.zeros(\n        ```", "```py\n                    self.num_layers, batch_size,\n        ```", "```py\n                    self.hidden_size).to(device)\n        ```", "```py\n                embedded = self.embedding(inputs)\n        ```", "```py\n                output, hidden = self.gru(embedded,\n        ```", "```py\n                    zero_hidden)\n        ```", "```py\n                output = torch.sigmoid(self.fc(output[:, -1]))\n        ```", "```py\n                return output\n        ```", "```py\n    vocab_size = train_dataset.tokenizer.vocab_size\n    ```", "```py\n    embedding_dim = 32\n    ```", "```py\n    hidden_dim = 32\n    ```", "```py\n    output_size = 1\n    ```", "```py\n    # Optionally, set the device to GPU if you have one device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    model = GRUClassifier(\n    ```", "```py\n        vocab_size=vocab_size,\n    ```", "```py\n        embedding_dim=embedding_dim,\n    ```", "```py\n        hidden_size=hidden_dim,\n    ```", "```py\n        output_size=output_size,\n    ```", "```py\n    ).to(device)\n    ```", "```py\n    random_data = torch.randint(0, vocab_size,\n    ```", "```py\n        size=(batch_size, max_words)).to(device)\n    ```", "```py\n    result = model(random_data)\n    ```", "```py\n    print('Resulting output tensor:', result.shape)\n    ```", "```py\n    print('Sum of the output tensor:', result.sum())\n    ```", "```py\nResulting output tensor: torch.Size([64, 1]) Sum of the output tensor: tensor(31.0246, device='cuda:0', grad_fn=<SumBackward0>)\n```", "```py\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    ```", "```py\n    criterion = nn.BCELoss()\n    ```", "```py\ndef epoch_step_IMDB(model, dataloader, device,\n    training_set: bool):\n        running_loss = 0.\n        correct = 0.\n        for i, data in enumerate(dataloader, 0):\n  # Get the inputs: data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.unsqueeze(1).to(device)\n            if training_set:\n                # Zero the parameter gradients\n                optimizer.zero_grad()\n                # Forward + backward + optimize\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            if training_set:\n                loss.backward()\n                optimizer.step()\n             # Add correct predictions for this batch\n                correct += (\n             (outputs > 0.5) == labels).float().sum()\n                # Compute loss for this batch\n                running_loss += loss.item()\n    return running_loss, correct\n```", "```py\ndef train_IMDB_classification(model, train_dataloader,\n    test_dataloader, criterion, device,\n    epochs: int = 20):\n        # Train the model\n        train_losses = []\n        test_losses = []\n        train_accuracy = []\n        test_accuracy = []\n    for epoch in range(20):\n        running_train_loss = 0.\n        correct = 0.\n        model.train()\n        running_train_loss, correct = epoch_step_IMDB(\n            model, train_dataloader, device,\n                training_set=True\n        )\n        # Compute and store loss and accuracy for this epoch\n        train_epoch_loss = running_train_loss / len(\n            train_dataloader)\n        train_losses.append(train_epoch_loss)\n        train_epoch_accuracy = correct / len(\n            train_dataset)\n        train_accuracy.append(\n            train_epoch_accuracy.cpu().numpy())\n        ## Evaluate the model on the test set\n        running_test_loss = 0.\n        correct = 0.\n        model.eval()\n        with torch.no_grad():\n            running_test_loss,\n            correct = epoch_step_IMDB(\n                model, test_dataloader, device,\n                training_set=False\n            )\n            test_epoch_loss = running_test_loss / len(\n                test_dataloader)\n            test_losses.append(test_epoch_loss)\n            test_epoch_accuracy = correct / len(\n                test_dataset)\n            test_accuracy.append(\n                test_epoch_accuracy.cpu().numpy())\n        # Print stats\n        print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\\\n    \\t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')\n    return train_losses, test_losses, train_accuracy,\n        test_accuracy\n```", "```py\n    train_losses, test_losses, train_accuracy, test_accuracy = train_IMDB_classification(model,\n    ```", "```py\n        train_dataloader, test_dataloader, criterion,\n    ```", "```py\n        device, epochs=20)\n    ```", "```py\n[epoch 20] Training: loss=0.040 accuracy=0.991 |  Test: loss=1.155 accuracy=0.751\n```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('loss (BCE)')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim,\n        hidden_size, output_size, num_layers=3):\n            super(LSTMClassifier, self).__init__()\n            self.hidden_size = hidden_size\n            self.num_layers = num_layers\n            self.embedding = nn.Embedding(\n                num_embeddings=vocab_size,\n                embedding_dim=embedding_dim)\n                self.lstm = nn.LSTM(\n                    input_size=embedding_dim,\n                    hidden_size=hidden_size,\n                    num_layers=num_layers,\n                    batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        h_0 = torch.zeros(self.num_layers, batch_size,\n            self.hidden_size)\n        c_0 = torch.zeros(self.num_layers, batch_size,\n            self.hidden_size)\n        embedded = self.embedding(inputs)\n        output,\n        (final_hidden_state, final_cell_state) = self.lstm(\n            embedded, (h_0, c_0))\n        output = torch.softmax(self.fc(output[:, -1]),\n            dim=1)\n        return output\n```", "```py\nsum(p.numel() for p in list(\n    model.parameters())[1:] if p.requires_grad)\n```", "```py\nkaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip\n```", "```py\npip install pandas numpy scikit-learn matplotlib torch transformers\n```", "```py\n        # Define GRU model\n        ```", "```py\n        class GRUClassifier(nn.Module):\n        ```", "```py\n            def __init__(self, vocab_size, embedding_dim,\n        ```", "```py\n                hidden_size, output_size, num_layers=3,\n        ```", "```py\n                dropout=0.25):\n        ```", "```py\n                    super(GRUClassifier, self).__init__()\n        ```", "```py\n                    self.num_layers = num_layers\n        ```", "```py\n                    self.hidden_size = hidden_size\n        ```", "```py\n                    self.embedding = nn.Embedding(\n        ```", "```py\n                        num_embeddings=vocab_size,\n        ```", "```py\n                        embedding_dim=embedding_dim)\n        ```", "```py\n                    self.dropout = nn.Dropout(dropout)\n        ```", "```py\n                    self.gru = nn.GRU(\n        ```", "```py\n                        input_size=embedding_dim,\n        ```", "```py\n                        hidden_size=hidden_size,\n        ```", "```py\n                        num_layers=num_layers,\n        ```", "```py\n                        batch_first=True, dropout=dropout)\n        ```", "```py\n                    self.fc = nn.Linear(hidden_size,\n        ```", "```py\n                        output_size)\n        ```", "```py\n            def forward(self, inputs):\n        ```", "```py\n                batch_size = inputs.size(0)\n        ```", "```py\n                zero_hidden = torch.zeros(self.num_layers,\n        ```", "```py\n                    batch_size, self.hidden_size).to(device)\n        ```", "```py\n                embedded = self.dropout(\n        ```", "```py\n                    self.embedding(inputs))\n        ```", "```py\n                output, hidden = self.gru(embedded,\n        ```", "```py\n                    zero_hidden)\n        ```", "```py\n                output = torch.sigmoid(self.fc(output[:, -1]))\n        ```", "```py\n                return output\n        ```", "```py\n    vocab_size = train_dataset.tokenizer.vocab_size\n    ```", "```py\n    embedding_dim = 32 hidden_dim = 32 output_size = 1\n    ```", "```py\n    # Optionally, set the device to GPU if you have one\n    ```", "```py\n    device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    model = GRUClassifier(\n    ```", "```py\n        vocab_size=vocab_size,\n    ```", "```py\n        embedding_dim=embedding_dim,\n    ```", "```py\n        hidden_size=hidden_dim,\n    ```", "```py\n         output_size=output_size, ).to(device)\n    ```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.BCELoss()\n```", "```py\n    train_losses, test_losses, train_accuracy, \n    ```", "```py\n    test_accuracy = train_IMDB_classification(model,\n    ```", "```py\n        train_dataloader, test_dataloader, criterion,\n    ```", "```py\n        device, epochs=20)\n    ```", "```py\n[epoch 20] Training: loss=0.248 accuracy=0.896 |  Test: loss=0.550 accuracy=0.785\n```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('loss (BCE)')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\nkaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip\n```", "```py\npip install pandas numpy scikit-learn matplotlib torch transformers\n```", "```py\nbatch_size = 64 max_words = 256\n# Initialize datasets and dataloaders\nTrain_dataset = TextClassificationDataset(train_data,\n    max_words)\ntest_dataset = TextClassificationDataset(test_data,\n    max_words)\ntrain_dataloader = DataLoader(train_dataset,\n    batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset,\n    batch_size=batch_size, shuffle=True)\n```", "```py\n    vocab_size = train_dataset.tokenizer.vocab_size\n    ```", "```py\n    embedding_dim = 32\n    ```", "```py\n    hidden_dim = 32\n    ```", "```py\n    output_size = 1\n    ```", "```py\n    # Optionally, set the device to GPU if you have one\n    ```", "```py\n    device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    model = GRUClassifier(\n    ```", "```py\n        vocab_size=vocab_size,\n    ```", "```py\n        embedding_dim=embedding_dim,\n    ```", "```py\n        hidden_size=hidden_dim,\n    ```", "```py\n        output_size=output_size, ).to(device)\n    ```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.BCELoss()\n```", "```py\n    train_losses, test_losses, train_accuracy, \n    ```", "```py\n    test_accuracy = train_IMDB_classification(model,\n    ```", "```py\n        train_dataloader, test_dataloader, criterion,\n    ```", "```py\n        device, epochs=20)\n    ```", "```py\n[epoch 20] Training: loss=0.022 accuracy=0.995 |  Test: loss=0.640 accuracy=0.859\n```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('loss (BCE)')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nreview_lengths = [len(tokens) for tokens in tokenizer(\n    train_data['review'].to_list())['input_ids']]\n```", "```py\nplt.hist(review_lengths, bins=50, log=True)\nplt.xlabel('Review length (#tokens)') plt.show()\n```", "```py\nprint('Average length:', np.mean(review_lengths))\nprint('Median length:', np.median(review_lengths))\n```", "```py\nAverage length: 309.757075 Median length: 231.0\n```"]