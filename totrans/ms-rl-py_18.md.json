["```py\n$ virtualenv pybenv\n$ source pybenv/bin/activate\n$ pip install pybullet --upgrade\n$ pip install gym\n$ pip install tensorflow==2.3.1\n$ pip install ray[rllib]==1.0.0\n$ pip install scikit-learn==0.23.2\n```", "```py\n$ python -m pybullet_envs.examples.enjoy_TF_AntBulletEnv_v0_2017may\n```", "```py\n    class CustomKuka(Kuka):\n        def __init__(self, *args, jp_override=None, **kwargs):\n            self.jp_override = jp_override\n            super(CustomKuka, self).__init__(*args, **kwargs)\n    ```", "```py\n        def reset(self):\n        ...\n            if self.jp_override:\n                for j, v in self.jp_override.items():\n                    j_ix = int(j) - 1\n                    if j_ix >= 0 and j_ix <= 13:\n                        self.jointPositions[j_ix] = v\n    ```", "```py\n    class CustomKukaEnv(KukaGymEnv):\n        def __init__(self, env_config={}):\n            renders = env_config.get(\"renders\", False)\n            isDiscrete = env_config.get(\"isDiscrete\", False)\n            maxSteps = env_config.get(\"maxSteps\", 2000)\n            self.rnd_obj_x = env_config.get(\"rnd_obj_x\", 1)\n            self.rnd_obj_y = env_config.get(\"rnd_obj_y\", 1)\n            self.rnd_obj_ang = env_config.get(\"rnd_obj_ang\", 1)\n            self.bias_obj_x = env_config.get(\"bias_obj_x\", 0)\n            self.bias_obj_y = env_config.get(\"bias_obj_y\", 0)\n            self.bias_obj_ang = env_config.get(\"bias_obj_ang\", 0)\n            self.jp_override = env_config.get(\"jp_override\")\n            super(CustomKukaEnv, self).__init__(\n                renders=renders, isDiscrete=isDiscrete, maxSteps=maxSteps\n            )\n    ```", "```py\n        def reset(self):\n            ...\n            xpos = 0.55 + self.bias_obj_x + 0.12 * random.random() * self.rnd_obj_x\n            ypos = 0 + self.bias_obj_y + 0.2 * random.random() * self.rnd_obj_y\n            ang = (\n                3.14 * 0.5\n                + self.bias_obj_ang\n                + 3.1415925438 * random.random() * self.rnd_obj_ang\n            )\n    ```", "```py\n            ...\n            self._kuka = CustomKuka(\n                jp_override=self.jp_override,\n                urdfRootPath=self._urdfRoot,\n                timeStep=self._timeStep,\n            )\n    ```", "```py\n        def step(self, action):\n            dz = -0.0005\n            ...\n    \t\t...\n                realAction = [dx, dy, dz, da, f]\n            obs, reward, done, info = self.step2(realAction)\n            return obs, reward / 1000, done, info\n    ```", "```py\n        def increase_difficulty(self):\n            deltas = {\"2\": 0.1, \"4\": 0.1}\n            original_values = {\"2\": 0.413184, \"4\": -1.589317}\n            all_at_original_values = True\n            for j in deltas:\n                if j in self.jp_override:\n                    d = deltas[j]\n                    self.jp_override[j] = max(self.jp_override[j] - d, original_values[j])\n                    if self.jp_override[j] != original_values[j]:\n                        all_at_original_values = False\n    ```", "```py\n            self.rnd_obj_x = min(self.rnd_obj_x + 0.05, 1)\n            self.rnd_obj_y = min(self.rnd_obj_y + 0.05, 1)\n            self.rnd_obj_ang = min(self.rnd_obj_ang + 0.05, 1)\n    ```", "```py\n            if self.rnd_obj_x == self.rnd_obj_y == self.rnd_obj_ang == 1:\n                if all_at_original_values:\n                    self.bias_obj_x = 0\n                    self.bias_obj_y = 0\n                    self.bias_obj_ang = 0\n    ```", "```py\nconfig[\"env_config\"] = {\n    \"jp_override\": {\"2\": 1.3, \"4\": -1}, \"rnd_obj_x\": 0, \n    \"rnd_obj_y\": 0, \"rnd_obj_ang\": 0, \"bias_obj_y\": 0.04}\ndef on_train_result(info):\n    result = info[\"result\"]\n    if result[\"episode_reward_mean\"] > 5.5:\n        trainer = info[\"trainer\"]\n        trainer.workers.foreach_worker(\n            lambda ev: ev.foreach_env(lambda env: env.increase_difficulty()))\nray.init()\ntune.run(\"PPO\", config=dict(config,\n                            **{\"env\": CustomKukaEnv,\n                               \"callbacks\": {\n                          \"on_train_result\": on_train_result}}\n                            ),\n          checkpoint_freq=10)\n```", "```py\n    class ALPKukaEnv(CustomKukaEnv):\n        def __init__(self, env_config={}):\n            ...\n            self.mins = [...]\n            self.maxs =  [...]\n            self.alp = ALPGMM(mins=self.mins, \n                         maxs=self.maxs, \n                               params={\"fit_rate\": 20})\n            self.task = None\n            self.last_episode_reward = None\n            self.episode_reward = 0\n            super(ALPKukaEnv, self).__init__(env_config)\n    ```", "```py\n        def reset(self):\n            if self.task is not None and self.last_episode_reward is not None:\n                self.alp.update(self.task, \n                                self.last_episode_reward)\n            self.task = self.alp.sample_task()\n            self.rnd_obj_x = self.task[0]\n            self.rnd_obj_y = self.task[1]\n            self.rnd_obj_ang = self.task[2]\n            self.jp_override = {\"2\": self.task[3], \n                                \"4\": self.task[4]}\n            self.bias_obj_y = self.task[5]\n            return super(ALPKukaEnv, self).reset()\n    ```", "```py\n        def step(self, action):\n            obs, reward, done, info = super(ALPKukaEnv,  self).step(action)\n            self.episode_reward += reward\n            if done:\n                self.last_episode_reward = self.episode_reward\n                self.episode_reward = 0\n            return obs, reward, done, info\n    ```", "```py\n    Agent ALP-GMM score: 3.71\n    Agent Manual score: -2.79\n    Agent No Curriculum score: 2.81\n    ```"]