<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer084">
			<h1 id="_idParaDest-96"><em class="italic"><a id="_idTextAnchor095"/>Chapter 8</em>: Deploying a DL Inference Pipeline at Scale</h1>
			<p>Deploying a <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) inference pipeline for production usage is both exciting and challenging. The exciting part is that, finally, the DL model pipeline can be used for prediction with real-world production data, which will provide real value to the business scenarios. However, the challenging part is that there are different DL model serving platforms and host environments. It is not easy to choose the right framework for the right model serving scenarios, which can minimize deployment complexity but provide the best model serving experiences in a scalable and cost-effective way. This chapter will cover the topics as an overview of different deployment scenarios and host environments, and then provide hands-on learning on how to deploy to different environments, including local and remote cloud environments using MLflow deployment tools. By the end of this chapter, you should be able to confidently deploy an MLflow DL inference pipeline to various host environments for either batching or real-time inference services.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding the landscape of deployment and hosting environments</li>
				<li>Deploying locally for batch and web service inference</li>
				<li>Deploying using Ray Serve and MLflow deployment plugins</li>
				<li>Deploying to AWS SageMaker – a complete end-to-end guide</li>
			</ul>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Technical requirements </h1>
			<p>The following items are required for this chapter's learning:</p>
			<ul>
				<li>GitHub repository code for this chapter: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08</a>.</li>
				<li>Ray serve and <strong class="source-inline">mlflow-ray-serve</strong> plugin: <a href="https://github.com/ray-project/mlflow-ray-serve">https://github.com/ray-project/mlflow-ray-serve</a>.</li>
				<li>AWS SageMaker: You will need to have an AWS account. You can create a free AWS account easily through the free signup website at <a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>.</li>
				<li>AWS <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>): <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a>.</li>
				<li>Docker Desktop: <a href="https://www.docker.com/products/docker-desktop/">https://www.docker.com/products/docker-desktop/</a>.</li>
				<li>Complete the example in <a href="B18120_07_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 7</em></a>, <em class="italic">Multi-Step Deep Learning Inference Pipeline,</em> of this book. This will give you a ready-to-deploy inference pipeline to use in this chapter.</li>
			</ul>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Understanding different deployment tools and host environments</h1>
			<p>There are different deployment tools in the MLOps technology stack that have different target use cases and host environments for deploying different model inference pipelines. In <a href="B18120_07_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 7</em></a>, <em class="italic">Multi-Step Deep Learning Inference Pipeline</em>, we learned the different inference scenarios and requirements and implemented a multi-step DL inference pipeline that can be deployed into a model hosting/serving environment. Now, we will learn how to deploy such a model to a few specific model hosting and serving environments. This is visualized in <em class="italic">Figure 8.1</em> as follows:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="Images/B18120_08_01.jpg" alt="Figure 8.1 – Using model deployment tools to deploy a model inference pipeline to &#13;&#10;a model hosting and serving environment&#13;&#10;" width="1186" height="347"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Using model deployment tools to deploy a model inference pipeline to a model hosting and serving environment</p>
			<p>As can be seen from <em class="italic">Figure 8.1</em>, there can be different deployment tools for different model hosting and serving environments. Here, we list the three typical scenarios as follows:</p>
			<ul>
				<li><strong class="bold">Batch inference at scale</strong>: If we want <a id="_idIndexMarker504"/>to do batch inference at a regular schedule, we can use the PySpark <strong class="bold">user defined function </strong>(<strong class="bold">UDF</strong>) to load an MLflow model <a id="_idIndexMarker505"/>flavor to do this, since we can leverage Spark's scalable computational approach on a distributed cluster (<a href="https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf">https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf</a>). We will show an example of how to do this in the next section. </li>
				<li><strong class="bold">Streaming inference at scale</strong>: This usually requires an endpoint that hosts the <strong class="bold">Model as a Service</strong> (<strong class="bold">MaaS</strong>). There exist quite a few tools and frameworks for <a id="_idIndexMarker506"/>production-grade deployment and model serving. We will compare a few tools in this section to understand how they work and how well they integrate with MLflow before we start learning how to do this type of deployment in this chapter.</li>
				<li><strong class="bold">On-device model inference</strong>: This is an emerging area called <strong class="bold">TinyML</strong>, which deploys ML/DL models in <a id="_idIndexMarker507"/>a resource-limited environment such as mobile, sensor, or edge device (<a href="https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html">https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html</a>). Two popular frameworks are <a id="_idIndexMarker508"/>PyTorch Mobile (<a href="https://pytorch.org/mobile/home/">https://pytorch.org/mobile/home/</a>) and TensorFlow Lite (<a href="https://www.tensorflow.org/lite">https://www.tensorflow.org/lite</a>). This is not the focus of this book. You are <a id="_idIndexMarker509"/>encouraged to check out some further reading <a id="_idIndexMarker510"/>for this area at the end of this chapter.</li>
			</ul>
			<p>Now, let's look at what kind of tools are available for deploying model inference as a service, especially those tools that have support for MLflow model deployment. There are three types of model deployment and serving tools, as follows:</p>
			<ul>
				<li><strong class="bold">MLflow built-in model deployment</strong>: This comes out of the box from MLflow releases, which includes deployments to a local web server, AWS SageMaker, and <a id="_idIndexMarker511"/>Azure ML. There is also a managed MLflow on Databricks that supports model serving in public review as of this writing, which we will not cover in this book since this is well presented in the official Databricks documentation (interested readers <a id="_idIndexMarker512"/>should look up the official documentation on this Databricks feature at this website: <a href="https://docs.databricks.com/applications/mlflow/model-serving.html">https://docs.databricks.com/applications/mlflow/model-serving.html</a>). However, we will show you how to use the MLflow built-in model deployment to deploy to local and remote AWS SageMaker in this chapter.</li>
				<li><strong class="bold">MLflow custom deployment plugins</strong>: MLflow provides an API for deploying to custom serving <a id="_idIndexMarker513"/>environments through MLflow deployment plugins (<a href="https://mlflow.org/docs/latest/plugins.html#deployment-plugins">https://mlflow.org/docs/latest/plugins.html#deployment-plugins</a>). Examples include <strong class="source-inline">mlflow-torchserv</strong> (<a href="https://github.com/mlflow/mlflow-torchserve">https://github.com/mlflow/mlflow-torchserve</a>), <strong class="source-inline">mlflow-ray-serve</strong> (<a href="https://github.com/ray-project/mlflow-ray-serve">https://github.com/ray-project/mlflow-ray-serve</a>), and <strong class="source-inline">mlflow-triton-plugin</strong> (<a href="https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin">https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin</a>). We will show how to use the <strong class="source-inline">mlflow-ray-serve</strong> plugin for deployment in this chapter.</li>
				<li><strong class="bold">Model serving tools that can deploy MLflow model flavors</strong>: These are usually generic <a id="_idIndexMarker514"/>model serving frameworks that support many types of models, including MLflow <a id="_idIndexMarker515"/>model flavors. Examples include <strong class="bold">Seldon MLServer</strong> (<a href="https://docs.seldon.io/projects/seldon-core/en/latest/servers/mlflow.html">https://docs.seldon.io/projects/seldon-core/en/latest/servers/mlflow.html</a>), <strong class="bold">Ray Serve</strong> (<a href="https://docs.ray.io/en/latest/serve/ml-models.html#integration-with-model-registries">https://docs.ray.io/en/latest/serve/ml-models.html#integration-with-model-registries</a>) and <strong class="bold">Triton Inference Server</strong>; only two <a id="_idIndexMarker516"/>MLflow <a id="_idIndexMarker517"/>model flavors – <strong class="bold">Open Neural Network Exchange</strong> (<strong class="bold">ONNX</strong>) and <strong class="bold">Triton</strong> – are supported at the time of writing (https://developer.nvidia.com/nvidia-triton-inference-server). We will <a id="_idIndexMarker518"/>show you how to use <strong class="bold">Ray Serve</strong> together <a id="_idIndexMarker519"/>with the <strong class="source-inline">mlflow-ray-serve</strong> plugin to deploy the MLflow Python model. Note that, although in this book we show how to use an MLflow customized plugin to deploy with a generic ML serve tool such as Ray Serve, it is important to note that a generic ML serve tool can do much more with or without an MLflow customized plugin.<p class="callout-heading">Optimize DL Inference through Specialized Inference Engines</p><p class="callout">There are some <a id="_idIndexMarker520"/>special MLflow <a id="_idIndexMarker521"/>model flavors such as <strong class="bold">ONNX</strong> (<a href="https://onnx.ai/">https://onnx.ai/</a>) and <strong class="bold">TorchScript</strong> (<a href="https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript">https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript</a>) that are specially designed for DL model inference runtime. We can convert a DL model into an ONNX model flavor (<a href="https://github.com/microsoft/onnxruntime">https://github.com/microsoft/onnxruntime</a>) or a TorchScript server (<a href="https://pytorch.org/serve/">https://pytorch.org/serve/</a>). As both ONNX and TorchScript are still evolving and are specifically <a id="_idIndexMarker522"/>designed for the original DL model part, but not the entire inference pipeline, we are not covering them in this chapter.</p></li>
			</ul>
			<p>Now that we have <a id="_idIndexMarker523"/>a good understanding of the varieties of the deployment tools and model serving frameworks, let's learn how to do the deployment in the following sections with concrete examples.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Deploying locally for batch and web service inference </h1>
			<p>For development and testing purposes, we usually need to deploy our model locally to verify it works as expected. Let's see how to do it for two scenarios: batch inference and web service inference.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Batch inference</h2>
			<p>For batch <a id="_idIndexMarker524"/>inference, follow <a id="_idIndexMarker525"/>these instructions:</p>
			<ol>
				<li>Make sure you have completed <a href="B18120_07_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 7</em></a>, <em class="italic">Multi-Step Deep Learning Inference Pipeline</em>. This will produce an MLflow <strong class="source-inline">pyfunc</strong> DL inference model pipeline URI that can be loaded using standard MLflow Python functions. The logged model can be uniquely located by the <strong class="source-inline">run_id</strong> and model name as follows:<p class="source-code">logged_model = 'runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model'</p></li>
			</ol>
			<p>The model can also be identified by the model name and version number using the model registry as follows:</p>
			<p class="source-code">logged_model = 'models:/inference_pipeline_model/6'</p>
			<ol>
				<li value="2">Follow the instructions under the <em class="italic">Batch inference at-scale using PySpark UDF function</em> section of this <strong class="source-inline">README.md</strong> file (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md</a>) to set up the local virtual environment, a full-fledged MLflow tracking server, and a few environment variables so that we can execute the code on your local environment.</li>
				<li>Load the model with the MLflow <strong class="source-inline">mlflow.pyfunc.spark_udf</strong> API to create a PySpark UDF function as follows. You may want to check out the <strong class="source-inline">batch_inference.py</strong> file from GitHub to follow through (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py</a>):<p class="source-code">loaded_model = <strong class="bold">mlflow.pyfunc.spark_udf</strong>(spark, model_uri=logged_model, result_type=StringType())</p></li>
			</ol>
			<p>This will wrap the inference pipeline as a PySpark UDF function with a return result type of <strong class="source-inline">String</strong>. This is because our model inference pipeline has a model signature requiring the output as a <strong class="source-inline">string</strong> type column.</p>
			<ol>
				<li value="4">Now, we <a id="_idIndexMarker526"/>can apply the PySpark UDF function to the input DataFrame. Note that the <a id="_idIndexMarker527"/>input DataFrame must have a <strong class="source-inline">text</strong> column with a <strong class="source-inline">string</strong> data type since that's what the model signature requires:<p class="source-code"><strong class="bold">df</strong> = <strong class="bold">df</strong>.withColumn('predictions', loaded_model())</p></li>
			</ol>
			<p>Because our model inference pipeline has defined a model signature, we don't need to specify any column parameters if it finds the <strong class="source-inline">text</strong> column in the input DataFrame, which is <strong class="source-inline">df</strong> in this example. Note that we can read a large volume of data using Spark's <strong class="source-inline">read</strong> API, which supports different data format reading, such as CSV, JSON, Parquet, and many more. In our example, we read the <strong class="source-inline">test.csv</strong> file from the IMDB dataset. This will leverage Spark's powerful distributed computation on a cluster if we have a large volume of data. This enables us to do batch inference at scale effortlessly.</p>
			<ol>
				<li value="5">To run the batch inference code from end to end, you should check out the complete code provided in the repository at this location: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py</a>. Make sure you <a id="_idIndexMarker528"/>replace the <strong class="source-inline">logged_model</strong> variable with your own <strong class="source-inline">run_id</strong> and model name or the <a id="_idIndexMarker529"/>registered model name and version before you run the following command in the <strong class="source-inline">batch</strong> folder:<p class="source-code"><strong class="bold">python batch_inference.py</strong></p></li>
				<li>You should see the output in <em class="italic">Figure 8.2</em> on the screen:</li>
			</ol>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="Images/B18120_08_02.jpg" alt="Figure 8.2 – Batch inference using PySpark UDF function&#13;&#10;" width="785" height="945"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Batch inference using PySpark UDF function</p>
			<p>As can be <a id="_idIndexMarker530"/>seen from <em class="italic">Figure 8.2</em>, the multi-step inference pipeline we loaded worked correctly and even detected non-English texts and duplicates, although the language <a id="_idIndexMarker531"/>detector probably produced some false positives. The output is a two-column <a id="_idIndexMarker532"/>DataFrame where the JSON response of the model prediction is saved in the <strong class="source-inline">predictions</strong> column. Note that you can use the same code provided in <strong class="source-inline">batch_inference.py</strong> in a Databricks notebook and process a very large volume of input data with a Spark cluster by changing the input data and the logged model location. </p>
			<p>Now that we know how to do batch inference at scale, let's see how to deploy to a local web service for the same model inference pipeline.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Model as a web service</h2>
			<p>We can deploy <a id="_idIndexMarker533"/>the same <a id="_idIndexMarker534"/>logged model inference pipeline to a web service locally and have an endpoint that accepts HTTP requests with an HTTP response. </p>
			<p>The local deployment is quite straightforward with just one command line. We can deploy a logged model or a registered model using the model URI as in the previous batch inference, as follows:</p>
			<p class="source-code">mlflow models serve -m models:/inference_pipeline_model/6</p>
			<p>You should be able to see the following:</p>
			<p class="source-code">2022/03/06 21:50:19 INFO mlflow.models.cli: Selected backend for flavor 'python_function'</p>
			<p class="source-code">2022/03/06 21:50:21 INFO mlflow.utils.conda: === Creating conda environment mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 ===</p>
			<p class="source-code">± |main U:1 ?:8 X| done</p>
			<p class="source-code">Solving environment: done</p>
			<p>This will create the conda environment using the logged model so that it will have all the dependencies to run. After the conda environment is created, you should see the following:</p>
			<p class="source-code">2022/03/06 21:52:11 INFO mlflow.pyfunc.backend: === Running command 'source /Users/yongliu/opt/miniconda3/bin/../etc/profile.d/conda.sh &amp;&amp; conda activate mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 1&gt;&amp;2 &amp;&amp; gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'</p>
			<p class="source-code">[2022-03-06 21:52:12 -0800] [97554] [INFO] Starting gunicorn 20.1.0</p>
			<p class="source-code">[2022-03-06 21:52:12 -0800] [97554] [INFO] Listening at: http://127.0.0.1:5000 (97554)</p>
			<p class="source-code">[2022-03-06 21:52:12 -0800] [97554] [INFO] Using worker: sync</p>
			<p class="source-code">[2022-03-06 21:52:12 -0800] [97561] [INFO] Booting worker with pid: 97561</p>
			<p>Now, the model is deployed as a web service and ready to accept HTTP requests for model <a id="_idIndexMarker535"/>prediction. Open <a id="_idIndexMarker536"/>a different Terminal window and type the following command to invoke the model web service to get a prediction response:</p>
			<p class="source-code">curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{</p>
			<p class="source-code">    "columns": ["text"],</p>
			<p class="source-code">    "data": [["This is the best movie we saw."], ["What a movie!"]]</p>
			<p class="source-code">}'</p>
			<p>We can see the following prediction response immediately:</p>
			<p class="source-code">[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/07b900a96af04037a956c74ef691396e/model\", \"inference_pipeline_model_uri\": \"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/07b900a96af04037a956c74ef691396e/model\", \"inference_pipeline_model_uri\": \"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\"}}"}]</p>
			<p>If you have followed the steps so far and saw the prediction results, you should feel very proud <a id="_idIndexMarker537"/>that you just deployed <a id="_idIndexMarker538"/>a DL model inference pipeline into a local web service! This is great for testing and debugging, and the behavior of the model will not change on a production web server, so we should make sure it works on a local web server.</p>
			<p>So far, we have learned how to use the built-in MLflow deployment tool. Next, we will see how to use a generic deployment tool, Ray Serve, to deploy an MLflow inference pipeline.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Deploying using Ray Serve and MLflow deployment plugins</h1>
			<p>A more generic way to <a id="_idIndexMarker539"/>do deployment is to use a framework such as Ray Serve (<a href="https://docs.ray.io/en/latest/serve/index.html">https://docs.ray.io/en/latest/serve/index.html</a>). Ray Serve has several advantages, such as DL model <a id="_idIndexMarker540"/>frameworks agnostics, native Python <a id="_idIndexMarker541"/>support, and supporting complex model <a id="_idIndexMarker542"/>composition inference <a id="_idIndexMarker543"/>patterns. Ray Serve supports all major DL frameworks and any arbitrary business logic. So, can we leverage both Ray Serve and MLflow to do model deployment and serve? The good news is that we can use the MLflow deployment plugins provided by Ray Serve to do this. Let's walk through how to use the <strong class="source-inline">mlflow-ray-serve</strong> plugin to do MLflow model deployment using Ray Serve (<a href="https://github.com/ray-project/mlflow-ray-serve">https://github.com/ray-project/mlflow-ray-serve</a>). Before we begin, we need to install the <strong class="source-inline">mlflow-ray-serve</strong> package:</p>
			<p class="source-code">pip install mlflow-ray-serve</p>
			<p>Then, we need to start a single node Ray cluster locally first using the following two commands:</p>
			<p class="source-code">ray start --head</p>
			<p class="source-code">serve start</p>
			<p>This will <a id="_idIndexMarker544"/>start a Ray cluster locally, and you <a id="_idIndexMarker545"/>can access its dashboard from your web browser at <strong class="source-inline">http://127.0.0.1:8265/#/</strong> as follows:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="Images/B18120_08_03.jpg" alt="Figure 8.3 – A locally running Ray cluster&#13;&#10;" width="1178" height="795"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – A locally running Ray cluster</p>
			<p><em class="italic">Figure 8.3</em> shows a locally running Ray cluster. You can then issue the following command to deploy <strong class="source-inline">inference_pipeline_model</strong> into Ray Serve as follows: </p>
			<p class="source-code">mlflow deployments create -t ray-serve -m runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model --name dl-inference-model-on-ray -C num_replicas=1</p>
			<p>This will show the following screen output:</p>
			<p class="source-code">2022-03-20 20:16:46,564    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379</p>
			<p class="source-code">2022-03-20 20:16:46,717    INFO api.py:242 -- Updating deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray</p>
			<p class="source-code">(ServeController pid=78159) 2022-03-20 20:16:46,784    INFO deployment_state.py:912 -- Adding 1 replicas to deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray</p>
			<p class="source-code">2022-03-20 20:17:10,309    INFO api.py:249 -- Deployment 'dl-inference-model-on-ray' is ready at `http://127.0.0.1:8000/dl-inference-model-on-ray`. component=serve deployment=dl-inference-model-on-ray</p>
			<p class="source-code">python_function deployment dl-inference-model-on-ray is created</p>
			<p>This means <a id="_idIndexMarker546"/>that an endpoint <a id="_idIndexMarker547"/>at <strong class="source-inline">http://127.0.0.1:8000/dl-inference-model-on-ray</strong> is ready to serve an online inference request! You can test this deployment using the Python code provided at <strong class="source-inline">chapter08/ray_serve/query_ray_serve_endpoint.py</strong> as follows:</p>
			<p class="source-code">python ray_serve/query_ray_serve_endpoint.py</p>
			<p>This will show results on the screen as follows:</p>
			<p class="source-code">2022-03-20 21:16:45,125    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379</p>
			<p class="source-code">[{'name': 'dl-inference-model-on-ray', 'info': Deployment(name=dl-inference-model-on-ray,version=None,route_prefix=/dl-inference-model-on-ray)}]</p>
			<p class="source-code">{</p>
			<p class="source-code">    "columns": [</p>
			<p class="source-code">        "text"</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    "index": [</p>
			<p class="source-code">        0,</p>
			<p class="source-code">        1</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    "data": [</p>
			<p class="source-code">        [</p>
			<p class="source-code">            "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/be2fb13fe647481eafa071b79dde81de/model\", \"inference_pipeline_model_uri\": \"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\"}}"</p>
			<p class="source-code">        ],</p>
			<p class="source-code">        [</p>
			<p class="source-code">            "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/be2fb13fe647481eafa071b79dde81de/model\", \"inference_pipeline_model_uri\": \"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\"}}"</p>
			<p class="source-code">        ]</p>
			<p class="source-code">    ]</p>
			<p class="source-code">}</p>
			<p>You should see the inference model response as expected. If you followed through up to this point, congratulations <a id="_idIndexMarker548"/>on your successful <a id="_idIndexMarker549"/>deployment using the <strong class="source-inline">mlflow-ray-serve</strong> MLflow deployment plugin! If you no longer need this Ray Serve instance, you can stop it by executing the following command line:</p>
			<p class="source-code">ray stop</p>
			<p>This will stop all running Ray instances on your local machine.</p>
			<p class="callout-heading">Deployment Using MLflow Deployment Plugins</p>
			<p class="callout">There are several <a id="_idIndexMarker550"/>MLflow deployment plugins. We just showed how to use <strong class="source-inline">mlflow-ray-serve</strong> to deploy a generic MLflow Python model, <strong class="source-inline">inference_pipeline_model</strong>. This opens doors to deploying to many target destinations where you can launch a Ray cluster in any cloud provider. We will not cover more details in this chapter as it's beyond the scope of this book. If you are <a id="_idIndexMarker551"/>interested, refer to the Ray documentation on how to launch cloud clusters (AWS, Azure, and <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>)): <a href="https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster">https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster</a>. Once there is a Ray cluster, you can follow the same procedure to deploy an MLflow model.</p>
			<p>Now that we know several ways to deploy locally and could further deploy to the cloud using Ray Serve if desirable, let's see how we can deploy to a cloud-managed inference service, AWS SageMaker, in the next section, since it is widely used and can provide a good lesson on how to deploy in a realistic scenario. </p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor102"/>Deploying to AWS SageMaker – a complete end-to-end guide</h1>
			<p>AWS SageMaker has a <a id="_idIndexMarker552"/>cloud-hosted model service managed by AWS. We <a id="_idIndexMarker553"/>will use AWS SageMaker as an example to show you how to deploy to a remote cloud provider for hosted web services that can serve real production traffic. AWS SageMaker has a suite of ML/DL-related services including supporting annotation and model training and many more. Here, we show how to <strong class="bold">bring your own model</strong> (<strong class="bold">BYOM</strong>) for deployment. This means that you have a model inference pipeline trained outside of AWS SageMaker, and now just need to deploy to SageMaker for <a id="_idIndexMarker554"/>hosting. Follow the next steps to prepare and deploy a DL sentiment model. A few prerequisites are required: </p>
			<ul>
				<li>You must have Docker Desktop running in your local environment.</li>
				<li>You must have an AWS account. You can create a free AWS account easily through the free signup website at <a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>.</li>
			</ul>
			<p>Once you have these requirements , activate the <strong class="source-inline">dl-model-chapter08</strong> conda virtual environment to follow through a few steps for deploying to SageMaker. We breakdown these steps into six subsections as follows:</p>
			<ol>
				<li value="1">Build a local SageMaker Docker image</li>
				<li>Add additional model artifacts layers onto the SageMaker Docker image</li>
				<li>Test local deployment with the newly built SageMaker Docker image</li>
				<li>Push the SageMaker Docker image to AWS Elastic Container Registry</li>
				<li>Deploy the inference pipeline model to create a SageMaker endpoint</li>
				<li>Query the SageMaker endpoint for online inference</li>
			</ol>
			<p>Let's start with the first step to build a local SageMaker Docker image.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>Step 1: Build a local SageMaker Docker image</h2>
			<p>We intentionally start with a local build without pushing to the AWS so that we can learn how to add additional layers on top of this basic image and verify everything locally before incurring any cloud cost:</p>
			<p class="source-code"><strong class="bold">mlflow sagemaker build-and-push-container --build --no-push -c mlflow-dl-inference</strong></p>
			<p>You will see a lot of screen outputs and at the end, it will show something like the following:</p>
			<p class="source-code"><strong class="bold">#15 exporting to image</strong></p>
			<p class="source-code"><strong class="bold">#15 sha256:e8c613e07b0b7ff33893b694f7759a10 d42e180f2b4dc349fb57dc6b71dcab00</strong></p>
			<p class="source-code"><strong class="bold">#15 exporting layers</strong></p>
			<p class="source-code"><strong class="bold">#15 exporting layers 8.7s done</strong></p>
			<p class="source-code"><strong class="bold">#15 writing image sha256:95bc539b021179e5e87087 012353ebb43c71410be535ef368d1121b550c57bd4 done</strong></p>
			<p class="source-code"><strong class="bold">#15 naming to docker.io/library/mlflow-dl-inference done</strong></p>
			<p class="source-code"><strong class="bold">#15 DONE 8.7s</strong></p>
			<p>If you see the image name <strong class="source-inline">mlflow-dl-inference</strong>, that means you have successfully created a SageMaker-compatible MLflow-model-serving Docker image. You can verify this by running the following command:</p>
			<p class="source-code"><strong class="bold">docker images | grep mlflow-dl-inference</strong></p>
			<p>You should see output like the following:</p>
			<p class="source-code"><strong class="bold">mlflow-dl-inference          latest                  95bc539b0211   6 minutes ago   2GB</strong></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor104"/>Step 2: Add additional model artifacts layers onto the SageMaker Docker image</h2>
			<p>Recall that our inference pipeline model builds on top of a fine-tuned DL model and we load the model through the MLflow PythonModel API's <strong class="source-inline">load_context</strong> function (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel</a>) without serializing the fine-tuned model itself. This is partly because MLflow cannot serialize the PyTorch DataLoader (<a href="https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading">https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading</a>) properly using pickle since the DataLoader does not implement pickle serialization as of this writing. This does give us an opportunity to learn how we can deploy when some of the dependencies cannot be serialized properly, especially when dealing with a real-world DL model. </p>
			<p class="callout-heading">Two Ways to Allow a Docker Container to Access an MLflow Tracking Server</p>
			<p class="callout">There are two ways to allow a Docker container such as <strong class="source-inline">mlflow-dl-inference</strong> to access and load a fine-tuned model at runtime. The first method is to allow the container to include the MLflow tracking server URL and access token. This may cause some security concerns in an enterprise environment since the Docker image now contains some security credentials. The second method is to directly copy all the referenced artifacts to create a new Docker image that's self-sufficient. At runtime, it does not have to know where the original MLflow tracking server is located since it has all model artifacts locally. This self-contained approach eliminates any concerns of security leaking. We use this second approach in this chapter for deployment. </p>
			<p>In this chapter, we will copy the referenced fine-tuned model into a new Docker image that's built on top of the basic <strong class="source-inline">mlflow-dl-inference</strong> Docker image. This will make a new self-contained Docker image without relying on any external MLflow tracking server. To do this, you need to either download the fine-tuned DL model from a model tracking server to your current <a id="_idIndexMarker555"/>local folder, or you can just run our MLproject's pipeline locally using the local filesystem as the MLflow tracking server backend. Follow the <em class="italic">Deploy to AWS SageMaker</em> section in the <strong class="source-inline">README.md</strong> file to reproduce the local MLflow runs to prepare a fine-tuned model and <strong class="source-inline">inference-pipeline-model</strong> in the local folder. For learning purposes, we have provided two example <strong class="source-inline">mlruns</strong> artifacts and the <strong class="source-inline">huggingface</strong> cache folder in the GitHub repository in the <strong class="source-inline">chapter08</strong> folder (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08</a>), so that we can start building a new Docker image right away by using these existing artifacts. </p>
			<p>To build a new Docker image, we need to create a Dockerfile as follows:</p>
			<pre class="source-code">FROM mlflow-dl-inference</pre>
			<pre class="source-code">ADD mlruns/1/meta.yaml  /opt/mlflow/mlruns/1/meta.yaml</pre>
			<pre class="source-code">ADD mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/ /opt/mlflow/mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/</pre>
			<pre class="source-code">ADD tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/ /opt/mlflow/tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/</pre>
			<p>The first line means that it starts with the existing <strong class="source-inline">mlflow-dl-inference</strong> Docker image, and the following three lines of <strong class="source-inline">ADD</strong> will copy one <strong class="source-inline">meta.yaml</strong> file and two folders to the corresponding locations in the Docker image. Note that if you already have produced your own runs by following the <strong class="source-inline">README</strong> file, then you do not need to add the third line. Note that, by default, when the Docker container starts, it automatically goes to this<strong class="source-inline">/opt/mlflow/</strong> working directory so everything needs to be copied to this folder for easy access. Also, note that the <strong class="source-inline">/opt/mlflow</strong> directory requires superuser permission, so you need to <a id="_idIndexMarker556"/>be prepared to enter your local machine's admin password (usually, on your own laptop, that's your own password).</p>
			<p class="callout-heading">Copy Privately Built Python Packages into Docker Images </p>
			<p class="callout">It is also possible to copy privately built Python packages into Docker images so that we can directly reference them in the <strong class="source-inline">conda.yaml</strong> file without going outside of the container itself. For example, we can copy a private Python wheel package, <strong class="source-inline">cool-dl-package-1.0.py3-none-any.whl</strong>, to the <strong class="source-inline">/usr/private-wheels/cool-dl-package/cool-dl-package-1.0-py3-none-any.whl</strong> Docker folder, and then we can point to this path in the <strong class="source-inline">conda.yaml</strong> file. This allows MLflow model artifacts to load these locally accessible Python packages successfully. In our current example, we don't use this approach since we haven't used any privately built Python packages. This is useful for future reference if you are interested in exploring this.</p>
			<p>Now, you can run the following command to build a new Docker image in the <strong class="source-inline">chapter08</strong> folder as follows:</p>
			<p class="source-code"><strong class="bold">docker build . -t mlflow-dl-inference-w-finetuned-model</strong></p>
			<p>This will build a new Docker image, <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong>, on top of <strong class="source-inline">mlflow-dl-inference</strong>. You should see the following output (only the first and last couple of lines are presented for brevity):</p>
			<p class="source-code"><strong class="bold">[+] Building 0.2s (9/9) FINISHED</strong></p>
			<p class="source-code"><strong class="bold"> =&gt; [internal] load build definition from Dockerfile                                                      0.0s</strong></p>
			<p class="source-code"><strong class="bold">…………</strong></p>
			<p class="source-code"><strong class="bold">=&gt; =&gt; naming to docker.io/library/mlflow-dl-inference-w-finetuned-model</strong></p>
			<p>Now, you have a new Docker image named <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong>, which contains the fine-tuned model. Now, we are ready to deploy our inference pipeline model using this new Docker image, which is SageMaker compatible.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Step 3: Test local deployment with the newly built SageMaker Docker image</h2>
			<p>Before we deploy to the cloud, let's test the deployment locally with this new SageMaker <a id="_idIndexMarker557"/>Docker image. MLflow provides a convenient way to test this locally using the following command:</p>
			<p class="source-code"><strong class="bold">mlflow sagemaker run-local -m runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model -p 5555 -i mlflow-dl-inference-w-finetuned-model</strong></p>
			<p>This command will start running the <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong> Docker container locally and deploy the inference pipeline model with a <strong class="source-inline">dc5f670efa1a4eac95683633ffcfdd79</strong> run ID into this container. </p>
			<p class="callout-heading">Fix a Potential Docker Error</p>
			<p class="callout">Note that you may <a id="_idIndexMarker558"/>encounter a Docker error saying <strong class="bold">The path /opt/mlflow/mlruns/1/ dc5f670efa1a4eac95683633ffcfdd79/artifacts/inference_pipeline_model is not shared from the host and is not known to Docker</strong>. You can configure shared paths from <strong class="bold">Docker</strong> | <strong class="bold">Preferences...</strong> | <strong class="bold">Resources</strong> | <strong class="bold">File Sharing</strong> to fix this Docker error.</p>
			<p>We already provided this inference pipeline model in the GitHub repository, so this should work out-of-the-box when you check out the repository in your local environment. The port for web service is <strong class="source-inline">5555</strong>. Once the command is running, you will see a lot of outputs on the screen, and finally, you should see the following:</p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [552] [INFO] Starting gunicorn 20.1.0</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [552] [INFO] Listening at: http://127.0.0.1:8000 (552)</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [552] [INFO] Using worker: gevent</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [565] [INFO] Booting worker with pid: 565</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [566] [INFO] Booting worker with pid: 566</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [567] [INFO] Booting worker with pid: 567</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [568] [INFO] Booting worker with pid: 568</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [569] [INFO] Booting worker with pid: 569</strong></p>
			<p class="source-code"><strong class="bold">[2022-03-18 01:47:20 +0000] [570] [INFO] Booting worker with pid: 570</strong></p>
			<p>This means that the service is up and running. You might see a few warnings about <a id="_idIndexMarker559"/>the PyTorch version not being compatible, but they can be safely ignored. Once this service is up and running, you can then test against it in a different Terminal window by issuing a <strong class="source-inline">curl</strong> web request as follows, like we have tried before:</p>
			<p class="source-code"><strong class="bold">curl http://127.0.0.1:5555/invocations -H 'Content-Type: application/json' -d '{</strong></p>
			<p class="source-code"><strong class="bold">    "columns": ["text"],</strong></p>
			<p class="source-code"><strong class="bold">    "data": [["This is the best movie we saw."], ["What a movie!"]]</strong></p>
			<p class="source-code"><strong class="bold">}'</strong></p>
			<p>Note that the port number is <strong class="source-inline">5555</strong> for the localhost. You should then see the response as follows:</p>
			<p class="source-code"><strong class="bold">[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]</strong></p>
			<p>You may wonder how this is different from the previous section's local web service for the inference model. The difference is that this time, we are using a SageMaker <a id="_idIndexMarker560"/>container locally, while previously, it was just a local web service without a Docker container. Having the SageMaker container tested locally is very important so that you don't waste time and money deploying a failed model service to the cloud.</p>
			<p>Next, we are ready to deploy this container to AWS SageMaker.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Step 4: Push the SageMaker Docker image to AWS Elastic Container Registry</h2>
			<p>Now, you can push your newly built <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong> Docker image to AWS <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) with the following command. Make sure you have your AWS access token and <a id="_idIndexMarker561"/>access ID set up correctly (the real one, not the local development one). Once you have your access key ID and token, run the following command to set up your access to the real AWS:</p>
			<p class="source-code"><strong class="bold">aws configure</strong></p>
			<p>Answer all the questions after executing the command and you will be ready to go. Now, you can run the following command to push the <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong> Docker image to the AWS ECR:</p>
			<p class="source-code"><strong class="bold">mlflow sagemaker build-and-push-container --no-build --push -c mlflow-dl-inference-w-finetuned-model</strong></p>
			<p>Make sure you don't build a new image with the <strong class="source-inline">--no-build</strong> option included in the command since we just want to push the image, not build a new one. You will see <a id="_idIndexMarker562"/>the following output, which shows the image is being pushed to the ECR. Note that in the following output, the AWS account is masked with <strong class="source-inline">xxxxx</strong>. You will see your account number showing in the output. Make sure you have the permission to write to the AWS ECR store:</p>
			<p class="source-code"><strong class="bold">2022/03/18 17:36:05 INFO mlflow.sagemaker: Pushing image to ECR</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 17:36:06 INFO mlflow.sagemaker: Pushing docker image mlflow-dl-inference-w-finetuned-model to xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1</strong></p>
			<p class="source-code"><strong class="bold">Created new ECR repository: mlflow-dl-inference-w-finetuned-model</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 17:36:06 INFO mlflow.sagemaker: Executing: aws ecr get-login-password | docker login  --username AWS  --password-stdin xxxxx.dkr.ecr.us-west-2.amazonaws.com;</strong></p>
			<p class="source-code"><strong class="bold">docker tag mlflow-dl-inference-w-finetuned-model xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1;</strong></p>
			<p class="source-code"><strong class="bold">docker push xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1</strong></p>
			<p class="source-code"><strong class="bold">Login Succeeded</strong></p>
			<p class="source-code"><strong class="bold">The push refers to repository [xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model]</strong></p>
			<p class="source-code"><strong class="bold">447db5970ca5: Pushed</strong></p>
			<p class="source-code"><strong class="bold">9d6787a516e7: Pushed</strong></p>
			<p class="source-code"><strong class="bold">1.23.1: digest: sha256:f49f85741bc2b82388e85c79f6621f4 d7834e19bdf178b70c1a6c78c572b4d10 size: 3271</strong></p>
			<p>Once this is done, if you go to the AWS website (for example, if you use the <strong class="source-inline">us-west-2</strong> region data center, the URL is <a href="https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2">https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2</a>), you should <a id="_idIndexMarker563"/>find your newly pushed image in the ECR with a folder named <strong class="source-inline">mlflow-dl-inference-w-finetuned-model</strong>. You will then find the image in this folder as follows (<em class="italic">Figure 8.4</em>): </p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="Images/B18120_08_04.jpg" alt="Figure 8.4 – AWS ECR repositories with mlflow-dl-inference-w-finetuned-model image tag 1.23.1&#13;&#10;" width="867" height="362"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – AWS ECR repositories with mlflow-dl-inference-w-finetuned-model image tag 1.23.1</p>
			<p>Note that the image tag number <strong class="bold">1.23.1</strong> in <em class="italic">Figure 8.4</em> is the MLflow version we used. This image has a full URI, which you can get using the <strong class="source-inline">Copy URI</strong> option. It will look as follows (with the AWS account masked with <strong class="source-inline">xxxxx</strong>):</p>
			<p class="source-code">xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1</p>
			<p>You will need this image URI to deploy to SageMaker in the next step. Let's now deploy to SageMaker to create an inference endpoint.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/>Step 5: Deploy the inference pipeline model to create a SageMaker endpoint</h2>
			<p>Now, it is time to deploy the inference pipeline model to SageMaker using this image URI we just pushed to the AWS ECR registry. We have included the <strong class="source-inline">sagemaker/deploy_to_sagemaker.py</strong> code in the <strong class="source-inline">chapter08</strong> folder in the GitHub repository. You will need to use the correct AWS role for the deployment. You can create a new <strong class="source-inline">AWSSageMakerExecutionRole</strong> role in your account and assign two permissions policies to this role, <strong class="source-inline">AmazonS3FullAccess</strong> and <strong class="source-inline">AmazonSageMakerFullAccess</strong>. In a real-world <a id="_idIndexMarker564"/>scenario, you might want to tighten the permission to a more restricted policy, but for learning purposes, this will work fine. The following figure shows the screen after the role is created:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="Images/B18120_08_05.jpg" alt="Figure 8.5 – Create a role that can be used for deployment in SageMaker&#13;&#10;" width="751" height="677"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Create a role that can be used for deployment in SageMaker</p>
			<p>You also need to create an S3 bucket for SageMaker to upload the model artifacts and deploy them to SageMaker. In our example, we created a bucket called <strong class="source-inline">dl-inference-deployment</strong>. When we execute the deployment script, as shown here, the model to be deployed will be first uploaded to the <strong class="source-inline">dl-inference-deployment</strong> bucket and then deployed to SageMaker. We have provided the complete deployment script in the <strong class="source-inline">chapter08/sagemaker/deploy_to_sagemaker.py</strong> GitHub repository so you <a id="_idIndexMarker565"/>can download and execute it as follows (as a reminder, before you run this script, make sure you reset the environment variable of <strong class="source-inline">MLFLOW_TRACKING_URI</strong> to empty, as in <strong class="source-inline">export MLFLOW_TRACKING_URI=</strong>):</p>
			<p class="source-code"><strong class="bold">sudo python sagemaker/deploy_to_sagemaker.py</strong></p>
			<p>This script executes the following two tasks:</p>
			<ol>
				<li value="1">Makes a copy of the local <strong class="source-inline">mlruns</strong> under the <strong class="source-inline">chapter08</strong> folder to a local <strong class="source-inline">/opt/mlflow</strong> folder so that SageMaker deployment code can pick up the <strong class="source-inline">inference-pipeline-model</strong> to upload. Because the <strong class="source-inline">/opt</strong> path is usually restricted, here we use <strong class="source-inline">sudo</strong> (superuser) to do this copy. This will prompt you to type in your user password on your laptop.</li>
				<li>Uses the <strong class="source-inline">mlflow.sagemaker.deploy</strong> API to create a new SageMaker endpoint, <strong class="source-inline">dl-sentiment-model</strong>.</li>
			</ol>
			<p>The code snippet is as follows:</p>
			<p class="source-code">mlflow.sagemaker.deploy(</p>
			<p class="source-code">    mode='create',</p>
			<p class="source-code">    app_name=endpoint_name,</p>
			<p class="source-code">    model_uri=model_uri,</p>
			<p class="source-code">    image_url=image_uri,</p>
			<p class="source-code">    execution_role_arn=role,</p>
			<p class="source-code">    instance_type='ml.m5.xlarge',</p>
			<p class="source-code">    bucket = bucket_for_sagemaker_deployment,</p>
			<p class="source-code">    instance_count=1,</p>
			<p class="source-code">    region_name=region</p>
			<p class="source-code">)</p>
			<p>The parameters need some explanations so that we fully understand all the preparation work that is needed:</p>
			<ul>
				<li><strong class="source-inline">model_uri</strong>: This is the inference pipeline model's URI. In our example, it is <strong class="source-inline">runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model</strong>.</li>
				<li><strong class="source-inline">image_url</strong>: This is the Docker image we uploaded to the AWS ECR. In our example, it is <strong class="source-inline">xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1</strong>. Note that you need to replace the masked AWS account number, <strong class="source-inline">xxxxx</strong>, with your actual account number.</li>
				<li><strong class="source-inline">execution_role_arn</strong>: This is the role we created to allow SageMaker to do <a id="_idIndexMarker566"/>the deployment. In our example, it is <strong class="source-inline">arn:aws:iam::565251169546:role/AWSSageMakerExecutionRole</strong>. Again, you need to replace <strong class="source-inline">xxxxx</strong> with your actual AWS account number.</li>
				<li><strong class="source-inline">bucket</strong>: This is the S3 bucket we created to allow SageMaker to upload the model and then do the actual deployment. In our example, it is <strong class="source-inline">dl-inference-deployment</strong>. </li>
			</ul>
			<p>The rest of the parameters are self-explanatory. </p>
			<p>After you execute the deployment script, you will see the following output (where <strong class="source-inline">xxxxx</strong> is the masked AWS account number):</p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:47 INFO mlflow.sagemaker: Using the python_function flavor for deployment!</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:47 INFO mlflow.sagemaker: tag response: {'ResponseMetadata': {'RequestId': 'QMAQRCTJT36TXD2H', 'HostId': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'x-amz-request-id': 'QMAQRCTJT36TXD2H', 'date': 'Sat, 19 Mar 2022 02:30:48 GMT', 'server': 'AmazonS3', 'content-length': '0'}, 'RetryAttempts': 0}}</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:47 INFO mlflow.sagemaker: Creating new endpoint with name: dl-sentiment-model ...</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:47 INFO mlflow.sagemaker: Created model with arn: arn:aws:sagemaker:us-west-2:xxxxx:model/dl-sentiment-model-model-qbca2radrxitkujn3ezubq</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:47 INFO mlflow.sagemaker: Created endpoint configuration with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint-config/dl-sentiment-model-config-r9ax3wlhrfisxkacyycj8a</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:48 INFO mlflow.sagemaker: Created endpoint with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint/dl-sentiment-model</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for the deployment operation to complete...</strong></p>
			<p class="source-code"><strong class="bold">2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for endpoint to reach the "InService" state. Current endpoint status: "Creating"</strong></p>
			<p>This may take several minutes (sometimes more than 10 minutes). You may see some warning messages regarding PyTorch version compatibility as you saw <a id="_idIndexMarker567"/>when doing local SageMaker deployment testing. You can also go directly to the SageMaker website and you will see the status of the endpoints starting with <strong class="bold">Creating</strong>, and then eventually turning to a green-colored <strong class="bold">InService</strong> status as follows: </p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="Images/B18120_08_06.jpg" alt="Figure 8.6 – AWS SageMaker dl-sentiment-model endpoint InService&#13;&#10;" width="1165" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – AWS SageMaker dl-sentiment-model endpoint InService</p>
			<p>If you see the <strong class="bold">InService</strong> status, then congratulations! You have successfully deployed a DL inference pipeline model into SageMaker and you can now use it for production traffic!</p>
			<p>Now that the status of the service is inService, you can query it using the command line in the next step.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor108"/>Step 6: Query the SageMaker endpoint for online inference</h2>
			<p>To query the SageMaker endpoint, you can use the following command line:</p>
			<p class="source-code"><strong class="bold">aws sagemaker-runtime invoke-endpoint --endpoint-name 'dl-sentiment-model' --content-type 'application/json; format=pandas-split' --body '{"columns":["text"], "data": [["This is the best movie we saw."], ["What a movie!"]]}' response.json</strong></p>
			<p>You <a id="_idIndexMarker568"/>will then see the output as follows:</p>
			<p class="source-code"><strong class="bold">{</strong></p>
			<p class="source-code"><strong class="bold">    "ContentType": "application/json",</strong></p>
			<p class="source-code"><strong class="bold">    "InvokedProductionVariant": "dl-sentiment-model-model-qbca2radrxitkujn3ezubq"</strong></p>
			<p class="source-code"><strong class="bold">}</strong></p>
			<p>The actual prediction results are stored in a local <strong class="source-inline">response.json</strong> file, which can be viewed by running the following command to show the content of the response:</p>
			<p class="source-code"><strong class="bold">cat response.json</strong></p>
			<p>This will display the content as follows:</p>
			<p class="source-code"><strong class="bold">[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]</strong></p>
			<p>This is the expected response pattern from our inference pipeline model! It is also <a id="_idIndexMarker569"/>possible to run the query against the SageMaker inference endpoint using Python code, which we have provided in the <strong class="source-inline">chapter08/sagemaker/ query_sagemaker_endpoint.py</strong> file in the GitHub repository. The core code snippet uses <strong class="bold">Boto3</strong> and the <strong class="source-inline">SageMakerRuntime</strong> client's <strong class="source-inline">invoke_endpoint</strong> to query, as follows:</p>
			<pre class="source-code">client = boto3.client('sagemaker-runtime') </pre>
			<pre class="source-code">response = client.invoke_endpoint(</pre>
			<pre class="source-code">    EndpointName=app_name, </pre>
			<pre class="source-code">    ContentType=content_type,</pre>
			<pre class="source-code">    Accept=accept,</pre>
			<pre class="source-code">    Body=payload</pre>
			<pre class="source-code">    )</pre>
			<p>The parameters for <strong class="source-inline">invoke_endpoint</strong> need some explanation:</p>
			<ul>
				<li><strong class="source-inline">EndpointName</strong>: This is the inference endpoint name. In our example, it is <strong class="source-inline">dl-inference-model</strong>.</li>
				<li><strong class="source-inline">ContentType</strong>: This is the MIME type of the input data in the request body. In our example, we use <strong class="source-inline">application/json; format=pandas-split</strong>.</li>
				<li><strong class="source-inline">Accept</strong>: This is the desired MIME type of the inference in the response body. In our <a id="_idIndexMarker570"/>example, we expect the <strong class="source-inline">text/plain</strong> string type. </li>
				<li><strong class="source-inline">Body</strong>: This is the actual text that we want to predict the sentiment using the DL model inference service. In our example, it is <strong class="source-inline">{"columns": ["text"],"data": [["This is the best movie we saw."], ["What a movie!"]]}</strong>.</li>
			</ul>
			<p>The full code is provided in the GitHub repository, and you can run it in the command line as follows:</p>
			<p class="source-code"><strong class="bold">python sagemaker/query_sagemaker_endpoint.py</strong></p>
			<p>You will see the following output on your Terminal screen:</p>
			<p class="source-code"><strong class="bold">Application status is: InService</strong></p>
			<p class="source-code"><strong class="bold">[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]</strong></p>
			<p>This is what we expect from our inference pipeline model's response! If you have followed <a id="_idIndexMarker571"/>this chapter up to here, congratulate yourself on successfully deploying our inference pipeline model into production in a remote cloud host, AWS SageMaker! When you are done following the lessons in this chapter, make sure to delete the endpoint so that it doesn't incur unnecessary costs.</p>
			<p>Let's summarize what we've learned in this chapter. </p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor109"/>Summary</h1>
			<p>In this chapter, we have learned different ways to deploy an MLflow inference pipeline model for both batch inference and online real-time inference. We started with a brief survey on different model serving scenarios (batch, streaming, and on-device) and looked at three different categories of tools for MLflow model deployment (the MLflow built-in deployment tool, MLflow deployment plugins, and generic model inference serving frameworks that could work with the MLflow inference model). Then, we covered several local deployment scenarios, using the PySpark UDF function to do batch inference and MLflow local deployment for web service. Afterward, we learned how to use Ray Serve in conjunction with the <strong class="source-inline">mlflow-ray-serve</strong> plugin to deploy an MLflow Python inference pipeline model into a local Ray cluster. This opens doors to deploy to any cloud platform such as AWS, Azure ML, or GCP, as long as we can set up a Ray cluster in the cloud. Finally, we provided a complete end-to-end guide on how to deploy to AWS SageMaker, focusing on a common scenario of BYOM, where we have a trained inference pipeline model that's built outside of AWS SageMaker and now needs to be deployed to AWS SageMaker for a hosting model service. Our step-by-step guide should provide you with the confidence to deploy an MLflow inference pipeline model for real production usage.</p>
			<p>Note that the landscape of deploying DL inference pipeline models is still evolving, and we just learned some foundational skills. You are encouraged to explore more from the <em class="italic">Further reading</em> section for more advanced topics.</p>
			<p>Now that we know how to deploy and host a DL inference pipeline, we will learn how to do model explainability in the next chapter, which is of great importance for trustworthy and interpretable model prediction results in many real-world scenarios.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor110"/>Further reading</h1>
			<ul>
				<li><em class="italic">An Introduction to TinyML</em>: <a href="https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79">https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79</a></li>
				<li><em class="italic">Performance Optimizations and MLFlow Integrations – Seldon Core 1.10.0 Released</em>: <a href="https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/">https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/</a></li>
				<li><em class="italic">Ray &amp; MLflow: Taking Distributed Machine Learning Applications to Production</em>: <a href="https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88">https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88</a></li>
				<li><em class="italic">Managing your machine learning lifecycle with MLflow and Amazon SageMaker</em>: <a href="https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/</a></li>
				<li><em class="italic">Deploy A Locally Trained ML Model In Cloud Using AWS SageMaker</em>: <a href="https://medium.com/geekculture/84af8989d065">https://medium.com/geekculture/84af8989d065</a></li>
				<li><em class="italic">PyTorch vs TensorFlow in 2022</em>: <a href="https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/">https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/</a></li>
				<li><em class="italic">Try Databricks: Free Trial or Community Edition</em>: <a href="https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition">https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition</a></li>
				<li><em class="italic">MLOps with MLflow and Amazon SageMaker Pipelines</em>: <a href="https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238">https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238</a> </li>
				<li><em class="italic">PyTorch JIT and TorchScript</em>: <a href="https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff">https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff</a></li>
				<li><em class="italic">ML Model Serving Best Tools</em>: <a href="https://neptune.ai/blog/ml-model-serving-best-tools">https://neptune.ai/blog/ml-model-serving-best-tools</a></li>
				<li><em class="italic">Deploying Machine Learning models to production — Inference service architecture patterns</em>: <a href="https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080">https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080</a></li>
				<li><em class="italic">How to Deploy Large-Size Deep Learning Models into Production</em>: <a href="https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33">https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33</a></li>
				<li><em class="italic">Serving ML models at scale using Mlflow on Kubernetes</em>: <a href="https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e">https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e</a></li>
				<li><em class="italic">When PyTorch meets MLflow</em>: <a href="https://mlops.community/when-pytorch-meets-mlflow/">https://mlops.community/when-pytorch-meets-mlflow/</a></li>
				<li><em class="italic">Deploy a model to an Azure Kubernetes Service Cluster</em>: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python</a></li>
				<li><em class="italic">ONNX and Azure Machine Learning: Create and accelerate ML models</em>: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx">https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx</a></li>
			</ul>
		</div>
	</div></body></html>