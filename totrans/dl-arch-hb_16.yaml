- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Governing Deep Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型治理
- en: 'Deploying a model is just the beginning of its journey. Once it’s out in the
    real world, it’s like a living thing – it requires efficient use to make the most
    of it, upgrades to stay sharp, care to perform consistently well, and, eventually,
    a graceful exit. Imagine a car on the road: you start driving, but you also need
    to use the car effectively, fuel it, maintain it, and eventually replace it or
    its components. The same goes for deep learning models in action.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个模型只是它旅程的开始。一旦它进入现实世界，它就像一个活生生的生命体——它需要高效使用以最大化效益，需要升级以保持锐利，需要维护以保持一致性表现，最终，还需要优雅地退出。想象一下路上的一辆车：你开始驾驶，但你也需要有效使用它、加油、保养，最终更换它或它的组件。深度学习模型的运作也同样如此。
- en: Model governance acts as the guiding force that oversees the use of a model
    and maintains constant vigilance over its performance and context to ensure the
    continuous, consistent, and dependable delivery of value through the model. In
    the realm of deep learning, model governance is crucial for ensuring that these
    complex models adhere to the highest standards of quality, reliability, and fairness.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型治理作为引导力量，监督模型的使用，并始终保持对其性能和背景的警觉，确保模型能够持续、一致、可靠地传递价值。在深度学习领域，模型治理至关重要，它确保这些复杂的模型符合最高的质量、可靠性和公平性标准。
- en: 'This chapter delves into the three fundamental pillars of model governance
    for deep learning models: steering the ship of model utilization, which focuses
    on the appropriate application of deep learning models, keeping a watchful eye
    on its performance on all fronts with model monitoring, and ensuring it stays
    at its best in the ever-evolving landscape of deep learning with model maintenance.
    By implementing a robust model governance framework, deep learning architects
    can effectively manage the challenges posed by these intricate models and harness
    their immense potential to drive valuable insights and decisions in production.
    In this chapter, we will learn about these pillars of model governance in detail.
    *Figure 16**.1* shows a holistic view of the concept of model governance that
    we will explore in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了深度学习模型治理的三个基本支柱：引导模型利用的方向，着重于深度学习模型的合理应用；通过模型监控在各个方面保持对其性能的警觉；以及通过模型维护确保其在不断发展的深度学习领域保持最佳状态。通过实施健全的模型治理框架，深度学习架构师可以有效管理这些复杂模型所带来的挑战，并充分利用其巨大潜力，推动生产中的有价值的洞察和决策。本章将详细介绍这些模型治理的支柱。*图
    16.1* 展示了我们将在本章探讨的模型治理概念的整体视图：
- en: '![Figure 16.1 – Holistic overview of model governance in the context of concepts
    that will be introduced in this chapter](img/B18187_16_1.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.1 – 本章将介绍的概念背景下，模型治理的整体概述](img/B18187_16_1.jpg)'
- en: Figure 16.1 – Holistic overview of model governance in the context of concepts
    that will be introduced in this chapter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1 – 本章将介绍的概念背景下，模型治理的整体概述
- en: 'Specifically, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主题：
- en: Governing deep learning model utilization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型利用的治理
- en: Governing a deep learning model through monitoring
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过监控治理深度学习模型
- en: Governing a deep learning model through maintenance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过维护治理深度学习模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter covers a practical example of monitoring metrics and setting up
    alerts, leveraging the code from the previous tutorial in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models in Production*. This tutorial requires you to
    have a Linux machine with an NVIDIA GPU device ideally in Ubuntu with Python 3.10
    and the `nvidia-docker` tool installed. Additionally, we will require the following
    Python libraries to be installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了监控指标和设置警报的实际示例，利用前一教程中的代码，参考[*第15章*](B18187_15.xhtml#_idTextAnchor217)，*在生产中部署深度学习模型*。本教程要求你使用安装了
    Python 3.10 和 `nvidia-docker` 工具的 Ubuntu 系统，并且拥有一台配有 NVIDIA GPU 的 Linux 机器。此外，我们还需要安装以下
    Python 库：
- en: '`numpy`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`transformers==4.21.3`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.21.3`'
- en: '`nvidia-tensorrt==8.4.1.5`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-tensorrt==8.4.1.5`'
- en: '`torch==1.12.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch==1.12.0`'
- en: '`transformers-deploy`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-deploy`'
- en: '`Tritonclient`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tritonclient`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在GitHub上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16)。
- en: Governing deep learning model utilization
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型利用的治理
- en: 'Model utilization, the first pillar of model governance for deep learning models,
    is crucial for the responsible and ethical deployment of these sophisticated tools.
    In this section, we will explore the integral aspects of model utilization, including
    guardrail filters, accountability, compliance, validation, shared access, transparency,
    and decision support systems. By comprehensively addressing these aspects, deep
    learning architects can ensure effective model utilization that maximizes value
    from the model while mitigating potential risks and unintended consequences. Let’s
    dive deeper into these aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 模型利用，深度学习模型治理的第一支柱，对于这些复杂工具的负责任和道德部署至关重要。在本节中，我们将探讨模型利用的各个方面，包括护栏过滤器、问责制、合规性、验证、共享访问、透明度和决策支持系统。通过全面解决这些方面，深度学习架构师可以确保有效的模型利用，最大化模型的价值，同时减轻潜在的风险和意外后果。让我们深入了解这些方面：
- en: '**Guardrail filters**: These play a crucial role in ensuring that models operate
    within established boundaries, minimizing the risks associated with inaccurate
    or harmful predictions. These filters help maintain the original purpose of the
    models. While the objectives of using a model’s predictions can significantly
    vary based on individual use cases, several common types of guardrails are widely
    applicable:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**护栏过滤器**：这些过滤器在确保模型在既定边界内运行、最小化与不准确或有害预测相关的风险方面起着至关重要的作用。它们有助于保持模型的初衷。虽然使用模型预测的目标可以根据具体的使用案例大不相同，但一些常见类型的护栏在广泛应用中具有普适性：'
- en: '**Prevent harmful use on a per-prediction basis**: Harmful use of the model
    or its predictions can encompass a wide range of issues, including biases related
    to sensitive attributes, malicious attacks such as adversarial attacks, and harassment-related
    text generation. *Figure 16.2* shows the OpenAI ChatGPT’s way of displaying its
    predictions after the guardrail of harmful use has been triggered.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止有害使用每次预测**：模型或其预测的有害使用可能涉及广泛的问题，包括与敏感属性相关的偏见、恶意攻击（如对抗性攻击）和与骚扰相关的文本生成。*图16.2*展示了OpenAI
    ChatGPT在触发有害使用护栏后显示其预测的方式。'
- en: '![Figure 16.2 – OpenAI ChatGPT’s harmful use guardrail triggered response](img/B18187_16_2.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.2 – OpenAI ChatGPT的有害使用护栏触发响应](img/B18187_16_2.jpg)'
- en: Figure 16.2 – OpenAI ChatGPT’s harmful use guardrail triggered response
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2 – OpenAI ChatGPT的有害使用护栏触发响应
- en: '**Prevent usage of unconfident predictions on a per-prediction basis**: To
    maintain the reliability of a model’s output, it is essential to prevent the use
    of predictions with low confidence. The issue, however, is that regression model
    predictions do not have prediction values that could be treated as a confidence
    score. Additionally, although classification model prediction typically has a
    softmax operation applied to allow predictions to add up to 1, it is not properly
    calibrated toward actual statistical probabilities. *Conformal predictions* are
    a more battle-tested statistical and robust technique to provide a robust confidence
    interval for each prediction, allowing for a better understanding of the model’s
    certainty. Additionally, input data that goes out of training data bounds or has
    drifted may deteriorate the model’s performance and can be treated as a special
    case of unconfident predictions without even generating the predictions.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止使用低置信度的预测**：为了保持模型输出的可靠性，必须防止使用置信度低的预测。然而，问题在于回归模型的预测没有可以作为置信度得分的预测值。此外，尽管分类模型预测通常会应用softmax操作，使得预测值的和为1，但它并没有适当地校准为实际的统计概率。*一致性预测*是一种经过实战考验的统计和强大的技术，能够为每个预测提供一个稳健的置信区间，从而更好地理解模型的确定性。此外，超出训练数据边界或发生漂移的输入数据可能会降低模型的性能，甚至在没有生成预测的情况下将其视为低置信度预测的特殊情况。'
- en: '**Prevent the use of an inaccurate model**: By continuously monitoring and
    assessing a model’s accuracy performance, one can determine when to stop the usage
    of a model, especially in high-risk use cases, and proceed to perform model maintenance,
    which is to retrain and update the model.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止使用不准确的模型**：通过持续监控和评估模型的准确性表现，可以确定何时停止使用某个模型，特别是在高风险的使用场景下，并进行模型维护，即重新训练和更新模型。'
- en: '**Mitigating bias**: Guardrail filters can help minimize bias related to sensitive
    attributes, such as race, gender, or ethnicity. By preventing the model from producing
    predictions that may lead to discriminatory outcomes, guardrail filters contribute
    to a more equitable and fair application of these technologies.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少偏见**：护栏过滤器有助于最小化与敏感属性（如种族、性别或民族）相关的偏见。通过防止模型产生可能导致歧视性结果的预测，护栏过滤器有助于更公平、公正地应用这些技术。'
- en: '**Prevent known data conditions that can negatively affect the model’s performance**:
    For example, face recognition systems should only predict on frontal, unobstructed
    faces without masks or glasses. Adversarial performance analysis, introduced in
    *Chapter 14*, *Analyzing Adversarial Performance*, must be performed prior to
    deployment to identify the traits that could negatively affect the model’s performance.
    During deployment, appropriate thresholds of the identified traits that are estimated
    to deteriorate the model’s performance can be applied as a guardrail for prediction
    prevention.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止已知数据条件对模型性能产生负面影响**：例如，面部识别系统应仅对正面无遮挡且没有戴口罩或眼镜的面部进行预测。在*第14章* *分析对抗性表现*中介绍的对抗性表现分析必须在部署前进行，以识别可能对模型性能产生负面影响的特征。在部署过程中，可以对已识别的特征施加适当的阈值，这些特征预计会恶化模型的表现，并作为预测预防的护栏。'
- en: '**Implement human-in-the-loop oversight only for critical predictions**: In
    high-stakes scenarios, such as medical drug recommendations, it is vital to involve
    human experts in the decision-making process, and higher-level experts when specific
    predictions are made.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅对关键预测实施人工干预**：在高风险场景中，例如医学药物推荐，涉及决策过程的人工专家是至关重要的，且在进行特定预测时，需要更高层次的专家参与。'
- en: '**Accountability**: This entails the clear assignment of roles and responsibilities,
    and addresses questions related to model ownership, compliance with regulations,
    training data, and the approval process at each stage of development. Accountability
    is a critical aspect of AI and machine learning systems, ensuring that there is
    a clear understanding of roles, responsibilities, and ownership throughout the
    model’s life cycle. It encompasses the following two facets:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：这涉及到角色和责任的明确分配，并解决与模型所有权、合规性、训练数据以及每个开发阶段的审批流程相关的问题。问责制是人工智能和机器学习系统的关键方面，确保在整个模型生命周期中，角色、责任和所有权有明确的理解。它包括以下两个方面：'
- en: '**Model ownership**: Clearly defining who owns the model is essential for establishing
    accountability. This includes determining the parties responsible for the model’s
    development, maintenance, and updates, as well as those who will be held liable
    for any adverse consequences resulting from the model’s use. Some additional key
    considerations related to model ownership are the following:'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型所有权**：明确模型的所有者对于建立问责制至关重要。这包括确定负责模型开发、维护和更新的各方，以及那些因模型使用而可能产生不良后果的责任方。与模型所有权相关的其他关键考虑因素包括：'
- en: '**Handling personnel changes**: In the event of a model owner’s departure or
    role change within the organization, a well-defined process should be in place
    to transfer ownership and responsibilities to another suitable individual or team.
    This ensures that the model continues to receive proper oversight and maintenance
    and that accountability remains clear.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理人员变动**：当模型所有者离职或角色变动时，应该有一个明确定义的流程，将所有权和责任转交给另一位合适的个人或团队。这确保了模型能够继续得到适当的监督和维护，且问责制得以清晰保持。'
- en: '**Shared access and default admin roles**: To promote effective model governance
    and minimize potential disruptions, it is vital to establish shared access and
    default admin roles. This allows multiple team members to oversee the model’s
    development, maintenance, and updates, reducing the dependency on a single individual.
    Such shared access should be accompanied by clear guidelines on roles and responsibilities
    to avoid confusion and maintain accountability.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享访问和默认管理员角色**：为了促进有效的模型治理并减少潜在的中断，建立共享访问和默认管理员角色至关重要。这允许多个团队成员监督模型的开发、维护和更新，从而减少对单个个体的依赖。此类共享访问应配备明确的角色和责任指南，以避免混乱并保持问责制。'
- en: '**Handling open source models**: Proprietary models are typically developed
    within a single organization, which makes it straightforward to deal with accountability
    with the considerations discussed previously. In open source models, the development
    process often involves multiple contributors from diverse backgrounds, which makes
    establishing accountability more challenging. To address this, it is essential
    to provide clear guidelines for contributions, maintain transparent documentation
    of the model’s development history, and implement community-driven governance
    structures or assign a core group of maintainers to oversee the project. As an
    alternative, a key model owner can be established in an organization that assumes
    all responsibility for using the open source model in that organization.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理开源模型**：专有模型通常在单一组织内开发，这使得处理问责制变得简单，遵循前述的相关考量。而在开源模型中，开发过程通常涉及来自不同背景的多个贡献者，这使得建立问责制变得更加具有挑战性。为了解决这个问题，必须提供明确的贡献指南，保持模型开发历史的透明文档，并实施社区驱动的治理结构，或者指定一组核心维护者来监督该项目。作为替代方案，可以在组织中确立一个主要模型负责人，负责该组织中开源模型的所有使用责任。'
- en: '**Predictions ownership**: Predictions ownership in high-risk use cases is
    crucial for maintaining accountability and ensuring accurate, reliable outcomes.
    Since raw predictions may not always be easily understandable, post-processing
    steps are often needed to convert them into more digestible insights or nested
    outcomes. Approvals of the outcomes at each post-processing stage further ensure
    the quality and relevance of the final outcomes, fostering the responsible and
    effective use of AI and machine learning models.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测责任**：在高风险应用场景中，预测责任对于保持问责制和确保准确、可靠的结果至关重要。由于原始预测结果可能并不总是容易理解，因此通常需要后处理步骤将其转化为更易于理解的洞察或嵌套结果。在每个后处理阶段对结果的批准进一步确保了最终结果的质量和相关性，从而促进了AI和机器学习模型的负责任和有效使用。'
- en: '**Model and prediction transparency**: This is essential for fostering trust
    and understanding in AI systems. This entails offering clear explanations and
    relevant information about the model’s development, including its architecture,
    training data, and methodology. Providing such insights enables users to grasp
    how the model generates predictions and ensures that the AI system aligns with
    ethical and responsible practices, ultimately contributing to better decision-making
    and more reliable outcomes. These can be the same explanations that were used
    to understand and compare different models and predictions during the model development
    stage.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型与预测透明度**：这对于促进对AI系统的信任和理解至关重要。这意味着要提供关于模型开发的清晰解释和相关信息，包括其架构、训练数据和方法论。提供这些洞察使用户能够理解模型如何生成预测，并确保AI系统与道德和负责任的实践一致，最终有助于更好的决策和更可靠的结果。这些解释可以是模型开发阶段用于理解和比较不同模型和预测的相同内容。'
- en: '**Decision support systems**: This involves building interfaces or platforms
    that enable decision-makers to interact with model predictions and insights. This
    includes providing user-friendly dashboards, reports, and visualization tools
    in the system while incorporating business rules, regulations, and policies into
    the decision-making process. This is again useful in high-risk use cases.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策支持系统**：这包括构建能够让决策者与模型预测和洞察互动的界面或平台。系统中应提供用户友好的仪表板、报告和可视化工具，同时将业务规则、法规和政策纳入决策过程中。这在高风险应用场景中再次具有重要作用。'
- en: Now, we will dive into the second component of model governance, which is about
    monitoring deployed models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨模型治理的第二个组成部分——监控已部署的模型。
- en: Governing a deep learning model through monitoring
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过监控来管理深度学习模型
- en: Model monitoring is essential for maintaining the performance, reliability,
    and fairness of deep learning models throughout their life cycle. As data landscapes
    and business requirements evolve, continuous monitoring enables the early detection
    of issues such as model drift, performance degradation, and potential biases,
    thereby ensuring the consistent delivery of accurate and valuable predictions.
    This process involves the collection and analysis of key performance metrics,
    the ongoing evaluation of model outputs against ground-truth data, and the identification
    of any emerging trends that could impact the model’s efficacy. By implementing
    a robust model monitoring framework, deep learning architects can proactively
    address challenges and make informed decisions about model updates, refinements,
    and retraining, ultimately optimizing the model’s value and mitigating risks associated
    with its deployment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控对于维护深度学习模型在整个生命周期中的性能、可靠性和公平性至关重要。随着数据环境和业务需求的变化，持续监控可以帮助及早发现问题，如模型漂移、性能下降和潜在的偏差，从而确保持续交付准确且有价值的预测。这个过程包括收集和分析关键性能指标，持续评估模型输出与真实数据的对比，并识别任何可能影响模型效能的新兴趋势。通过实施一个强大的模型监控框架，深度学习架构师可以主动应对挑战，并做出有关模型更新、优化和再训练的明智决策，从而最大化模型的价值并减少与模型部署相关的风险。
- en: 'Model monitoring holds value only when it results in corrective actions addressing
    deteriorating performance or concerning conditions. Thus, the objective of monitoring
    should be to identify and rectify undesirable behavior. The actions that can be
    taken are more broadly grouped into the third pillar of model governance, called
    model maintenance, which we will discuss separately in the next section. Now,
    let’s delve into the various categories and specific metrics for a deployed machine
    learning model, accompanied by examples of conditions that can prompt the initiation
    of model maintenance procedures:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控只有在能够采取纠正措施来应对性能下降或不良情况时才具有价值。因此，监控的目标应该是识别和纠正不良行为。可以采取的措施更广泛地归入模型治理的第三支柱——模型维护，我们将在下一节单独讨论。现在，让我们深入探讨已部署机器学习模型的各种类别和具体指标，并举例说明哪些条件可能促使启动模型维护程序：
- en: '**Model accuracy-based performance metrics**: These are the typical model evaluation
    metrics we introduced more comprehensively in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161),
    *Exploring Model Evaluation Methods*, such as accuracy, recall, precision, F1
    score, AUC-ROC, and log-loss. The same metrics that were used for model evaluation
    in the model development and delivery model insights stage should be reused here.
    These metrics can be monitored when the true labels can be obtained at a future
    time, in two ways:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型准确性的性能指标**：这些是我们在[*第10章*](B18187_10.xhtml#_idTextAnchor161)《探索模型评估方法》中更全面介绍的典型模型评估指标，如准确率、召回率、精确度、F1分数、AUC-ROC和对数损失。在模型开发和交付模型洞察阶段使用的相同评估指标应在此重用。当在未来能够获得真实标签时，这些指标可以通过两种方式进行监控：'
- en: '**Naturally**: When the use case is a time-series use case to predict a future
    target or the target is just not immediately accessible to the model owner, the
    targets can be obtained in the future naturally'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然情况**：当使用场景是时间序列预测未来目标，或者目标对模型拥有者并非立即可得时，目标可以在未来自然获得。'
- en: '**Manual labeling**: Labeling is recommended to be carried out in a regular
    cadence with a sample of the historical production input data to verify the validity
    of the model performance'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动标注**：建议定期进行标注，选取一部分历史生产输入数据样本来验证模型性能的有效性。'
- en: Conditions that can cause a trigger of model maintenance here are using the
    same use case validity thresholds that were referred to in the model building
    and evaluation experimentation process. As emphasized in *Chapter 10*, *Exploring
    Model Evaluation Methods*, this threshold should ideally be tied to the business
    metrics threshold in some way.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导致模型维护触发的条件是使用在模型构建和评估实验过程中提到的相同使用场景有效性阈值。正如在*第10章*《探索模型评估方法》中强调的那样，这个阈值理想情况下应该与业务指标阈值有所关联。
- en: '**Data quality metrics**: Data quality metrics provide essential insights into
    the validity, characteristics, and consistency of the input data. Data quality
    is linked to the accuracy and bias performance of the model and thus any deviations
    from the norm can potentially cause accuracy degradations. Examples of such metrics
    are the following:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing or incomplete data count**: This refers to the number of instances
    in the dataset where the data is either absent or not fully available. This can
    impact the accuracy and reliability of the model, as it may not have enough information
    to infer a prediction from.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invalid data bounds count**: This refers to the instances where data values
    fall outside the acceptable or expected range. This can lead to incorrect model
    predictions, as the model may infer from incorrect data points that were not learned
    from.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier and anomaly indicator metrics**: They are used to identify unusual
    or extreme data points that deviate significantly from the overall pattern or
    trend in the dataset. This has the same root cause as invalid data bounds.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift**: This occurs when the distribution of input features in the
    data changes over time. This can happen due to various reasons, such as evolving
    data sources, changing user behavior, or external factors influencing the data
    generation process. Data drift may lead to a decline in model performance as the
    model was trained on a different distribution of data and may not generalize well
    to the new distribution. Monitoring for data drift helps in identifying when retraining
    or adjusting the model is necessary to maintain its accuracy and effectiveness.
    In [*Chapter 17*](B18187_17.xhtml#_idTextAnchor247), *Managing Drift Effectively
    in a Dynamic Environment*, we will dive into the techniques that we can use to
    detect data drift focused on deep learning-specific data inputs.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift**: It refers to the change in the relationship between input
    features and the target variable over time. This change can cause a previously
    accurate model to degrade in performance as the model’s learned patterns no longer
    align with the evolving relationships. This is also related to the label consistency
    metric introduced in the data quality section in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015),
    *Deep Learning* *Life Cycle*.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System performance metrics**: These metrics help ensure that the deployed
    model meets the operational requirements. The key subgroups under system performance
    metrics are the following:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference latency**: Refers to the measurement of the time taken by the model
    to generate predictions or output from the input data. Low latency is crucial
    for real-time applications and user experiences, as it ensures the model provides
    quick and timely results.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput**: Measures the number of predictions or outputs the model can
    generate within a specific time frame. High throughput is vital for handling large-scale
    data processing and maintaining the desired level of performance, especially in
    high-demand scenarios.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：衡量模型在特定时间范围内能够生成的预测或输出的数量。高吞吐量对于处理大规模数据并保持所需的性能水平至关重要，尤其是在高需求场景中。'
- en: '**Resource utilization**: Evaluates the efficiency of resource usage, such
    as CPU, memory, and storage, by the model during its operation. Optimizing resource
    utilization ensures that the model can run efficiently on the available infrastructure,
    reducing costs and allowing for better scalability.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用率**：评估模型在运行过程中对资源（如CPU、内存和存储）的使用效率。优化资源利用率可以确保模型在可用的基础设施上高效运行，降低成本并提升可扩展性。'
- en: '**Queueing delay and request counts**: Queueing delay refers to the waiting
    time experienced by each request before being processed by the deployed deep learning
    model. Monitoring the queueing delay and the number of requests can help identify
    potential bottlenecks in the system and optimize the model’s capacity to handle
    multiple requests simultaneously.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排队延迟和请求数量**：排队延迟指的是每个请求在被部署的深度学习模型处理前所经历的等待时间。监控排队延迟和请求数量有助于识别系统中的潜在瓶颈，并优化模型同时处理多个请求的能力。'
- en: '**Alert and incident metrics**: These metrics help ensure timely identification
    and resolution of problems, enabling optimal system performance. They are as follows:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报和事件指标**：这些指标有助于确保及时识别和解决问题，从而实现系统性能的最佳化。它们如下：'
- en: '**Alert frequency**: This metric refers to the number of alerts generated over
    a specific time period, indicating potential issues or anomalies in the system.
    Monitoring alert frequency helps identify patterns and trends, enabling proactive
    measures to prevent or mitigate recurring problems.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报频率**：此指标指的是在特定时间段内生成的警报数量，表示系统中的潜在问题或异常。监控警报频率有助于识别模式和趋势，从而采取主动措施防止或缓解重复出现的问题。'
- en: '**Alert severity**: This measures the degree of impact an issue has on overall
    system performance. By categorizing alerts based on severity, it is possible to
    prioritize and address the most critical issues first, ensuring efficient use
    of resources and minimizing negative impacts on the system.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报严重性**：这衡量了一个问题对整体系统性能的影响程度。通过根据严重性对警报进行分类，可以优先处理最关键的问题，从而确保资源的高效利用，并最大程度地减少对系统的负面影响。'
- en: '**Incident resolution time**: This is the time taken to address and resolve
    incidents arising from alerts. Tracking this metric helps evaluate the effectiveness
    of the incident response process and identify areas for improvement, ultimately
    leading to faster resolution times and better system performance.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件解决时间**：这是处理和解决由警报引发的事件所花费的时间。跟踪这个指标有助于评估事件响应过程的有效性，并识别需要改进的地方，最终实现更快的解决时间和更好的系统性能。'
- en: '**Model fairness and bias metrics**: The same metrics that were introduced
    in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    to compare different models in development, can also be applied to monitor model
    fairness on a deployed model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型公平性和偏差指标**：在[**第13章**](B18187_13.xhtml#_idTextAnchor196)中介绍的用于比较不同开发模型的相同指标，也可以应用于监控已部署模型的公平性。'
- en: '**Business metrics**: Monitoring business-related metrics is crucial for evaluating
    the impact of a deployed deep learning model on the organization’s goals and ensuring
    its alignment with business objectives. Not everything can be monitored with numbers,
    so figure out the components that are quantifiable. Here are some metrics to consider:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务指标**：监控与业务相关的指标对于评估部署的深度学习模型对组织目标的影响至关重要，并确保其与业务目标的对齐。并非所有东西都能通过数字来监控，因此要找出那些可以量化的组件。以下是一些需要考虑的指标：'
- en: '**Key Performance Indicators** (**KPIs**): Identify and track KPIs that are
    directly influenced by the model’s predictions, such as revenue, customer satisfaction,
    return on investment, or operational efficiency. This helps assess the model’s
    overall contribution to the business.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键绩效指标**（**KPIs**）：识别并跟踪受模型预测直接影响的关键绩效指标，例如收入、客户满意度、投资回报率或运营效率。这有助于评估模型对业务的整体贡献。'
- en: '**User adoption and engagement**: Monitor how users interact with the model,
    including usage patterns, frequency, and feedback. This can provide insights into
    the model’s relevance, ease of use, and overall effectiveness in addressing user
    needs.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户采纳与参与度**：监控用户与模型的互动方式，包括使用模式、频率和反馈。这可以为模型的相关性、易用性及其在满足用户需求方面的整体效果提供洞察。'
- en: Incorporating the monitoring of the various metric groups not only provides
    a comprehensive view of deep learning model performance, reliability, and fairness
    but also facilitates the identification of emerging trends and patterns. By closely
    monitoring these metrics, potential issues, such as model drift, performance degradation,
    and biases, can be proactively addressed, ensuring consistent delivery of accurate
    predictions. This also means that analyzing patterns from the monitored metrics
    is crucial in developing improvement plans to enhance deep learning model performance
    and address any potential issues.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监控各种指标组的变化，不仅可以全面了解深度学习模型的性能、可靠性和公平性，还有助于识别新兴趋势和模式。通过密切监控这些指标，可以积极解决潜在问题，如模型漂移、性能下降和偏见，确保准确预测的持续交付。这也意味着分析监控指标中的模式对于制定改进计划以提升深度学习模型性能和解决潜在问题至关重要。
- en: To effectively analyze and consume the metrics that are monitored, it is recommended
    to consolidate the key metrics in a comprehensive dashboard, which allows for
    easy tracking and assessment of the model’s overall health, and ultimately enhances
    the monitoring process. Grafana, a popular open source analytics and monitoring
    platform, can effectively meet these requirements by offering a variety of features
    and integrations. As we move forward, we will explore a practical tutorial on
    monitoring deep learning models by using NVIDIA Triton Inference Server, Prometheus,
    and Grafana.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为有效分析和利用所监控的指标，建议将关键指标整合到一个全面的仪表板中，以便轻松跟踪和评估模型的整体健康状况，并最终增强监控过程。Grafana，一个流行的开源分析和监控平台，可以通过提供各种功能和集成有效地满足这些要求。随着我们的进展，我们将探讨如何通过使用NVIDIA
    Triton推理服务器、Prometheus和Grafana来监控深度学习模型的实用教程。
- en: Monitoring a deployed deep learning model with NVIDIA Triton Server, Prometheus,
    and Grafana
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NVIDIA Triton Server、Prometheus和Grafana监控部署的深度学习模型
- en: NVIDIA Triton Server hosts configured metrics via a REST HTTP API in Prometheus
    format, offering real-time insights without persisting historical data. To persist
    metrics data over time, Prometheus needs to be configured to connect with NVIDIA
    Triton Server. While Prometheus tracks and logs metrics over time, it lacks visualization
    capabilities. This is where Grafana comes in. It’s a platform that can leverage
    Prometheus-logged data to create dynamic dashboards with custom graphs and tables.
    Prometheus conveniently shares its logged information through a separate REST
    HTTP API, facilitating Grafana’s seamless connectivity. Additionally, Grafana
    allows alert rules to be set up reliably.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA Triton Server通过Prometheus格式的REST HTTP API托管配置的指标，提供实时洞察，无需保留历史数据。要持续保存指标数据，需要配置Prometheus与NVIDIA
    Triton Server连接。虽然Prometheus可以随时间跟踪和记录指标，但它缺乏可视化能力。这就是Grafana的用武之地。它是一个平台，可以利用Prometheus记录的数据创建具有自定义图形和表格的动态仪表板。Prometheus通过单独的REST
    HTTP API方便地共享其记录的信息，从而实现与Grafana的无缝连接。此外，Grafana允许可靠地设置警报规则。
- en: The first step in monitoring is to plan the metrics that we want to monitor,
    which we will discuss next.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的第一步是计划我们想要监控的指标，我们将在接下来讨论。
- en: Choosing metrics to monitor
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择要监控的指标
- en: 'Any deployed model hosted through NVIDIA Triton Server will by default support
    a variety of standard metrics. These metrics are the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过NVIDIA Triton Server托管的任何部署模型默认支持各种标准指标。这些指标包括以下内容：
- en: '**Inference request metrics**: Success count, failure count, inference count,
    and execution count'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理请求指标**：成功次数、失败次数、推理次数和执行次数'
- en: '**GPU-related metrics**: Power usage, power limit, energy consumption, GPU
    utilization, GPU total memory, and GPU used memory'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与GPU相关的指标**：功率使用、功率限制、能量消耗、GPU利用率、GPU总内存和GPU已使用内存'
- en: '**CPU-related metrics**: CPU utilization, CPU total memory, and CPU used memory'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与CPU相关的指标**：CPU利用率、CPU总内存和CPU已使用内存'
- en: '**Response cache metrics**: Cache hit count, cache miss count, cache hit time,
    and cache miss time'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应缓存指标**：缓存命中次数、缓存未命中次数、缓存命中时间和缓存未命中时间'
- en: 'Note that these metrics can be manually disabled. In this practical example,
    we will be leveraging the deployed language model implementation from the previous
    chapter in using NVIDIA Triton Server and additionally using the Prometheus and
    Grafana tools. The default standard metrics that NVIDIA Triton Server logs are
    useful, but we also need potential custom metrics that can be useful for a business
    and are specific to a language model. It is well documented that NVIDIA Triton
    Server supports custom metrics through their C API, which means you need to develop
    C code! However, a fairly new way to support custom metrics, since NVIDIA Triton
    Server version 23.05, is that you can define custom metrics for NVIDIA Triton
    Server using Python! We will be exploring this new feature in our practical tutorial,
    where we will be exploring the following custom metrics for a language model that
    can be useful:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of tokens processed**: The larger the input data, the longer a request
    can take'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of tokens generated**: The larger the number of output tokens, the
    longer a request can take'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flesch reading score**: This is a reading comprehension metric that measures
    how well a text can be understood, which can be a useful business metric, as generated
    text needs to be well understood to be useful'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are ready to dive into the practical example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Tracking and visualizing the chosen metrics over time
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we start, make sure you have installed `nvidia-docker`, Prometheus,
    Node Exporter, and Grafana version v10.0.3\. Also, make sure Prometheus and Grafana
    are callable from any location in the command line. Let’s start the process in
    a step-by-step manner, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We are leveraging the code from [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models to Production*. The first change that is needed
    here is that we will make changes on top of `TritonPythonModel` in the `model.py`
    file. The Custom Metrics API in Python from NVIDIA allows you to define and log
    metrics directly in the three methods that you can define in `TritonPythonModel`.
    These methods are `initialize`, `execute`, and `finalize`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, the additional libraries that we will use are `textstat` and `nltk`,
    which will be used to compute the readability score:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step is to initialize the metric logging instance in the `initialize`
    method. Prometheus supports four metric types: counters for increasing values,
    gauges for fluctuating values, histograms for observing value distribution, and
    summaries for tracking quantiles in data. The three custom metrics we plan to
    add are inherently fluctuating values, and any histograms can be created in Grafana.
    Let’s define the metric family that we will use. You can set the name, description,
    and type of metric for a family. Additionally, you can create many metric families
    for any metric logical group. For our case, all three metrics we plan for are
    business metrics and are fluctuating values:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s define the metrics in this metric family:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Conveniently, you can do versioning of the metric in case any logic needs to
    be changed, which makes for a more robust monitoring process.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will be logging the metrics in every execution. We will be defining
    a helper method that will in turn be executed at the end of the `execute` method:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will use this helper method under the `execute` method:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we need to start the NVIDIA Triton Server `nvidia-docker` instance with
    the same command, which is the following:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After that, we are in the Docker environment, where the next step is to install
    the necessary libraries:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: For production usage, please be sure to create a Docker image where all the
    libraries are fixed, and you don’t need to manually install libraries anymore.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can execute the `python triton_client.py` command in the command line
    and get your predictions.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The default metrics and the custom metrics are immediately hosted in the URL
    `http://localhost:8002/metrics`, where you can view the real-time metrics in text
    form. `localhost` can be replaced with the IP of your remote server if you are
    using one. The following snippet shows the real-time Prometheus-formatted metrics
    that can be found at the preceding URL:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As these are only real-time metrics, we need to set up a local server or use
    an online Prometheus server. In this step, we will opt for a locally hosted Prometheus
    server where the following commands need to be run in the command line:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to add the NVIDIA Triton Server endpoint into the Prometheus configuration
    file to track metrics. To do that, execute `sudo gedit /etc/prometheus/prometheus.yml`
    in the command line and add the following job details:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that, Prometheus is all set up to log metrics from NVIDIA Triton Server.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prometheus hosts its web app by default with port `9090`. So, accessing the
    link `localhost:9090` in a web browser will take you to the Prometheus home page.
    Going to the **Status** tab and clicking on **Targets** in the dropdown will show
    the following screenshot, which verifies that the Triton endpoint is being tracked.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 16.3 – Prometheus web app home page on the left and targets that Prometheus
    is tracking and polling metrics from on the right](img/B18187_16_3.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – Prometheus web app home page on the left and targets that Prometheus
    is tracking and polling metrics from on the right
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus by default doesn’t include user account enforcement but it can be
    configured to be enforced.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will set up Grafana to connect to the locally hosted Prometheus instance.
    First, we have to start up the Grafana service by executing the following command
    in the command line:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 16.4 – Screenshots showing how to navigate to the Add data source
    page in the Grafana web app](img/B18187_16_4.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – Screenshots showing how to navigate to the Add data source page
    in the Grafana web app
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Next, click on Prometheus as the data source, where you will be presented with
    the screen shown in *Figure 16**.5 (a)*. Set the Prometheus default hosted web
    app link to `http://localhost:9090` and click on **Save & Test**. This should
    result in the success screen shown in *Figure* *16**.5 (b)*.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击Prometheus作为数据源，您将看到如*图16.5 (a)*所示的屏幕。将Prometheus默认托管的Web应用链接设置为`http://localhost:9090`，然后点击**保存并测试**。这应该会显示如*图16.5
    (b)*所示的成功界面。
- en: '![Figure 16.5 – Grafana Prometheus data source settings tab in (a) and successfully
    created screen (b)](img/B18187_16_5.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – Grafana Prometheus数据源设置标签页（a）和成功创建的屏幕（b）](img/B18187_16_5.jpg)'
- en: Figure 16.5 – Grafana Prometheus data source settings tab in (a) and successfully
    created screen (b)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – Grafana Prometheus数据源设置标签页（a）和成功创建的屏幕（b）
- en: 'With that, you will see that the data source has been created as shown in *Figure
    16**.6 (a)*. At this point, we will be able to create a dashboard to visualize
    the metrics we are monitoring. Grafana allows you to create dashboards in three
    ways: importing through its publicly shared dashboard IDs, importing through an
    exported dashboard JSON file, and creating a new dashboard. In Grafana, you can
    create many types of visualizations manually using the in-built visualization
    UI builder system or the **PromQL**-based visualizations and choose how you want
    them to be displayed. However, in this tutorial, we will be using a ready-made
    dashboard with visualizations by importing it through a dashboard JSON file. To
    do that, navigate to the dashboard page using the same three-line button dropdown
    shown in *Figure 16**.4 (a)*. Once, you are on the dashboard page, click on **New**
    and then on **Import**, as shown in *Figure* *16**.6 (b)*.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，您将看到数据源已成功创建，如*图16.6 (a)*所示。此时，我们可以创建一个仪表盘来可视化我们正在监控的指标。Grafana允许通过三种方式创建仪表盘：通过其公开共享的仪表盘ID导入，通过导出的仪表盘JSON文件导入，以及创建新的仪表盘。在Grafana中，您可以使用内置的可视化UI构建器系统或基于**PromQL**的可视化手动创建多种类型的可视化，并选择它们的展示方式。然而，在本教程中，我们将通过导入一个已有的仪表盘JSON文件来使用现成的可视化。为此，使用*图16.4
    (a)*所示的三行按钮下拉菜单导航到仪表盘页面。进入仪表盘页面后，点击**新建**，然后点击**导入**，如*图16.6 (b)*所示。
- en: '![Figure 16.6 – Grafana Data sources tab showing the created data source and
    the Dashboards tab showing the dropdown of the New button](img/B18187_16_6.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – Grafana数据源标签页显示已创建的数据源，仪表盘标签页显示新建按钮的下拉菜单](img/B18187_16_6.jpg)'
- en: Figure 16.6 – Grafana Data sources tab showing the created data source and the
    Dashboards tab showing the dropdown of the New button
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – Grafana数据源标签页显示已创建的数据源，仪表盘标签页显示新建按钮的下拉菜单
- en: Drag the provided `Triton Inference Server-1692252636911.json` file straight
    into the import area and then connect to the Prometheus database you created,
    and you’ll see the dashboard shown in *Figure 16**.7*.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提供的`Triton Inference Server-1692252636911.json`文件直接拖入导入区域，然后连接到您创建的Prometheus数据库，您将看到如*图16.7*所示的仪表盘。
- en: '![Figure 16.7 – A custom Grafana dashboard for the monitoring tutorial](img/B18187_16_7.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7 – 用于监控教程的自定义Grafana仪表盘](img/B18187_16_7.jpg)'
- en: Figure 16.7 – A custom Grafana dashboard for the monitoring tutorial
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 – 用于监控教程的自定义Grafana仪表盘
- en: Note that there were two GPUs in the machine that generated this metric, which
    is why there are two GPU stats. This visualization effectively represents most
    of the default NVIDIA Triton Server metrics, along with the three extra custom
    metrics we added, by displaying them on a graph that captures their historical
    values up to the present moment. However, hardware-resource-specific stats are
    an exception, as they are shown only in real-time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成该指标的机器上有两块GPU，这也是为什么会有两个GPU统计数据。此可视化有效地展示了大多数默认的NVIDIA Triton Server指标，并通过图形显示了这三个额外的自定义指标，这些图形捕捉了它们的历史值直到当前时刻。然而，硬件资源特定的统计数据是一个例外，它们仅以实时方式展示。
- en: Now, the component missing from monitoring is to create rules and conditions
    that would be considered an alarming incident, called the incident alerting component.
    Monitoring deployed deep learning models without alerts is like having a security
    camera but no alarm. You won’t know if something has gone wrong until it’s too
    late to do anything about it. Incidents can include deteriorating model accuracy,
    consistently delayed responses, consistent resource bottlenecks, consistently
    unexpected output variations during the monitoring of deployed deep learning models,
    and hardware failures. Grafana has an in-built alert management, notifications
    management, and contact management system that we will leverage in the following
    section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，监控中缺少的部分是创建规则和条件，这些规则和条件被认为是警报事件，称为事件警报组件。没有警报的深度学习模型监控就像有了安防摄像头，却没有警报系统。你不会知道问题何时发生，直到为时已晚。事件可能包括模型准确度下降、响应延迟、资源瓶颈、在监控已部署深度学习模型时的意外输出变化和硬件故障等。Grafana
    具有内建的警报管理、通知管理和联系人管理系统，我们将在接下来的部分中利用这些功能。
- en: Setting up alerts with Grafana
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Grafana 设置警报
- en: 'Let’s go through the steps on how to set up alerts with Grafana:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用 Grafana 设置警报的步骤：
- en: Click on the **Alerting** tab, shown in *Figure 16**.4 (a)*, and then on **Alert
    rules**. You will see the screen shown in *Figure 16**.8*.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**警报**标签，如*图 16**.4 (a)*所示，然后点击**警报规则**。你将看到如*图 16**.8*所示的界面。
- en: '![Figure 16.8 – Alert rules tab settings for NVIDIA Triton request failure
    alert rule](img/B18187_16_8.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.8 – NVIDIA Triton 请求失败警报规则的警报规则标签设置](img/B18187_16_8.jpg)'
- en: Figure 16.8 – Alert rules tab settings for NVIDIA Triton request failure alert
    rule
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8 – NVIDIA Triton 请求失败警报规则的警报规则标签设置
- en: In this example, we will set an alert to trigger when there is any failed NVIDIA
    Triton Server inference request. So, in the same tab, choose the `nv_inference_request_failure`
    metric tab, and set the threshold to a number that is lower than 1 so that a single
    failed request will trigger the alarm. In *Figure 16**.8*, the number is set to
    `0.8`.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将设置一个警报，当有任何 NVIDIA Triton Server 推理请求失败时触发。所以，在同一标签页中，选择`nv_inference_request_failure`指标标签，并将阈值设置为小于
    1 的数值，这样单个失败请求就会触发警报。如*图 16**.8*所示，数值被设置为`0.8`。
- en: Next, set the evaluation interval to be one minute and to raise an alarm only
    if there are consistent request failures for five minutes straight, as shown in
    *Figure 16**.9*. Then, click on the **Save and** **exit** button.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将评估间隔设置为一分钟，并且仅在连续五分钟内出现请求失败时才会触发警报，如*图 16**.9*所示。然后，点击**保存并退出**按钮。
- en: '![Figure 16.9 – Evaluation interval settings for alert rules](img/B18187_16_9.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.9 – 警报规则的评估间隔设置](img/B18187_16_9.jpg)'
- en: Figure 16.9 – Evaluation interval settings for alert rules
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9 – 警报规则的评估间隔设置
- en: 'There are three possible statuses that alerts in Grafana can have: **Normal**,
    which indicates that the condition wasn’t triggered; **Pending**, which indicates
    that the condition was partially triggered but there isn’t a consistent behavior
    yet; and **Firing**, which indicates that the condition has been consistently
    satisfied and an alarm has been triggered. Now that an alert rule is saved and
    created, you will see the screen shown in *Figure 16**.10 (a)*, where the status
    is **Normal**. *Figure 16**.10 (b)* shows the **Pending** stage, where a failure
    has been detected but is not yet consistent enough to send an alert. *Figure 16**.10
    (c)*, on the other hand, shows the **Firing** stage, where the failure has consistently
    happened per the configured time interval.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 中的警报可能有三种状态：**正常**，表示条件未触发；**待处理**，表示条件部分触发，但行为还不一致；**触发**，表示条件已持续满足，并且警报已被触发。现在，警报规则已保存并创建，你将看到如*图
    16**.10 (a)*所示的界面，状态为**正常**。*图 16**.10 (b)*展示了**待处理**阶段，其中检测到失败，但还不够一致，无法触发警报。另一方面，*图
    16**.10 (c)*展示了**触发**阶段，其中故障已按照配置的时间间隔持续发生。
- en: '![Figure 16.10 – Alert status of Normal in (a), Pending in (b), and Firing
    in (c)](img/B18187_16_10.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.10 – 正常状态（a）、待处理状态（b）和触发状态（c）的警报状态](img/B18187_16_10.jpg)'
- en: Figure 16.10 – Alert status of Normal in (a), Pending in (b), and Firing in
    (c)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10 – 正常状态（a）、待处理状态（b）和触发状态（c）的警报状态
- en: 'To configure where and who these alerts will be sent to, we’ll work with the
    **Contact points** and **Notification policies** tabs in the **Alerting** section.
    Let’s start by clicking on the **Contact points** tab to set up the individuals
    who will receive notifications. You can even organize these into groups, but for
    simplicity in this tutorial, we’ll have notifications sent to ourselves. Grafana
    offers various contact platform integrations: Alertmanager, Cisco Webex Teams,
    DingDing, Discord, Email, Google Chat, Kafka REST policy, LINE, Microsoft Teams,
    Opsgenie, PagerDuty, Pushover, Sensu Go, Slack, Telegram, Threema Gateway, VictorOps,
    Webhook, and WeCom. To keep things straightforward, we’ll choose a widely available
    integration type: email. Grafana uses the **sSMTP** software for sending emails,
    so ensure you have an email account with credentials set up before proceeding.
    Within the contact points settings, provide your name and email, then click on
    **Test** to generate a test notification to confirm that the credentials are accurate.
    Once you’ve verified that you’ve received the email notification, save your settings.
    Refer to *Figure 16**.11* for an example of the settings interface.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置这些警报发送到哪里以及发送给谁，我们将使用**联系人**和**通知策略**选项卡，这些都在**警报**部分。首先，点击**联系人**选项卡，设置将接收通知的人员。你甚至可以将这些人员组织成小组，但为了简化本教程，我们将通知发送给自己。Grafana
    提供了多种联系方式集成：Alertmanager、Cisco Webex Teams、钉钉、Discord、电子邮件、Google Chat、Kafka REST
    策略、LINE、Microsoft Teams、Opsgenie、PagerDuty、Pushover、Sensu Go、Slack、Telegram、Threema
    Gateway、VictorOps、Webhook 和 WeCom。为了简化操作，我们将选择一个广泛可用的集成类型：电子邮件。Grafana 使用**sSMTP**软件发送电子邮件，因此在继续操作之前，请确保你已设置了电子邮件账户并配置了凭据。在联系人设置中，输入你的姓名和电子邮件，然后点击**测试**以生成一个测试通知，确认凭据准确。确认收到电子邮件通知后，保存设置。有关设置界面的示例，请参见*图16**.11*。
- en: '![Figure 16.11 – Contact points tab with email set up](img/B18187_16_11.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图16.11 – 设置电子邮件的联系人选项卡](img/B18187_16_11.jpg)'
- en: Figure 16.11 – Contact points tab with email set up
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 – 设置电子邮件的联系人选项卡
- en: Next, we need to link up the contact as part of the default notification policy.
    Proceed to the **Notification policies** tab, click on **Settings**, and change
    the default contact point to be the email contact we set in *step 4*. You will
    then see a similar screen to *Figure 16**.12*.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将联系人与默认通知策略关联起来。转到**通知策略**选项卡，点击**设置**，并将默认联系点更改为我们在*第4步*中设置的电子邮件联系人。然后你将看到一个类似于*图16**.12*的界面。
- en: '![Figure 16.12 – The default notification policy set up to notify us](img/B18187_16_12.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图16.12 – 设置为通知我们的默认通知策略](img/B18187_16_12.jpg)'
- en: Figure 16.12 – The default notification policy set up to notify us
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12 – 设置为通知我们的默认通知策略
- en: Now, we are all set up to receive email notifications! As a challenge, try to
    figure out ways you can make the inference server request fail, and if you can’t,
    change the rule to something that will definitely trigger so you can get an example
    actual alert notification come through email. *Figure 16**.13* shows the example
    email that you will get through a mobile phone interface.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经设置好接收电子邮件通知！作为一个挑战，尝试找出一些方法让推理服务器请求失败，如果无法做到，修改规则为某个一定会触发的条件，这样你就能通过电子邮件收到一个实际的警报通知示例。*图16**.13*展示了你将通过手机界面收到的示例电子邮件。
- en: '![Figure 16.13 – Example triggered alert email notification with the Firing
    status](img/B18187_16_13.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图16.13 – 示例触发的警报电子邮件通知，状态为Firing](img/B18187_16_13.jpg)'
- en: Figure 16.13 – Example triggered alert email notification with the Firing status
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13 – 示例触发的警报电子邮件通知，状态为Firing
- en: With that, we have successfully set up a monitoring and alerting system for
    a deployed deep learning model! A notable caveat in this implementation is the
    fact that metrics are bundled up in the same execution as the model. A solution
    to make it decoupled to not increase the runtime for prediction-specific inference
    is to use the C API instead to build the custom metrics. If the time needed to
    get the metrics logged is not crucial, you can also consider hosting another “model”
    in NVIDIA Triton Server that takes in outputs from the prediction-specific model
    and log metrics. NVIDIA Triton Server also provides a tool called `perf_client`,
    which evaluates the runtime of different configurations, helping you optimize
    your system’s performance. Specifically, the tool measures and reports the throughput
    and latency with different load conditions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: However, just having monitoring and alerts doesn’t provide a full picture of
    model monitoring. We need to dive into those numbers, cross-reference them, spot
    connections, and find patterns. It’s like checking the fuel efficiency, tire pressure,
    and engine temperature of a car to ensure a smooth ride.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, alerts alone won’t fix issues. They’re like the car’s warning
    lights – they tell you something’s up, but you still need to pull over, pop the
    hood, and fix the problem. That’s where model maintenance comes in. In the next
    section, we’ll explore how to not only detect issues but also take action to keep
    your model running smoothly and efficiently over time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Governing a deep learning model through maintenance
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics logging, dashboard building, logged metrics analysis, and alerts are
    essential components of model monitoring, but they are only effective when followed
    by appropriate actions, which are covered under model maintenance. Model maintenance
    is akin to a skilled pit crew in a car race, regularly fine-tuning and optimizing
    the performance of deep learning models to keep them running efficiently and effectively.
    Like how a pit crew conducts rapid repairs, refuels, and adjusts the car’s components
    to adapt to changing race conditions, model maintenance involves updating the
    models to account for environmental changes, improving and refining the models
    with new data obtained from feedback loops, and performing incident responses
    on miscellaneous issues. This ensures that the models consistently stay on track,
    deliver valuable insights, and drive informed decision-making in the ever-evolving
    landscape of data and business requirements.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Key aspects of model maintenance comprehensively include the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**Establishing a feedback loop**: Establishing a feedback loop is vital for
    capturing real-world outcomes and validating model predictions, enabling deep
    learning practitioners to identify areas for improvement, and adapting the model
    accordingly.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retraining**: Retraining is an essential part of model maintenance, ensuring
    that the model stays up to date with the latest data and trends, thereby maintaining
    its accuracy and relevance. Regular retraining enables the model to learn from
    new insights and adapt to evolving data landscapes, ensuring consistent performance.
    Fortunately, for deep learning models, a fine-tuning process can be employed,
    which is much faster than a full retraining process. Two use cases that highlight
    the importance of frequent updates with model retraining are the following:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新训练**：重新训练是模型维护的一个关键环节，确保模型能够跟上最新的数据和趋势，从而保持准确性和相关性。定期重新训练可以使模型从新的见解中学习，并适应不断变化的数据环境，保证稳定的性能。幸运的是，对于深度学习模型，可以使用微调过程，这比完全重新训练过程要快得多。以下是两个强调频繁更新和模型重新训练重要性的使用案例：'
- en: '**E-commerce product recommendation**: Consumer preferences and product availability
    change rapidly in e-commerce. To provide relevant product recommendations, deep
    learning models need to be retrained frequently, maybe weekly or even daily, to
    understand the latest trends and customer behavior.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子商务产品推荐**：在电子商务中，消费者偏好和产品可用性变化迅速。为了提供相关的产品推荐，深度学习模型需要频繁地进行重新训练，可能是每周甚至每天一次，以理解最新的趋势和客户行为。'
- en: '**Social media sentiment analysis**: Social media platforms are constantly
    evolving with new trends, hashtags, and user behaviors. To accurately gauge public
    sentiment and opinion, deep learning models need to be retrained frequently, maybe
    quarterly, to account for these changes.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体情感分析**：社交媒体平台不断发展，出现新的趋势、标签和用户行为。为了准确评估公众情感和意见，深度学习模型需要经常重新训练，可能是每季度一次，以适应这些变化。'
- en: '**Incident response handling**: When alerts signal potential issues, it’s vital
    to have a dedicated response team to triage and address the problem promptly.
    This team should be well equipped to investigate the root cause, implement corrective
    measures, and prevent similar issues from recurring in the future. Let’s discover
    response-handling recommendations for different groups of incidents:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件响应处理**：当警报信号显示潜在问题时，拥有一个专门的响应团队来及时处理和解决问题至关重要。该团队应该具备充分的能力，调查根本原因，采取纠正措施，并防止类似问题在未来再次发生。让我们来看看不同类型事件的响应处理建议：'
- en: '**Data-related incidents**: These incidents occur when the model receives incorrect,
    incomplete, or biased input data. To handle such issues, the response team should
    work closely with the data provider to identify the cause, correct the data, and
    retrain the model as needed.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据相关事件**：这些事件发生在模型接收到不正确、不完整或有偏的数据输入时。处理此类问题时，响应团队应该与数据提供者紧密合作，识别原因，纠正数据，并在需要时重新训练模型。'
- en: '**Model performance incidents**: These incidents involve the model generating
    inaccurate or unexpected predictions. Proper handling requires collaboration between
    the model owner (responsible for model creation or approving the model usage)
    and the prediction owner (responsible for approving the usage of the predictions),
    as described in the *Governing deep learning model utilization* section earlier.
    They should analyze the model’s performance, identify potential issues in its
    architecture or training, and implement improvements to ensure better performance
    in the future.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能事件**：这些事件涉及模型生成不准确或出乎意料的预测。正确的处理方式需要模型拥有者（负责模型创建或批准模型使用）与预测拥有者（负责批准预测使用）之间的合作，正如前面*深度学习模型利用治理*部分所描述的那样。两者应该分析模型的表现，识别架构或训练中潜在的问题，并实施改进，确保未来能够提供更好的性能。'
- en: '**Infrastructure-related incidents**: These incidents are caused by hardware
    or software failures, affecting the model’s deployment environment. The response
    team should work with the infrastructure provider or team to resolve the issue
    and ensure the model runs smoothly.'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施相关事件**：这些事件由硬件或软件故障引起，影响模型的部署环境。响应团队应该与基础设施提供商或团队合作，解决问题，确保模型能够顺利运行。'
- en: '**Security incidents**: These incidents involve unauthorized access, data breaches,
    or other malicious activities targeting the model. The response team should follow
    the organization’s security policies, identify the threat, and take appropriate
    measures to mitigate the risk.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全事件**：这些事件涉及未经授权的访问、数据泄露或其他恶意活动，目标是模型。响应团队应该遵循组织的安全政策，识别威胁，并采取适当的措施来减轻风险。'
- en: '**Compliance and regulatory incidents**: These incidents occur when the model’s
    output or operation violates legal or regulatory requirements. The response team
    should work with legal and compliance teams to address the violation and modify
    the model to comply with the necessary regulations.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By comprehensively considering model maintenance components shared here, organizations
    can effectively address challenges associated with deployed deep learning models,
    ensuring their continuous improvement and alignment with business requirements.
    Traditionally, these maintenance actions are executed manually after alerts are
    raised. However, it is possible to schedule custom tasks to be executed automatically
    given an alert event. Consider using Apache Airflow to orchestrate your desired
    automated tasks from your model monitoring alerts. Apache Airflow is like a conductor
    for your data tasks, allowing you to choreograph and schedule complex workflows
    in a directed acyclic graph format. It lets you define, automate, and monitor
    sequences of tasks, making sure they happen in the right order and at the right
    time. However, there are some inherent limitations and risks with creating automated
    tasks from model monitoring alerts, which we will briefly explore next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Exploring limitations and risks of using automated tasks triggered by model
    monitoring alerts
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While automating tasks based on model monitoring alerts can save time and resources,
    it also comes with limitations and potential risks that need to be considered
    when implementing such an approach. Some limitations of automating tasks based
    on model monitoring alerts are the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity of issues**: Some issues may be too complex or nuanced to be handled
    effectively by an automated process. For example, in a deep learning model for
    medical image analysis, an automated task might be triggered to retrain the model
    when the monitoring alerts indicate a drop in accuracy. However, the complexity
    of the issue may stem from an imbalance in the training data, such as an underrepresentation
    of a certain disease, which cannot be resolved by simply retraining the model.
    In this case, automated processes might not be able to effectively address the
    root cause of the problem.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of context**: Automated tasks may lack the ability to consider the broader
    context of an issue or understand its potential impact on other aspects of the
    system.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a deep learning model that predicts customer churn based on various
    behavioral and demographic factors. An automated task might be set up to send
    promotional offers to customers identified as high risk for churn. However, the
    task may not have the context to consider external factors, such as a recent negative
    publicity event or a widespread service outage, which might be causing a temporary
    increase in churn risk. This lack of context may lead to unnecessary promotional
    offers and an ineffective use of resources.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Inadequate or inappropriate responses**: Automated tasks might not always
    choose the most appropriate action in response to an alert, potentially leading
    to suboptimal outcomes. For example, an AI model monitoring social media posts
    for harmful content may detect a post containing offensive language. An automated
    response system might remove the post or ban the user immediately, without considering
    the possibility of false positives or the post’s broader context (e.g., quoting
    offensive language to criticize it).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for risks associated with enabling automated tasks with model monitoring
    alerts, they are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Over-reliance on automation**: Relying too heavily on automated tasks can
    lead to a lack of human oversight and expertise in the model maintenance process.
    This may result in overlooking subtle patterns and trends that only human intuition
    can detect, potentially leading to suboptimal model performance.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inaccurate or premature triggers**: Automated tasks are often triggered by
    specific conditions in the monitored metrics. If these conditions are not carefully
    defined, tasks may be triggered inaccurately or prematurely, leading to unnecessary
    or even detrimental actions being taken on the model.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inflexibility**: Automated tasks are typically designed for specific scenarios
    or issues and may not be flexible enough to handle unforeseen or complex situations.
    This could limit their effectiveness in addressing unique challenges that arise
    during model maintenance.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk of compounding errors**: When automated tasks are executed based on
    erroneous alerts or inaccurate metrics, they can compound the issue by making
    unnecessary or incorrect adjustments to the model. This may lead to further deterioration
    in model performance or even irreversible damage.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security risks**: Automating tasks based on alerts can expose the model and
    its infrastructure to potential security risks, especially if the automation system
    is not adequately secured. Unauthorized access or manipulation of the automation
    system could lead to unintended consequences or malicious actions on the model.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To mitigate these limitations and risks, it is essential to strike a balance
    between automation and human involvement in the model maintenance process. This
    can be achieved by incorporating human-in-the-loop systems, ensuring proper validation
    and calibration of monitoring metrics and alerts, and implementing robust security
    measures to protect the automation infrastructure.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have covered all the components of deep learning model governance.
    This holistic three-pillar approach to model governance ultimately enables organizations
    to consistently and continuously harness the full potential of deep learning models,
    driving valuable insights and informed decision-making in the real world.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the three fundamental pillars of model governance
    for deep learning models: model utilization, model monitoring, and model maintenance.
    Model utilization ensures the effective, efficient, ethical, and responsible utilization
    of deep learning models, while model monitoring allows for ongoing evaluation
    of performance, identification of potential bias or drift, and infrastructure-related
    metrics. Model maintenance, on the other hand, focuses on regular updates and
    refinements to keep models aligned with evolving data landscapes and business
    requirements.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We also dove into and learned about the technical steps for monitoring deep
    learning models using NVIDIA Triton Server, Prometheus, and Grafana. By diligently
    considering the components for model governance, deep learning architects can
    effectively manage the challenges posed by these complex models in production
    and consistently harness their potential for driving valuable insights and decisions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will further dive deeper into the details of drift detection
    for deep learning models.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
