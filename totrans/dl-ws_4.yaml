- en: 4\. Deep Learning for Text – Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will begin our foray into **Natural Language Processing**
    for text. We will start by using the **Natural Language Toolkit** to perform text
    preprocessing on raw text data, where we will tokenize the raw text and remove
    punctuations and stop words. As we progress through this chapter, we will implement
    classical approaches to text representation, such as one-hot encoding and the
    **TF-lDF** approach. This chapter demonstrates the power of word embeddings and
    explains the popular deep learning-based approaches for embeddings. We will use
    the **Skip-gram** and **Continuous Bag of Words** algorithms to generate our own
    word embeddings. We will explore the properties of the embeddings, the different
    parameters of the algorithms, and generate vectors for phrases. By the end of
    this chapter, you will be able to handle text data and start using word embeddings
    by using pre-trained models, as well as your own embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does Siri know exactly what to do when you ask her to "*play a mellow song
    from the 80s*"? How does Google find the most relevant results for even your ill-formed
    search queries in a fraction of a second? How does your translation app translate
    text from German to English almost instantly? How does your email client protect
    you and automatically identify all those malicious spam/phishing emails? The answer
    to all these questions, and what powers many more amazing applications, is using
    **Natural Language Processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've dealt with structured, numeric data – images that were also numeric
    matrices. In this chapter, we'll begin our discussion by talking about handling
    text data and unlock the skills needed to harness this goldmine of unstructured
    information. We will discuss a key idea in this chapter – representation, particularly
    using embeddings. We will discuss the considerations and implement the approaches
    for representation. We will begin with the simplest approaches and end with word
    embeddings – an amazingly powerful approach for representing text data. Word embeddings
    will help you get state-of-the-art results in NLP tasks when coupled with deep
    learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP is a field concerned with helping machines make sense of natural (human)
    language. As shown in the following figure, NLP resides at the intersection of
    linguistics, computer science, and artificial intelligence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Where NLP fits'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Where NLP fits'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a vast field – think of all the places language (spoken and written)
    is used. NLP enables and powers the kind of applications listed in the preceding
    figure, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification of documents into categories (text classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation between languages, say, German to English (sequence-to-sequence learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically classifying the sentiment of a tweet or a movie review (sentiment
    analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots that reply to your query instantly, 24/7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we go any further, we need to acknowledge and appreciate that NLP isn''t
    easy. Consider the following sentence: "*The boy saw a man with a telescope.*"'
  prefs: []
  type: TYPE_NORMAL
- en: Who had the telescope? Did the boy use a telescope to see the man through it?
    Or was the man carrying a telescope with him? There is an ambiguity that we can't
    resolve with this sentence alone. Maybe some more context will help us figure
    this out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider this sentence, then: "*Rahim convinced Mohan to buy a television
    for himself.*" Who was the TV bought for – Rahim or Mohan? This is another case
    of ambiguity that we may be able to resolve with more context, but again, it may
    be very difficult for a machine/program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider another example: "*Rahim has quit skydiving.*" This sentence
    implies that Rahim did a fair amount of skydiving. There is a presupposition in
    this sentence, which is hard for a machine to infer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Language is a complex system that uses symbols (words/terms) and combines them
    in many ways to communicate ideas. Making sense of language is not always very
    easy, and there are many reasons for this. Ambiguity is by far the biggest reason:
    words can have different meanings in different contexts. Add to that subtext,
    different perspectives, and so on. We can never be sure if the same words are
    understood the same way by different people. A poem can be interpreted in many
    ways by those who read it, where each reader brings their unique perspective and
    understanding of the world and employs them to make sense of the poem in their
    own way.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The emergence of deep learning has had a strong positive impact on many fields,
    and NLP is no exception. By now, you can appreciate that deep learning approaches
    have given us accuracies like never before, and this has helped us improve in
    many areas. There are several tasks in NLP that have gained tremendously from
    deep learning approaches. Applications that use sentiment prediction, machine
    translation, and chatbots previously required a lot of manual intervention. With
    deep learning and NLP, these tasks are completely automated and bring with them
    impressive performance. The simple, high-level view shown in *Figure 4.2* shows
    how deep learning can be used for processing natural language. Deep learning provides
    us with not only great representations of natural language that machines can understand
    but also very powerful modeling approaches well suited for tasks in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Deep learning for NLP'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.2: Deep learning for NLP'
  prefs: []
  type: TYPE_NORMAL
- en: That being said, we need to be cautious to avoid underestimating the difficulty
    of getting machines to perform tasks involving human language and the field of
    NLP. Deep learning hasn't solved all of the challenges in NLP, but it has indeed
    caused a paradigm shift in the way several tasks in NLP are approached and has
    helped advance some applications in this field, making otherwise difficult tasks
    accessible and easy for anyone and everyone. We will perform some of these in
    *Chapter 5*, *Deep Learning for Sequences*.
  prefs: []
  type: TYPE_NORMAL
- en: One such key task is text data representation – which is, in simple terms, converting
    raw text into something a model would understand. Word embeddings constitute a
    deep learning-based approach that has changed the game and gives a very powerful
    representation of text. We'll discuss embeddings in detail and create our own
    embeddings later in this chapter. First, let's get our hands dirty by working
    with some text and performing some very important data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Text Data Handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's get some test data into Python to begin. First, we'll create some toy
    data of our own and get familiar with the tools. Then, we'll use Lewis Carrol's
    classic work, "*Alice's Adventures in Wonderland*", which is available through
    Project Gutenberg ([gutenberg.org](http://gutenberg.org)). Conveniently enough,
    we have this easily accessible through the **Natural Language ToolKit** (**NLTK**),
    a great library for performing NLP from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code implementations for this chapter can be found at [https://packt.live/3gEgkSP](https://packt.live/3gEgkSP).
    All the code in this chapter must be run in a single Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLTK should come with the Anaconda distribution. If not, you can install NLTK
    by using the following command in the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should work on Windows. For macOS and Linux, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dummy data can be created using the following command (we''re using Jupyter
    Notebooks here; feel free to use any interface):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have the text in `raw_txt`, which is a string variable, so now, we're ready
    to start processing it.
  prefs: []
  type: TYPE_NORMAL
- en: Text Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text preprocessing refers to the process of getting the text data ready for
    your primary analysis/model. Regardless of your end goal – which could be sentiment
    analysis, classification, clustering, or any of the many others – you need to
    get your raw text data cleaned up and ready for analysis. This is the first part
    of any application involving NLP.
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean by **clean up**, and when is the text data ready? We know that
    the text data we encounter in our day-to-day lives can be very messy (think about
    social media, product reviews, service reviews, and so on) and has various imperfections.
    Depending on the task at hand and the kind of data you're dealing with, the imperfections
    you care about will vary, and **cleaning up** can mean very different things.
    As an example, in some applications, preprocessing could just mean "dividing the
    sentences into individual terms." The steps you take here can and will have an
    impact on the final outcome of your analysis. Let's discuss this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in preprocessing is inevitably **tokenization** – splitting the
    raw input text sequence into **tokens**. In simple terms, it is breaking the raw
    text into constituent elements that you want to work on. This token can be a paragraph,
    sentence, word, or even a character. If you want to separate a paragraph into
    sentences, then you would tokenize the paragraph into sentences. If you want to
    separate the words in a sentence, then you would tokenize the sentence into words.
  prefs: []
  type: TYPE_NORMAL
- en: For our raw text, first, we want to separate the sentences. To do so, we have
    multiple options in Python – here, we'll use the tokenize API in NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We'll be using Jupyter Notebooks throughout this book, which is something that
    we recommend. However, feel free to use any IDE you wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can use the API, we have to `import nltk` and download the `punkt`
    sentence tokenizer. Then, we need to import the `tokenize` library. All this can
    be done using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenize API has utilities to extract different levels of tokens (sentences,
    words, or characters) for different types of data (a very handy tweet tokenizer,
    too). We''ll use the `sent_tokenize()` method here. The `sent_tokenize()` method
    breaks input text into constituent sentences. Let''s see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us the following individual sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the output, it seems like `sent_tokenize()` is doing a pretty good
    job. It has correctly identified the sentence boundaries and given us the four
    sentences, as expected. Let''s assign the result to a variable for ease of handling
    and let''s check the data type of the result and its constituents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, it's a list with four elements, where each element contains the
    sentence as a string.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try breaking sentences into individual words using the `word_tokenize()`
    method. This method breaks a given sentence into its constituent words. It uses
    smart rules to figure out word boundaries. Let''s use list comprehension (comprehensions
    in Python are a concise approach to constructing new sequences) for a bit for
    convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as expected – the elements of the resulting list are lists themselves,
    containing the words that form the sentence. Let''s also print out the first two
    elements of the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The sentences have been broken into individual words. We can also see that contractions
    like "we'll" have been broken into constituents, that is, "we" and "'ll". All
    punctuation (commas, periods, exclamation marks, and so on) are separate tokens.
    This is very convenient for us if we wish to remove them, which we will do later.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing Case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common step is to normalize case – we usually don't want "car", "CAR",
    "Car", and "caR" to be treated as separate entities. To do so, we typically convert
    all text into lowercase (we could also convert it into uppercase if we wanted).
  prefs: []
  type: TYPE_NORMAL
- en: All strings in Python have a `lower()` method to them, so converting a string
    variable (`strvar`) into lowercase is as simple as `strvar.lower()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We could have used this right in the beginning, before tokenization, and it
    would have been as simple as `raw_txt = raw_txt.lower()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will normalize the case of our data using the `lower()` method after tokenizing
    into individual sentences. We''ll accomplish this with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print out a couple of sentences to see what the result looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the output has all the terms in lowercase this time. We've taken
    the raw text, broken it into sentences, normalized the case, and then broken that
    down into words. Now, we have all the tokens that we need, but we still seem to
    have a lot of punctuation marks as tokens that we need to get rid of. Let's go
    ahead and perform more "cleanup".
  prefs: []
  type: TYPE_NORMAL
- en: Removing Punctuation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can see that the data currently has all punctuation as separate tokens.
    Again, bear in mind that there could be tasks where punctuations could be important.
    As an example, when performing sentiment analysis, that is, predicting if the
    sentiment in the text is positive or negative, an exclamation can add value. For
    our task, let''s remove these since we''re only interested in representing the
    terms of language. To do so, we need to have a list of all the punctuation marks
    we want to remove. Luckily, we have such a list in the string base library in
    Python, which we can simply import and assign to a list variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: All the usual punctuation marks are available here. If there are any additional
    punctuation marks you want to remove, you can simply add them to the `list_punct`
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a function to remove punctuation from a given list of tokens.
    This function will expect a list of tokens, from which it will drop the tokens
    that are available in the `list_punct` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test this out on some dummy tokens using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The function works as intended. Now, we need to pass the `txt_words` variable
    we modified in the previous section to the `drop_punct` function we just created.
    We will store our result in a new variable called `txt_words_nopunct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding output, the function we created has removed
    all the punctuation marks from our raw text. Now, the data looks much cleaner
    without the punctuation, but we still need to get rid of non-informative terms.
    We'll discuss that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Removing Stop Words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In day-to-day language, we have a lot of terms that don''t add a lot of information/value*.
    These are typically referred to as "stop words". We can think of these as belonging
    to two broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General/functional**: These are filler words in the language that don''t
    provide a lot of information but help stitch together other informative words
    to form meaningful sentences, such as "the", "an", "of", and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Contextual**: These aren''t general functional terms, but given the context,
    don''t add a lot of value. If you''re working with reviews of a mobile phone,
    where all reviews are talking about the phone, the term "phone" itself may not
    add a lot of information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*The notion of "value" changes with each task. Functional words such as "the"
    and "and" may not be important for, say, automatic document categorization into
    subjects, but can be very important for other applications, such as part-of-speech
    tagging (identifying verbs, adjectives, nouns, pronouns, and so on).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Functional stop words are conveniently built into NLTK. We just need to import
    them and then we can store them in a variable. Once stored, they can be accessed
    just like any Python list. Let''s import them and see how many of these words
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have 179 built-in stop words. Let''s also print some of
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can see that most of these terms are very commonly used "filler" terms that
    have a "functional" role in the language, and don't add a lot of information.
  prefs: []
  type: TYPE_NORMAL
- en: Now, removing stop words can be done the same way we removed punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.01: Tokenizing, Case Normalization, Punctuation, and Stop Word Removal'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will remove stop words from the data, and also apply everything
    we have learned so far. We'll start by performing tokenization (sentences and
    words); then, we'll perform case normalization, followed by punctuation and stop
    word removal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before commencing this exercise, ensure that you are using a Jupyter Notebook
    where you have downloaded both the `punkt` sentence tokenizer and the `stopwords`
    corpus, as demonstrated in the *Text Preprocessing* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll keep the code concise this time. We''ll be defining and manipulating
    the `raw_txt` variable. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to import `nltk` and the `tokenize` module from
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `raw_txt` variable so that it contains the text "`Welcome to the
    world of deep learning for NLP! We''re in this together, and we''ll learn together.
    NLP is amazing, and deep learning makes it even more fun. Let''s learn!`":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `sent_tokenize()` method to separate the raw text into individual sentences
    and store the result in a variable. Use the `lower()` method to convert the string
    into lowercase before tokenizing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `txt_sents` variable we've just created will be used later on in the chapter
    as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using list comprehension, apply the `word_tokenize()` method to separate each
    sentence into its constituent words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `punctuation` from the `string` module and convert it into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the built-in stop words for English from NLTK and save them in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a combined list that contains the punctuations as well as the NLTK stop
    words. Note that we can remove them together in one go:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will remove stop words and punctuation from the input
    sentence, provided as a collection of tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove redundant tokens by applying the function to the tokenized sentences
    and store the result in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the first cleaned-up sentence from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the stop words removed, the result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we performed all the cleanup steps we've learned about so
    far. This time around, we combined certain steps and made the code more concise.
    These are some very common steps that we should apply when dealing with text data.
    You could try to further optimize and modularize by defining a function that returns
    the result after all the processing steps. We encourage you to try it out.
  prefs: []
  type: TYPE_NORMAL
- en: So far, the steps in the cleanup process were steps that got rid of tokens that
    weren't very useful in our assessment. But there are a few more things we could
    do to make our data even better – we can try using our understanding of the language
    to combine tokens, identify tokens that have practically the same meaning, and
    remove further redundancy. A couple of popular approaches are stemming and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The variables we have created in this exercise will be used in later sections
    of the chapter as well. Ensure that you're completing this exercise first before
    moving to the upcoming exercises and activities.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and Lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"Eat", "eats", "eating", "ate" – aren''t they all just variations of the same
    word, all referring to the same action? In most text and spoken language, in general,
    we have multiple forms of the same word. Typically, we don''t want these to be
    considered as separate tokens. A search engine would need to return similar results
    if the query is "red shoes" or "red shoe"– it would be a terrible search experience
    otherwise. We acknowledge that such cases are very common and that we need a strategy
    to handle such cases. But what should we do with the variants of a word? A reasonable
    approach is to map them all to a common token so that they are all treated the
    same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stemming is a rule-based approach to achieve normalization by reducing a word
    to its "stem". The stem is the root of the word before any affixes (an element
    added to make a variant) are added. This approach is rather simple – chop off
    the suffix to get the stem. A popular algorithm is the **Porter stemming** algorithm,
    which applies a series of such rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Examples of the Porter stemming algorithm''s rule-based approach'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.3: Examples of the Porter stemming algorithm''s rule-based approach'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The full set of Porter stemming algorithm rules can be found at [http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the Porter stemming algorithm in action. Let''s import the `PorterStemmer`
    function from the `''stem''` module in NLTK and create an instance of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the stemmer works on individual tokens, not sentences as a whole.
    Let''s see how the stemmer stems the word "`driving`":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how we can apply this to a whole sentence. Note that we will have
    to tokenize the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used for tokenizing the sentence and applying the stemmer
    to each term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the stemmer has correctly reduced "mustered" to "muster" and
    "driving" to "drive", while "drove" is untouched. Also, note that the result of
    a stemmer need not be a valid English word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lemmatization is a more sophisticated approach that refers to a dictionary
    and finds a valid root form (the lemma) of the word. Lemmatization works best
    when the part of speech of the word is also provided – it considers the role the
    term is playing and returns the appropriate form. The output from a lemmatization
    step is always a valid English word. However, lemmatization is computationally
    very expensive, and for it to work well, it needs the part-of-speech tag, which
    typically isn''t available in the data. Let''s have a brief look at it. First,
    let''s import `WordNetLemmatizer` from `nltk.stem` and instantiate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the `lemmatizer` on the term `ponies`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: For our discussions, stemming is sufficient. The result from stemming may not
    always be a valid word. For example, `poni` is the stem for `ponies` but isn't
    a valid English word. Also, there may be some inaccuracies, but for the objective
    of mapping to a common word, this crude method works just fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Stemming Our Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will continue with data preprocessing. We removed the
    stop words and punctuation in the previous exercise. Now, we will use the Porter
    stemming algorithm to stem the tokens. Since we''ll be using the `txt_words_nostop`
    variable we created previously, let''s continue with the same Jupyter Notebook
    we created in *Exercise 4.01*, *Tokenizing, Case Normalization, Punctuation, and
    Stop Word Removal*. The variable, at this point, will contain the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `PorterStemmer` from NLTK using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the stemmer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the stemmer to the first sentence in `txt_words_nostop`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we print the result, we get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the stemmer to all the sentences in the data. You could use loops, or
    a nested list comprehension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the output using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It looks like plenty of modifications have been made by the stemmer. Many of
    the words aren't valid anymore but are still recognizable, and that's okay.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we used the Porter stemming algorithm to stem the terms of
    our tokenized data. Stemming works on individual terms, so it needs to be applied
    after tokenizing into terms. Stemming reduced some terms to their base form, which
    weren't necessarily valid English words.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Stemming and Lemmatization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond stemming and lemmatization, there are many specific approaches to handle
    word variations. We have techniques such as phonetic hashing to identify spelling
    variations of a word induced by pronunciations. Then, there is spelling correction
    to identify and rectify errors in spelling. Another potential step is abbreviation
    handling so that *television* and *TV* are treated the same. The result from these
    steps can be further augmented by performing domain-specific term handling. You
    get the drift… there are a lot of steps possible, and, depending on your data
    and the criticality of your application, you may include some of these in your
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: In general, though, the steps we performed together are largely sufficient –
    case normalization, tokenization, stop word, and punctuation removal, followed
    by stemming/lemmatization. These are some common steps that most NLP applications
    include.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Text Corpora Using NLTK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we've performed these steps on dummy data that we created. Now, it's
    time to try out our newly acquired skills on a larger and more authentic text.
    First, let's acquire that text – Lewis Carrol's classic work, "*Alice's Adventures
    in Wonderland*", which is available through Project Gutenberg and accessible through
    NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may need to download the `''gutenberg''` corpus through NLTK. First, import
    NLTK using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the `nltk.download()` command to open up an app, that is, the **NLTK
    Downloader** interface (shown in the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the app has multiple tabs. Click the **Corpora** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: NLTK Downloader'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: NLTK Downloader'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Corpora` tab, scroll down until you reach `gutenberg`. If the status
    is `not installed`, go ahead and click the `Download` button in the lower-left
    corner. That should install the `gutenberg` corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: NLTK Downloader''s Corpora tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: NLTK Downloader''s Corpora tab'
  prefs: []
  type: TYPE_NORMAL
- en: 'Close the interface. Now, you can access some classic texts right from NLTK.
    We''ll read in the text and store it in a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The text is stored in `alice_raw`, which is one big character string. Let''s
    have a look at the first few characters of this string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We can see the raw text in the output, which contains the usual imperfections
    that we expect – varying case, stop words, punctuation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We're ready. Let's test out our skills through an activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Text Preprocessing of the ''Alice in Wonderland'' Text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will apply all the preprocessing steps you''ve learned
    about so far to a much larger, real text. We''ll work with the text for Alice
    in Wonderland that we stored in the `alice_raw` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The text currently looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: By the end of this activity, you will have cleaned and tokenized the data, removed
    a lot of imperfections, removed stop words and punctuation, and have applied stemming
    on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before beginning this activity, make sure you have the `gutenberg` corpus installed
    and the `alice_raw` variable created, as shown in the previous section titled
    *Downloading Text Corpora Using NLTK*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps you need to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing in the same Jupyter Notebook, use the raw text in the `'alice_raw'`
    variable. Change the raw text to lowercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import punctuation from the `string` module and the stop words from NLTK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable holding the contextual stop words, that is, `--` and `said`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a master list for stop words to remove that contain terms from punctuation,
    NLTK stop words and contextual stop words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a function to drop these tokens from any input sentence (tokenized).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `PorterStemmer` algorithm from NLTK to perform stemming on the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out the first five sentences from the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 405.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The expected output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at what we have achieved so far and what lies ahead.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've learned how to perform text preprocessing – the process of getting
    the text data ready for our primary analysis/model. We started with raw text data
    that has, potentially, many imperfections. We learned how to handle many of these
    imperfections and are now at a juncture where we are comfortable with handling
    text data and getting it ready for further analysis. This is an important first
    part in any NLP application. So, we took raw text data and got clean data in return.
    What's next?
  prefs: []
  type: TYPE_NORMAL
- en: The next section is a very important one since it has a very strong bearing
    on the quality of your analysis. It is known as representation. Let's discuss
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Text Representation Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have processed our raw input data into cleaned text. Now, we need to transform
    this cleaned text into something a predictive model understands. But what does
    a predictive model understand? Does it understand the different words? Does it
    read a word as we do? Can it work with the text that we supply to it?
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you understand that models work on numbers. The input to a model is
    a stream of numbers. It doesn''t understand images, but it can work with matrices
    and numbers representing those images. For handling images, the key idea is to
    convert them into numbers and generate features out of them. The idea is the same
    for text: we need to convert the text into numbers, which will act as features
    for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation** is all about converting the text into numbers/features that
    the model understands. Doesn''t sound like there is much to it, right? If you
    think that, then here''s something for you to consider: input features are very
    important for any modeling exercise, and representation is the process of creating
    those features. It has a very significant effect on the outcome of your model
    and is a process that you should pay a great deal of attention to.'
  prefs: []
  type: TYPE_NORMAL
- en: How do you go about text representation, then? What's the "best" way to represent
    text, if there is such a thing at all? Let's discuss a few approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Classical Approaches to Text Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text representation approaches have evolved significantly over the years, and
    the advent of neural networks and deep neural networks has made a significant
    impact on the way we now represent text (more on that later). We have come a long
    way indeed: from handcrafting features to marking if a certain word is present
    in the text, to creating powerful representations such as word embeddings. While
    there are a lot of approaches, some more suitable for the task than the others,
    we will discuss a few major classical approaches and work with all of them in
    Python.'
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoding is, perhaps, one of the most intuitive approaches toward text
    representation. A one-hot encoded feature for a word is a binary indicator of
    the term being present in the text. It's a simple approach that is easy to interpret
    – the presence or absence of a word. To understand this better, let's consider
    our sample text before stemming, and let's see how one-hot encoding works for
    a particular term of interest, say, `nlp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the text currently looks like using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the text looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Our word of interest is `nlp`. Here''s what the one-hot encoded feature for
    it would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: One-hot encoded feature for ''nlp'''
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: One-hot encoded feature for ''nlp'''
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the feature is `1`, but only for the sentences where the term
    `nlp` is present and is `0` otherwise. We can make such indicator variables for
    each word that we''re interested in. So, if we''re interested in three terms,
    we make three such features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: One-hot encoded features for ''nlp'', ''deep'', and ''learn'''
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: One-hot encoded features for ''nlp'', ''deep'', and ''learn'''
  prefs: []
  type: TYPE_NORMAL
- en: Let's recreate this using Python in an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.03: Creating One-Hot Encoding for Our Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will replicate the preceding example. The target terms
    are `nlp`, `deep`, and `learn`. We will create a one-hot encoded feature for these
    terms using our own function and store the result in a `numpy` array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we''ll be using the `txt_words_nostop` variable we created in *Exercise
    4.01*, *Tokenizing, Case Normalization, Punctuation, and Stop Word Removal*. So,
    you will need to continue this exercise in the same Jupyter Notebook. Follow these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Print out the `txt_words_nostop` variable to see what we''re working with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a list with the target terms, that is, `"nlp", "deep", "learn"`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that takes in a single tokenized sentence and returns a `0`
    or `1` for each target term, depending on its presence in the text. Note that
    the length of the output is fixed at `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We're iterating over the target terms and checking if they're available in the
    input sentence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply the function to each sentence in our text and store the result in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy`, create a `numpy` `array` from the result, and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The array''s output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the output contains four rows, one for each sentence. Each of
    the columns in the array contains the one-hot encoding for a target term. The
    values for "learn" are 0, 1, 0, 1, which is consistent with our expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we saw how we can generate features from text using one-hot
    encoding. The example used a list of target terms. This may work when you have
    a very specific objective in mind where we know exactly which terms are useful.
    Indeed, this was the method that was heavily employed until a few years ago, where
    people handcrafted features from text. In many situations, this is not feasible
    – since we don't know exactly which terms are important, we use one-hot encoding
    for a large number of terms (5,000, 10,000, or even more).
  prefs: []
  type: TYPE_NORMAL
- en: The other aspect is whether the presence/absence of the term enough for most
    situations. Do we not want to include more information? Maybe the frequency of
    the term instead of just its presence, or maybe even some other smarter measure?
    Let's see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We discussed that one-hot encoding merely indicates the presence or absence
    of a term. A reasonable argument here is that the frequency of terms is also important.
    It may be that a term that''s present more times in a document is more important
    for the document. Maybe representing the term by its frequency is a better approach
    than simply the indicator. The frequency approach is straightforward – for each
    term, count the number of times it appears in a particular text. If a term is
    absent from the document/text, it gets a 0\. We do this for all the terms in our
    vocabulary. Therefore, we have as many features as the number of words in our
    vocabulary (something we can choose; this can be thought of as a hyperparameter).
    We should note that after the preprocessing steps, the "*terms*" that we''re working
    with are tokens that may not be valid words in the language:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: The *vocabulary* is the superset of all the terms that we'll use in the final
    model. Vocabulary size refers to the number of unique terms in the vocabulary.
    You could have 20,000 unique terms in the raw text but choose to work with the
    most frequent 10,000 terms; this would be the effective vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following image; if we had *N* documents and had *V* (`t1, t2,
    t3` … `t`V) words in our working vocabulary, the representation for the data would
    be a matrix of dimensions *N × V*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Document-term matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Document-term matrix'
  prefs: []
  type: TYPE_NORMAL
- en: This matrix is our **Document-Term Matrix** (**DTM**) – where each row represents
    a document, and each column represents a term. The values in the cells can represent
    some measure (count, or any other measure). We'll work with term frequencies in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could create our own function again, but we have a very handy utility called
    `''CountVectorizer''` for this in `scikit-learn` that we''ll use instead. Let''s
    familiarize ourselves with it, beginning by importing the utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The vectorizer can work with raw text, as well as tokenized data (as in our
    case). To work on the raw text, we would use the following code, where we will
    create a DTM with term frequencies from our raw text (`txt_sents`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, let''s take a quick look at the contents of this variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the contents of the `txt_sents` variable have been overwritten while working
    on *Activity 4.01,* *Text Preprocessing of the 'Alice in Wonderland' Text*, you
    can revisit *Step 3* of *Exercise 4.01,* *Tokenizing, Case Normalization, Punctuation,
    and Stop Word Removal* and redefine the variable so that its contents match the
    preceding output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s instantiate the vectorizer. Note that we need to provide the vocabulary
    size. This picks the top *n* terms from the data for creating the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We chose five terms here; the result will contain five columns in the matrix.
    Let''s train (`''fit''`) the vectorizer on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The vectorizer has now learned a vocabulary – the top five terms – and has
    created an index for each term in the vocabulary. Let''s have a look at the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding attribute gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We can see which terms have been picked (the top five).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply the vectorizer to the data to create the DTM. A minor detail:
    the result from a vectorizer is a sparse matrix. To view it, we''ll convert it
    into an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Have a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The second document (the second row) has a frequency of `2` for the last two
    terms. What are those terms? Well, indices 3 and 4 are the terms `''together''`
    and `''we''`, respectively. Let''s print out the original text to see if the output
    is as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: This is just as we expected, and it looks like the count vectorizer works just
    fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the vectorizer tokenizes the sentence as well. If you don''t want
    that and want to use preprocessed tokens instead (`txt_words_stem`), you simply
    need to pass a dummy tokenizer and preprocessor to `CountVectorizer`. Let''s see
    how that works. First, we create a function that does nothing and simply returns
    the tokenized sentence/document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll instantiate the vectorizer to use this function as the preprocessor
    and tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we''re fitting and transforming the data in one step using the `fit_transform()`
    method from the tokenizer, and then we view the result. The method identifies
    the unique terms as the *vocabulary* when fitting on the data, then counts and
    returns the occurrence of each term for each document when transforming. Let''s
    see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The output array will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the output is different from that of the previous result. Is
    this difference expected? To understand, let''s look at the vocabulary of the
    vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re working with preprocessed data, remember? We have already removed stop
    words and stemmed. Let''s try printing out the input data just to be sure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the DTM is working according to the new vocabulary and the frequencies
    that were obtained after preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: So, this was the second approach of generating features from text data, that
    is, using the frequencies of the terms. In the next section, we will look at another
    very popular method.
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Does the high frequency of a term in a document mean that the word is very important
    for the document? Not really. What if that term is very common in all the documents?
    A common assumption that's employed in text data handling is that if a term is
    present in all documents, it may not be very differentiating or important for
    this particular document at hand. Seems like a reasonable assumption. Once more,
    let's consider the example of the term "*mobile*" when we're working with mobile
    phone reviews. The term is likely to be present in a very high proportion of reviews.
    But if your task is identifying the sentiment in the reviews, the term may not
    add a lot of information.
  prefs: []
  type: TYPE_NORMAL
- en: We can bump up the importance of terms that are present in the document but
    rare in the entire data and decrease the importance of terms that are present
    in most of the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TF-IDF method, which stands for *Term Frequency – Inverse Document Frequency*,
    defines **Inverse Document Frequency** (**IDF**) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Equation for TF-IDF'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.9: Equation for TF-IDF'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the total number of documents, while *df(t)* is the number of documents
    where the term *t* occurs. This is used as a factor to adjust the term frequency.
    You can see that it works just as we want it to – it increases the importance
    for rare terms, and decreases it for common terms. Note that there are variations
    to this formula, but we''ll stick with what `scikit-learn` uses. Like `CountVectorizer`,
    the TF-IDF vectorizer tokenizes the sentence and learns the vocabulary, but instead
    of returning the counts for a term in a document, it returns the adjusted (IDF-multiplied)
    counts.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's apply this interesting new approach to our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.04: Document-Term Matrix with TF-IDF'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll implement the third approach to feature generation
    from text – TF-IDF. We will use scikit-learn''s `TfidfVectorizer` utility and
    create the DTM for our raw text data. Since we''re using the `txt_sents` variable
    we created earlier in this chapter, we''ll need to use the same Jupyter Notebook.
    The text contained in the variable currently looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the contents of the `txt_sents` variable have been overwritten while working
    on *Activity 4.01*, *Text Preprocessing of the 'Alice in Wonderland' Text*, you
    can revisit *Step 3* of *Exercise 4.01*, *Tokenizing, Case Normalization, Punctuation,
    and Stop Word Removal* and redefine the variable so that its contents match the
    preceding output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `TfidfVectorizer` utility from `scikit learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `vectorizer` with a vocabulary size of `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the `vectorizer` on the raw data of `txt_sents`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the vocabulary learned by the `vectorizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The trained vocabulary will look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the vocabulary is the same as that of the count vectorizer. This
    is expected. We're not changing the vocabulary; we're adjusting its importance
    for the documents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transform the data using the trained vectorizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the resulting DTM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can clearly see that the output values are different from the frequencies
    and that the values less than 1 indicate that many values have been lowered after
    multiplication with IDF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also need to see the IDF for each of the terms in the vocabulary to check
    if the factor is indeed working as we expect it to. Print out the IDF values for
    the terms using the `idf_` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The terms `'and'`, `'deep'`, and `'learn'` have a lower IDF, while the terms
    `'together'` and `'we'` have a higher IDF. This is just as we expect it to be
    – the terms `'together'` and `'we'` appear only in one document, while the others
    appear in two. So, the TF-IDF scheme is indeed giving more importance to rarer
    words.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we saw how we can represent text using the TF-IDF approach.
    We also saw how the approach downweighs more frequent terms by noticing that the
    IDF values were lower for higher-frequency terms. We ended up with a DTM containing
    the TF-IDF values for the terms.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the Classical Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've just looked at three approaches to the classical way of text representation.
    We began with one-hot encoding, where the feature for a term was simply marking
    its presence in a document. The count/frequency-based approach attempted to add
    the importance of the term by using its frequency in a document. The TF-IDF approach
    attempted to use a "normalized" importance value of the term, factoring in how
    common the term is across the documents.
  prefs: []
  type: TYPE_NORMAL
- en: All three approaches that we've discussed so far fall under the "*Bag of Words*"
    approach to representation. So, why are they called "*Bag of Words*"? For a couple
    of reasons. The first reason is that they don't retain the order of the tokens
    – once in the bag, the position of the terms/tokens doesn't matter. The second
    reason is that this approach retains features for individual terms. So, for each
    document, you have, in a way, a "mixed bag of tokens", or a "*bag of words*",
    for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: The result from all three approaches had a dimensionality of *N × V*, where
    *N* is the number of documents and *V* is the vocabulary size. Note that all three
    representations are very sparse – a typical sentence is very short (maybe 20 words),
    but the vocabulary size is typically in the thousands, resulting in most of the
    cells of the DTM being 0\. This doesn't seem ideal. Well, there's this and a few
    more shortcomings of such representations, which we'll see shortly, that have
    led to the success of deep learning-based methods for representation. Let's discuss
    these ideas next.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Representation for Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why are word embeddings so popular? Why are we claiming they are amazingly powerful?
    What makes them so special? To understand and appreciate word embeddings, we need
    to acknowledge the shortcomings of the representations so far.
  prefs: []
  type: TYPE_NORMAL
- en: The terms "*footpath*" and "*sidewalk*" are synonyms. Do you think the approaches
    we've discussed so far will be able to capture this information? Well, you could
    manually go in and replace "*sidewalk*" with "*footpath*" so that both have the
    same token eventually, but can you do this for all possible synonyms in the language?
  prefs: []
  type: TYPE_NORMAL
- en: The terms "*hot*" and "*cold*" are antonyms. Do the previous Bag-of-Words representations
    capture this? What about "*dog*" being a type of "*animal*"? "*Cockpit*" being
    a part of a "*plane*"? Differentiating between a dog's bark and a tree's bark?
    Can you handle all these cases manually?
  prefs: []
  type: TYPE_NORMAL
- en: All the preceding are examples of "**semantic associations**" between the terms
    – simply put, their meanings are linked in some way or another. Bag-of-words representations
    can't capture these. This is where the notion of distributional semantics comes
    in. The key idea of distributional semantics is that terms with similar distributions
    have similar meanings.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick, fun quiz for you: Guess the meaning of the term *furbaby* from the
    following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '"*I adopted a young Persian furbaby a month back. Like all furbabys, it loves
    to scratch its back and hates water, but unlike other furbabys, it miserably fails
    at catching a mouse.*"'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have guessed it right: *furbaby* is referring to a cat. This was easy,
    wasn''t it?'
  prefs: []
  type: TYPE_NORMAL
- en: 'But how did you do that? Nowhere has the term cat been used. You looked at
    the context (the terms surrounding it) for "*furbaby*" and, based on your understanding
    of language and the world, you figured that these terms are generally associated
    with cats. You intuitively used this notion: words with similar meaning appear
    in similar contexts. If "*furbaby*" and "*cat*" appeared in similar contexts,
    their meaning must be similar.'
  prefs: []
  type: TYPE_NORMAL
- en: '"*You shall know a word by the company it keeps.*"'
  prefs: []
  type: TYPE_NORMAL
- en: This famous, and now overused, quote by John Firth captures this idea very well.
    It's overused for all the right reasons. Let's see how this notion is employed
    in word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Word Embeddings and Word Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embeddings are representations of each term as a vector with low dimensionality.
    The one-hot encoded representation for a term was also a vector, but with dimensionality
    in the several thousands. Word embeddings/word vectors have much lower dimensionality
    and result from distributional semantics-based approaches – essentially, the representation
    captures the notion that words with similar meanings appear in similar contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors attempt to capture the meanings of terms. This idea makes them
    very powerful, assuming, of course, they have been created correctly. With word
    vectors, vector operations such as adding/subtracting vectors and dot products
    are possible and have some very interesting meanings. There is also this great
    property that items with similar meanings are spatially closer. All of this leads
    to some amazing results.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very interesting result is that word vectors can perform well on analogy
    tasks. Analogy tasks are defined as tasks of the format – "*a* is to *b* as *x*
    is to ?" – that is, find an entity that has the same relation to *x* as *b* has
    to *a*. As an example, if you ask "man is to uncle as a woman is to ?", the result
    would be "`aunt`" (more on this later). You can also find out semantic regularities
    between terms – the relationships between terms and sets of terms. Let''s look
    at the following figure, which is based on word vectors/embeddings, to understand
    this better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Semantic relationships between terms'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.10: Semantic relationships between terms'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows some examples. The vectors can have high dimensionality
    (up to 300 or even more), so dimensionality reduction to two dimensions is performed
    to visualize it. The dotted connection between the two terms represents the relation
    between the terms. The direction of this connection is the important bit. On the
    left panel, we can see that the segment connecting `slow` and `slower` is parallel
    to the segment connecting `short` and `shorter`. What does this mean? This means
    that the word embeddings learned that the relationship between `short` and `shorter`
    is the same as the relationship between `slow` and `slower`. Likewise, the embeddings
    learned that the relationship between `clearer` and `clearest` is the same as
    that between `darker` and `darkest`. Pretty neat, right?
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the right-hand side of *Figure 4.10* shows that the embeddings learned
    that the relationship between `sir` and `madam` is the same as that between `king`
    and `queen`. Embeddings have also captured other kinds of semantic associations
    between terms, which we discussed in the previous section. Isn't that amazing?
  prefs: []
  type: TYPE_NORMAL
- en: This would not be possible with the approaches we discussed earlier. Word embeddings
    truly are working around the "meaning" of terms. We hope you can already appreciate
    the utility and power of word vectors. If you're not convinced yet, we'll soon
    be working with them and will see this for ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: To generate word embeddings, there are several algorithms we can use. We will
    discuss two major approaches and apprise you of some other popular approaches.
    We will see how the distributional semantics approach is leveraged to derive these
    word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: word2vec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back in school, to test if we understood the meaning of certain terms, our
    language teachers used a very popular technique: "*fill in the blanks*". Based
    on the words around it, we needed to identify the word that would best fill that
    blank. If you understood the meaning well, you would do well. Think about this
    – isn''t this distributional semantics?'
  prefs: []
  type: TYPE_NORMAL
- en: In the *'furbaby'* example, you could predict the term `'cat'` because you understood
    the contexts and terms it occurs with. The exercise was effectively a "fill-in-the-blank"
    exercise. You could fill the blank only because you understood the meaning of
    'cat'.
  prefs: []
  type: TYPE_NORMAL
- en: If you can predict a term given some context around it, you understand the meaning
    of the term.
  prefs: []
  type: TYPE_NORMAL
- en: 'This simple idea is exactly the formulation behind the `word2vec` algorithm.
    The `word2vec` algorithm/process is a prediction exercise, a massive "fill-in-the-blank"
    exercise, in a way. In short, this is what the algorithm does:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the contextual words, predict the missing target word.
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to it. The `word2vec` algorithm predicts the target word
    given the context. Let's understand how these are defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence, "*The Persian cat eats fish and hates bathing*." We
    define context as some fixed number of the terms to the left and right of the
    target word, which is in the center. For our example, let `''cat''` be the target
    word, and let''s take two words on either side of the target as our context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: "cat" as the target term'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: "cat" as the target term'
  prefs: []
  type: TYPE_NORMAL
- en: 'The five terms together form a `''window''`, which has the target term at the
    center and the context terms around it. In this example, since we are considering
    two terms on either side, the window size is 2 (more on these parameters later).
    The window is a sliding one and moves over the terms in the sentence. The next
    window would have `''eats''` at the center, with `''cat''` now becoming part of
    the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Windows for the target term'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: Windows for the target term'
  prefs: []
  type: TYPE_NORMAL
- en: '`C1`, `C2`, `C3`, and `C4` denote the contexts for each window. In `C3`, "fish"
    is the target word, which is predicted using the terms "cat", "eats", "and", and
    "hates". The formulation is clear, but how does the model learn the representations?
    Let''s discuss that next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: The CBOW architecture with an example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.13: The CBOW architecture with an example'
  prefs: []
  type: TYPE_NORMAL
- en: The model shown in the preceding figure uses a neural network with a single
    hidden layer. The output layer is for the target term and is one-hot encoded with
    *V* outputs, one for each term – the predicted term, which is `'cat'`, is the
    term that gets `'hot'` in the output, of course. The input layer for the context
    terms is also size *V*, but fires for all the terms in the context. The hidden
    layer is of dimensionality *V x D* (where *D* is the dimension of the vectors).
    This hidden layer is where these magical representations of the terms are learned.
    Note that there is just one input layer, as the weights matrix *W* suggests.
  prefs: []
  type: TYPE_NORMAL
- en: While the network trains, predicting the target word better with each epoch,
    the parameters of the hidden layer are also getting updates. These parameters
    are effectively D-length vectors for each term. This D-length vector for a term
    is our word embedding for that term. After the iterations complete, we would have
    learned our word embeddings for all the terms in the vocabulary. Pretty neat,
    isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach we just discussed is the CBOW approach to training word vectors.
    The context is a simple bag of words (as we discussed in the previous section
    on classical approaches; order doesn''t matter, remember), hence the name. There
    is another popular approach, the Skip-gram approach, which inverts the approach
    of the CBOW method – it predicts the context words based on the center word. This
    approach may seem a little less intuitive initially but works well. We''ll discuss
    the differences between the results from CBOW and Skip-gram later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14: The Skip-gram architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.14: The Skip-gram architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the CBOW approach in action in Python. We'll create our own word embeddings
    and assess if we can indeed get the amazing results we have been claiming so far.
  prefs: []
  type: TYPE_NORMAL
- en: Training Our Own Word Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many implementations of the `word2vec` algorithm available in different
    packages. We will use the implementation in **Gensim**, which is a great package
    for many NLP tasks. The implementation of word2vec in Gensim is close to the original
    paper by *Mikolov et al.* in 2013 ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
    Gensim also supports other algorithms for word embeddings; more on this later.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t have Gensim installed, you can install it by typing the following
    command into a Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset we''ll use is the `text8` corpus ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)),
    which is the first billion characters from Wikipedia. It should, therefore, cover
    data from a variety of topics, not specific to one domain. Conveniently, Gensim
    has a utility (the `downloader` API) to read in the data. Let''s read in the data
    after importing the `downloader` utility from Gensim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This step downloads the `text8` data and can take a while, depending on your
    internet connectivity. Alternatively, the data is available here ([https://packt.live/3gKXU2D](https://packt.live/3gKXU2D))
    to be downloaded and read using the `Text8Corpus` utility in Gensim, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The `text8` data is now available as an iterable, which can simply be passed
    to the `word2vec` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we train the embeddings, to make the results reproducible, let''s set
    the seed as `1` for random number generation using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although we have set the seed, there are more causes for variation of results.
    Some of this is because of an internal hash seed that the Python version on your
    system may use. Using multiple cores can also cause the results to vary. In any
    case, while the values you see may be different, and there could be some changes
    in the order of the results, the output you see should largely agree with ours.
    Note that this applies to all the practical elements pertaining to word vectors
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s train our first word embedding by using the `word2Vec` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'This may take a minute or two, or less, depending on your system. Once complete,
    we will have our trained word vectors in the model and have access to multiple
    handy utilities to work with these word vectors. Let''s access the word vector/embedding
    for a term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: The embedding for "animal"'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: The embedding for "animal"'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have a series of numbers – the vector for the term. Let''s find the length
    of the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The length of the vector is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'The representation for each term is now a vector of length 100 (the length
    is a hyperparameter we can change; we used the default setting to get started).
    The vector for any term can be accessed as we did previously. Among the other
    handy utilities is the `most_similar()` method, which helps us find the terms
    that are the most similar to a target term. Let''s see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: The output is a list of tuples, with each tuple containing the term and its
    similarity score with the term "animal".
  prefs: []
  type: TYPE_NORMAL
- en: We can see `insect`, `animals`, `insects`, and `mammal` in the top-most similar
    terms to "animal". This seems like a very good result, right? But how is the similarity
    being calculated? Words are being represented by vectors, and the vectors are
    trying to capture meaning – the similarity between terms is the similarity between
    their corresponding vectors. The `most_similar()` method uses **cosine similarity**
    between the vectors and returns the terms with the highest values. The value corresponding
    to each term in the result is the cosine similarity with the target word's vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cosine similarity measures are suitable here as we expect terms that are similar
    in meaning to be spatially together. Cosine similarity is the cosine of the angle
    between the vectors. Terms with similar meaning and representation will have an
    angle closer to 0 and a similarity score closer to 1, whereas terms with completely
    unrelated meanings will have an angle closer to 90, and a cosine similarity closer
    to 0\. Let''s see what the model has learned as top terms related to "happiness":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The most similar items turn out to be the following (the most similar ones
    are at the top):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Humanity, mankind, goodness, righteousness, and compassion -- we have some life
    lessons here. It seems to have learned what many people seemingly can't figure
    out in their entire lifetime. Remember, it is just a series of matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Regularities in Word Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned earlier that these representations capture regularities in language
    and are good at solving simple analogy tasks. The offsets between vector embeddings
    seem to capture the analogical relationship between words. So, for example, *"king"
    - "man" + "woman"* is expected to result in "*queen*". Let's see if the model
    that we trained on the `text8` corpus also understands some regularities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `most_similar()` method here, which allows us to add and subtract
    vectors from each other. We''ll provide `''king''` and `''woman''` as vectors
    to add to each other, use `''man''` to subtract from the result, and then check
    out the five terms that are the most similar to the resulting vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The top result is `'queen'`. Looks like the model is capturing these regularities.
    Let's try out another example. "Man" is to "uncle" as "woman" is to ? Or in an
    arithmetic form, what is the vector closest to *uncle - man + woman = ?*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: This seems to be working great. Notice that all the top five results are for
    the feminine gender. So, we took `uncle`, removed the masculine elements, added
    feminine elements, and now we have some really good results.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some other examples of vector arithmetic. We can take vectors
    for two different terms and average them to arrive at vectors for a phrase as
    well. Let's try it for ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Taking the average of individual vectors is just one of the many ways of arriving
    at phrase vectors. Variations range from weighted averages to more complex mathematical
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.05: Vectors for Phrases'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will begin to create vectors for two different phrases,
    `get happy` and `make merry`, by taking the average of the individual vectors.
    We will find a similarity between the representations for the phrases. You will
    need to continue this exercise in the same Jupyter Notebook we have been using
    throughout this chapter. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the vector for the term "*get*" and store it in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the vector for the term "*happy*" and store it in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector as the element-wise average of the two vectors, `(v1 + v2)/2`.
    This is our vector for the entire phrase "get happy":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, extract vectors for the terms "*make*" and "*merry*":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector for the phrase by averaging the individual vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the `cosine_similarities()` method in the model, find the cosine similarity
    between the two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The cosine similarity comes out as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is a cosine similarity of about `0.58`, which is positive and much
    higher than `0`. This means that the model thinks the phrases "get happy" and
    "make merry" are similar in meaning. Not bad, right? Instead of a simple average,
    we could use weighted averages, or come up with more sophisticated methods of
    combining individual vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we saw how we could use vector arithmetic to represent phrases,
    instead of individual terms, and we saw that meaning is still captured. This brings
    us to a very important lesson – *vector arithmetic on word embeddings has meaning*.
  prefs: []
  type: TYPE_NORMAL
- en: These vector arithmetic operations work on the meaning of terms, resulting in
    some very interesting results.
  prefs: []
  type: TYPE_NORMAL
- en: We hope you now appreciate the power of word embeddings. We realize that these
    results come from just some matrix multiplication and take a minute to train on
    our dataset. Word embeddings are almost magical, and it is pleasantly surprising
    how such a simple prediction formulation results in such a powerful representation.
  prefs: []
  type: TYPE_NORMAL
- en: When we created the word vectors previously, we didn't pay much attention to
    the controls/parameters. There are many, but only some have a significant impact
    on the quality of the representations. We will now come to understand the different
    parameters of the `word2vec` algorithm and see the effect of changing these for
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Parameters – "size" of the Vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `size` parameter of the `word2vec` algorithm is the length of the vector
    for each term. By default, as we saw earlier, this is 100\. We will try reducing
    this parameter and assess the differences, if any, in the results. Let''s retrain
    the word embeddings, with `size` as 30 this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s check the analogy task from earlier, that is, `king - man + woman`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: We can see that `queen` isn't present in the top five results. It looks like
    by using a very low dimensionality, we aren't capturing enough information in
    the representation for a term.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Parameters – "window size"
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `window size` parameter defines the context; concretely, the window size
    is the number of terms to the left and to the right of the target term while building
    the context. The effect of this parameter is not very obvious. The general observation
    is that when you use a higher window size (say, 20), the top similar terms seem
    to be terms that are used along with the target term, not necessarily having a
    similar meaning. On the other hand, reducing the window size (to, say, 2), returns
    the top terms that are very similar in meaning, and are synonyms in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram versus CBOW
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Choosing between Skip-gram and CBOW as the learning algorithm is exercised
    by setting `sg = 1` for Skip-gram (the default is `sg = 0`, that is, CBOW). Recall
    that the Skip-gram approach predicts the context words based on the central target
    word. This flips the formulation of CBOW, where the context words are used to
    predict the target word. But how do we choose between the two? What are the benefits
    of one over the other? To see for ourselves, let''s train embeddings using Skip-gram
    and compare some results with what we had for CBOW. To begin, let''s take a particular
    example for CBOW. First, we''ll recreate the CBOW word vectors with the default
    vector size by not specifying the size parameter. Oeuvre is a term for the body
    of work of an artist/performer. We''ll see the most similar terms for the uncommon
    term, `oeuvre`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'The following terms come out as the most similar terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that most results are the names of artists (`swinburne`, `kurosawa`,
    and `baglione`) or food dishes (chateaubriand). None of the top five results are
    close in meaning to the target term. Now, let''s retrain our vectors using the
    Skip-gram method and see the result on the same task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the top terms are much closer in meaning (`masterful`, `orchestration`,
    `showcasing`). So, the Skip-gram method seems to work better for rare words.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this so? The CBOW method smooths over a lot of the distributional statistics
    by effectively averaging overall context words (remember, all the context terms
    together go as an input), while Skip-gram does not. When you have a small dataset,
    the smoothing that's done by CBOW is desirable. If you have a small/moderately
    sized dataset, and if you are concerned about the representation of rare terms,
    then Skip-gram is a good option.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very important decision while training your word vectors is the underlying
    data. The patterns and similarities will be learned from the data you supply to
    the algorithm, and we expect the model to learn differently from data from different
    domains, different kinds of settings, and so on. To appreciate this, we load different
    corpora from different contexts and see how the embeddings vary.
  prefs: []
  type: TYPE_NORMAL
- en: The Brown corpus is a collection of general text, collected from 15 different
    topics to make it general (from politics to religion, books to music, and many
    other themes). It contains 500 text samples and about 1 million words. The "movie"
    corpus contains movie-review data from IMDb. Both of these are available in NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.06: Training Word Vectors on Different Datasets'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will train our own word vectors on the Brown corpus and
    the IMDb movie reviews corpus. We will assess the differences in the representations
    learned and the effect of the underlying training data. Follow these steps to
    complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Brown and IMDb movie reviews corpus from NLTK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corpora have a convenient method, `sent()`, to extract the individual sentences
    and words (tokenized sentences, which can be directly passed to the `word2vec`
    algorithm). Since both the corpora are rather small, use the Skip-gram method
    to create the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have two embeddings that have been learned on different contexts for
    the same term. Let's see the most similar terms for `money` from the model on
    the Brown corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print out the *top five terms* most similar to `money` from the model that
    were learned on the Brown corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the top term is `'job'`; fair enough. Let's see what the model
    learned regarding movie reviews.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print out the top five terms most similar to `money` from the model that learned
    from the movie corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following are the top terms:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The top terms are `cash` and `ransom`. Considering the language being used in
    movies, and thus in movie reviews, this isn't very surprising.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we created word vectors using different datasets and saw that
    the representations for the same terms and the associations that were learned
    are very affected by the underlying data. So, choose your data wisely.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pre-Trained Word Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've trained our own word embeddings using the small datasets we had
    access to. The folks at the Stanford NLP group have trained word embeddings on
    6 billion tokens with 400,000 terms in the vocabulary. Individually, we will not
    have the resources to handle this scale. Fortunately, the Stanford NLP group has
    been benevolent enough to make these trained embeddings available to the general
    public so that people like us can benefit from their work. The trained embeddings
    are available on the GloVe page ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick note on GloVe: the method that''s used for training is slightly different.
    The objective is modified to make the similar terms occur closer in space, in
    a little more explicit fashion. You can read about the details on the project
    page for GloVe ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)),
    which also has a link to the original paper proposing it. The end result, however,
    is very similar in performance to word2vec.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll download the `glove.6B.zip` file from the GloVe project page. The file
    contains 50D, 100D, 200D, and 300D vectors. We''ll work with the 100D vectors
    here. Please unzip the file and make sure you have the text files in your working
    directory. The trained vectors are available as a text file, and the format is
    slightly different. We''ll use the `glove2word2vec` utility that''s available
    in Gensim to convert into a format that Gensim can easily load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'We specified the input and the output file and ran the `glove2word2vec` utility.
    As the name suggests, the utility takes in word vectors in GloVe format and converts
    them into `word2vec` format. After this, the `word2vec` models can understand
    these embeddings easily. Now, let''s load the `keyed` word vectors from the text
    file (reformatted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'With this done, we have the GloVe embeddings in the model, along with all the
    handy utilities we had for the embeddings model from word2vec. Let''s check out
    the top terms similar to `"money"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'For closure, let''s also check how this model performs on the king and queen
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have these embeddings in a model, we can work with them the same
    way we worked with the embeddings we created previously and can benefit from the
    larger dataset and vocabulary and the processing power used by the contributing
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: Bias in Embeddings – A Word of Caution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When discussing regularities and analogies, we saw the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king – man + woman = queen*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s great that the embeddings are capturing these regularities by learning
    from the text data. Let''s try something similar to a profession. Let''s see the
    term closest to *doctor – man + woman*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'The output regarding the top five results will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s not the kind of result we want. Doctors are males, while females are
    nurses? Let''s try another example. This time, let''s try what the model thinks
    regarding females as corresponding to "smart" for "males":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following top five results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the top terms are `'cute'`, `'dumb'`, and `'crazy'`. That's
    not good at all.
  prefs: []
  type: TYPE_NORMAL
- en: What's happening here? Is this seemingly great representation approach sexist?
    Is the word2vec algorithm sexist? There definitely is bias in the resulting word
    vectors, but think about where the bias is coming from. It's the underlying data
    that uses `'nurse'` for females in contexts where `'doctor'` is used for males.
    It is, therefore, the underlying text that contains the bias, not the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This topic has recently gained significant attention, and there is ongoing research
    around ways to assess and get rid of biases from the learned embeddings, but a
    good approach is to avoid biases in the data to begin with. If you trained word
    embeddings on YouTube comments, don't be surprised if they contain all kinds of
    extreme biases. You're better off avoiding text data that you suspect to have
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: Other Notable Approaches to Word Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We worked with the word2vec approach primarily, and we briefly looked at the
    GloVe approach. While these are the most popular approaches, there are a few other
    approaches worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FastText**: Created by **Facebook''s AI Research** (**FAIR**) lab, it uses
    subword information to enrich the word embeddings. You can read more about it
    on the official page ([https://research.fb.com/downloads/fasttext/](https://research.fb.com/downloads/fasttext/)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**WordRank**: Treats the embeddings problem as a word-ranking problem. Its
    performance is similar to word2vec in several tasks. You can read more about this
    at [https://arxiv.org/abs/1506.02761](https://arxiv.org/abs/1506.02761).'
  prefs: []
  type: TYPE_NORMAL
- en: Other than these, some popular libraries now have pre-trained embeddings available
    (SpaCy is a good example). The choices are aplenty. We can't do a detailed treatment
    of these choices here, but please do explore the options.
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed a lot of ideas around representation in this chapter. Now, let's
    implement these ideas with the help of an activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Text Representation for Alice in Wonderland'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous activity, we tokenized and performed basic preprocessing of
    the text. In this activity, we will advance this process by using representation
    approaches for the text. You will create your own embeddings from the data and
    see the kind of relations we have. You will also utilize pre-trained embeddings
    to represent the data in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that you'll need to have completed *Activity 4.01*, *Text Preprocessing
    of the 'Alice in Wonderland' Text*, to proceed with this activity. In that activity,
    we performed stop word removal on the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll continue using the same Jupyter Notebook that we used for *Activity 4.01*,
    *Text Preprocessing of the 'Alice in Wonderland' Text*. We'll work on the result
    of the stop word removal step we got in that activity (let's say it is stored
    in a variable called `alice_words_nostop`). Print the first three sentences from
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: Import `word2vec` from Gensim and train your word embeddings with default parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the terms most similar to `rabbit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a window size 2, retrain the word vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the terms most similar to `rabbit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrain the word vectors using the Skip-gram method with a window size of `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the terms most similar to `rabbit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the representation for the phrase `white rabbit` by averaging the vectors
    for `white` and `rabbit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the representation for `mad hatter` by averaging the vectors for `mad`
    and `hatter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the cosine similarity between these two phrases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load pre-trained GloVe embeddings of size 100D.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find representations for `white rabbit` and `mad hatter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the cosine similarity between the two phrases. Has the cosine similarity
    changed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a result of this activity, we will have our own word vectors that have been
    trained on "Alice's Adventures in Wonderland" and have representation for the
    terms available in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 407.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began by discussing the peculiarities of text data and how
    ambiguity makes NLP difficult. We discussed that there are two key ideas in working
    with text – preprocessing and representation. We discussed the many tasks involved
    in preprocessing, that is, getting your data cleaned up and ready for analysis.
    We saw various approaches to removing imperfections from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Representation was the next big aspect – we understood the considerations in
    representing text and converting text into numbers. We looked at various approaches,
    beginning with classical approaches, which included one-hot encoding, the count-based
    approach, and the TF-IDF method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings are a whole new approach to representing text that leverage
    ideas from distributional semantics – terms that appear in similar contexts have
    similar meanings. The word2vec algorithm smartly exploits this idea by formulating
    a prediction problem: predict a target word given the context. It uses a neural
    network for the prediction and, in the process, learns vector representations
    for the terms.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw that these representations are amazing as they seem to capture meaning,
    and simple arithmetic operations gave some very interesting and meaningful results.
    You can even create representations for phrases or even sentences/documents using
    word vectors. This sets the stage for later when we use word embeddings in more
    sophisticated deep learning architectures for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll continue our exploration of sequences by applying
    deep learning approaches such as recurrent neural networks and one-dimensional
    convolutions to them.
  prefs: []
  type: TYPE_NORMAL
