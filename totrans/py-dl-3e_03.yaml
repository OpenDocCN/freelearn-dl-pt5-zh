- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Deep Learning Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: In this chapter, we will introduce **deep learning** (**DL**) and **deep neural
    networks** (**DNNs**) – that is, **neural networks** (**NNs**) with multiple hidden
    layers. You might be wondering what the point of using more than one hidden layer
    is, given the universal approximation theorem. This is in no way a naive question,
    and for a long time, NNs were used in that way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将介绍**深度学习**（**DL**）和**深度神经网络**（**DNNs**）——即具有多个隐藏层的**神经网络**（**NNs**）。你可能会疑惑，既然有通用逼近定理，为什么还需要使用多个隐藏层？这个问题并非天真，长期以来，神经网络确实是以这种方式使用的。
- en: Without going into too much detail, one reason is that approximating a complex
    function might require a huge number of units in the hidden layer, making it impractical
    to use. There is also another, more important, reason for using deep networks,
    which is not directly related to the number of hidden layers, but to the level
    of learning. A deep network does not simply learn to predict output *Y* given
    input, *X*; it also understands the basic features of the input. It’s able to
    learn abstractions of features of input samples, understand the basic characteristics
    of the samples, and make predictions based on those characteristics. This level
    of abstraction is missing in other basic **machine learning** (**ML**) algorithms
    and shallow NNs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要过多细节解释，原因之一是，逼近一个复杂的函数可能需要隐藏层中大量的单元，这使得使用它变得不切实际。还有另一个更为重要的原因，虽然它与隐藏层的数量无关，但与学习的层次相关。一个深度网络不仅仅是学习如何根据输入*X*预测输出*Y*；它还能够理解输入的基本特征。它能够学习输入样本特征的抽象，理解样本的基本特性，并基于这些特性进行预测。这种抽象层次在其他基本的**机器学习**（**ML**）算法和浅层神经网络中是缺失的。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论以下主要主题：
- en: Introduction to DL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Fundamental DL concepts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: Deep neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: Training deep networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度网络
- en: Applications of DL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: Introducing popular DL libraries
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍流行的深度学习库
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and Keras
    as part of **TensorFlow** (**TF**). If you don’t have an environment set up with
    these tools, fret not – the example is available as a Jupyter notebook on Google
    Colab. You can find the code examples in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和Keras作为**TensorFlow**（**TF**）的一部分来实现示例。如果你还没有设置好这些工具的环境，别担心——示例代码已经作为Jupyter笔记本文件提供在Google
    Colab上。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03)。
- en: Introduction to DL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: 'In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton published a milestone
    paper titled *ImageNet Classification with Deep Convolutional Neural Networks*
    ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)).
    The paper describes their use of NNs to win the ImageNet competition of the same
    year, which we mentioned in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047). At
    the end of their paper, they noted that the network’s performance degrades even
    if a single layer is removed. Their experiments demonstrated that removing any
    of the middle layers resulted in an about 2% top-1 accuracy loss of the model.
    They concluded that network depth is important for the performance of the network.
    The basic question is: what makes the network’s depth so important?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 发表了一篇具有里程碑意义的论文，题为 *使用深度卷积神经网络进行ImageNet分类*
    ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf))。该论文描述了他们使用神经网络（NN）赢得同年ImageNet竞赛的过程，这一点我们在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中提到过。论文结尾指出，即使移除单一层，网络的性能也会下降。他们的实验表明，移除任何中间层都会导致模型大约2%的Top-1准确率损失。他们得出结论，网络的深度对性能至关重要。那么，基本问题是：是什么让网络的深度如此重要呢？
- en: 'A typical English saying is a picture is worth a thousand words. Let’s use
    this approach to understand what DL is. We’ll use images from the highly cited
    paper *Convolutional Deep Belief Networks for Scalable Unsupervised Learning of
    Hierarchical Representations* ([https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf](https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf)).
    Here, the authors trained an NN with pictures of different categories of either
    objects or animals. The following figure shows how the different layers of the
    network learn different characteristics of the input data. In the first layer,
    the network learns to detect some small basic features, such as lines and edges,
    which are common for all images in all categories:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的英文谚语是“一图胜千言”。让我们用这种方法来理解什么是深度学习（DL）。我们将使用来自广泛引用的论文《卷积深度信念网络：可扩展的无监督学习层次表示》中的图像（[https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf](https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf)）。在这篇论文中，作者用不同类别的物体或动物图片训练了一个神经网络（NN）。下图展示了网络的不同层如何学习输入数据的不同特征。在第一层，网络学习检测一些小的基础特征，例如线条和边缘，这些特征在所有类别的所有图像中都是常见的：
- en: '![Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom)
    after training](img/B19627_03_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 第一层权重（上）和第二层权重（下）训练后的结果](img/B19627_03_1.jpg)'
- en: Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom)
    after training
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 第一层权重（上）和第二层权重（下）训练后的结果
- en: 'But the next layers, which we can see in the following figure, combine those
    lines and edges to compose more complex features that are specific to each category.
    In the first row of the bottom-left image, we can see how the network can detect
    different features of human faces, such as eyes, noses, and mouths. In the case
    of cars, these would be wheels, doors, and so on, as seen in the second image
    from the left in the following figure. These features are **abstract** – that
    is, the network has learned the generic shape of a feature (such as a mouth or
    a nose) and can detect this feature in the input data, despite the variations
    it might have:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是接下来的层，如下图所示，将这些线条和边缘结合起来，组成了更复杂的特征，这些特征对每个类别都是特定的。在左下角图像的第一行中，我们可以看到网络如何检测到人脸的不同特征，例如眼睛、鼻子和嘴巴。对于汽车而言，这些特征可能是车轮、车门等等，如下图中的第二张图所示。这些特征是**抽象的**——即，网络已经学会了一个特征（如嘴巴或鼻子）的通用形状，并且能够在输入数据中检测到这个特征，尽管它可能具有不同的变化：
- en: '![Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer
    (bottom) weights learned for a speciﬁc object category (class). Column 5 represents
    the weights learned for a mixture of four object categories (faces, cars, airplanes,
    and motorbikes)](img/B19627_03_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 第1至第4列表示为特定物体类别（类）学习的第二层（上）和第三层（下）权重。第5列表示为四个物体类别（人脸、汽车、飞机和摩托车）混合学习的权重](img/B19627_03_2.jpg)'
- en: Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer
    (bottom) weights learned for a speciﬁc object category (class). Column 5 represents
    the weights learned for a mixture of four object categories (faces, cars, airplanes,
    and motorbikes)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 第1至第4列表示为特定物体类别（类）学习的第二层（上）和第三层（下）权重。第5列表示为四个物体类别（人脸、汽车、飞机和摩托车）混合学习的权重
- en: In the second row of the preceding figure, we can see how, in the deeper layers,
    the network combines these features in even more complex ones, such as faces and
    whole cars. One strength of DNNs is that they can learn these high-level abstract
    representations by themselves, deducting them from the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图像的第二行中，我们可以看到，在更深的层次中，网络将这些特征组合成更复杂的特征，如人脸和完整的汽车。深度神经网络（DNN）的一个优势是，它们能够自主地学习这些高层次的抽象表示，并从训练数据中推导出这些表示。
- en: Next, let’s discuss these properties of DNNs in more detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地讨论DNN的这些特性。
- en: Fundamental DL concepts
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: In 1801, Joseph Marie Charles invented the **Jacquard loom**. Charles was not
    a scientist, but simply a merchant. The Jacquard loom used a set of punched cards,
    where each card represented a pattern to be reproduced on the loom. At the same
    time, each card was an abstract representation of that pattern. Punched cards
    have been used, for example, in the tabulating machine invented by Herman Hollerith
    in 1890, or in the first computers as a means to input code. In the tabulating
    machine, the cards were simply abstractions of samples to be fed into the machine
    to calculate statistics on a population. But in the Jacquard loom, their use was
    subtler, and each card represented the abstraction of a pattern that could be
    combined with others to create more complex patterns. The punched card is an abstract
    representation of a feature of reality, the final weaved design.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 1801年，Joseph Marie Charles 发明了**贾卡尔织机**。Charles 不是一名科学家，而仅仅是一个商人。贾卡尔织机使用了一套打孔卡片，每张卡片代表着织机上要复制的图案。同时，每张卡片也是该图案的抽象表示。例如，打孔卡片曾被用在1890年由
    Herman Hollerith 发明的统计机器中，或者作为第一代计算机输入代码的一种方式。在统计机器中，卡片仅仅是要输入机器以计算人口统计数据的样本的抽象。然而，在贾卡尔织机中，卡片的使用更加微妙，每张卡片代表了可以与其他卡片组合起来形成更复杂图案的模式抽象。打孔卡片是现实特征的抽象表示，最终编织出来的设计就是这一抽象的体现。
- en: In a way, the Jacquard loom sowed the seeds of what DL is today, the definition
    of reality through the representations of its features. A DNN does not simply
    recognize what makes a cat a cat, or a squirrel a squirrel, but it understands
    what features are present in a cat and a squirrel, respectively. It learns to
    design a cat or a squirrel using those features. If we were to design a weaving
    pattern in the shape of a cat using a Jacquard loom, we would need to use punched
    cards that have whiskers on the nose, such as those of a cat, and an elegant and
    slender body. Conversely, if we were to design a squirrel, we would need to use
    a punched card that makes a furry tail. A deep network that learns basic representations
    of its output can make classifications using the assumptions it has made. For
    example, if there is no furry tail, it will probably not be a squirrel, but rather
    a cat. In this way, the amount of information the network learns is much more
    complete and robust, and the most exciting part is that DNNs learn to do this
    automatically.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，贾卡尔织机为今天的深度学习（DL）播下了种子，它通过对特征的表示定义了现实的含义。深度神经网络（DNN）不仅仅是识别猫是什么，或者松鼠是什么，它更理解猫和松鼠分别具备哪些特征。它学会了如何利用这些特征来设计一只猫或一只松鼠。如果我们要用贾卡尔织机设计一个猫形状的织物图案，我们需要使用具有猫鼻子上的胡须，且有优雅而纤细的身体的打孔卡片。相反，如果我们要设计一只松鼠，则需要使用能够表现松鼠毛茸茸尾巴的打孔卡片。一个学习了基本特征表示的深度网络，可以根据它所做的假设进行分类。例如，如果没有毛茸茸的尾巴，它可能就不是松鼠，而是猫。通过这种方式，网络学习到的信息更为完整且更具鲁棒性，最令人兴奋的是，深度神经网络能够自动完成这一学习过程。
- en: Feature learning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征学习
- en: 'To illustrate how DL works, let’s consider the task of recognizing a simple
    geometric figure, such as a cube, as seen in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明深度学习是如何工作的，我们来考虑一个简单的几何图形识别任务，例如识别一个立方体，见下图所示：
- en: '![Figure 3.3 – An abstraction of an NN representing a cube. Diﬀerent layers
    encode features with diﬀerent levels of abstraction](img/B19627_03_3.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 一个表示立方体的神经网络（NN）的抽象图。不同的层次编码了具有不同抽象层级的特征](img/B19627_03_3.jpg)'
- en: Figure 3.3 – An abstraction of an NN representing a cube. Diﬀerent layers encode
    features with diﬀerent levels of abstraction
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 一个表示立方体的神经网络（NN）的抽象图。不同的层次编码了具有不同抽象层级的特征。
- en: The cube is composed of edges (or lines), which intersect in vertices. Let’s
    say that each possible point in the three-dimensional space is associated with
    a unit (forget for a moment that this will require an infinite number of units).
    All the points/units are in the first (input) layer of a multilayer feedforward
    network. An input point/unit is active if the corresponding point lies on a line.
    The points/units that lie on a common line (edge) have strong positive connections
    to a single common edge/unit in the next layer. Conversely, they have negative
    connections to all other units in the next layer. The only exceptions are the
    units that lie on the vertices. Each such unit lies simultaneously on three edges
    and is connected to its three corresponding units in the subsequent layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立方体由边（或线）组成，这些边在顶点处相交。假设三维空间中的每一个可能点都与一个单位相关联（暂时忽略这将需要无穷多个单位）。所有这些点/单位都位于多层前馈网络的第一层（输入层）。如果相应的点位于一条线上，则输入点/单位是激活的。位于同一条线（边）上的点/单位与下一层中的单一公共边/单位之间有强的正向连接。相反，它们与下一层中所有其他单位之间有负向连接。唯一的例外是位于顶点上的单位。每个这样的单位同时位于三条边上，并与下一层中的三个对应单位相连接。
- en: Now, we have two hidden layers, with different levels of abstraction – the first
    for points and the second for edges. However, this is not enough to encode a whole
    cube in the network. Let’s try this with another layer for vertices. Here, each
    three active edges/units of the second layer, which form a vertex, have a significant
    positive connection to a single common vertex/unit of the third layer. Since an
    edge of the cube forms two vertices, each edge/unit will have positive connections
    to two vertices/units and negative connections to all others. Finally, we’ll introduce
    the last hidden layer (the cube). The four vertices/units forming the cube will
    have positive connections to a single cube/unit from the cube/layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了两个隐藏层，分别具有不同的抽象层次——第一个层次处理点，第二个层次处理边。然而，这还不足以在网络中编码一个完整的立方体。我们尝试通过增加一个顶点层来解决这个问题。在这里，第二层中每三个激活的边/单位（形成一个顶点）与第三层中的单一公共顶点/单位之间有显著的正向连接。由于立方体的每条边形成两个顶点，因此每条边/单位将与两个顶点/单位有正向连接，并与所有其他单位有负向连接。最后，我们将引入最后一个隐藏层（立方体）。形成立方体的四个顶点/单位将与来自立方体/层的单一立方体/单位之间有正向连接。
- en: This cube representation example is oversimplified, but we can draw several
    conclusions from it. One of them is that DNNs lend themselves well to hierarchically
    organized data. For example, an image consists of pixels, which form lines, edges,
    regions, and so on. This is also true for speech, where the building blocks are
    called **phonemes**, as well as text, where we have characters, words, and sentences.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立方体表示的例子过于简化，但我们可以从中得出几个结论。其中之一是深度神经网络（DNN）非常适合层次化组织的数据。例如，一张图像由像素组成，这些像素形成线条、边缘、区域等。同样，语音也如此，其中的基本构件叫做**音素**，而文本则有字符、单词和句子。
- en: In the preceding example, we dedicated layers to specific cube features deliberately,
    but in practice, we wouldn’t do that. Instead, a deep network will “discover”
    features automatically during training. These features might not be immediately
    obvious and, in general, wouldn’t be interpretable by humans. Also, we wouldn’t
    know the level of the features encoded in the different layers of the network.
    Our example is more akin to classic ML algorithms, where the user has to use their
    own experience to select what they think are the best features. This process is
    called **feature engineering**, and it can be labor-intensive and time-consuming.
    Allowing a network to automatically discover features is not only easier, but
    those features are highly abstract, which makes them less sensitive to noise.
    For example, human vision can recognize objects of different shapes, sizes, in
    different lighting conditions, and even when their view is partly obscured. We
    can recognize people with different haircuts and facial features, even when they
    wear a hat or a scarf that covers their mouth. Similarly, the abstract features
    the network learns will help it recognize faces better, even in more challenging
    conditions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们有意为特定的立方体特征分配了层，但实际上，我们不会这么做。相反，深度网络会在训练过程中自动“发现”特征。这些特征可能不会立刻显现出来，而且通常人类也无法解释它们。此外，我们无法知道网络中不同层所编码的特征的层次。我们的例子更像是经典的机器学习（ML）算法，在这些算法中，用户必须凭借自己的经验选择他们认为最好的特征。这一过程被称为**特征工程**，它可能既费力又耗时。让网络自动发现特征不仅更容易，而且这些特征通常非常抽象，从而使它们对噪声的敏感度较低。例如，人类视觉可以识别不同形状、大小、光照条件下的物体，甚至在物体的视角部分被遮挡时也能识别。我们能认出不同发型和面部特征的人，即使他们戴着帽子或围巾遮住嘴巴。类似地，网络学习到的抽象特征将帮助它更好地识别面孔，即使在更具挑战性的条件下。
- en: In the next section, we’ll discuss some of the reasons DL has become so popular.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论深度学习（DL）变得如此流行的一些原因。
- en: The reasons for DL’s popularity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习流行的原因
- en: If you’ve followed ML for some time, you may have noticed that many DL algorithms
    are not new. **Multilayer perceptrons** (**MLPs**) have been around for nearly
    50 years. Backpropagation was discovered a couple of times but finally gained
    recognition in 1986\. Yann LeCun, a famous computer scientist, perfected his work
    on convolutional networks in the 1990s. In 1997, Sepp Hochreiter and Jürgen Schmidhuber
    invented long short-term memory, a type of recurrent NN still in use today. In
    this section, we’ll try to understand why we have AI summer now, and why we only
    had AI winters ([https://en.wikipedia.org/wiki/AI_winter](https://en.wikipedia.org/wiki/AI_winter))
    before.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经关注机器学习（ML）一段时间，你可能会注意到许多深度学习（DL）算法并不新鲜。**多层感知机**（**MLPs**）已经存在近50年。反向传播算法曾多次被发现，但最终在1986年获得了认可。著名计算机科学家Yann
    LeCun在1990年代完善了他的卷积网络工作。在1997年，Sepp Hochreiter和Jürgen Schmidhuber发明了长短期记忆网络（LSTM），这是一种至今仍在使用的递归神经网络（RNN）。在这一节中，我们将尝试理解为什么如今我们迎来了AI的春天，而之前只有AI的冬天（[https://en.wikipedia.org/wiki/AI_winter](https://en.wikipedia.org/wiki/AI_winter)）。
- en: The first reason is that today, we have a lot more data than in the past. The
    rise of the internet and software in different industries has generated a lot
    of computer-accessible data. We also have more benchmark datasets, such as ImageNet.
    With this comes the desire to extract value from that data by analyzing it. And,
    as we’ll see later, DL algorithms work better when they are trained with a lot
    of data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是，今天我们拥有的数据比过去多得多。互联网和软件在各个行业的兴起生成了大量可以通过计算机访问的数据。我们还拥有更多的基准数据集，例如ImageNet。随着这些数据的增加，人们也希望通过分析数据来提取价值。正如我们稍后会看到的，深度学习（DL）算法在使用大量数据进行训练时表现得更好。
- en: The second reason is the increased computing power. This is most visible in
    the drastically increased processing capacity of **graphical processing units**
    (**GPUs**). NNs are organized in such a way as to take advantage of this parallel
    architecture. Let’s see why. As we learned in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047),
    units from a network layer are not connected to units from the same layer. We
    also learned that we could represent many layer operations as matrix multiplications.
    Matrix multiplication is embarrassingly parallel (trust me, this is a term – you
    can Google it!). The computation of each output cell is not related to the computation
    of any other output cell. Therefore, we can compute all of the outputs in parallel.
    Not coincidentally, GPUs are well suited for highly parallel operations like this.
    On the one hand, a GPU has a high number of computational cores compared to a
    **central processing unit** (**CPU**). Even though a CPU core is faster than a
    GPU one, we can still compute a lot more output cells in parallel. But what’s
    even more important is that GPUs are optimized for memory bandwidth, while CPUs
    are optimized for latency. This means that a CPU can fetch small chunks of memory
    very quickly but will be slow when it comes to fetching large chunks. The GPU
    does the opposite. For matrix multiplication in a deep network with a lot of wide
    layers, bandwidth becomes the bottleneck, not latency. In addition, the L1 cache
    of the GPU is much faster than the L1 cache for the CPU and is also larger. The
    L1 cache represents the memory of the information that the program is likely to
    use next, and storing this data can speed up the process. Much of the memory gets
    reused in DNNs, which is why L1 cache memory is important.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是计算能力的提升。最显著的表现就是**图形处理单元**（**GPU**）的处理能力大幅提高。神经网络的组织方式使得它能够充分利用这种并行架构。让我们看看为什么。正如我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中所学到的，网络层的单元与同一层的单元没有直接连接。我们还学到了，许多层的操作可以表示为矩阵乘法。矩阵乘法是显著并行的（相信我，这是一个术语——你可以去Google查找！）。每个输出单元的计算与其他输出单元的计算无关。因此，我们可以并行计算所有的输出。并且，GPU非常适合执行这样的高并行操作。一方面，GPU的计算核心数量远超**中央处理单元**（**CPU**）。尽管CPU的核心速度比GPU核心更快，但我们仍然可以在GPU上并行计算更多的输出单元。但更重要的是，GPU在内存带宽方面进行了优化，而CPU则优化了延迟。这意味着CPU可以非常快速地获取小块内存，但当需要获取大块内存时则会变得较慢。而GPU则相反。对于一个深度网络中有许多宽层的矩阵乘法，带宽成为瓶颈，而不是延迟。此外，GPU的L1缓存比CPU的L1缓存更快，而且更大。L1缓存代表了程序下一步可能使用的信息存储，存储这些数据可以加速处理过程。在深度神经网络（DNN）中，大量的内存会被重复使用，这也是L1缓存非常重要的原因。
- en: In the next section, *Deep neural networks*, we’ll give a more precise definition
    of the key NN architectures that will be thoroughly introduced in the coming chapters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节《深度神经网络》中，我们将给出神经网络关键架构的更精确定义，并将在接下来的章节中详细介绍这些架构。
- en: Deep neural networks
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: 'We could define DL as a class of ML techniques, where information is processed
    in hierarchical layers to understand representations and features from data in
    increasing levels of complexity. In practice, all DL algorithms are NNs, which
    share some common basic properties. They all consist of a graph of interconnected
    operations, which operate with input/output tensors. Where they differ is network
    architecture (or the way units are organized in the network), and sometimes in
    the way they are trained. With that in mind, let’s look at the main classes of
    NNs. The following list is not exhaustive, but it represents most NN types in
    use today:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将深度学习（DL）定义为机器学习（ML）技术的一类，其中信息通过分层处理，以逐步深入的方式理解数据中的表示和特征，复杂度逐渐增加。实际上，所有深度学习算法都是神经网络（NN），它们共享一些基本的共同特性。它们都由一个互联操作的图构成，操作过程使用输入/输出张量。它们的不同之处在于网络架构（或网络中单元的组织方式），有时也体现在训练方法上。考虑到这一点，让我们来看看神经网络的主要类型。以下列表并不详尽，但它代表了今天大多数使用中的神经网络类型：
- en: '**Multilayer perceptron** (**MLP**): An NN with feedforward propagation, fully
    connected layers, and at least one hidden layer. We introduced MLPs in [*Chapter
    2*](B19627_02.xhtml#_idTextAnchor047).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）：一种具有前馈传播、全连接层且至少有一个隐藏层的神经网络。我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中介绍了MLP。'
- en: '**Convolutional neural network** (**CNN**): A CNN is a feedforward NN with
    several types of special layers. For example, convolutional layers apply a filter
    to the input image (or sound) by sliding that filter all across the incoming signal,
    to produce an *n*-dimensional activation map. There is some evidence that units
    in CNNs are organized similarly to how biological cells are organized in the visual
    cortex of the brain. We’ve mentioned CNNs several times so far, and that’s not
    a coincidence – today, they outperform all other ML algorithms on many computer
    vision and NLP tasks. We’ll discuss CNNs in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）：CNN是一种前馈神经网络，具有几种特殊类型的层。例如，卷积层通过滑动滤波器到输入图像（或声音）上，从而应用该滤波器，生成*n*维激活图。有证据表明，CNN中的单元以类似生物细胞在大脑视觉皮层中的组织方式进行组织。到目前为止，我们已经提到过CNN很多次，这并非巧合——今天，CNN在许多计算机视觉和自然语言处理任务中优于所有其他机器学习算法。我们将在[*第4章*](B19627_04.xhtml#_idTextAnchor107)中讨论CNN。'
- en: '**Recurrent neural network** (**RNN**): This type of NN has an internal state
    (or memory), which is based on all, or part of, the input data that’s already
    been fed to the network. The output of a recurrent network is a combination of
    its internal state (memory of inputs) and the latest input sample. At the same
    time, the internal state changes to incorporate newly input data. Because of these
    properties, recurrent networks are good candidates for tasks that work on sequential
    data, such as text or time series data. We’ll discuss recurrent networks in [*Chapter
    6*](B19627_06.xhtml#_idTextAnchor185).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）：这种类型的神经网络具有一个内部状态（或记忆），该状态基于已输入网络的所有或部分数据。循环网络的输出是其内部状态（输入的记忆）和最新输入样本的组合。同时，内部状态会发生变化，以纳入新输入的数据。由于这些特性，循环网络非常适合处理顺序数据任务，例如文本或时间序列数据。我们将在[*第6章*](B19627_06.xhtml#_idTextAnchor185)中讨论循环网络。'
- en: '**Transformer**: Like RNNs, the transformer is suited to work with sequential
    data. It uses a mechanism called **attention**, which allows it *direct simultaneous
    access* to all elements of the input sequence. This is unlike an RNN, which processes
    the sequence elements one by one and updates its internal state after each element.
    As we’ll see in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202), the attention
    mechanism has several major advantages over the classic RNNs. Because of this,
    in recent years, transformers have superseded RNNs in many tasks.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：与RNN类似，transformer适合处理序列数据。它使用一种叫做**注意力机制**的方式，使得模型能够*直接同时访问*输入序列中的所有元素。这与RNN不同，后者是逐个处理序列元素，并在每个元素之后更新其内部状态。正如我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中看到的，注意力机制相较于经典的RNN具有多个重要优势。正因为如此，近年来，transformer已经在许多任务中取代了RNN。'
- en: '**Autoencoders**: As we mentioned in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016),
    autoencoders are a class of unsupervised learning algorithms, in which the output
    shape is the same as the input, which allows the network to better learn basic
    representations.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**：正如我们在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中提到的，自编码器是一类无监督学习算法，其输出形状与输入相同，这使得网络能够更好地学习基本表示。'
- en: Now that we’ve outlined the major types of DNNs, let’s discuss how to train
    them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了主要的深度神经网络类型，让我们讨论一下如何训练它们。
- en: Training deep neural networks
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练深度神经网络
- en: Historically, the scientific community has understood that deeper networks have
    greater representational power compared to shallow ones. However, there were various
    challenges in training networks with more than a few hidden layers. We now know
    that we can successfully train DNNs using a combination of gradient descent and
    backpropagation, just as we discussed in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
    In this section, we’ll see how to improve them so that we can solve some of the
    problems that exist uniquely for DNNs and not shallow NNs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，科学界一直认为，深度网络相比浅层网络具有更强的表示能力。然而，训练拥有多个隐藏层的网络曾面临许多挑战。我们现在知道，结合梯度下降和反向传播，我们可以成功训练深度神经网络（DNN），正如我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中讨论过的那样。在本节中，我们将看到如何改进这些网络，以解决一些只在深度神经网络中出现的问题，而不是浅层神经网络。
- en: 'The first edition of this book included networks such as **Restricted Boltzmann
    Machines** (**RBMs**) and **Deep Belief Networks** (**DBNs**). They were popularized
    by Geoffrey Hinton, a Canadian scientist, and one of the most prominent DL researchers.
    Back in 1986, he was also one of the inventors of backpropagation. RBMs are a
    special type of generative NN, where the units are organized into two layers,
    namely visible and hidden. Unlike feedforward networks, the data in an RBM can
    flow in both directions – from visible to hidden units, and vice versa. In 2002,
    Prof. Hinton introduced **contrastive divergence**, which is an unsupervised algorithm
    for training RBMs. In 2006, he introduced **deep belief networks** (**DBNs**),
    which are DNNs that are formed by stacking multiple RBMs. Thanks to their novel
    training algorithm, it was possible to create a DBN with more hidden layers than
    had previously been possible. But even with contrastive divergence, training a
    DBN is not easy. It is a two-step process:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第一版包括了如**限制玻尔兹曼机**（**RBMs**）和**深度信念网络**（**DBNs**）等网络。它们由加拿大科学家Geoffrey Hinton推广，他是最著名的深度学习研究者之一。早在1986年，他也是反向传播算法的发明者之一。RBM是一种特殊类型的生成性神经网络，其中单元被组织成两个层次，即可见层和隐藏层。与前馈网络不同，RBM中的数据可以双向流动——从可见单元到隐藏单元，反之亦然。2002年，Hinton教授引入了**对比散度**，这是一种用于训练RBM的无监督算法。2006年，他引入了**深度信念网络**（**DBNs**），这些是通过堆叠多个RBM形成的深度神经网络。由于其创新的训练算法，DBN可以拥有比以前更多的隐藏层。但即便有了对比散度，训练一个DBN也不是件容易的事。这是一个两步过程：
- en: First, we have to train each RBM with contrastive divergence, and gradually
    stack them on top of each other. This phase is called **pre-training**.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须使用对比散度训练每个RBM，并逐渐将它们堆叠在一起。这个阶段叫做**预训练**。
- en: In effect, pre-training serves as a sophisticated weight initialization algorithm
    for the next phase, called **fine-tuning**. With fine-tuning, we transform the
    DBN into a regular MLP and continue training it using supervised backpropagation
    and gradient descent, in the same way as we saw in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，预训练作为下一阶段的一个复杂的权重初始化算法，叫做**微调**。通过微调，我们将DBN转化为一个常规的MLP，并继续使用监督反向传播和梯度下降训练它，就像我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中看到的那样。
- en: Thanks to some algorithmic advances, it’s now possible to train deep networks
    using plain old backpropagation, thus effectively eliminating the pre-training
    phase. These advances rendered DBNs and RBMs obsolete. They are, without a doubt,
    interesting from a research perspective, but they are rarely used in practice
    anymore and we’ll omit them from this edition.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些算法的进展，现在可以使用传统的反向传播算法训练深度网络，从而有效地消除了预训练阶段。这些进展使得DBN和RBM变得过时。它们无疑在研究中非常有趣，但在实践中已经很少使用，我们将在本版本中省略它们。
- en: Next, let’s discuss the algorithmic advances that made training of NNs with
    backpropagation possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一些使得使用反向传播训练神经网络成为可能的算法进展。
- en: Improved activation functions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进的激活函数
- en: 'But why is training deep networks so hard? One of the main challenges that
    pre-training solved is the so-called **vanishing gradients** problem. To understand
    it, we’ll assume that we’ll use backpropagation to train a regular MLP with multiple
    hidden layers and logistic sigmoid activation at each layer. Let’s focus on the
    sigmoid function (the same applies to tanh). As a reminder, it is computed as
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/233.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/234.png):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '那么，为什么训练深度网络如此困难呢？预训练解决的主要挑战之一就是所谓的**梯度消失**问题。为了理解这一点，我们假设使用反向传播训练一个普通的多层感知机（MLP），该网络具有多个隐藏层，并在每个层使用逻辑
    sigmoid 激活函数。我们先聚焦于 sigmoid 函数（tanh 函数的情况也是如此）。提醒一下，sigmoid 函数的计算公式为 ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/233.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/234.png):'
- en: '![Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted)
    (left); consecutive sigmoid activations, which “squash” the data (right)](img/B19627_03_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 逻辑 sigmoid（未中断）及其导数（中断）（左）；连续的 sigmoid 激活，将数据“压缩”（右)](img/B19627_03_4.jpg)'
- en: Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted)
    (left); consecutive sigmoid activations, which “squash” the data (right)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 逻辑 sigmoid（未中断）及其导数（中断）（左）；连续的 sigmoid 激活，将数据“压缩”（右）
- en: 'The vanishing gradients manifest themselves in the following ways:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失现象表现为以下几种方式：
- en: In the forward phase, the outputs of the first sigmoid layer are represented
    by the blue uninterrupted line (both left and right images in the preceding figure)
    and fall in the range (0, 1). The dotted lines on the right image represent the
    sigmoid activations of each of the consecutive layers after the first. Even after
    three layers, we can see that the activation is “squashed” in a narrow range and
    converges to around 0.66, regardless of the input value. For example, if the input
    value of the first layer is 2, then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.881</mml:mn></mml:math>](img/235.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.71</mml:mn></mml:math>](img/236.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.67</mml:mn></mml:math>](img/237.png),
    and so on. This peculiarity of the sigmoid function acts as an eraser of any information
    coming from the preceding layers.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now know that to train an NN, we need to compute the derivative of the activation
    function (along with all the other derivatives) for the backward phase. The derivative
    of the sigmoid function is represented by the green interrupted line on the left
    image in the preceding figure. We can see that it has a significant value in a
    very narrow interval, centered around 0, and converges toward 0 in all other cases.
    In networks with many layers, the derivative would likely converge to 0 when propagated
    to the first layers of the network. Effectively, this means we cannot propagate
    the error to these layers and we cannot update their weights in a meaningful way.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thankfully, the **ReLU** activation we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)
    can solve both of these problems with a single stroke. To recap, the following
    figure shows the ReLU graph and its derivative:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)](img/B19627_03_5.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLU has the following desirable properties:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: It is **idempotent**. If we pass a value through an arbitrary number of ReLU
    activations, it will not change; for example, *ReLU(2) = 2*, *ReLU(ReLU(2)) =
    2*, and so on. This is not the case for a sigmoid. The idempotence of ReLU makes
    it theoretically possible to create networks with more layers compared to the
    sigmoid.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 **幂等的**。如果我们通过任意次数的 ReLU 激活传递一个值，它将保持不变；例如，*ReLU(2) = 2*，*ReLU(ReLU(2)) =
    2*，依此类推。这对于 sigmoid 函数来说并不成立。ReLU 的幂等性使得理论上可以创建比 sigmoid 更深层的网络。
- en: We can also see that its derivative is either 0 or 1, regardless of the backpropagated
    value. In this way, we can avoid vanishing gradients in the backward pass as well.
    Strictly speaking, the derivative ReLU at value 0 is undefined, which makes the
    ReLU only semi-differentiable (more information about this can be found at [https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)).
    But in practice, it works well enough.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以看到，它的导数无论反向传播的值如何，都是 0 或 1。通过这种方式，我们还可以避免在反向传播中梯度消失的问题。严格来说，ReLU 在值为 0
    时的导数是未定义的，这使得 ReLU 只在半微分意义下有效（关于这一点的更多信息可以参考 [https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)）。但在实践中，它足够有效。
- en: It creates sparse activations. Let’s assume that the weights of the network
    are initialized randomly through normal distribution. Here, there is a 0.5 chance
    that the input for each ReLU unit is < 0\. Therefore, the output of about half
    of all activations will also be 0\. The sparse activations have several advantages,
    which we can roughly summarize as Occam’s razor in the context of NNs – it’s better
    to achieve the same result with a simpler data representation than a complex one.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它产生稀疏的激活。假设网络的权重通过正态分布随机初始化。在这种情况下，每个 ReLU 单元的输入有 0.5 的概率小于 0。因此，大约一半的激活输出也将为
    0。这种稀疏激活有几个优势，我们可以粗略地总结为在神经网络中的奥卡姆剃刀原则——用更简单的数据表示来实现相同的结果，比复杂的表示方式更好。
- en: It’s faster to compute in both the forward and backward passes.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在前向和反向传播中计算速度更快。
- en: 'Despite these ReLU advantages, during training, the network weights can be
    updated in such a way that some of the ReLU units in a layer will always receive
    inputs smaller than 0, which, in turn, will cause them to permanently output 0
    as well. This phenomenon is known as **dying ReLUs**. To solve this, several ReLU
    modifications have been proposed. The following is a non-exhaustive list:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ReLU 有这些优势，但在训练过程中，网络权重可能会被更新到某些 ReLU 单元总是接收小于 0 的输入，从而导致它们始终输出 0。这种现象被称为
    **死亡 ReLU（dying ReLUs）**。为了解决这个问题，已经提出了几种 ReLU 的改进方法。以下是一个非详尽的列表：
- en: '**Leaky ReLU**: When the input is larger than 0, leaky ReLU repeats its input
    in the same way as the regular ReLU does. However, when *x < 0*, the leaky ReLU
    outputs *x* multiplied by some constant, α (*0 < α < 1*), instead of 0\. The following
    diagram shows the leaky ReLU formula, its derivative, and their graphs for *α
    =* *0.2*:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：当输入大于 0 时，Leaky ReLU 与普通 ReLU 相同，直接输出输入值。然而，当 *x < 0* 时，Leaky
    ReLU 输出 *x* 与某个常数 α (*0 < α < 1*) 的乘积，而不是 0。下图展示了 Leaky ReLU 的公式、它的导数以及它们的图形，*α
    = 0.2*：'
- en: '![Figure 3.6 – The leaky ReLU activation function](img/B19627_03_6.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – Leaky ReLU 激活函数](img/B19627_03_6.jpg)'
- en: Figure 3.6 – The leaky ReLU activation function
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – Leaky ReLU 激活函数
- en: '**Parametric ReLU** (**PReLU**; see *Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification*, [https://arxiv.org/abs/1502.01852):](https://arxiv.org/abs/1502.01852):)
    This activation is the same as the leaky ReLU, but *α* is tunable and is adjusted
    during training.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parametric ReLU** (**PReLU**；参见 *深入探讨激活函数：超越人类水平的 ImageNet 分类*， [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852))：该激活函数与
    Leaky ReLU 相同，但 *α* 是可调的，并且在训练过程中会进行调整。'
- en: '**Exponential linear unit** (**ELU**; see *Fast and Accurate Deep Network Learning
    by Exponential Linear Units (ELUs)*, [https://arxiv.org/abs/1511.07289):](https://arxiv.org/abs/1511.07289):)
    When the input is larger than 0, ELU repeats its input in the same way as ReLU
    does. However, when *x < 0*, the ELU output becomes ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/238.png),
    where *α* is a tunable parameter. The following diagram shows the ELU formula,
    its derivative, and their graphs for *α =* *0.2*:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – The ELU activation function](img/B19627_03_7.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – The ELU activation function
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaled exponential linear unit** (**SELU**; see *Self-Normalizing Neural
    Networks*, [https://arxiv.org/abs/1706.02515):](https://arxiv.org/abs/1706.02515):)
    This activation is like ELU, except that the output (both smaller and larger than
    0) is scaled with an additional training parameter, *λ*. The SELU is part of a
    larger concept called **self-normalizing NNs** (**SNNs**), which is described
    in the source paper.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sigmoid Linear Unit (SiLU)**, **Gaussian Error Linear Unit** (**GELU**; see
    *Gaussian Error Linear Units (GELUs)*, [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)),
    and **Swish** (see *Searching for Activation Functions*, [https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)):
    This is a collection of three similar (but not the same) functions that closely
    resemble ReLU but are differentiable at the 0 point. For the sake of simplicity,
    we’ll only show the SiLU graph (*σ* is the sigmoid function):'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – The SiLU activation function](img/B19627_03_8.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – The SiLU activation function
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have softmax, which is the activation function of the output layer
    in classification problems. Let’s assume that the output of the final network
    layer is a vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/239.png).
    Each of the *n* elements represents one of *n* classes, to which the input sample
    might belong. To determine the network prediction, we’ll take the index, *i*,
    of the highest value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/240.png),
    and assign the input sample to the class it represents. However, we can also interpret
    the network output as a probability distribution of a discrete random variable
    – that is, each value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png),
    represents the probability that the input sample belongs to that particular class.
    To help us with this, we’ll use the softmax activation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow></mfrac></mrow></mrow></math>](img/242.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'It has the following properties:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The denominator in the formula acts as a normalizer. This is important for
    the probability interpretation we just introduced:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/243.png),
    is constrained within the [0, 1] range, which allows us to treat it as a probability
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The total sum of values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/244.png)
    is equal to 1: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mi>j</mi><mrow
    /></msubsup><mrow><mi>f</mi><mfenced open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/245.png),
    which also aligns with the probability interpretation'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A bonus (in fact, obligatory) is that the function is differentiable.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The softmax activation has one more subtle property. Before we normalize the
    data, we transform each vector component exponentially with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/246.png).
    Let’s imagine that two of the vector components are ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/247.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math>](img/248.png).
    Here, we would have exp(1) = 2.7 and exp(2) = 7.39\. As we can see, the ratios
    between the components before and after the transformation are very different
    – 0.5 and 0.36\. In effect, the softmax function increases the probability of
    higher scores compared to lower ones.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, **softmax** is often used in combination with the **cross-entropy
    loss** function. It compares the difference between the estimated class probabilities
    and the actual class distribution (the difference is known as cross-entropy).
    We can define the cross-entropy loss for a single training sample as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/249.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/250.png)
    is the estimated probability of the output belonging to class *j* (out of *n*
    total classes) and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/251.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/252.png)
    is the actual probability. The actual distribution, *P(X)*, is usually a one-hot-encoded
    vector, where the real class has a probability of 1, and all others have a probability
    of 0\. In this case, the cross-entropy loss will only capture the error on the
    target class and will discard all other errors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned how to prevent vanishing gradients and we’re able to
    interpret the NN output as a probability distribution, we’ll focus on the next
    challenge in front of DNNs – overfitting.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: DNN regularization
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve learned that an NN can approximate any function. But with great
    power comes great responsibility. The NN may learn to approximate the noise of
    the target function rather than its useful components. For example, imagine that
    we are training an NN to classify whether an image contains a car or not, but
    for some reason, the training set contains mostly red cars. It may turn out that
    the NN will associate the color red with the car, rather than its shape. Now,
    if the network sees a green car in inference mode, it may not recognize it as
    such because the color doesn’t match. As we discussed in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016),
    this problem is referred to as overfitting and it is central to ML (and even more
    so in deep networks). In this section, we’ll discuss several ways to prevent it.
    Such techniques are collectively known as regularization.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到神经网络（NN）可以逼近任何函数。但强大的能力伴随着巨大的责任。神经网络可能会学习逼近目标函数的噪声，而不是其有用的部分。例如，假设我们正在训练一个神经网络来分类图像是否包含汽车，但由于某种原因，训练集大多数是红色的汽车。结果可能是，神经网络会将红色与汽车关联，而不是其形状。现在，如果网络在推理模式下看到一辆绿色的汽车，它可能无法识别为汽车，因为颜色不匹配。正如我们在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中讨论的那样，这个问题被称为过拟合，它是机器学习（ML）的核心问题（在深度网络中尤为严重）。在本节中，我们将讨论几种防止过拟合的方法。这些技术统称为正则化。
- en: 'In the context of NNs, these regularization techniques usually impose some
    artificial limitations or obstacles on the training process to prevent the network
    from approximating the target function too closely. They try to guide the network
    to learn generic rather than specific approximation of the target function in
    the hope that this representation will generalize well on previously unseen examples
    of the test dataset. Let’s start with regularization techniques that apply to
    the input data before we feed it to the NN:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的上下文中，这些正则化技术通常会在训练过程中施加一些人工的限制或障碍，以防止网络过度逼近目标函数。它们试图引导网络学习目标函数的一般性而非特定的逼近方式，期望这种表示能够在之前未见过的测试数据集示例上良好地泛化。让我们先从应用于输入数据的正则化技术开始，然后再将其输入到神经网络中：
- en: '**Min-max normalization**: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math>](img/253.png).
    Here, *x* is a single element of the input vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/254.png)
    is the smallest element of the training dataset, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/255.png)
    is the largest element. This operation scales all the inputs in the [0, 1] range.
    For example, a grayscale image will have a min color value of 0 and a max color
    value of 255\. Then, a pixel with an intensity of 125 would have a scaled value
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>125</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>255</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.49</mml:mn></mml:math>](img/256.png).
    Min-max is fast and easy to implement. One problem with this normalization is
    that data outliers could have an outsized impact on the result over the whole
    dataset. For example, if a single erroneous element has a very large value, it
    will enter the formula as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/257.png)
    and it will drive all normalized dataset values toward 0.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard score** (or **z-score**): ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:math>](img/258.png).
    It handles data outliers better than min-max. To understand how, let’s focus on
    the formula:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/259.png)
    is the mean value of all elements of the dataset, where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    is a single element of the input vector and *N* is the total size of the dataset.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/261.png)
    is the **standard deviation** of all dataset elements. It measures how far apart
    the dataset values are from the mean value. There is also **variance,** **![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/262.png)**![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/263.png),
    which removes the square root from the standard deviation. The variance is theoretically
    correct but is less intuitive than standard deviation, which is measured in the
    same units as the original data, *x*.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, we can compute μ and σ per sample if it’s not practical to compute
    them over the entire dataset. The standard score maintains the dataset’s mean
    value close to 0 and its standard deviation close to 1.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data augmentation**: This is where we artificially increase the size of the
    training set by applying random modifications to the training samples before feeding
    them to the network. In the case of images, these would be rotation, skew, scaling,
    and so on.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next class of regularization techniques are applied within the DNN structure
    itself:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: Here, we randomly and periodically remove some of the units of
    a layer (along with their input and output connections) from the network. During
    a training mini-batch, each unit has a probability, *p*, of being stochastically
    dropped. This is to ensure that no unit ends up relying too much on other units
    and “learns” something useful for the NN instead. Dropout is only applied during
    the training phase and all the units in the network fully participate during the
    inference phase. In the following figure, we can see a dropout for fully connected
    layers:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 3.9 – An example of dropout on full\uFEFFy connected layers](img/B19627_03_9.jpg)"
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – An example of dropout on fully connected layers
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch normalization** (**BN**; see *Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift*, [https://arxiv.org/abs/1502.03167):](https://arxiv.org/abs/1502.03167):)
    This is a way to apply data processing, not unlike the standard score, for the
    hidden layers of the network. It normalizes the outputs of the hidden layer for
    each mini-batch (hence the name) in a way that maintains its mean activation value
    close to 0 (**re-centering**) and its standard deviation close to 1 (**re-scaling**).
    The intuition is that as information is propagated through the layers, these values
    can deviate from the desired values. Let’s say that the mini-batch is represented
    by an *m×n* matrix, **X**. Each row of **X**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/264.png),
    represents a single input vector (this vector is an output of a preceding layer).
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/265.png)
    is the *j*-th element of the *i*-th vector. We can compute BN for each matrix
    element in the following way:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/266.png):
    This is the mini-batch mean. We compute a single *μ* value over all cells of the
    mini-batch matrix.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/267.png):
    This is the mini-batch variance. We compute a single ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi>σ</mi><mrow
    /><mn>2</mn></msubsup></mrow></math>](img/268.png) value over all cells of the
    mini-batch matrix.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:math>](img/269.png):
    We normalize each cell of the matrix. *ε* is a constant that’s added for numerical
    stability, so the denominator cannot become 0.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>B</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/270.png):
    This formula represents the scale and shift of the original data. *γ* and *β*
    are learnable parameters and we compute them over each location, *ij* (![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/272.png)),
    over all cells of the mini-batch matrix.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer normalization** (**LN**; see *Layer Normalization*, [https://arxiv.org/abs/1607.06450):](https://arxiv.org/abs/1607.06450):)
    LN is similar to BN, but with one key difference: the mean and variance are computed
    separately over each mini-batch sample. This is unlike BN, where these values
    are computed across the whole mini-batch. As with BN, the mini-batch is an *m×n*
    matrix, **X**, and each row vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/273.png),
    is the output of a preceding layer, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/274.png)
    is the *j*-th element of the *i*-th vector. Then, we have the following for the
    *i*-th input vector:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/275.png)'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/276.png)'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mstyle scriptlevel="+1"><mfrac><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>μ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mrow></math>](img/277.png)'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>L</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/278.png)'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '**Root mean square layer normalization** (**RMSNorm**; see [https://arxiv.org/abs/1910.07467):](https://arxiv.org/abs/1910.07467):)
    The authors of RMSNorm argue that the main benefit of LN comes just from the re-scaling,
    rather than the combination of re-centering and re-scaling. Therefore, RMSNorm
    is a simplified and faster version of LN, which only applies re-scaling using
    the root mean square statistic. We’ll use the same notation as with LN. So, we
    can define RMSNorm as follows:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/279.png).'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png):
    Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/281.png)
    is the gain parameter used to re-scale the standardized summed inputs (it is set
    to 1 at the beginning). It is equivalent to the *γ* parameter in BN.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the difference between BN and LN. On the left,
    we compute single *μ* and *σ* values across the whole mini-batch. To the right,
    we can see ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/282.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/283.png)
    for each row:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – BN and LN computation of μ and σ](img/B19627_03_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – BN and LN computation of μ and σ
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The final type of regularization we’ll introduce is **L2 regularization**.
    This technique adds a special regularization term to the cost function. To understand
    it, let’s take the MSE cost. We can add L2 regularization to it in the following
    way (the underscored part of the formula):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/284.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    is one of *m* total network weights and *λ* is the weight decay coefficient. The
    rationale is that if the network weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/174.png),
    are large, then the cost function will also increase. In effect, weight decay
    penalizes large weights (hence the name). This prevents the network from relying
    too heavily on a few features associated with these weights. There is less chance
    of overfitting when the network is forced to work with multiple features. In practical
    terms, when we compute the derivative of the weight decay cost function (the preceding
    formula) concerning each weight and then propagate it to the weights themselves,
    the weight update rule changes from the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/287.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/288.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: With this discussion of DNN regularization, we’ve covered our theoretical base.
    Next, let’s see what the real-world applications of DNNs are.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Applications of DL
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML in general, and DL in particular, is producing more and more astonishing
    results in terms of the quality of predictions, feature detection, and classification.
    Many of these recent results have made the news. Such is the pace of progress
    that some experts are worrying that machines will soon be more intelligent than
    humans. But I hope that any such fears you might have will be alleviated after
    you have read this book. For better or worse, we’re still far from machines having
    human-level intelligence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we mentioned how DL algorithms
    have occupied the leaderboard of the ImageNet competition. They are successful
    enough to make the jump from academia to industry.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s talk about some real-world use cases of DL:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, new cars have a suite of safety and convenience features that aim
    to make the driving experience safer and less stressful. One such feature is automated
    emergency braking if the car sees an obstacle. Another one is lane-keeping assist,
    which allows the vehicle to stay in its current lane without the driver needing
    to make corrections with the steering wheel. To recognize lane markings, other
    vehicles, pedestrians, and cyclists, these systems use a
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forward-facing camera. One of the most prominent suppliers of such systems,
    Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/)), has produced
    custom chips that use CNNs to detect these objects on the road ahead. To give
    you an idea of the importance of this sector, in 2017, Intel acquired Mobileye
    for $15.3 billion. This is not an outlier, and Tesla’s famous Autopilot system
    also relies on CNNs to achieve the same results. The former director of AI at
    Tesla, Andrej Karpathy ([https://karpathy.ai/](https://karpathy.ai/)), is a well-known
    researcher in the field of DL. We can speculate that future autonomous vehicles
    will also use deep networks for computer vision.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both Google’s **Vision API** ([https://cloud.google.com/vision/](https://cloud.google.com/vision/))
    and Amazon’s **Rekognition** ([https://aws.amazon.com/rekognition/](https://aws.amazon.com/rekognition/))
    services use DL models to provide various computer vision capabilities. These
    include recognizing and detecting objects and scenes in images, text recognition,
    face recognition, content moderation, and so on.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these APIs are not enough, you can run your own models in the cloud. For
    example, you can use Amazon’s AWS DL AMIs (short for **Amazon Machine Images**;
    see [https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)),
    which are virtual machines that come configured with some of the most popular
    DL libraries. Google offers a similar service with their Cloud AI ([https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/)),
    but they’ve gone one step further. They created **tensor processing units** (**TPUs**;
    see https://cloud.google.com/tpu/) – microprocessors that are optimized for fast
    NN operations such as matrix multiplication and activation functions.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DL has a lot of potential for medical applications. However, strict regulatory
    requirements, as well as patient data confidentiality, have slowed down its adoption.
    Nevertheless, we’ll identify several areas in which DL could have a high impact:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical imaging is an umbrella term for various non-invasive methods of creating
    visual representations of the inside of the body. Some of these include **magnetic
    resonance images** (**MRIs**), ultrasound, **computed axial tomography** (**CAT**)
    scans, X-rays, and histology images. Typically, such an image is analyzed by a
    medical professional to determine the patient’s condition.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer-aided diagnosis, and computer vision, in particular, can help specialists
    by detecting and highlighting important features of images. For example, to determine
    the degree of malignancy of colon cancer, a pathologist would have to analyze
    the morphology of the glands using histology imaging. This is a challenging task
    because morphology can vary greatly. A DNN could segment the glands from the image
    automatically, leaving the pathologist to verify the results. This would reduce
    the time needed for analysis, making it cheaper and more accessible.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Another medical area that could benefit from DL is the analysis of medical history
    records. When a doctor diagnoses a patient’s condition and prescribes treatment,
    they consult the patient’s medical history first. A DL algorithm could extract
    the most relevant and important information from those records, even if they are
    handwritten. In this way, the doctor’s job would be made easier, and the risk
    of errors would also be reduced.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One area where DNNs have already had an impact is in protein folding. Proteins
    are large, complex molecules, whose function depends on their 3D shape. The building
    blocks of proteins are amino acids, and their sequence determines the shape of
    the protein. The protein folding problem seeks to understand the relationship
    between the initial amino acid sequence and the final 3D shape of the protein.
    DeepMind’s **AlphaFold 2** model (believed to be based on transformers; see [https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe](https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe))
    has managed to predict 200 million protein structures, which represents almost
    all known cataloged proteins.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Google’s Neural Machine Translation API ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    uses – you guessed it – DNNs for machine translation.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siri ([https://machinelearning.apple.com/2017/10/01/hey-siri.html](https://machinelearning.apple.com/2017/10/01/hey-siri.html)),
    Google Assistant, and Amazon Alexa ([https://aws.amazon.com/deep-learning/](https://aws.amazon.com/deep-learning/))
    rely on deep networks for speech recognition.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AlphaGo** is an AI machine based on DL that made the news in 2016 by beating
    the world Go champion, Lee Sedol. AlphaGo had already made the news, in January
    2016, when it beat the European champion, Fan Hui. At the time, however, it seemed
    unlikely that it could go on to beat the world champion. Fast-forward a couple
    of months and AlphaGo was able to achieve this remarkable feat by sweeping its
    opponent in a 4-1 victory series. This was an important milestone because Go has
    many more possible game variations than other games, such as chess, and it’s impossible
    to consider every possible move in advance. Also, unlike chess, in Go, it’s very
    difficult to even judge the current position or value of a single stone on the
    board. In 2017, DeepMind released an updated version of AlphaGo called **AlphaZero**
    ([https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)), and in
    2019, they released a further update called **MuZero** ([https://arxiv.org/abs/1911.08265](https://arxiv.org/abs/1911.08265)).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools such as GitHub Copilot ([https://github.com/features/copilot](https://github.com/features/copilot))
    and ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)) utilize generative
    DNN models to transform natural language requests into source code snippets, functions,
    and entire programs. We already mentioned Stable Diffusion ([https://stability.ai/blog/stable-diffusion-public-release](https://stability.ai/blog/stable-diffusion-public-release))
    and DALL-E ([https://openai.com/dall-e-2/](https://openai.com/dall-e-2/)), which
    can generate realistic images based on text description.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this short list, we aimed to cover the main areas in which DL is applied,
    such as computer vision, NLP, speech recognition, and **reinforcement learning**
    (**RL**). This list is not exhaustive, however, as there are many other uses for
    DL algorithms. Still, I hope this has been enough to spark your interest. Next,
    we’ll formally introduce two of the most popular DL libraries – PyTorch and Keras.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Introducing popular DL libraries
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already implemented a simple example with PyTorch in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016).
    In this section, we’ll introduce this library, and Keras, more systemically. Let’s
    start with the common features of most DNN libraries:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: All libraries use Python.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic unit for data storage is the **tensor**. Mathematically, the definition
    of a tensor is more complex, but in the context of DL libraries, they are multi-dimensional
    (with an arbitrary number of axes) arrays of base values.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NNs are represented as a **computational graph** of operations. The nodes of
    the graph represent the operations (weighted sum, activation function, and so
    on). The edges represent the flow of data, which is how the output of one operation
    serves as an input for the next one. The inputs and outputs of the operations
    (including the network inputs and outputs) are tensors.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All libraries include **automatic differentiation**. This means that all you
    need to do is define the network architecture and activation functions, and the
    library will automatically figure out all of the derivatives required for training
    with backpropagation.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we’ve referred to GPUs in general, but in reality, the vast majority
    of DL projects work exclusively with NVIDIA GPUs. This is because of the better
    software support NVIDIA provides. These libraries are no exception – to implement
    GPU operations, they rely on the CUDA Toolkit ([https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit))
    in combination with the cuDNN library ([https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)).
    cuDNN is an extension of CUDA, built specifically for DL applications. As mentioned
    in the *Applications of DL* section, you can also run your DL experiments in the
    cloud.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch is an independent library, while Keras is built on top of TF and acts
    as a user-friendly TF interface. We’ll continue by implementing a simple classification
    example using both PyTorch and Keras.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Classifying digits with Keras
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras exists either as a standalone library with TF as the backend or as a
    sub-component of TF itself. You can use it in both flavors. To use Keras as part
    of TF, we need only to install TF itself. Once we’ve done this, we can use the
    library with the following import:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The standalone Keras supports different backends besides TF, such as Theano.
    In this case, we can install Keras itself and then use it with the following import:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The large majority of Keras’s use is with the TF backend. The author of Keras
    recommends using the library as a TF component (the first option) and we’ll do
    so in the rest of this book.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll use Keras via TF to classify the images of the MNIST
    dataset. It’s comprised of 70,000 examples of digits that have been handwritten
    by different people. The first 60,000 are typically used for training and the
    remaining 10,000 for testing:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – A sample of digits taken from the MNIST dataset](img/B19627_03_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – A sample of digits taken from the MNIST dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll build a simple MLP with one hidden layer. Let’s start:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of Keras is that it can import this dataset for you without
    you needing to explicitly download it from the web (it will download it for you):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `(X_train, Y_train)` is the training images and labels, and `(X_validation,
    Y_validation)` is the test images and labels.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to modify the data so that we can feed it to the NN. `X_train` contains
    60,000 28×28 pixel images, and `X_validation` contains 10,000\. To feed them to
    the network as inputs, we want to reshape each sample as a 784-pixel-long array,
    rather than a 28×28 two-dimensional matrix. We’ll also normalize them in the [0:1]
    range. We can accomplish this with these two lines:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The labels indicate the value of the digit depicted in the images. We want
    to convert this into a 10-entry **one-hot-encoded** vector comprised of 0s and
    just one 1 in the entry corresponding to the digit. For example, 4 is mapped to
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Conversely, our network will have 10 output units:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the NN. In this case, we’ll use the `Sequential` model, where each layer
    serves as an input to the next. In Keras, `Dense` means a fully connected layer.
    We’ll use a network with one hidden layer with 100 units, BN, ReLU activation,
    and softmax output:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can define our gradient descent parameters. We’ll use the Adam optimizer
    and categorical cross-entropy loss (this is cross entropy, optimized for softmax
    outputs):'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, run the training for 100 epochs and a batch size of 100\. In Keras, we
    can do this with the `fit` method, which iterates over the dataset internally.
    Keras will default to GPU training, but if a GPU is not available, it will fall
    back to the CPU:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'All that’s left to do is add code to evaluate the network’s accuracy on the
    test data:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And that’s it. The validation accuracy will be about 97.7%, which is not a great
    result, but this example runs in less than 30 seconds on a CPU. We can make some
    simple improvements, such as a larger number of hidden units, or a higher number
    of epochs. We’ll leave those experiments to you so that you can familiarize yourself
    with the code.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To see what the network has learned, we can visualize the weights of the hidden
    layer. The following code allows us to obtain them:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Reshape the weights for each unit back to a 28×28 two-dimensional array and
    then display them:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see the result in the following figure:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units](img/B19627_03_12.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us see the example for PyTorch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Classifying digits with PyTorch
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll implement the same example that we did in the *Classifying
    digits with Keras* section but this time with PyTorch. Let’s start:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll select the device we’re using (CPU or GPU). We’ll try with the
    GPU first and fall back to the CPU if the GPU is not available:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Like Keras, PyTorch supports MNIST out of the box. Here’s how we can instantiate
    the train and validation sets:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The dataset is automatically downloaded and split into training and validation
    parts. The `ToTensor()` transformation converts the images from `numpy` arrays
    into PyTorch tensors and normalizes them in the [0:1] range (as opposed to [0:255]
    originally). The `torch.flatten` transform flattens the two-dimensional 28×28
    images to a one-dimensional 784 tensor so that we can feed it to the NN.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll wrap the datasets in `DataLoader` instances:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The data `DataLoader` instance takes care of creating mini-batches and shuffles
    the data randomly. They are also iterators, which supply mini-batches one at a
    time.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we’ll define the NN `model`. We’ll use the same MLP with a single hidden
    layer, as in the Keras example:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This definition is like the one in Keras. One difference is that the `Linear`
    (fully connected) layers require both input and output dimensions since they cannot
    extract the output dimension of the preceding layer. The activations are defined
    as separate operations.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s define the cross-entropy loss and the Adam optimizer:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can define the `train_model` function, which, as its name suggests,
    takes care of training the model. It takes our predefined `model`, `cost_function`,
    `optimizer`, and `data_loader` and runs the training for a single epoch:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unlike Keras and its `fit` function, we have to implement the PyTorch training
    ourselves. `train_model` iterates over all mini-batches provided by `train_loader`.
    For each mini-batch, `optimizer.zero_grad()` resets the gradients from the previous
    iteration. Then, we initiate the forward and backward passes, and finally the
    weight updates.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We’ll also define the `test_model` function, which will run the model in inference
    mode to check its results:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: BN and dropout layers are not used in evaluation (only in training), so `model.eval()`
    turns them off. We iterate over the validation set, initiate a forward pass, and
    aggregate the validation loss and accuracy.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s run the training for 20 epochs:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This model achieves 97.6% accuracy.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained what DL is and how it’s related to DNNs. We discussed
    the different types of DNNs and how to train them, and we paid special attention
    to various regularization techniques that help with the training process. We also
    mentioned many real-world applications of DL and tried to analyze the reasons
    for its efficiency. Finally, we introduced two of the most popular DL libraries,
    namely PyTorch and Keras. We also implemented identical MNIST classification examples
    with both libraries.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll discuss how to solve classification tasks over more
    complex image datasets with the help of convolutional networks – one of the most
    popular and effective deep network models. We’ll talk about their structure, building
    blocks, and what makes them uniquely suited to computer vision tasks. To spark
    your interest, let’s recall that convolutional networks have consistently won
    the popular ImageNet challenge since 2012, delivering top-five accuracy from 74.2%
    to 99%.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2:'
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Neural Networks for Computer Vision
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In this part, we’ll introduce **convolutional neural networks** (**CNNs**) –
    a type of neural network suitable for computer vision applications. Building on
    top of the first three chapters, we’ll discuss the rationale behind CNNs, their
    building blocks, and their architecture. We’ll also outline the most popular CNN
    models in use today. Finally, we’ll focus on the advanced applications of CNNs
    – object detection, image segmentation, and image generation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19627_04.xhtml#_idTextAnchor107), *Computer Vision with Convolutional
    Networks*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19627_05.xhtml#_idTextAnchor146), *Advanced Computer Vision
    Applications*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
