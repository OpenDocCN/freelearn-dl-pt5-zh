<html><head></head><body>
<div id="_idContainer124">
<h1 class="chapter-number" id="_idParaDest-197"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.1.1">14</span></h1>
<h1 id="_idParaDest-198"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.2.1">Analyzing Adversarial Performance</span></h1>
<p><span class="koboSpan" id="kobo.3.1">An adversary, in the context of machine learning models, refers to an entity or system that actively seeks to exploit or undermine the performance, integrity, or security of these models. </span><span class="koboSpan" id="kobo.3.2">They can be malicious actors, algorithms, or systems designed to target vulnerabilities within machine learning models. </span><span class="koboSpan" id="kobo.3.3">Adversaries perform adversarial attacks, where they intentionally input misleading or carefully crafted data to deceive the model and cause it to make incorrect or </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">unintended predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Adversarial attacks can range from subtle perturbations of input data to sophisticated methods that exploit the vulnerabilities of specific algorithms. </span><span class="koboSpan" id="kobo.5.2">The objectives of adversaries can vary depending on the context. </span><span class="koboSpan" id="kobo.5.3">They may attempt to bypass security measures, gain unauthorized access, steal sensitive information, or cause disruption in the model’s intended functionality. </span><span class="koboSpan" id="kobo.5.4">Adversaries can also target the fairness and ethics of machine learning models, attempting to exploit biases or discrimination present in the training data or model design. </span><span class="koboSpan" id="kobo.5.5">One example of adversaries targeting fairness and ethics in machine learning models is in the context of facial recognition systems. </span><span class="koboSpan" id="kobo.5.6">Consider that a facial recognition system has a bias and it performs better for men than for women. </span><span class="koboSpan" id="kobo.5.7">Adversaries can exploit this bias by deliberately manipulating their appearance to mislead the system. </span><span class="koboSpan" id="kobo.5.8">They may use makeup, hairstyles, or accessories to confuse the facial recognition algorithms and make it harder for the system to accurately identify them. </span><span class="koboSpan" id="kobo.5.9">By doing so, adversaries can exploit the system’s weaknesses and potentially evade detection or misdirect law </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">enforcement efforts.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">To counter adversaries and adversarial attacks, the best first step is to analyze the adversarial performance of the trained machine learning models. </span><span class="koboSpan" id="kobo.7.2">This analysis allows for a better understanding of potential vulnerabilities and weaknesses in the models, enabling the development of targeted mitigation methods. </span><span class="koboSpan" id="kobo.7.3">Additionally, evaluating the adversarial performance can provide insights into the effectiveness of existing mitigation strategies and guide the improvement of future model designs. </span><span class="koboSpan" id="kobo.7.4">As an added benefit, it helps ensure that your model is well-equipped to handle any possible natural changes that may occur in its deployed environment, even in the absence of specific adversaries targeting </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">the system.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we will go through the adversarial performance evaluation of image, text, and audio data-based models separately. </span><span class="koboSpan" id="kobo.9.2">Specifically, the following topics will </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">be discussed:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Using data augmentations for </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">adversarial analysis</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Analyzing adversarial performance for </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">audio-based models</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Analyzing adversarial performance for </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">image-based models</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Exploring adversarial analysis for </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">text-based models</span></span></li>
</ul>
<h1 id="_idParaDest-199"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">This chapter includes some practical implementations in the Python programming language. </span><span class="koboSpan" id="kobo.20.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">matplotlib</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">scikit-learn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">numpy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.25.1">pytorch</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">accelerate==0.15.0</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">captum</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">catalyst</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">adversarial-robustness-toolbox</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">torchvision</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">pandas</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">The code files for this chapter are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14"><span class="No-Break"><span class="koboSpan" id="kobo.34.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.35.1">.</span></span></p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.36.1">Using data augmentations for adversarial analysis</span></h1>
<p><span class="koboSpan" id="kobo.37.1">The core of the</span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.38.1"> adversarial performance</span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.39.1"> analysis method focuses on utilizing data augmentations. </span><span class="koboSpan" id="kobo.39.2">Data augmentation refers to the process of introducing realistic variations to existing data programmatically. </span><span class="koboSpan" id="kobo.39.3">Data augmentations are commonly employed during the model training process to enhance the validation performance and generalizability of deep learning models. </span><span class="koboSpan" id="kobo.39.4">However, we can also leverage augmentations as an evaluation method to ensure the robustness of performance under various conditions. </span><span class="koboSpan" id="kobo.39.5">By applying augmentations during evaluation, practitioners can obtain a more detailed and comprehensive estimation of the model’s performance when deployed </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">in production.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Adversarial performance analysis offers two main advantages. </span><span class="koboSpan" id="kobo.41.2">Firstly, it assists in building a more generalizable model by enabling better model selection during validation in training and after training between multiple trained models. </span><span class="koboSpan" id="kobo.41.3">This is achieved through the use of augmentation prerequisite metrics. </span><span class="koboSpan" id="kobo.41.4">Certain use cases might have special conditions that are not necessarily representative of what is available in the raw validation or holdout partition. </span><span class="koboSpan" id="kobo.41.5">Augmentations can help change the representation of the evaluation dataset to mimic conditions in production. </span><span class="koboSpan" id="kobo.41.6">Secondly, adversarial performance analysis can be used to establish targeted guardrails when the model is deployed in production. </span><span class="koboSpan" id="kobo.41.7">By thoroughly assessing the model’s performance under different adversarial conditions, practitioners can set up specific thresholds, actions, and operating guidelines to ensure the model’s behavior aligns with </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">their requirements.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Choosing all possible augmentations that you can think of will undoubtedly help align performance expectations in different conditions. </span><span class="koboSpan" id="kobo.43.2">However, thoughtfully chosen augmentations for adversarial analysis can help you </span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.44.1">more effectively </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.45.1">extract value from the process instead of doing it merely for an understanding. </span><span class="koboSpan" id="kobo.45.2">Here are a few recommendations when performing adversarial performance analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">using augmentations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Consider choosing augmentations that you can detect, measure, and control</span></strong><span class="koboSpan" id="kobo.48.1">: Do you have a system or a machine learning model that can already detect the component that an augmentation can change? </span><span class="koboSpan" id="kobo.48.2">Having a measurable component that you want to perform adversarial analysis associated with a chosen augmentation can help set up actual guardrails in production. </span><span class="koboSpan" id="kobo.48.3">Guardrails can range from rejecting automated prediction-based decisions from a model and delegating the decision to a human reviewer, to requiring the user or participant in the system that utilizes the machine learning model to resubmit input data that follows the requirements of the system. </span><span class="koboSpan" id="kobo.48.4">An example of this is having a guardrail that makes sure that the face is straight without any tilting in a face </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">verification system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Think of conditions that are more likely to happen in real-life deployments</span></strong><span class="koboSpan" id="kobo.51.1">: By focusing on augmentations that mimic realistic conditions, practitioners can assess the model’s robustness and performance in situations that are relevant to its </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">intended deployment.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Evaluate the performance of the model at various degrees of strength of a chosen augmentation to understand performance more comprehensively</span></strong><span class="koboSpan" id="kobo.54.1">: Knowing the range of values where performance is at its peak and its bottom will help you make the proper actions. </span><span class="koboSpan" id="kobo.54.2">However, some augmentation methods only have a binary parameter such as added or not added augmentation and it’s okay to just compare the difference in performance from applying and not applying. </span><span class="koboSpan" id="kobo.54.3">For example, whether a horizontal flip has been applied </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">or not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Consider evaluating performance jointly with multiple augmentations</span></strong><span class="koboSpan" id="kobo.57.1">: Real-world situations often involve a combination of factors that can affect the performance of a model. </span><span class="koboSpan" id="kobo.57.2">By testing the model’s performance with multiple augmentations applied simultaneously, you can better understand its ability to handle complex scenarios and identify potential weaknesses that may not be apparent when evaluating single </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">augmentations individually.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Consider using popular adversarial examples or methods that generate adversarial examples</span></strong><span class="koboSpan" id="kobo.60.1">: Utilizing well-known adversarial examples or techniques can help you identify common vulnerabilities in your model that may have been overlooked. </span><span class="koboSpan" id="kobo.60.2">By identifying and mitigating them, you would have already defended against a significant portion of potential attacks, as these popular methods are more likely to be employed </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">by adversaries.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Grouping real data with targeted traits for assessment can be more effective for adversarial performance analysis instead of using augmentations</span></strong><span class="koboSpan" id="kobo.63.1">: Sometimes, augmentations can’t replicate real-life situations properly, so collecting and analyzing against real data samples with specific adversarial characteristics can provide a more accurate assessment of the model’s performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">real-world scenarios.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.65.1">In short, evaluate </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.66.1">augmentations</span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.67.1"> that are actionable. </span><span class="koboSpan" id="kobo.67.2">In general, valuable model insights are the ones that are actionable by any means. </span><span class="koboSpan" id="kobo.67.3">Next, we will go through our first practical example of adversarial analysis using </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">audio-based models.</span></span></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.69.1">Analyzing adversarial performance for audio-based models</span></h1>
<p><span class="koboSpan" id="kobo.70.1">Adversarial analysis </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.71.1">for audio-based </span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.72.1">models requires audio augmentations. </span><span class="koboSpan" id="kobo.72.2">In this section, we will be leveraging the open source </span><strong class="source-inline"><span class="koboSpan" id="kobo.73.1">audiomentations</span></strong><span class="koboSpan" id="kobo.74.1"> library to apply audio augmentation methods. </span><span class="koboSpan" id="kobo.74.2">We will analyze the adversarial accuracy-based performance of a speech recognition model practically. </span><span class="koboSpan" id="kobo.74.3">The accuracy metric we’ll use is the </span><strong class="bold"><span class="koboSpan" id="kobo.75.1">Word Error Rate</span></strong><span class="koboSpan" id="kobo.76.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.77.1">WER</span></strong><span class="koboSpan" id="kobo.78.1">), which</span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.79.1"> is a commonly used metric in automatic speech recognition and machine translation systems. </span><span class="koboSpan" id="kobo.79.2">It measures the dissimilarity between a system’s output and the reference transcription or translation by calculating the sum of word substitutions, insertions, and deletions divided by the total number of reference words, resulting in a percentage value. </span><span class="koboSpan" id="kobo.79.3">The formula for WER is </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.81.1">W</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.82.1">E</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.83.1">R</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.84.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.85.1">(</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.86.1">S</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.87.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.88.1">I</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.89.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.90.1">D</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.91.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.92.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.93.1">N</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">the following:</span></span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.96.1">S</span></span><span class="koboSpan" id="kobo.97.1"> represents the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">word substitutions</span></span></li>
<li><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.99.1">I</span></span><span class="koboSpan" id="kobo.100.1"> represents the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">word insertions</span></span></li>
<li><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.102.1">D</span></span><span class="koboSpan" id="kobo.103.1"> represents the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">word deletions</span></span></li>
<li><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.105.1">N</span></span><span class="koboSpan" id="kobo.106.1"> is the total number of words in the reference transcription </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">or translation</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.108.1">The following augmentations are considered for </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">the analysis:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.110.1">Pronunciation speed augmentation</span></strong><span class="koboSpan" id="kobo.111.1">: Altering the speed of word pronunciation can have a significant impact on WER. </span><span class="koboSpan" id="kobo.111.2">Increasing the speed (time compression) may lead to more errors due to compressed phonetic information, while decreasing the speed (time expansion) may result in more accurate transcriptions. </span><span class="koboSpan" id="kobo.111.3">As words can be long and short, syllables per minute will be a good estimator of this without any special machine </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">learning model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">Speech pitch</span></strong><span class="koboSpan" id="kobo.114.1">: Changing the pitch of speech can affect the perception and recognition of spoken words. </span><span class="koboSpan" id="kobo.114.2">Augmentations such as pitch shifting can introduce variations in pitch, which can influence WER performance. </span><span class="koboSpan" id="kobo.114.3">Women and men generally have different pitch ranges, which can work as a proxy for measuring this, so we will not directly analyze pitch in this topic. </span><span class="koboSpan" id="kobo.114.4">Pitch can be measured either by machine learning models or rule-based </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">scientific methods.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.116.1">Background noise</span></strong><span class="koboSpan" id="kobo.117.1">: The presence of background noise can negatively impact speech recognition systems. </span><span class="koboSpan" id="kobo.117.2">Background noise can be created algorithmically, such as Gaussian noise, or it can be strategically chosen types of real-world background noise that can exist in real environments, such as car or motorbike sounds. </span><span class="koboSpan" id="kobo.117.3">However, its presence can’t be detected simply and has to depend on a machine learning model or manual </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">environment controls.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.119.1">Speech loudness/magnitude</span></strong><span class="koboSpan" id="kobo.120.1">: The loudness or volume of speech can play a crucial role in speech recognition. </span><span class="koboSpan" id="kobo.120.2">Increasing or decreasing the loudness of the speech can introduce variability that reflects real-world conditions. </span><span class="koboSpan" id="kobo.120.3">Common speech datasets are collected in a closed environment without any external noise. </span><span class="koboSpan" id="kobo.120.4">This makes it easy to control the loudness of the speech by using simple </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">mathematical methods.</span></span></li>
</ul>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.122.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.123.1">.1</span></em><span class="koboSpan" id="kobo.124.1"> shows the adversarial performance analysis graph plots of four components: pronunciation speed performance </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.125.1">analysis</span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.126.1"> in graph </span><em class="italic"><span class="koboSpan" id="kobo.127.1">(a)</span></em><span class="koboSpan" id="kobo.128.1">, Gaussian noise performance analysis in graph </span><em class="italic"><span class="koboSpan" id="kobo.129.1">(b)</span></em><span class="koboSpan" id="kobo.130.1">, speech loudness performance analysis in graph </span><em class="italic"><span class="koboSpan" id="kobo.131.1">(c)</span></em><span class="koboSpan" id="kobo.132.1">, and real-life background noise performance analysis (motorbikes noise) in </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">graph </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">(d)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.136.1"><img alt="Figure 14.1 – Adversarial analysis results for a speech recognition model with WER" src="image/B18187_14_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.137.1">Figure 14.1 – Adversarial analysis results for a speech recognition model with WER</span></p>
<p><span class="koboSpan" id="kobo.138.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.139.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.140.1">.1(a)</span></em><span class="koboSpan" id="kobo.141.1">, the performance seems to be the best at between 2.5 to 4 syllables spoken per second across all categories, and males seem to be more susceptible to performance degradations going out of this range. </span><span class="koboSpan" id="kobo.141.2">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.142.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.143.1">.1(b)</span></em><span class="koboSpan" id="kobo.144.1">, the models perform at an optimum level at every SNR value after 30 dB. </span><span class="koboSpan" id="kobo.144.2">Therefore, a simple guardrail would be to ensure speech is always at least 30 dB louder than any background noise, which can be measured by hardware. </span><span class="koboSpan" id="kobo.144.3">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.145.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.146.1">.1(c)</span></em><span class="koboSpan" id="kobo.147.1">, it’s obvious that the performance for females doesn’t specifically degrade no matter how loud or soft the speech sounds made are. </span><span class="koboSpan" id="kobo.147.2">However, for males, the best performance can be obtained when the absolute magnitude of the voice is at around the 20 dB range and not over it. </span><span class="koboSpan" id="kobo.147.3">This shows some form of bias of the model toward gender. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.148.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.149.1">.1(d)</span></em><span class="koboSpan" id="kobo.150.1"> shows</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.151.1"> that there isn’t </span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.152.1">any particular special behavior of motorbike noises versus Gaussian noise. </span><span class="koboSpan" id="kobo.152.2">Consider evaluating more real-life background noises you can think of at the end of the </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">practical steps!</span></span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.154.1">Executing adversarial performance analysis for speech recognition models</span></h2>
<p><span class="koboSpan" id="kobo.155.1">Let’s start a step-by-step</span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.156.1"> practical example </span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.157.1">that will show how to obtain the adversarial performance analysis results of the speech recognition model presented in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.158.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.159.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.161.1">We will be using the Speech2Text model from the Hugging Face open source platform, which is an English speech-trained transformer-based speech recognition model. </span><span class="koboSpan" id="kobo.161.2">Let’s start by importing the necessary libraries, the highlights of which are </span><strong class="source-inline"><span class="koboSpan" id="kobo.162.1">matplotlib</span></strong><span class="koboSpan" id="kobo.163.1"> for graph plotting, </span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">numpy</span></strong><span class="koboSpan" id="kobo.165.1"> for array handling, </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">pytorch</span></strong><span class="koboSpan" id="kobo.167.1"> for handling the PyTorch Speech2Text model, </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">audiomentations</span></strong><span class="koboSpan" id="kobo.169.1"> for augmentations, the Speech2Text model, and the preprocessor from </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.171.1">
import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm_notebook
import evaluate
import syllables
from audiomentations import (AddBackgroundNoise, AddGaussianNoise,
                             AddGaussianSNR, LoudnessNormalization, PitchShift,
                             Shift, TimeStretch)
from datasets import load_dataset
from transformers import (Speech2TextForConditionalGeneration,
                          Speech2TextProcessor)</span></pre></li> <li><span class="koboSpan" id="kobo.172.1">Next, we will load the trained Speech2Text model and preprocessor, and assign the model to a </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">GPU device:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.174.1">
device = torch.device("cuda")
model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")
model.to(device)</span></pre></li> <li><span class="koboSpan" id="kobo.175.1">The dataset we will use as a base to evaluate the model is the English speech recognition </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">fleurs</span></strong><span class="koboSpan" id="kobo.177.1"> dataset from Google, which conveniently contains gender information that allows us to indirectly evaluate pitch performance differences and also perform bias analysis. </span><span class="koboSpan" id="kobo.177.2">Let’s download and load </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">the dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.179.1">
ds = load_dataset("google/fleurs", 'en_us', split="validation")</span></pre></li> <li><span class="koboSpan" id="kobo.180.1">Next, we will load the WER evaluation method from the Hugging Face </span><strong class="source-inline"><span class="koboSpan" id="kobo.181.1">evaluate</span></strong><span class="koboSpan" id="kobo.182.1"> library and define the helper method</span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.183.1"> to</span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.184.1"> extract the WER scores from the </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">Speech2Text model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.186.1">
wer = evaluate.load("wer")
all_gender = np.array(ds['gender'])
gender_map = {'female':1, 'male':0}
def get_wer_scores(dataset, transcriptions=None, sampling_rates=None, is_hg_ds=False):
    all_wer_score = []
    for idx, audio_data in tqdm_notebook(enumerate(dataset), total=len(dataset)):
        inputs = processor(
            audio_data["audio"]["array"] if is_hg_ds else audio_data,
            sampling_rate=audio_data["audio"]["sampling_rate"] if is_hg_ds else sampling_rates[idx],
            return_tensors="pt"
        )
        generated_ids = model.generate(
            inputs["input_features"].to(device), attention_mask=inputs["attention_mask"].to(device)
        )
        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)
        wer_score = wer.compute(
            predictions=transcription,
            references=[audio_data['transcription'] if is_hg_ds else transcriptions[idx]]
        )
        all_wer_score.append(wer_score)
    all_wer_score = np.array(all_wer_score)
    wer_score_results = {}
    for gender in gender_map.keys():
        gender_idx  = np.where(all_gender == gender_map[gender])[0]
        wer_score_results[gender + '_wer_score'] = all_wer_score[gender_idx].mean()
    wer_score_results['wer_score'] = all_wer_score.mean()
    return wer_score_results</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.187.1">Three scores will be returned here, which are the male-specific score, the female-specific score, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">overall score.</span></span></p></li> <li><span class="koboSpan" id="kobo.189.1">As a follow-up, we will define the main method that will apply the augmentation to all the baseline samples and obtain a </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">WER score:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.191.1">
def get_augmented_samples_wer_results(
    all_baseline_samples, augment, transcriptions, all_sampling_rates
):
    all_augmented_samples = []
    for idx, audio_sample in enumerate(all_baseline_samples):
        augmented_samples = augment(samples=audio_sample, sample_rate=all_sampling_rates[idx])
        all_augmented_samples.append(augmented_samples)
    results = get_wer_scores(
        all_augmented_samples, transcriptions, sampling_rates=all_sampling_rates, is_hg_ds=False
    )
    return results</span></pre></li> <li><span class="koboSpan" id="kobo.192.1">First, we will analyze the </span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.193.1">adversarial</span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.194.1"> performance for the pronunciation speed component using the time stretch method from </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">audiomentation</span></strong><span class="koboSpan" id="kobo.196.1">. </span><span class="koboSpan" id="kobo.196.2">The dataset contains audio data with different numbers of syllables spoken per second, so we have to make sure all of the audio data has the same number of syllables per second before starting the analysis. </span><span class="koboSpan" id="kobo.196.3">Let’s start by finding the mean number of syllables spoken </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">per second:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.198.1">
all_syllables_per_second = []
for audio_data in ds:
    num_syllables = syllables.estimate(audio_data['transcription'])
    syllables_per_second = num_syllables / (audio_data['num_samples'] / audio_data['audio']['sampling_rate'])
    all_syllables_per_second.append(syllables_per_second)
average_syllables_per_second = np.mean(all_syllables_per_second)</span></pre></li> <li><span class="koboSpan" id="kobo.199.1">Now, we can obtain the initial set of baseline audio samples for </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">pronunciation-based analysis:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.201.1">
all_baseline_speed_audio_samples = []
transcriptions = []
all_sampling_rates = []
for idx, audio_data in tqdm_notebook(enumerate(ds), total=len(ds)):
    rate = average_syllables_per_second / all_syllables_per_second[idx]
    augment = TimeStretch(min_rate=rate, max_rate=rate, p=1.0)
    augmented_samples = augment(
        samples=audio_data['audio']['array'], sample_rate=audio_data['audio']['sampling_rate']
    )
    transcriptions.append(audio_data['transcription'])
    all_sampling_rates.append(audio_data['audio']['sampling_rate'])
    all_baseline_speed_audio_samples.append(augmented_samples)</span></pre></li> <li><span class="koboSpan" id="kobo.202.1">Finally, we will perform</span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.203.1"> the </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.204.1">adversarial WER analysis using a range of different speed-up and </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">speed-down rates:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.206.1">
rates = np.linspace(0.1, 1, 9).tolist() + list(range(1, 11))
wer_results_by_rate = []
for rate_to_change in tqdm_notebook(rates):
    augment = TimeStretch(min_rate=rate_to_change, max_rate=rate_to_change, p=1.0)
    results = get_augmented_samples_wer_results(
        all_baseline_speed_audio_samples, augment, transcriptions, all_sampling_rates
    )
    wer_results_by_rate.append(results)</span></pre></li> <li><span class="koboSpan" id="kobo.207.1">By running the following plotting code, you will obtain the graph shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.208.1">Figure 14</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.209.1">.1(a)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.211.1">
labels = ["female", "male", "overall"]
plt.xlabel("Syllables per second")
plt.ylabel("WER")
for idx, gender in enumerate(["female_", "male_", ""]):
    plt.plot(
        [average_syllables_per_second * i for i in rates],
        [wr[gender + 'wer_score'] for wr in wer_results_by_rate],
        label=labels[idx]
    )
plt.legend()</span></pre></li> <li><span class="koboSpan" id="kobo.212.1">Now, we will move forward to the next augmentation component that algorithmically generates Gaussian background noise. </span><span class="koboSpan" id="kobo.212.2">In this example, we will be controlling the </span><strong class="bold"><span class="koboSpan" id="kobo.213.1">signal-to-noise ratio</span></strong><span class="koboSpan" id="kobo.214.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.215.1">SNR</span></strong><span class="koboSpan" id="kobo.216.1">) of the</span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.217.1"> speech signal and the Gaussian noise. </span><span class="koboSpan" id="kobo.217.2">In this case, the original data can be used as the baseline without </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">any equalization:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.219.1">
baseline_samples = [audio_data['audio']['array'] for audio_data in ds]
snr_rates = np.linspace(1, 100, 25)
wer_results_by_snr = []
for snr_rate in tqdm_notebook(snr_rates):
    all_augmented_samples = []
    augment = AddGaussianSNR(
        min_snr_in_db=snr_rate,
        max_snr_in_db=snr_rate,
        p=1.0
    )
    results = get_augmented_samples_wer_results(
        baseline_samples, augment, transcriptions, all_sampling_rates
    )
    wer_results_by_snr.append(results)</span></pre></li> <li><span class="koboSpan" id="kobo.220.1">By running th</span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.221.1">e following</span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.222.1"> plotting code, you will obtain the graph shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.223.1">Figure 14</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.224.1">.1(b)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.226.1">
plt.xlabel("SNR (dB)")
plt.ylabel("WER")
for idx, gender in enumerate(["female_", "male_", ""]):
    plt.plot(
        snr_rates,
        [wr[gender + 'wer_score'] for wr in wer_results_by_snr],
        label=labels[idx]
    )
plt.legend()</span></pre></li> <li><span class="koboSpan" id="kobo.227.1">Next, we will analyze the WER performance at different speech loudness. </span><span class="koboSpan" id="kobo.227.2">The dataset from </span><a href="https://huggingface.co/datasets/google/fleurs"><span class="koboSpan" id="kobo.228.1">https://huggingface.co/datasets/google/fleurs</span></a><span class="koboSpan" id="kobo.229.1"> was made in a closed environment without background noise, which makes it straightforward to compute magnitude with the raw speech audio data array with the </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.231.1">
Wer_results_by_loudness = []
loudness_db = np.linspace(-31, 100, 25)
for db in tqdm_notebook(loudness_db):
    augment = LoudnessNormalization(
        min_lufs_in_db=db,
        max_lufs_in_db=db,
        p=1.0
    )
    results = get_augmented_samples_wer_results(baseline_samples, augment, transcriptions, all_sampling_rates)
    wer_results_by_loudness.append(results)</span></pre></li> <li><span class="koboSpan" id="kobo.232.1">By running the following plotting code, you will obtain the graph shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.233.1">Figure 14</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.234.1">.1(c)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.236.1">
labels = ["female", "male", "overall"]
plt.xlabel("SNR (dB)")
plt.ylabel("WER")
for idx, gender in enumerate(["female_", "male_", ""]):
    plt.plot(
        loudness_db,
        [wr[gender + 'wer_score'] for wr in wer_results_by_loudness],
        label=labels[idx]
    )
plt.legend()</span></pre></li> <li><span class="koboSpan" id="kobo.237.1">Finally, we will be </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.238.1">analyzing </span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.239.1">the adversarial performance of real background noise from the real world. </span><span class="koboSpan" id="kobo.239.2">We will use motorbike sounds from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">Freesound50k</span></strong><span class="koboSpan" id="kobo.241.1"> dataset and mix them into the original audio data at different SNRs with the </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.243.1">
snrs = np.linspace(-50, 50, 20)
wer_results_by_background_noise_snr = []
for snr in tqdm_notebook(snrs):
    augment = AddBackgroundNoise(
        sounds_path="motorbikes",
        min_snr_in_db=snr,
        max_snr_in_db=snr,
        p=1.0
    )
    results = get_augmented_samples_wer_results(baseline_samples, augment, transcriptions, all_sampling_rates)
    wer_results_by_background_noise_snr.append(results)</span></pre></li> <li><span class="koboSpan" id="kobo.244.1">By running the following plotting code, you will obtain the graph shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.245.1">Figure 14</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.246.1">.1(d)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.248.1">
plt.xlabel("SNR (dB)")
plt.ylabel("WER")
for idx, gender in enumerate(["female_", "male_", ""]):
    plt.plot(
        snrs,
        [wr[gender + 'wer_score'] for wr in wer_results_by_background_noise_snr],
        label=labels[idx]
    )
plt.legend()</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.249.1">With that, we have attempted to analyze the adversarial performance of a speech recognition model. </span><span class="koboSpan" id="kobo.249.2">Consider extending the analysis on your end and try to evaluate multiple augmentations jointly. </span><span class="koboSpan" id="kobo.249.3">For example, consider scenarios where multiple background voices are present. </span><span class="koboSpan" id="kobo.249.4">Augmenting audio with different magnitudes of background voices can simulate scenarios with varying levels </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">of interference.</span></span></p>
<p><span class="koboSpan" id="kobo.251.1">After performing the analysis, apart from using these augmentations during training to mitigate poor performance, you can add guardrails in the production environment. </span><span class="koboSpan" id="kobo.251.2">As an example, consider the situation in which speech recognition outputs are used as input to an LLM model such as ChatGPT to prevent wrong results under the hood. </span><span class="koboSpan" id="kobo.251.3">An example guardrail would be to reroute the system to ask for manual verification</span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.252.1"> of the speech recognized before submitting it to ChatGPT when specific background noises are detected in the</span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.253.1"> audio background. </span><span class="koboSpan" id="kobo.253.2">Making subsequent actionable processes is crucial to unlocking value from insights. </span><span class="koboSpan" id="kobo.253.3">Next, let’s discover adversarial analysis for </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">image-based models.</span></span></p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.255.1">Analyzing adversarial performance for image-based models</span></h1>
<p><span class="koboSpan" id="kobo.256.1">Augmentations-based </span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.257.1">adversarial analysis </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.258.1">can also be applied to image-based models. </span><span class="koboSpan" id="kobo.258.2">The key here is to discover possible degradations of accuracy-based performance in original non-existent conditions in the validation dataset. </span><span class="koboSpan" id="kobo.258.3">Here are some examples of components that could be evaluated by augmentations for the </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">image domain:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Object of interest size</span></strong><span class="koboSpan" id="kobo.261.1">: In use cases that use CCTV camera image input, adversarial analysis can help us set up the camera with an appropriate distance so that optimal performance can be achieved. </span><span class="koboSpan" id="kobo.261.2">The original image can be iteratively resized into various sizes and overlayed on top of a base black image to </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">perform analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.263.1">The roll orientation of the object of interest</span></strong><span class="koboSpan" id="kobo.264.1">: Pitch and yaw orientation is not straightforward to augment. </span><span class="koboSpan" id="kobo.264.2">However, rotation augmentation can help stress test roll orientation performance. </span><span class="koboSpan" id="kobo.264.3">Optimal performance can be enforced by any pose orientation detection model </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">or system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.266.1">Level of blurriness</span></strong><span class="koboSpan" id="kobo.267.1">: Images can be blurred and there are off-the-shelf image blurriness detectors from the OpenCV library for this. </span><span class="koboSpan" id="kobo.267.2">Blur augmentation can help stress test </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">this component.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">The intensity of natural environmental events such as snow, rain, fog, and sun rays</span></strong><span class="koboSpan" id="kobo.270.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">albumentations</span></strong><span class="koboSpan" id="kobo.272.1"> library provides rain, snow, fog, and </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">sun augmentations!</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.274.1">In addition to the method of using augmentation for adversarial analysis to assess performance under different conditions, numerous other widely recognized and extensively researched approaches exist for conducting adversarial attacks against image-based models. </span><span class="koboSpan" id="kobo.274.2">The term “popular” here also means that the techniques are easily accessible to potential attackers, allowing them to readily experiment with these methods. </span><span class="koboSpan" id="kobo.274.3">Consequently, it becomes crucial to thoroughly analyze such attacks due to their increased likelihood and </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">potential impact.</span></span></p>
<p><span class="koboSpan" id="kobo.276.1">These attacks try to obtain an adversarial image to fool the model through the optimization of one of </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.278.1">An image perturbation matrix that acts as a noise mixer to the original image while maintaining a high perceived visual similarity to the </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">original image</span></span></li>
<li><span class="koboSpan" id="kobo.280.1">An image patch that can be digitally overlayed on the original image and printed in the real world to evade </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.281.1">detection </span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.282.1">or confuse </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">the model</span></span></li>
</ul>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.284.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.285.1">.2</span></em><span class="koboSpan" id="kobo.286.1"> shows examples of these two optimization approaches for </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">adversarial attacks:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.288.1"><img alt="Figure 14.2 – Examples of adversarial image patches and adversarial image noise mixers" src="image/B18187_14_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.289.1">Figure 14.2 – Examples of adversarial image patches and adversarial image noise mixers</span></p>
<p><span class="koboSpan" id="kobo.290.1">The top left adversarial image patch is targeted </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.291.1">on the </span><strong class="bold"><span class="koboSpan" id="kobo.292.1">YOLOv2</span></strong><span class="koboSpan" id="kobo.293.1"> image detection model on the class that represents a traffic stop sign and is capable of fooling the model to predict some other random class when printed and patched on the stop sign physically in the real world. </span><span class="koboSpan" id="kobo.293.2">The patch can also transfer its adversarial properties to </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.294.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.295.1">Faster-RCNN</span></strong><span class="koboSpan" id="kobo.296.1"> image object detection model. </span><span class="koboSpan" id="kobo.296.2">The bottom left adversarial image patch is an image that’s been optimized to fool the YOLOv2 detector model into not detecting a human when the printed patch is anywhere on a person. </span><span class="koboSpan" id="kobo.296.3">The right adversarial image noise mixer example shows the original image when added with the noise on the right produces an image that is visually indistinguishable from the original image. </span><span class="koboSpan" id="kobo.296.4">The ResNet50 model, which was used to build the model in the previous chapter, accurately predicts the right person on the original image but fails to predict the right person after adding the noise shown in the top right of the figure. </span><span class="koboSpan" id="kobo.296.5">Image patches are relevant attacks for CCTV based on real-time computer vision applications. </span><span class="koboSpan" id="kobo.296.6">For example, thieves want to prevent facial object detection. </span><span class="koboSpan" id="kobo.296.7">An adversarial image noise mixer is relevant for use cases where a user can provide their own data. </span><span class="koboSpan" id="kobo.296.8">For example, social media platforms and media-sharing platforms such as Instagram and YouTube want to filter and control media that can be uploaded using machine learning, and users would want to bypass the machine </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">learning guardrails.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">Audio noise mixers and audio patches can also be employed in adversarial attacks against audio-based models, following a similar approach as with image-based models. </span><span class="koboSpan" id="kobo.298.2">Audio noise mixers introduce carefully crafted noise to the original audio signal to create an adversarial example that maintains a high perceived auditory similarity to the original audio while fooling the model. </span><span class="koboSpan" id="kobo.298.3">This can be particularly relevant in applications such as voice recognition systems and audio content filtering, where adversaries might attempt to bypass security measures or manipulate system outputs. </span><span class="koboSpan" id="kobo.298.4">Audio patches, on the other hand, involve creating and overlaying adversarial audio segments onto the original audio signal. </span><span class="koboSpan" id="kobo.298.5">These patches can be designed to either mask certain elements in the audio or introduce new elements that deceive the model. </span><span class="koboSpan" id="kobo.298.6">For instance, adversarial audio can be played naturally in the environment with any audio speaker device to evade voice identification systems or to trick speech recognition models into misinterpreting specific words </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">or phrases.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">These techniques are engineered forms of adversarial attacks that are categorized into either techniques that require access to the neural network gradients and the model itself, or techniques that only require the prediction probabilities </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.301.1">or logits</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.302.1"> of the model to optimize an image. </span><span class="koboSpan" id="kobo.302.2">Since it is highly unlikely that an attacker would have access to the neural network gradients and the model itself, it is more practical to evaluate adversarial image generation methods that only require the prediction probabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">or logits.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">An attacker can opt to randomly generate the noise perturbation matrix and hope that it’ll work to fool the model. </span><span class="koboSpan" id="kobo.304.2">However, some algorithms can automatically optimize the generation of a useful noise perturbation matrix, so long as you have access to the probabilities or logits. </span><span class="koboSpan" id="kobo.304.3">One such algorithm that requires only the prediction logits to produce the noise needed to fool a model is called the </span><strong class="bold"><span class="koboSpan" id="kobo.305.1">HopSkipJump</span></strong><span class="koboSpan" id="kobo.306.1"> algorithm. </span><span class="koboSpan" id="kobo.306.2">This</span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.307.1"> algorithm aims to minimize the number of queries that are required to generate effective adversarial examples. </span><span class="koboSpan" id="kobo.307.2">Here is a summary of </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">the algorithm:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.309.1">Initialization</span></strong><span class="koboSpan" id="kobo.310.1">: The algorithm starts by initializing the target class and the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">adversarial example.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.312.1">Main loop</span></strong><span class="koboSpan" id="kobo.313.1">: The algorithm iteratively performs a series of steps until it successfully generates an adversarial example or reaches a predefined </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">query limit:</span></span><ol><li class="upper-roman"><strong class="bold"><span class="koboSpan" id="kobo.315.1">Hop</span></strong><span class="koboSpan" id="kobo.316.1">: In this step, the algorithm performs a local search to find a perturbation that moves the initial adversarial example closer to the target class while ensuring that the perturbed example </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">remains adversarial.</span></span></li><li class="upper-roman"><strong class="bold"><span class="koboSpan" id="kobo.318.1">Skip</span></strong><span class="koboSpan" id="kobo.319.1">: If the </span><em class="italic"><span class="koboSpan" id="kobo.320.1">hop</span></em><span class="koboSpan" id="kobo.321.1"> step fails to find a suitable perturbation, the algorithm attempts a more global search by skipping some pixels. </span><span class="koboSpan" id="kobo.321.2">This step is designed to explore a larger search </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">space efficiently.</span></span></li><li class="upper-roman"><strong class="bold"><span class="koboSpan" id="kobo.323.1">Jump</span></strong><span class="koboSpan" id="kobo.324.1">: If both the </span><em class="italic"><span class="koboSpan" id="kobo.325.1">hop</span></em><span class="koboSpan" id="kobo.326.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.327.1">skip</span></em><span class="koboSpan" id="kobo.328.1"> steps fail, the algorithm resorts to a jump operation, which performs a random perturbation on a small subset of pixels to encourage exploration of </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">different directions.</span></span></li><li class="upper-roman"><strong class="bold"><span class="koboSpan" id="kobo.330.1">Decision</span></strong><span class="koboSpan" id="kobo.331.1">: After each perturbation step (</span><em class="italic"><span class="koboSpan" id="kobo.332.1">hop</span></em><span class="koboSpan" id="kobo.333.1">, </span><em class="italic"><span class="koboSpan" id="kobo.334.1">skip</span></em><span class="koboSpan" id="kobo.335.1">, or </span><em class="italic"><span class="koboSpan" id="kobo.336.1">jump</span></em><span class="koboSpan" id="kobo.337.1">), the algorithm queries the target model to obtain its predicted class label for the </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">perturbed example.</span></span></li><li class="upper-roman"><strong class="bold"><span class="koboSpan" id="kobo.339.1">Stopping criteria</span></strong><span class="koboSpan" id="kobo.340.1">: The algorithm terminates if it successfully generates an adversarial example – that is, the target model predicts the target class for the perturbed example. </span><span class="koboSpan" id="kobo.340.2">It can also stop if the number of queries exceeds a </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">predefined threshold.</span></span></li></ol></li>
</ol>
<p><span class="koboSpan" id="kobo.342.1">The HopSkipJump algorithm combines both local and global search strategies to efficiently explore the space of adversarial examples while minimizing the number of queries to the target model. </span><span class="koboSpan" id="kobo.342.2">This is one of the many attacks </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.343.1">readily available </span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.344.1">in an open source adversarial attack toolkit at </span><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox"><span class="koboSpan" id="kobo.345.1">https://github.com/Trusted-AI/adversarial-robustness-toolbox</span></a><span class="koboSpan" id="kobo.346.1">. </span><span class="koboSpan" id="kobo.346.2">We will go through practical steps that allow us to get the adversarial image noise mixer results seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.347.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.348.1">.2</span></em><span class="koboSpan" id="kobo.349.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">more next.</span></span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.351.1">Executing adversarial performance analysis for a face recognition model</span></h2>
<p><span class="koboSpan" id="kobo.352.1">Before we start, please </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.353.1">put yourselves </span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.354.1">in the shoes and mindset of an attacker! </span><span class="koboSpan" id="kobo.354.2">The steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.356.1">First, we will load the necessary libraries. </span><span class="koboSpan" id="kobo.356.2">The highlights are </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">albumentations</span></strong><span class="koboSpan" id="kobo.358.1"> for augmentation, </span><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">torch</span></strong><span class="koboSpan" id="kobo.360.1"> for the neural network model, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">art</span></strong><span class="koboSpan" id="kobo.362.1"> for the adversarial example </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">generation algorithm:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.364.1">
import os
import albumentations as albu
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from albumentations.pytorch.transforms import ToTensorV2
from PIL import Image
from sklearn.model_selection import StratifiedShuffleSplit
from torchvision import models
from tqdm import tqdm_notebook
import evaluate
from art.attacks.evasion import HopSkipJump
from art.estimators.classification import BlackBoxClassifier
import matplotlib.pyplot as plt
from catalyst.contrib.layers import ArcFace</span></pre></li> <li><span class="koboSpan" id="kobo.365.1">Next, we will reuse the trained face classification model we built in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.366.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.367.1">, </span><em class="italic"><span class="koboSpan" id="kobo.368.1">Exploring Bias and Fairness</span></em><span class="koboSpan" id="kobo.369.1">, which is a ResNet50 backbone model </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">with ArcFace:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.371.1">
device = torch.device("cuda")
class ArcResNet50(nn.Module):
    def __init__(self, num_classes):
        super(ArcResNet50, self).__init__()
        self.model =  models.resnet50(pretrained=True)
        self.model.fc = nn.Linear(self.model.fc.in_features, self.model.fc.in_features)
        self.head = ArcFace(self.model.fc.out_features, num_classes, s=13, m=0.15)
    def forward(self, x, targets=None):
        output = self.model(x)
        outputs = self.head(output, targets)
        return outputs
num_classes = 10000
model = ArcResNet50(num_classes=num_classes)
model.to(device)
model_path = '../CHAPTER_13/experiments/face_modelv10'
state_dict = torch.load(os.path.join(model_path, "model.last.pth"))
model.load_state_dict(state_dict)</span></pre></li> <li><span class="koboSpan" id="kobo.372.1">We will also reuse the </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">same dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.374.1">
train = pd.read_csv('../CHAPTER_13/face_age_gender.csv')
image_path = train['image_path'].values
targets = train['target'].values
name2class = {name: idx for idx, name in enumerate(sorted(set(targets)))}
id_targets = np.array([name2class[target] for target in targets])</span></pre></li> <li><span class="koboSpan" id="kobo.375.1">To start </span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.376.1">using</span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.377.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">HopSkipJump</span></strong><span class="koboSpan" id="kobo.379.1"> class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">art</span></strong><span class="koboSpan" id="kobo.381.1"> library, we need to define an </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">art</span></strong><span class="koboSpan" id="kobo.383.1"> library black box classifier instance. </span><span class="koboSpan" id="kobo.383.2">The class is a wrapper that sets the expected generated image noise matrix shape, and has access to only a fully isolated prediction method of any image-based model. </span><span class="koboSpan" id="kobo.383.3">First, let’s prepare the inference prediction method so that the art classifier can </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">use it:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.385.1">
transforms = albu.Compose([
    albu.Resize(224, 224),
    albu.Normalize(),
    ToTensorV2()
])
def predict(x):
    if len(x.shape) == 3:
        img = transforms(image=x)["image"].unsqueeze(0).to(device)
    else:
        batch_img = []
        for img in x:
            img = transforms(image=img)["image"].to(device)
            batch_img.append(img)
        img = torch.stack(batch_img)
    with torch.inference_mode():
        output = model(img)
    return output.cpu().numpy()</span></pre></li> <li><span class="koboSpan" id="kobo.386.1">Next, let’s take an example image from the dataset and create the black box classifier with the isolated prediction </span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.387.1">method, the </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.388.1">target image shape, the number of classes, and the range of accepted </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">pixel values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.390.1">
target_image = np.array(Image.open(os.path.join("../CHAPTER_13", image_path[1])).resize((224, 224))).astype(np.float32)
classifier = BlackBoxClassifier(
predict, target_image.shape, num_classes, clip_values=(0, 255))</span></pre></li> <li><span class="koboSpan" id="kobo.391.1">Now, we will initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">HopSkipJump</span></strong><span class="koboSpan" id="kobo.393.1"> attack class with the classifier and set the max iteration per evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">10</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.397.1">
attack = HopSkipJump(classifier=classifier, targeted=False, max_iter=10, max_eval=1000, init_eval=10)</span></pre></li> <li><span class="koboSpan" id="kobo.398.1">We will be running the algorithm for just 30 iterations. </span><span class="koboSpan" id="kobo.398.2">At every multiple of the tenth iteration, we will evaluate the algorithm by printing the minimum and maximum pixel difference to the original image, plotting the generated adversarial image, and predicting the </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">generated image:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.400.1">
x_adv = None
for i in range(3):
    x_adv = attack.generate(x=np.array([target_image]), x_adv_init=x_adv)
    print("class label %d." </span><span class="koboSpan" id="kobo.400.2">% np.argmax(classifier.predict(x_adv)[0]))
    plt.imshow(x_adv[0].astype(np.uint8))
    plt.show(block=False)
    print(np.min(x_adv[0] - target_image), np.max(x_adv[0] - target_image))</span></pre></li> <li><span class="koboSpan" id="kobo.401.1">Finally, we will plot the original image, obtain the model’s prediction on the original image, and print the </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">actual label:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.403.1">
plt.imshow(target_image.astype(np.uint8))
print(np.argmax(classifier.predict(np.array([target_image.astype(np.float32)]))[0]))
print(id_targets[1])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.404.1">The results are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.405.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.406.1">.3</span></em><span class="koboSpan" id="kobo.407.1">. </span><span class="koboSpan" id="kobo.407.2">The more steps you take, the more visually similar the generated adversarial is to the original image while still fooling the model! </span><span class="koboSpan" id="kobo.407.3">The minimum and maximum pixel difference here provides a sense of how similar the generated image is to the original image, which will transfer to </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">visual similarity:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.409.1"><img alt="Figure 14.3 – The adversarial performance of a single face example" src="image/B18187_14_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.410.1">Figure 14.3 – The adversarial performance of a single face example</span></p>
<p><span class="koboSpan" id="kobo.411.1">With just the </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.412.1">predictions</span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.413.1"> of the model, we successfully fooled the model into misidentifying a facial identity! </span><span class="koboSpan" id="kobo.413.2">To </span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.414.1">advance, it would be beneficial to gain a deeper understanding of the model’s vulnerability to perturbation attacks by examining additional examples. </span><span class="koboSpan" id="kobo.414.2">To benchmark the method more widely, we need a way to ensure that the adversarial image that was generated is visually similar to the original image so that it makes sense from a likelihood standpoint. </span><span class="koboSpan" id="kobo.414.3">From the result shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.415.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.416.1">.3</span></em><span class="koboSpan" id="kobo.417.1">, it’s safe to assume that anything lower than the absolute minimum and maximum difference of 10 pixels should retrain a high enough visual similarity to fool a human evaluator into believing nothing is wrong while being able to fool the model. </span><span class="koboSpan" id="kobo.417.2">We will use the same HopSkipJump algorithm and a total of 30 iterations but will add a condition that the generated adversarial image needs to be under 10 pixels maximum and have a minimum absolute difference from the original image. </span><span class="koboSpan" id="kobo.417.3">If it is not, we will use the prediction on the original image treating it as a failure to generate a meaningful </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">adversarial attack:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.419.1">Let’s start by taking 1,000 random stratified samples so that the evaluation can be done </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">more quickly:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.421.1">
splitter = StratifiedShuffleSplit(test_size=.02, n_splits=2, random_state = 7)
split = splitter.split(image_path, targets)
_, val_inds = next(split)
val_inds = val_inds[:1000]</span></pre></li> <li><span class="koboSpan" id="kobo.422.1">Next, we will </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.423.1">compute the </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.424.1">predictions using the mentioned workflow and use the original images on the </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">1,000 images:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.426.1">
all_adversarial_hsj_predicted_class = []
all_predicted_class = []
for idx, path in tqdm_notebook(enumerate(image_path[val_inds]), total=len(val_inds)):
    img = np.array(Image.open(os.path.join("../CHAPTER_13", path))).astype(np.float32)
    classifier = BlackBoxClassifier(
        predict, img.shape, num_classes, clip_values=(0, 255))
    label = id_targets[idx]
    predicted_class = np.argmax(classifier.predict(np.array([img]))[0])
    attack = HopSkipJump(
        classifier=classifier, targeted=False, max_iter=10, max_eval=1000, init_eval=10, verbose=False)
    x_adv = None
    for i in range(3):
        x_adv = attack.generate(x=np.array([img]), x_adv_init=x_adv)
    adversarial_hsj_predicted_class = np.argmax(classifier.predict(x_adv)[0])
    if (np.min(x_adv - img) &gt;= 10.0 or np.max(x_adv - img) &gt;= 10.0):
        adversarial_hsj_predicted_class = predicted_class
    all_predicted_class.append(predicted_class)
all_adversarial_hsj_predicted_class.append(adversarial_hsj_predicted_class)</span></pre></li> <li><span class="koboSpan" id="kobo.427.1">Now, let’s </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.428.1">compare the accuracy performance of the model on the original images and </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.429.1">on the adversarial images workflow we went through previously in </span><em class="italic"><span class="koboSpan" id="kobo.430.1">s</span></em><em class="italic"><span class="koboSpan" id="kobo.431.1">tep 7</span></em><span class="koboSpan" id="kobo.432.1">. </span><span class="koboSpan" id="kobo.432.2">We will be using the Hugging Face </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">evaluate</span></strong><span class="koboSpan" id="kobo.434.1"> library’s accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">metric method:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.436.1">
accuracy_metric = evaluate.load("accuracy")
print(accuracy_metric.compute(references=id_targets[val_inds],
    predictions=all_predicted_class)
print(accuracy_metric.compute(references=id_targets[val_inds],
    predictions=all_adversarial_ba_predicted_class)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.437.1">This will result in an accuracy of 56.9 on the original images and 30.9 accuracy with the adversarial </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">image workflow!</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.439.1">In this practical </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.440.1">example, we managed to use an automated algorithm to identify adversarial noise mixers that can successfully fool the model we built in the </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">previous chapter!</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">Image models are highly susceptible to adversarial noise mixer types and adversarial image patch types due to their divergence from natural occurrences in the real world, which models may struggle to properly digest. </span><span class="koboSpan" id="kobo.442.2">While machine learning models excel at learning patterns and features from real-world data, they often struggle to handle synthetic perturbations that deviate significantly from the typical environmental conditions. </span><span class="koboSpan" id="kobo.442.3">These adversarial techniques exploit the vulnerabilities of image models, causing them to misclassify or produce erroneous outputs. </span><span class="koboSpan" id="kobo.442.4">Consequently, understanding and addressing </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.443.1">these vulnerabilities becomes crucial in building robust defenses against such attacks. </span><span class="koboSpan" id="kobo.443.2">So, consider diving into the adversarial robustness toolbox repository to explore more adversarial examples and attack algorithms! </span><span class="koboSpan" id="kobo.443.3">As a recommendation to mitigate adversarial attacks in your image model, consider using adversarial targeted augmentation such as Gaussian noise and random pixel perturbator during training. </span><span class="koboSpan" id="kobo.443.4">By incorporating these augmentations during training, models can learn to be more resilient to synthetic perturbations, thereby increasing their </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">overall robustness.</span></span></p>
<p><span class="koboSpan" id="kobo.445.1">Next, we will delve into adversarial attacks that target </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">text-based models.</span></span></p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.447.1">Exploring adversarial analysis for text-based models</span></h1>
<p><span class="koboSpan" id="kobo.448.1">Text-based models</span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.449.1"> can sometimes have </span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.450.1">performance vulnerabilities toward the usage of certain words, a specific inflection of a word stem, or a different form of the same word. </span><span class="koboSpan" id="kobo.450.2">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">an example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.452.1">
Supervised Use Case: Sentiment Analysis
Prediction Row: {"Text": "I love this product!", "Sentiment": "Positive"}
Adversarial Example: {"Text": "I l0ve this product!", "Sentiment": "Negative"}</span></pre> <p><span class="koboSpan" id="kobo.453.1">So, adversarial analysis can be done by benchmarking performance on when you add important words to a sentence versus without. </span><span class="koboSpan" id="kobo.453.2">To mitigate such attacks, similar word replacement augmentation can be applied </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">during training.</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">However, when</span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.456.1"> it comes to text-based models in the modern day, most widely adopted models now rely on a pre-trained language modeling foundation. </span><span class="koboSpan" id="kobo.456.2">This allows them to be capable of understanding natural language even after domain fine-tuning, and as a result, a more complex adversarial attack that utilizes natural language deception can be used. </span><span class="koboSpan" id="kobo.456.3">Consequently, it is crucial to thoroughly analyze and develop robust defense mechanisms against these sophisticated adversarial attacks that exploit natural language deception, to ensure the reliability and security of modern </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">text-based models.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">Think of natural language deception to be similar to how humans try to deceive each other through natural language speech. </span><span class="koboSpan" id="kobo.458.2">Just as people may employ various tactics to mislead or manipulate others, such as social engineering, context poisoning, and linguistic exploitation, these same methods can be used to trick a text model. </span><span class="koboSpan" id="kobo.458.3">Here is an example of a natural language-based deception on a spam/malicious email detection machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">use case:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.460.1">
Supervised Use Case: Spam/Malicious Email Detection
Prediction Row: {"Email Content": "Click the link below to claim your free iPhone.", "Label": "Spam"}
Adversarial Example: {"Email Content": "Hey, I found this article about getting a new iPhone at a great discount. </span><span class="koboSpan" id="kobo.460.2">Here's the link to check it out.", "Label": "Not Spam"}</span></pre> <p><span class="koboSpan" id="kobo.461.1">Social engineering </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.462.1">involves using psychological manipulation to deceive others into divulging sensitive information or performing specific actions. </span><span class="koboSpan" id="kobo.462.2">Context poisoning refers to the deliberate introduction of misleading or irrelevant information to confuse the recipient. </span><span class="koboSpan" id="kobo.462.3">Meanwhile, linguistic exploitation takes advantage of the nuances and ambiguities of language to create confusion or misinterpretation. </span><span class="koboSpan" id="kobo.462.4">By understanding and addressing these deceptive techniques between humans and applying them to adversarial text analysis, we can enhance the resilience of text-based models against adversarial attacks and maintain their accuracy and reliability. </span><span class="koboSpan" id="kobo.462.5">Unfortunately, there isn’t an algorithmic way to benchmark such natural language deceptions that completely reform sentences. </span><span class="koboSpan" id="kobo.462.6">We will need to rely on collecting real-world deception data to perform adversarial analysis on </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">this component.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">However, there</span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.465.1"> is another form of natural language adversarial attack worth mentioning that is becoming more widely used today to attack general questions and answer LLMs such as ChatGPT. </span><span class="koboSpan" id="kobo.465.2">Instead of reformatting the entire original text data, an additional malicious context is used as a pretext for the original text data. </span><span class="koboSpan" id="kobo.465.3">LLM API providers usually have built-in guardrails to prevent explicit, offensive, objectionable, discriminative, and personal attacks, as well as harassment, violence, illegal activities, misinformation, propaganda, self-harm, and any inappropriate content. </span><span class="koboSpan" id="kobo.465.4">This is to ensure the responsible use of AI and to prevent anything that can negatively impact individuals and society. </span><span class="koboSpan" id="kobo.465.5">However, a popular adversarial attack called “jailbreak” can remove all content generation restrictions ChatGPT has enforced. </span><span class="koboSpan" id="kobo.465.6">The jailbreak attack method is an engineered prompt that’s shared publicly by people and can be used as a pretext before any actual user prompt. </span><span class="koboSpan" id="kobo.465.7">Many versions of such engineered prompts are shared publicly and can be found easily through Google searches, allowing everybody in the world to attack ChatGPT. </span><span class="koboSpan" id="kobo.465.8">Fortunately, OpenAI has been diligently mitigating such jailbreak adversarial attacks </span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.466.1">as soon as they get their hands on the prompts. </span><span class="koboSpan" id="kobo.466.2">New versions of the jailbreak prompt get introduced pretty frequently and OpenAI has gotten stuck in a continuous loop trying to mitigate newly engineered </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">jailbreak attacks.</span></span></p>
<p><span class="koboSpan" id="kobo.468.1">In conclusion, it’s crucial to perform adversarial analysis on text models to enhance their resilience against deceptive techniques and attacks. </span><span class="koboSpan" id="kobo.468.2">By understanding human deception and staying vigilant against evolving threats, we can improve the reliability and security of these text models beyond mere accuracy on crafted </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">testing data.</span></span></p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.470.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.471.1">In this chapter, the concept of adversarial performance analysis for machine learning models was introduced. </span><span class="koboSpan" id="kobo.471.2">Adversarial attacks aim to deceive models by intentionally inputting misleading or carefully crafted data to cause incorrect predictions. </span><span class="koboSpan" id="kobo.471.3">This chapter highlighted the importance of analyzing adversarial performance to identify potential vulnerabilities and weaknesses in machine learning models and to develop targeted mitigation methods. </span><span class="koboSpan" id="kobo.471.4">Adversarial attacks can target various aspects of machine learning models, which include their bias and fairness behavior, and their accuracy-based performance. </span><span class="koboSpan" id="kobo.471.5">For instance, facial recognition systems may be targeted by adversaries who exploit biases or discrimination present in the training data or </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">model design.</span></span></p>
<p><span class="koboSpan" id="kobo.473.1">We also explored practical examples and techniques for analyzing adversarial performance in image, text, and audio data-based models. </span><span class="koboSpan" id="kobo.473.2">For image-based models, various approaches such as object size, orientation, blurriness, and environmental conditions were discussed. </span><span class="koboSpan" id="kobo.473.3">We also practically explored an algorithmic approach that’s used to generate a noise matrix so that it can mix and perturb the original image and generate an adversarial image capable of fooling a trained face classifier model; this was taken from </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.474.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.475.1">, </span><em class="italic"><span class="koboSpan" id="kobo.476.1">Exploring Bias and Fairness</span></em><span class="koboSpan" id="kobo.477.1">. </span><span class="koboSpan" id="kobo.477.2">For audio-based models, augmentations such as pronunciation speed, speech pitch, background noise, and speech loudness were analyzed, while for text-based models, word variation-based attacks, natural language deception, and jailbreak attacks </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">were explored.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">In conclusion, adversarial analysis is essential for enhancing the resilience of machine learning models against deceptive techniques and attacks. </span><span class="koboSpan" id="kobo.479.2">By understanding human deception and staying vigilant against evolving adversarial threats, we can improve the reliability and security of our neural </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">network models.</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">In the next chapter, we will be moving on to the next stage in the deep learning life cycle and explore the world of deep learning models </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">in production.</span></span></p>
</div>


<div class="Content" id="_idContainer125">
<h1 id="_idParaDest-207" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.1.1">Part 3 – DLOps</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part of the book, you will dive into the exciting realm of deploying, monitoring, and governing deep learning models in production, drawing parallels with MLOps and DevOps. </span><span class="koboSpan" id="kobo.2.2">This part will provide you with a comprehensive understanding of the essential components required to ensure the success and impact of your deep learning models in production with </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">real-world utilization.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">Throughout the chapters in this part, we’ll explore the various aspects of deploying deep learning models in production, touching upon important considerations such as hardware infrastructure, model packaging, and user interfaces. </span><span class="koboSpan" id="kobo.4.2">We’ll also delve into the three fundamental pillars of model governance, which are model utilization, model monitoring, and model maintenance. </span><span class="koboSpan" id="kobo.4.3">You’ll learn about the concept of drift and its impact on the performance of deployed deep learning models over time, as well as strategies to handle drift effectively. </span><span class="koboSpan" id="kobo.4.4">We’ll also discuss the benefits of AI platforms such as DataRobot, which streamline the complex stages of the machine learning life cycle and accelerate the creation, training, deployment, and governance of intricate deep </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">As a bonus, building upon the foundational transformer method from </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.7.1">Chapter 6</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.8.1">, Understanding Neural Network Transformers</span></em><span class="koboSpan" id="kobo.9.1">, we’ll delve into </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">Large Language Model (LLM</span></strong><span class="koboSpan" id="kobo.11.1">) solutions, which are revolutionizing various domains. </span><span class="koboSpan" id="kobo.11.2">You’ll learn about architecting LLM solutions and building autonomous agents, equipping you with the knowledge to harness </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">their potential.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">This part contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">following chapters:</span></span></p>
<ul>
<li><a href="B18187_15.xhtml#_idTextAnchor217"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 15</span></em></a><em class="italic"><span class="koboSpan" id="kobo.16.1">, Deploying Deep Learning Models in Production</span></em></li>
<li><a href="B18187_16.xhtml#_idTextAnchor238"><em class="italic"><span class="koboSpan" id="kobo.17.1">Chapter 16</span></em></a><em class="italic"><span class="koboSpan" id="kobo.18.1">, Governing Deep Learning Models</span></em></li>
<li><a href="B18187_17.xhtml#_idTextAnchor247"><em class="italic"><span class="koboSpan" id="kobo.19.1">Chapter 17</span></em></a><em class="italic"><span class="koboSpan" id="kobo.20.1">, Managing Drift Effectively in a Dynamic Environment</span></em></li>
<li><a href="B18187_18.xhtml#_idTextAnchor265"><em class="italic"><span class="koboSpan" id="kobo.21.1">Chapter 18</span></em></a><em class="italic"><span class="koboSpan" id="kobo.22.1">, Exploring the DataRobot AI Platform</span></em></li>
<li><a href="B18187_19.xhtml#_idTextAnchor286"><em class="italic"><span class="koboSpan" id="kobo.23.1">Chapter 19</span></em></a><em class="italic"><span class="koboSpan" id="kobo.24.1">, Architecting LLM Solutions</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer126">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer127">
</div>
</div>
</body></html>