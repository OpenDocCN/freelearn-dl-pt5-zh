["```py\n    MNIST           = f'{S3_IMAGE}mnist_png.tgz'\n    ```", "```py\n    S3_IMAGE     = f'{S3}imageclas/'\n    S3  = 'https://s3.amazonaws.com/fast-ai-'\n    ```", "```py\n    https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz\n    ```", "```py\n    mnist_png\n    ├── testing\n    │   ├── 0\n    │   ├── 1\n    │   ├── 2\n    │   ├── 3\n    │   ├── 4\n    │   ├── 5\n    │   ├── 6\n    │   ├── 7\n    │   ├── 8\n    │   └── 9\n    └── training\n         ├── 0\n         ├── 1\n         ├── 2\n         ├── 3\n         ├── 4\n         ├── 5\n         ├── 6\n         ├── 7\n         ├── 8\n         └── 9\n    ```", "```py\n         ADULT_SAMPLE           = f'{URL}adult_sample.tgz'\n         BIWI_SAMPLE            = f'{URL}biwi_sample.tgz'\n         CIFAR                     = f'{URL}cifar10.tgz'\n         COCO_SAMPLE            = f'{S3_COCO}coco_sample.tgz'\n         COCO_TINY               = f'{S3_COCO}coco_tiny.tgz'\n         HUMAN_NUMBERS         = f'{URL}human_numbers.tgz'\n         IMDB                       = f'{S3_NLP}imdb.tgz'\n         IMDB_SAMPLE            = f'{URL}imdb_sample.tgz'\n         ML_SAMPLE               = f'{URL}movie_lens_sample.tgz'\n         ML_100k                  = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n         MNIST_SAMPLE           = f'{URL}mnist_sample.tgz'\n         MNIST_TINY              = f'{URL}mnist_tiny.tgz'\n         MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n         PLANET_SAMPLE         = f'{URL}planet_sample.tgz'\n         PLANET_TINY            = f'{URL}planet_tiny.tgz'\n         IMAGENETTE              = f'{S3_IMAGE}imagenette2.tgz'\n         IMAGENETTE_160        = f'{S3_IMAGE}imagenette2-160.tgz'\n         IMAGENETTE_320        = f'{S3_IMAGE}imagenette2-320.tgz'\n         IMAGEWOOF               = f'{S3_IMAGE}imagewoof2.tgz'\n         IMAGEWOOF_160         = f'{S3_IMAGE}imagewoof2-160.tgz'\n         IMAGEWOOF_320         = f'{S3_IMAGE}imagewoof2-320.tgz'\n         IMAGEWANG               = f'{S3_IMAGE}imagewang.tgz'\n         IMAGEWANG_160         = f'{S3_IMAGE}imagewang-160.tgz'\n         IMAGEWANG_320         = f'{S3_IMAGE}imagewang-320.tgz'\n    ```", "```py\n         # image classification datasets\n         CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n         CARS            = f'{S3_IMAGE}stanford-cars.tgz'\n         CIFAR_100     = f'{S3_IMAGE}cifar100.tgz'\n         CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n         FLOWERS        = f'{S3_IMAGE}oxford-102-flowers.tgz'\n         FOOD            = f'{S3_IMAGE}food-101.tgz'\n         MNIST           = f'{S3_IMAGE}mnist_png.tgz'\n         PETS            = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n         # NLP datasets\n         AG_NEWS                        = f'{S3_NLP}ag_news_csv.tgz'\n         AMAZON_REVIEWS              = f'{S3_NLP}amazon_review_full_csv.tgz'\n         AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n         DBPEDIA                        = f'{S3_NLP}dbpedia_csv.tgz'\n         MT_ENG_FRA                    = f'{S3_NLP}giga-fren.tgz'\n         SOGOU_NEWS                    = f'{S3_NLP}sogou_news_csv.tgz'\n         WIKITEXT                       = f'{S3_NLP}wikitext-103.tgz'\n         WIKITEXT_TINY               = f'{S3_NLP}wikitext-2.tgz'\n         YAHOO_ANSWERS               = f'{S3_NLP}yahoo_answers_csv.tgz'\n         YELP_REVIEWS                 = f'{S3_NLP}yelp_review_full_csv.tgz'\n         YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n         # Image localization datasets\n         BIWI_HEAD_POSE      = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n         CAMVID                  = f'{S3_IMAGELOC}camvid.tgz'\n         CAMVID_TINY           = f'{URL}camvid_tiny.tgz'\n         LSUN_BEDROOMS        = f'{S3_IMAGE}bedroom.tgz'\n         PASCAL_2007           = f'{S3_IMAGELOC}pascal_2007.tgz'\n         PASCAL_2012           = f'{S3_IMAGELOC}pascal_2012.tgz'\n         # Audio classification datasets\n         MACAQUES               = 'https://storage.googleapis.com/ml-animal-sounds-datasets/macaques.zip'\n         ZEBRA_FINCH           = 'https://storage.googleapis.com/ml-animal-sounds-datasets/zebra_finch.zip'\n         # Medical Imaging datasets\n         SIIM_SMALL            = f'{S3_IMAGELOC}siim_small.tgz'\n    ```", "```py\n    path = untar_data(URLs.PETS)\n    ```", "```py\n    oxford-iiit-pet\n    ├── annotations\n    │   ├── trimaps\n    │   └── xmls\n    └── images\n    ```", "```py\n    path = untar_data(URLs.ADULT_SAMPLE)\n    ```", "```py\n    df = pd.read_csv(path/'adult.csv')\n    ```", "```py\n    df.shape\n    ```", "```py\n    df.nunique()\n    ```", "```py\n    df.isnull().sum()\n    ```", "```py\n    df_young = df[df.age <= 40]\n    df_young.head()\n    ```", "```py\n    path = untar_data(URLs.WIKITEXT_TINY)\n    ```", "```py\n    df_train = pd.read_csv(path/'train.csv')\n    ```", "```py\n    df_train = pd.read_csv(path/'train.csv',header=None)\n    ```", "```py\n    df_test = pd.read_csv(path/'test.csv',header=None)\n    ```", "```py\n    df_combined = pd.concat([df_train,df_test])\n    ```", "```py\n    print(\"df_train: \",df_train.shape)\n    print(\"df_test: \",df_test.shape)\n    print(\"df_combined: \",df_combined.shape)\n    ```", "```py\n    df_tok, count = tokenize_df(df_combined,[df_combined.columns[0]])\n    ```", "```py\n    print(\"very common word (count['the']):\", count['the'])\n    print(\"moderately common word (count['prepared']):\", count['prepared'])\n    print(\"rare word (count['gaga']):\", count['gaga'])\n    ```", "```py\n    path = untar_data(URLs.FLOWERS)\n    ```", "```py\n    (path/'jpg').ls()\n    ```", "```py\n    img_files = get_image_files(path)\n    img = PILImage.create(img_files[100])\n    img\n    ```", "```py\n    path = untar_data(URLs.BIWI_HEAD_POSE)\n    ```", "```py\n    path.ls()\n    ```", "```py\n    (path/\"05\").ls()\n    ```", "```py\n    path = untar_data(URLs.ADULT_SAMPLE)\n    ```", "```py\n    df = pd.read_csv(path/'adult.csv')\n    df.isnull().sum()\n    ```", "```py\n    procs = [FillMissing,Categorify]\n    dep_var = 'salary'\n    cont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n    ```", "```py\n    df_no_missing = TabularPandas(df, procs, cat, cont, y_names = dep_var)\n    ```"]