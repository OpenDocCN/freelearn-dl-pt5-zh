- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural Language Data – Finding and Preparing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will teach you how to identify and prepare data for processing
    with natural language understanding techniques. It will discuss data from databases,
    the web, and different kinds of documents, as well as privacy and ethics considerations.
    The Wizard of Oz technique will be covered briefly. If you don’t have access to
    your own data, or if you wish to compare your results to those of other researchers,
    this chapter will also discuss generally available and frequently used corpora.
    It will then go on to discuss preprocessing steps such as stemming and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sources of data and annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring privacy and observing ethical considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally available corpora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application-specific types of preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing among preprocessing techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding sources of data and annotating it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is where all **natural language processing** (**NLP**) projects start.
    Data can be in the form of written texts or transcribed speech. The purpose of
    data is to teach an NLP system what it should do when it’s given similar data
    in the future. Specific collections of data are also called *corpora* or *datasets*,
    and we will often use these terms interchangeably. Very recently, large pretrained
    models have been developed that greatly reduce the need for data in many applications.
    However, these pretrained models, which will be discussed in detail in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193), do not in most cases eliminate the need
    for application-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: Written language data can be of any length, ranging from very short texts such
    as tweets to multi-page documents or even books. Written language can be interactive,
    such as a record of a chatbot session between a user and a system, or it can be
    non-interactive, such as a newspaper article or blog. Similarly, spoken language
    can be long or short. Like written language, it can be interactive, such as a
    transcription of a conversation between two people, or non-interactive, such as
    broadcast news. What all NLP data has in common is that it is language, and it
    consists of words, in one or more human languages, that are used by people to
    communicate with each other. The goal of every NLP project is to take that data,
    process it by using specific algorithms, and gain information about what the author
    or authors of the data had in mind when they created the data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first steps in any NLP project is finding the right data. For that,
    you’ll want to consider your goals in doing the project.
  prefs: []
  type: TYPE_NORMAL
- en: Finding data for your own application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a specific, practical application in mind that you’d like to build,
    it’s often easy to know what kind of data you need. For example, to build an enterprise
    assistant (one of the types of interactive applications shown in *Figure 1**.3*),
    you’ll need examples of conversations between users and either human agents or
    interactive systems of the kind you would like to build. These examples could
    already be available in the form of existing call center records.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples we will consider in the following subsections are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversations in call centers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chat logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message boards and customer reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Conversations in call centers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’re planning to build a voice assistant that performs customer support,
    in many cases, the goal is to offload some work from an existing call center.
    In that case, there will frequently be many transcripts of previous calls between
    human agents and customers that can serve as data for training an application.
    The customers’ questions will be examples of what the application will need to
    understand, and the agents’ responses will be examples of how the system should
    respond. There will need to be an annotation process that assigns an overall intent,
    or customer goal, to each customer utterance. Most of the time, the annotation
    process will also need to label entities within the utterance. Before annotation
    of the intents and entities begins, there should be an initial design step where
    the intents and entities are determined.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been annotated, it can be used as training data for applications.
    The training process will vary significantly depending on the technologies used.
    We will cover the use of the data in training in detail in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173)
    and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  prefs: []
  type: TYPE_NORMAL
- en: Chat logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have a website that includes a chat window, the questions typed by customers
    can serve as training data, just like transcripts from call center conversations.
    The only difference in the data is that call center data is based on speech rather
    than typing. Otherwise, the annotation, design, and training processes will be
    very similar. The data itself will be a bit different since typed inputs tend
    to be shorter than spoken inputs and will contain spelling errors.
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Databases can often be a good source of enterprise data. Databases often contain
    free text fields, where a user can enter any information they like. Free text
    fields are used for information such as narrative summaries of incident reports,
    and they often contain rich information that’s not captured elsewhere in the other
    database fields. NLP can be extremely valuable for learning from this rich information
    because the contents of free text fields can be classified and analyzed using
    NLP techniques. This analysis can provide additional insight into the topic of
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: Message boards and customer reviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like free text fields, message boards and customer support forums contain unformatted
    inputs from customers. Customer support message boards and customer product reviews
    can be valuable sources of data about product failures as well as customer attitudes
    about products. Although this information can be reviewed by human analysts, human
    analysis is time-consuming and expensive. In many cases, this information is abundant
    and can be used as the basis of very useful NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve talked about application-specific data and how you can find and
    analyze it for the purposes of the application. On the other hand, sometimes you
    will be interested in analyzing data as part of a research project. The next section
    discusses where you can obtain data for a research project.
  prefs: []
  type: TYPE_NORMAL
- en: Finding data for a research project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your goal is to contribute to the science of NLP, or if you just want to
    be able to compare your algorithms to other researchers’ work, the kind of data
    you need will be quite different from the data discussed previously. Rather than
    finding data (possibly proprietary data inside your enterprise) that no one’s
    ever used before, you will want to use data that’s freely available to other researchers.
    Ideally, this data will already be public, but if not, it’s important for you
    to make it available to others in the NLP research community so that they can
    replicate your work. Scientific conferences or journals will nearly always require
    that any newly collected data be made available before a paper can be presented
    or published. We will now discuss several ways of collecting new data.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although there are many sources of pre-existing data, sometimes you will not
    find exactly what you need from currently available resources. Perhaps you want
    to work on a very specific technical topic, or perhaps you’re interested in a
    rapidly changing topic such as COVID-19\. You may need data that’s specific to
    a particular local area or seasonal data that’s only applicable to certain times
    of the year. For all of these reasons, it may be necessary to collect data specifically
    for your project.
  prefs: []
  type: TYPE_NORMAL
- en: There are several good ways to collect data under these circumstances, including
    **application programming interfaces** (**APIs**), crowdsourcing data, and Wizard
    of Oz. We will review these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: APIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some social media services have feeds that can be accessed from APIs. Twitter,
    for example, has an API that developers can use to access Twitter services ([https://developer.twitter.com/en/docs/twitter-api](https://developer.twitter.com/en/docs/twitter-api)).
  prefs: []
  type: TYPE_NORMAL
- en: Crowdsourcing data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some data can be generated by human workers using platforms such as Amazon’s
    Mechanical Turk ([https://www.mturk.com/](https://www.mturk.com/)). The data to
    be generated has to be clearly described for crowdworkers, along with any parameters
    or constraints on the data to be generated. Data that is easy for the average
    person to understand, as opposed to technical or specialized scientific data,
    is especially suitable for crowdsourcing. Crowdsourcing can be an effective way
    to obtain data; however, for this data to be useful, some precautions have to
    be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to make sure that the crowdworkers have adequate instructions
    so that the data they create is sufficiently similar to the real data that the
    system will encounter during deployment. Crowdworker data has to be monitored
    to make sure that the crowdworkers are properly following instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crowdworkers have to have the right knowledge in order to create data that’s
    appropriate for specialized applications. For example, if the crowdworkers are
    to generate medical reports, they have to have medical backgrounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crowdworkers have to have sufficient knowledge of the language that they’re
    generating data for in order to ensure that the data is representative of the
    real data. It is not necessary for the data to be perfectly grammatical – language
    encountered during deployment is not, especially speech data. Insisting on perfectly
    grammatical data can lead to stilted, unrealistic data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wizard of Oz
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The **Wizard of Oz** (**WoZ**) method of collecting data is based on setting
    up a computer-human interaction situation where a system appears to be processing
    the user’s inputs, but in fact, the processing is done by a human behind the scenes.
    The *Wizard of Oz* reference is from the line in the movie where the wizard says,
    “*Pay no attention to the man behind the curtain*,” who is actually controlling
    a projection of what is supposed to be the wizard. The idea behind the technique
    is that if the user believes that a system is doing the processing, the user’s
    behavior will represent how they would behave with an actual system. While the
    WoZ method can provide very high-quality data, it is expensive, since the setup
    must be carefully arranged so that subjects in the experiment are unaware that
    they’re interacting with an automated system. You can find details about the WoZ
    paradigm at [https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment](https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment),
    and more information about conducting a WoZ experiment here: [https://www.answerlab.com/insights/wizard-of-oz-testing](https://www.answerlab.com/insights/wizard-of-oz-testing).'
  prefs: []
  type: TYPE_NORMAL
- en: In data collection, we are not only interested in the language itself but also
    in additional information that describes the data, or *metadata*. The next section
    will describe metadata in general, and then continue discussing an extremely important
    type of metadata, *annotation*.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets often include metadata. Metadata refers to information about the data
    rather than the data itself. Almost any information that data providers think
    might be useful in further processing can be included as metadata. Some of the
    most common types of metadata include the human language of the data, the speaker
    and time and place of speech for spoken data, and the author of written data.
    If spoken data is the result of a speech recognition process, the speech recognizer’s
    confidence is usually included as metadata. The next section covers annotation,
    which is probably the most important type of metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most important types of metadata is the intended NLP result for a
    text, or the annotation.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, newly collected data will need to be annotated, unless the
    experiment to be done involves unsupervised learning (more on unsupervised learning
    in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)). Annotation is the process
    of associating an input with the NLP result that the trained system is intended
    to produce. The system *learns* how to analyze data by processing the annotated
    examples, and then applies that learning to new, unannotated examples.
  prefs: []
  type: TYPE_NORMAL
- en: Since annotation is actually the *supervision* in supervised learning, data
    used in unsupervised learning experiments does not need to be annotated with the
    intended NLP result.
  prefs: []
  type: TYPE_NORMAL
- en: There are several software tools that can be used to annotate NLP text data.
    For example, the **General Architecture for Text Engineering** (**GATE**) ([https://gate.ac.uk/](https://gate.ac.uk/))
    includes a well-tested user interface that enables annotators to assign meanings
    to documents and parts of documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will “learn about the” transcription of speech
    data and the question of inter-annotator agreement.
  prefs: []
  type: TYPE_NORMAL
- en: Transcription
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transcription, or converting the speech contained in audio files to its written
    form, is a necessary annotation step for speech data. If the speech does not contain
    significant amounts of noise, commercial **automatic speech recognition** (**ASR**)
    systems such as Nuance Dragon ([https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html](https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html))
    can provide a fairly accurate first pass at transcribing audio files. If you do
    use commercial ASR for transcription, the results should still be reviewed by
    a researcher in order to catch and correct any errors made by the ASR system.
    On the other hand, if the speech is very noisy or quiet, or if it contains speech
    from several people talking over each other, commercial ASR systems will probably
    make too many errors for the automatic transcription results to be useful. In
    that situation, manual transcription software such as TranscriberAG ([http://transag.sourceforge.net/](http://transag.sourceforge.net/))
    can be used. You should keep in mind that manual transcription of noisy or otherwise
    problematic speech is likely to be a slow process because the transcriber has
    to first understand the speech in order to transcribe it.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-annotator agreement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Annotators will not always agree on the correct annotation for any given data
    item. There can be general differences in opinion about the correct annotation,
    especially if the annotation instructions are unclear. In addition, annotators
    might not be thinking carefully about what the annotations should be, or the data
    that the annotators are being asked to annotate might be inherently subjective.
    For these reasons, annotation of the same data is often done by several annotators,
    especially if the data is subjective. Annotating an emotion associated with text
    or speech is a good example of data that has a lot of potential for disagreement.
    The degree of agreement among annotators is called **inter-annotator agreement**
    and is measured by what is known as the **kappa statistic**. The kappa statistic
    is preferable to just computing a percentage agreement because it takes into account
    the possibility that the annotators might agree by chance.
  prefs: []
  type: TYPE_NORMAL
- en: '`nltk.metrics.agreement`, that can be used to calculate inter-annotator agreement.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed the process of obtaining your own data, but there
    are also many pre-existing datasets available that have already been annotated
    and are freely available. We’ll discuss pre-existing data in the next section.
    When we start working directly with data in later chapters, we will be using pre-existing
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Generally available corpora
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to get data is to use pre-existing corpora that cover the kind
    of problem that you’re trying to solve. With pre-existing corpora, you will not
    need to collect the data, and you will probably not need to annotate it unless
    the pre-existing annotations are not suitable for your problem. Any privacy questions
    will have been addressed before the dataset was published. An added advantage
    to using pre-existing corpora is that other researchers have probably published
    papers describing their work on the corpus, which you can compare to your own
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are many standard preexisting datasets that you can download
    and work with, covering almost any NLP problem. Some are free and others are available
    for a fee.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preexisting datasets are available from a number of organizations, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linguistic Data Consortium ([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/)):
    Provides a wide variety of text and speech data in many languages and also manages
    donated data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face ([https://huggingface.co/](https://huggingface.co/)): Provides
    datasets in many languages, as well as NLP models. Some of the popular datasets
    available from Hugging Face include movie reviews, product reviews, and Twitter
    emotion categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaggle ([https://www.kaggle.com/](https://www.kaggle.com/)): Provides many
    datasets, including user-contributed datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**European language resources association** (**ELRA**) ([http://www.elra.info/en/](http://www.elra.info/en/)):
    A European organization that provides multilingual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data distributed with NLP libraries such as NLTK and spaCy: NLP libraries include
    dozens of corpora of all sizes and languages. Much of the data is annotated in
    support of many different types of applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Government data: Governments collect vast amounts of data, including text data,
    which is often publicly available and can be used for research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Librispeech [https://www.openslr.org/12](https://www.openslr.org/12): A large
    dataset of read speech, based on audiobooks read for people with visual impairments.
    Primarily used for speech projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to this point, we have covered the topics of obtaining data and adding metadata,
    including annotation. Before data is used in NLP applications, we also need to
    ensure that it is used ethically. Ethical considerations are the topic of the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring privacy and observing ethical considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Language data, especially data internal to an enterprise, may contain sensitive
    information. Examples that come to mind right away are medical and financial data.
    When an application deals with these kinds of topics, it is very likely to contain
    sensitive information about health or finances. Information can become even more
    sensitive if it is associated with a specific person. This is called **personally
    identifiable information** (**PII**), which is defined by the United States Department
    of Labor as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “*Any representation of information that permits the identity of an individual
    to whom the information applies to be reasonably inferred by either direct or
    indirect means*” ([https://www.dol.gov/general/ppii](https://www.dol.gov/general/ppii)).
    This is a broad and complex issue, a full treatment of which is out of the scope
    of this book. However, it’s worth discussing a few important points specific to
    NLP applications that should be considered if you need to deal with any kind of
    sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the privacy of training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of generally available corpora, the data has typically been prepared
    so that sensitive information has been removed. If you are dealing with data you
    have obtained yourself, on the other hand, you will have to consider how to deal
    with sensitive information that you may encounter in the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: One common strategy is to replace sensitive data with placeholders, such as
    `<NAME>`, `<LOCATION>`, and `<PHONENUMBER>`. This allows the training process
    to learn how to process the natural language without exposing any sensitive data.
    This should not affect the ability of the trained system to process natural language
    because it would be rare that an application would classify utterances differently
    depending on specific names or locations. If the classification depends on more
    specific information than just, for example, `<LOCATION>`, a more specific placeholder
    can be used, such as a city or country name. It is also actually helpful to use
    placeholders in training because it reduces the chances of overfitting the trained
    model on the specific names in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the privacy of runtime data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At runtime, when an application is deployed, incoming data will naturally contain
    sensitive data that the users enter in order to accomplish their goals. Any precautions
    that are normally taken to secure data entered with forms (non-NLP data such as
    social security numbers or credit card numbers) will of course apply to data entered
    with natural language text and speech. In some cases, the handling of this data
    will be subject to regulations and legislation, which you will need to be aware
    of and follow.
  prefs: []
  type: TYPE_NORMAL
- en: Treating human subjects ethically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language and speech data can be collected in the course of an experiment,
    such as a WoZ study, when human users or subjects provide speech and text for
    research. Universities and other research institutions have committees that review
    the planned procedures for any experiments with human subjects to ensure that
    the subjects are treated ethically. For example, subjects must provide informed
    consent to the experiment, they must not be harmed, their anonymity must be protected,
    deceptive practices must be avoided, and they must be able to withdraw from the
    study at any time. If you are collecting data in an experimental context, make
    sure to find out about the rules at your institution regarding approval by your
    human subjects committee or the equivalent. You may need to allocate extra lead
    time for the approval process to go through.
  prefs: []
  type: TYPE_NORMAL
- en: Treating crowdworkers ethically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If crowdworkers such as Amazon Mechanical Turk workers are used to create or
    annotate data, it is important to remember to treat them fairly – most importantly,
    to pay them fairly and on time – but also to make sure they have the right tools
    to do their jobs and to listen respectfully to any concerns or questions that
    they might have about their tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have identified a source of data and discussed ethical issues, let’s
    move on to what we can do to get the data ready to be used in an application,
    or preprocessing. We will first cover general topics in preprocessing in the next
    section, and then discuss preprocessing techniques for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the data is available, it usually needs to be cleaned or preprocessed before
    the actual natural language processing begins.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major goals in preprocessing data. The first goal is to remove
    items that can’t be processed by the system – these might include items such as
    emojis, HTML markup, spelling errors, foreign words, or some Unicode characters
    such as *smart quotes*. There are a number of existing Python libraries that can
    help with this, and we’ll be showing how to use them in the next section, *Removing
    non-text*. The second goal is addressed in the section called *Regularizing text*.
    We regularize text so that differences among words in the text that are not relevant
    to the application’s goal can be ignored. For example, in some applications, we
    might want to ignore the differences between uppercase and lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: There are many possible preprocessing tasks that can be helpful in preparing
    natural language data. Some are almost universally done, such as tokenization,
    while others are only done in particular types of applications. We will be discussing
    both types of preprocessing tasks in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Different applications will need different kinds of preprocessing. Other than
    the most common preprocessing tasks, such as tokenization, exactly what kinds
    of preprocessing need to be done has to be carefully considered. A useful preprocessing
    step for one kind of application can completely remove essential information that’s
    needed in another kind of application. Consequently, for each preprocessing step,
    it’s important to think through its purpose and how it will contribute to the
    effectiveness of the application. In particular, you should use preprocessing
    steps thoughtfully if you will be using **large language models** (**LLMs**) in
    your NLP application. Since they are trained on normal (unregularized) text, regularizing
    input text to LLMs will make the input text less similar to the training text.
    This usually causes problems with machine learning models such as LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Removing non-text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many natural language components are only able to process textual characters,
    but documents can also contain characters that are not text. Depending on the
    purpose of the application, you can either remove them from the text completely
    or replace them with equivalent characters that the system is able to process.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the idea of *non-text* is not an all-or-none concept. What is considered
    *non-text* is to some extent application-specific. While the standard characters
    in the ASCII character set ([https://www.ascii-code.com/](https://www.ascii-code.com/))
    are clear examples of text, other characters can sometimes be considered non-text.
    For example, your application’s *non-text* could include such items as currency
    symbols, math symbols, or texts written in scripts other than the main script
    of the rest of the text (such as a Chinese word in an otherwise English document).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next two sections, we will look at removing or replacing two common
    examples of non-text: emojis and smart quotes. These examples should provide a
    general framework for removing other types of non-text.'
  prefs: []
  type: TYPE_NORMAL
- en: Removing emojis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common types of non-text is emojis. Social media posts are very
    likely to contain emojis, but they are a very interesting kind of text for NLP
    processing. If the natural language tools being used in your application don’t
    support emojis, the emojis can either be removed or replaced with their text equivalents.
    In most applications, it is likely that you will want to remove or replace emojis,
    but it is also possible in some cases that your NLP application will be able to
    interpret them directly. In that case, you won’t want to remove them.
  prefs: []
  type: TYPE_NORMAL
- en: One way to replace emojis is to use regular expressions that search for the
    Unicode (see [https://home.unicode.org/](https://home.unicode.org/) for more information
    about Unicode) representations of emojis in the text. Still, the set of emojis
    is constantly expanding, so it is difficult to write a regular expression that
    covers all possibilities. Another approach is to use a Python library that directly
    accesses the Unicode data from [unicode.org](https://unicode.org), which defines
    standard emojis ([https://home.unicode.org/](https://home.unicode.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: One package that can be used to remove or replace emojis is `demoji` ([https://pypi.org/project/demoji/](https://pypi.org/project/demoji/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `demoji` is just a matter of installing it and running over text that
    may contain undesired emojis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install `demoji`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, given a text that contains emojis, run to replace the emojis with descriptions
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or, if you want to remove the emojis completely or replace them with a specific
    alternative of your choosing, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, *Figure 5.1* shows a text that includes an emoji of a birthday
    cake and shows how this can be replaced with the description `:birthday cake:`,
    or simply removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Replacing or removing emojis](img/B19005_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Replacing or removing emojis
  prefs: []
  type: TYPE_NORMAL
- en: Even if the emoji doesn’t cause problems with running the software, leaving
    the emoji in place means that any meaning associated with the emoji will be ignored,
    because the NLP software will not understand the meaning of the emoji. If the
    emoji is replaced with a description, some of its meaning (for example, that the
    emoji represents a birthday cake) can be taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: Removing smart quotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Word processing programs sometimes automatically change typed quotation marks
    into *smart quotes*, or *curly quotes*, which look better than straight quotes,
    but which other software may not be prepared for. Smart quotes can cause problems
    with some NLP software that is not expecting smart quotes. If your text contains
    smart quotes, they can easily be replaced with the normal Python string replacement
    method, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the replacement straight quotes in the `replace` method need to be
    escaped with a backslash, like any use of literal quotes.
  prefs: []
  type: TYPE_NORMAL
- en: The previous two sections covered removing non-text items such as emojis and
    smart quotes. We will now talk about some techniques for regularizing text or
    modifying it to make it more uniform.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be covering the most important techniques for regularizing
    text. We will talk about the goals of each technique and how to apply it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Nearly all NLP software operates on the level of words, so the text needs to
    be broken into words for processing to work. In many languages, the primary way
    of separating text into words is by whitespaces, but there are many special cases
    where this heuristic doesn’t work. In *Figure 5**.2*, you can see the code for
    splitting on whitespace and the code for tokenization using NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Python code for tokenization by splitting on whitespace and
    using NLTK’s tokenization](img/B19005_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Python code for tokenization by splitting on whitespace and using
    NLTK’s tokenization
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code in *Figure 5**.2* results in the tokenizations shown in *Table
    5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Example** | **Issue** | **Result from splitting** **on whitespace** | **NLTK
    result** |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk here.` | Punctuation should not be included in the token | `[''``Walk'',
    ''here.'']` | `[''Walk'', ''``here'', ''.'']` |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk` `here.` | Extra whitespace should not count as a token | `[''Walk'',
    '''', ''``here.'']` | `[''Walk'', ''``here'', ''.'']` |'
  prefs: []
  type: TYPE_TB
- en: '| `Don''t` `walk here.` | The contraction “don’t” should count as two tokens
    | `["Don''t", ''``walk'', ''here.'']` | `[''Do'', "n''t", ''walk'', ''``here'',
    ''.'']` |'
  prefs: []
  type: TYPE_TB
- en: '| `$``100` | The “$” should be a separate token | `[''$``100'']` | `[''$'',
    ''``100'']` |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Tokenization results by splitting on whitespace and using NLTK’s
    tokenization
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at *Table 5.1*, we can see that the simple heuristic of splitting the
    text on whitespace results in errors in some cases:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first row of *Table 5.1*, we can compare the two approaches when punctuation
    occurs at the end of a token, but there is no whitespace between the token and
    punctuation. This means that just separating tokens on whitespace results in incorrectly
    including the punctuation in the token. This means that `walk`, `walk,`, `walk.`,
    `walk?`, `walk;`, `walk:`, and `walk!` will all be considered to be different
    words. If all of those words appear to be different, any generalizations found
    in training based on one version of the word won’t apply to the other versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second row, we can see that if split on whitespace, two whitespaces in
    a row will result in an extra blank token. The result of this will be to throw
    off any algorithms that take into account the fact that two words are next to
    each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contractions also cause problems when splitting on whitespace is used for tokenization.
    Contractions of two words won’t be separated into their components, which means
    that the **natural language understanding** (**NLU**) algorithms won’t be able
    to take into account that *do not* and *don’t* have the same meaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, when presented with words with monetary amounts or other measurements,
    the algorithms won’t be able to take into account that *$100* and *100 dollars*
    have the same meaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is tempting to try to write regular expressions to take care of these exceptions
    to the generalization that most words are surrounded by whitespace. However, in
    practice, it is very hard to capture all of the cases. As we attempt to cover
    more cases with regular expressions, the regular expressions will become more
    complex and difficult to maintain. For that reason, it is preferable to use a
    library such as NLTK’s, which has been developed over many years and has been
    thoroughly tested. You can try different texts with this code to see what kinds
    of results you get with different tokenization approaches.
  prefs: []
  type: TYPE_NORMAL
- en: As *Table 5.1* shows, either way, the result will be a list of strings, which
    is a convenient form for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Lower casing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In languages that use uppercase and lowercase in their writing systems, most
    documents contain words with upper and lowercase letters. As in the case of tokenization,
    having the same word written in slightly different formats means that data from
    one format won’t apply to data in the other formats. For example, *Walk*, *walk*,
    and *WALK* will all count as different words. In order to make them all count
    as the same word, the text is normally all in lowercase. This can be done by looping
    over a list of word tokens and applying the `lower()` Python function, as shown
    in *Figure 5**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Converting text to all lowercase](img/B19005_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Converting text to all lowercase
  prefs: []
  type: TYPE_NORMAL
- en: Converting all words to lowercase has some drawbacks, however. Case differences
    are sometimes important for meaning. The biggest example of this is that it will
    make it hard for NLP software to tell the difference between proper names and
    ordinary words, which differ in case. This can cause errors in **part of speech**
    (**POS**) tagging or **named entity recognition** (**NER**) if the tagger or named
    entity recognizer is trained on data that includes case differences. Similarly,
    words can sometimes be written in all caps for emphasis. This may indicate something
    about the sentiment expressed in the sentence – perhaps the writer is excited
    or angry – and this information could be helpful in sentiment analysis. For these
    reasons, the position of each preprocessing step in a pipeline should be considered
    to make sure no information is removed before it’s needed. This will be discussed
    in more detail in the *Text preprocessing pipeline* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Words in many languages appear in different forms depending on how they’re used
    in a sentence. For example, English nouns have different forms depending on whether
    they’re singular or plural, and English verbs have different forms depending on
    their tense. English has only a few variations, but other languages sometimes
    have many more. For example, Spanish has dozens of verb forms that indicate past,
    present, or future tenses or whether the subject of the verb is first, second,
    or third person, or singular or plural. Technically, in linguistics, these different
    forms are referred to as **inflectional morphology**. The endings themselves are
    referred to as **inflectional morphemes**.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in speech or text that is directed toward another person, these different
    forms are very important in conveying the speaker’s meaning. However, if the goal
    of our NLP application is to classify documents into different categories, paying
    attention to different forms of a word is likely not to be necessary. Just as
    with punctuation, different forms of words can cause them to be treated as completely
    separate words by NLU processors, despite the fact that the words are very similar
    in meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming** and **lemmatization** are two similar methods of regularizing
    these different forms. Stemming is the simpler approach, so we will look at that
    first. Stemming basically means removing specific letters that end some words
    and that are frequently, but not always, inflectional morphemes – for example,
    the *s* at the end of *walks*, or the *ed* at the end of *walked*. Stemming algorithms
    don’t have any knowledge of the actual words in a language; they’re just guessing
    what may or may not be an ending. For that reason, they make a lot of mistakes.
    They can make mistakes by either removing too many letters or not enough letters,
    resulting in two words being collapsed that are actually different words, or not
    collapsing words that should be treated as the same word.'
  prefs: []
  type: TYPE_NORMAL
- en: '`PorterStemmer` is a widely used stemming tool and is built into NLTK. It can
    be used as shown in *Figure 5**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Results from stemming with PorterStemmer](img/B19005_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Results from stemming with PorterStemmer
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the results include a number of mistakes. While `walked` becomes
    `walk`, and `going` becomes `go`, which are good results, the other changes that
    the stemmer made are errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`exercise` ![](img/01.png) `exercis`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`every` ![](img/01.png) `everi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evening` ![](img/01.png) `even`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this` ![](img/01.png) `thi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Porter stemmer also only works for English because its stemming algorithm
    includes heuristics, such as removing the *s* at the end of words, that only apply
    to English. NLTK also includes a multilingual stemmer, called the Snowball stemmer,
    that can be used with more languages, including Arabic, Danish, Dutch, English,
    Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian,
    Russian, Spanish, and Swedish.
  prefs: []
  type: TYPE_NORMAL
- en: However, since these stemmers don’t have any specific knowledge of the words
    of the languages they’re applied to, they can make mistakes, as we have seen.
    A similar but more accurate approach actually makes use of a dictionary, so that
    it doesn’t make errors like the ones listed previously. This approach is called
    **lemmatization**.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatizing and part of speech tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lemmatization, like stemming, has the goal of reducing variation in the words
    that occur in the text. However, lemmatization actually replaces each word with
    its root word (found by looking the word up in a computational dictionary) rather
    than simply removing what looks like suffixes. However, identifying the root word
    often depends on the part of speech, and lemmatization can be inaccurate if it
    doesn’t know the word’s part of speech. The part of speech can be identified through
    **part of speech tagging**, covered in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059),
    which assigns the most probable part of speech to each word in a text. For that
    reason, lemmatization and part of speech tagging are often performed together.
  prefs: []
  type: TYPE_NORMAL
- en: For the dictionary in this example, we’ll use WordNet, developed at Princeton
    University ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)),
    an important source of information about words and their parts of speech. The
    original WordNet was developed for English, but WordNets for other languages have
    also been developed. We briefly mentioned WordNet in the *Semantic analysis* section
    of [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059), because WordNet contains semantic
    information as well as information about parts of speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we’ll just use the part of speech information, not the semantic
    information. *Figure 5**.5* shows importing the WordNet lemmatizer, the tokenizer,
    and the part of speech tagger. We then have to align the names of the parts of
    speech between WordNet and the part of speech tagger, because the part of speech
    tagger and WordNet don’t use the same names for the parts of speech. We then go
    through the text, lemmatizing each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve
    walked every evening this week”](img/B19005_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve
    walked every evening this week”
  prefs: []
  type: TYPE_NORMAL
- en: As the lemmatized result shows, many of the words in the input text have been
    replaced by their lemmas – `going` is replaced with `go`, `is` is replaced with
    `be`, and `walked` is replaced with `walk`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `evening` hasn’t been replaced by *even*, as it was in the stemming
    example. If *evening* had been the present participle of the verb *even*, it would
    have been replaced by `even`, but *even* isn’t the root word for *evening* here.
    In this case, *evening* just refers to the time of day.
  prefs: []
  type: TYPE_NORMAL
- en: Stopword removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Stopwords** are extremely common words that are not helpful in distinguishing
    documents and so they are often removed in classification applications. However,
    if the application involves any kind of detailed sentence analysis, these common
    words are needed so that the system can figure out what the analysis should be.'
  prefs: []
  type: TYPE_NORMAL
- en: Normally, stopwords include words such as pronouns, prepositions, articles,
    and conjunctions. Which words should be considered to be stopwords for a particular
    language is a matter of judgment. For example, spaCy has many more English stopwords
    (326) than NLTK (179). These specific stopwords were chosen by the spaCy and NLTK
    developers because they thought those stopwords would be useful in practice. You
    can use whichever one you find more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the stopwords provided by each system.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to run the NLTK and spaCy systems, you may need to do some preliminary
    setup. If you are working in a command-line or terminal environment, you can ensure
    that NLTK and spaCy are available by entering the following commands on the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install -U pip` `setuptools wheel`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pip install -``U spacy`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`python -m spacy` `download en_core_web_sm`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the other hand, if you’re working in the (recommended) Jupyter Notebook environment
    covered in [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085), you can enter the
    same commands in a Jupyter code cell but precede each command with `!`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve confirmed that your NLTK and spaCy environments are set up, you
    can look at the NLTK stopwords by running the code in *Figure 5**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Viewing the first few stopwords for NLTK](img/B19005_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Viewing the first few stopwords for NLTK
  prefs: []
  type: TYPE_NORMAL
- en: Note that *Figure 5**.6* just shows the first few stopwords. You can see them
    all by running the code in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the spaCy stopwords, run the code in *Figure 5**.7*, which also just
    shows the first few stopwords:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Viewing the first few stopwords for spaCy](img/B19005_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Viewing the first few stopwords for spaCy
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the stopwords provided by both packages, we can see that the two sets
    have a lot in common, but there are differences as well. In the end, the stopwords
    you use are up to you. Both sets work well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Removing punctuation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Removing punctuation can also be useful since punctuation, like stopwords, appears
    in most documents and therefore doesn’t help distinguish document categories.
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation can be removed by defining a string of punctuation symbols and removing
    items in that string with regular expressions, or by removing every non-alphanumeric
    word in the text. The latter approach is more robust because it’s easy to overlook
    an uncommon punctuation symbol.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to remove punctuation can be seen in *Figure 5**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Removing punctuation](img/B19005_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Removing punctuation
  prefs: []
  type: TYPE_NORMAL
- en: The original text is the value of the `text_to_remove_punct` variable, in *Figure
    5**.8*, which contains several punctuation marks – specifically, an exclamation
    mark, a comma, and a period. The result is the value of the `tokens_no_punct`
    variable, shown in the last line.
  prefs: []
  type: TYPE_NORMAL
- en: Spelling correction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Correcting misspelled words is another way of removing noise and regularizing
    text input. Misspelled words are much less likely to have occurred in training
    data than correctly spelled words, so they will be harder to recognize when a
    new text is being processed. In addition, any training that includes the correctly
    spelled version of the word will not recognize or be able to make use of data
    including the incorrectly spelled word. This means that it’s worth considering
    adding a spelling correction preprocessing step to the NLP pipeline. However,
    we don’t want to automatically apply spelling correction in every project. Some
    of the reasons not to use spelling correction are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Some types of text will naturally be full of spelling mistakes – for example,
    social media posts. Because spelling mistakes will occur in texts that need to
    be processed by the application, it might be a good idea not to try to correct
    spelling mistakes in either the training or runtime data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spelling error correction doesn’t always do the right thing, and a spelling
    correction that results in the wrong word won’t be helpful. It will just introduce
    errors into the processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some types of texts include many proper names or foreign words that aren’t known
    to the spell checker, which will try to correct them. Again, this will just introduce
    errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you do choose to use spelling corrections, there are many spell checkers
    available in Python. One recommended spell checker is `pyspellchecker`, which
    can be installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The spell-checking code and the result can be seen in *Figure 5**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Spell-checking with pyspellchecker](img/B19005_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Spell-checking with pyspellchecker
  prefs: []
  type: TYPE_NORMAL
- en: You can see from *Figure 5**.9* that it’s easy to make mistakes with spell-checking.
    `pyspellchecker` correctly changes `agains` to `against`, but it also made a mistake
    correcting `Ms.` to `is` and it didn’t know anything about the name `Ramalingam`,
    so it didn’t have a correction for that.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that input generated by **ASR** will not contain spelling errors,
    because ASR can only output words in their dictionaries, which are all correctly
    spelled. Of course, ASR output can contain mistakes, but those mistakes will just
    be the result of substituting a wrong word for the word that was actually spoken,
    and they can’t be corrected by spelling correction.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that stemming and lemmatization can result in tokens that aren’t real
    words, which you don’t want to be corrected. If spelling correction is used in
    a pipeline, make sure that it occurs before stemming and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding contractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to increase the uniformity of data is to expand contractions – that
    is, words such as *don’t* would be expanded to their full form, *do not*. This
    will allow the system to recognize an occurrence of *do* and an occurrence of
    *not* when it finds *don’t*.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have reviewed many generic preprocessing techniques. Next, let’s
    move on to more specific techniques that are only applicable to certain types
    of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Application-specific types of preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preprocessing topics we have covered in the previous sections are generally
    applicable to many types of text in many applications. Additional preprocessing
    steps can also be used in specific applications, and we will cover these in the
    next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Substituting class labels for words and numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes data includes specific words or tokens that have equivalent semantics.
    For example, a text corpus might include the names of US states, but for the purposes
    of the application, we only care that *some* state was mentioned – we don’t care
    which one. In that case, we can substitute a *class token* for the specific state
    name. Consider the interaction in *Figure 5**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Class token substitution](img/B19005_05_10New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Class token substitution
  prefs: []
  type: TYPE_NORMAL
- en: If we substitute the class token, `<state_name>`, for `Texas`, all of the other
    state names will be easier to recognize, because instead of having to learn 50
    states, the system will only have to learn about the general class, `<state_name>`.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to use class tokens is if the texts contain alphanumeric tokens
    such as dates, phone numbers, or social security numbers, especially if there
    are too many to enumerate. Class tokens such as `<social_security_number>` can
    be substituted for the actual numbers. This has the added benefit of masking,
    or *redacting*, sensitive information such as specific social security numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Redaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the *Ensuring privacy and observing ethical considerations*
    section, data can contain sensitive information such as people’s names, health
    information, social security numbers, or telephone numbers. This information should
    be redacted before the data is used in training.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-specific stopwords
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both NLTK and spaCy have the capability to add and remove stopwords from their
    lists. For example, if your application has some very common domain-specific words
    that aren’t on the built-in stopword list, you can add these words to the application-specific
    stopword list. Conversely, if some words that are normally stopwords are actually
    meaningful in the application, these words can be removed from the stopwords.
  prefs: []
  type: TYPE_NORMAL
- en: A good example is the word *not*, which is a stopword for both NLTK and spaCy.
    In many document classification applications, it’s fine to consider *not* as a
    stopword; however, in applications such as sentiment analysis, *not* and other
    related words (for example, *nothing* or *none*) can be important clues for a
    negative sentiment. Removing them can cause errors if sentences such as *I do
    not like this product* becomes *I do like this product*. In that case, you should
    remove *not* and other negative words from the list of stopwords you are using.
  prefs: []
  type: TYPE_NORMAL
- en: Remove HTML markup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the application is based on web pages, they will contain HTML formatting
    tags that aren’t useful in NLP. The Beautiful Soup library ([https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/))
    can perform the task of removing HTML tags. While Beautiful Soup has many functions
    for working with HTML documents, for our purposes, the most useful function is
    `get_text()`, which extracts the text from an HTML document.
  prefs: []
  type: TYPE_NORMAL
- en: Data imbalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text classification, where the task is to assign each document to one of a set
    of classes, is one of the most common types of NLP applications. In every real-life
    classification dataset, some of the classes will have more examples than others.
    This problem is called **data imbalance**. If the data is severely imbalanced,
    this will cause problems with machine learning algorithms. Two common techniques
    for addressing data imbalance are **oversampling** and **undersampling**. Oversampling
    means that some of the items in the less frequent classes are duplicated, and
    undersampling means that some of the items in the more common classes are removed.
    Both approaches can be used at the same time – frequent classes can be undersampled
    while infrequent classes can be oversampled. We will be discussing this topic
    in detail in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248).
  prefs: []
  type: TYPE_NORMAL
- en: Using text preprocessing pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting your data ready for NLP often involves multiple steps, and each step
    takes the output of the previous step and adds new information, or in general,
    gets the data further prepared for NLP. A sequence of preprocessing steps like
    this is called a **pipeline**. For example, an NLP pipeline could include tokenization
    followed by lemmatization and then stopword removal. By adding and removing steps
    in a pipeline, you can easily experiment with different preprocessing steps and
    see whether they make a difference in the results.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines can be used to prepare both training data to be used in learning a
    model, as well as test data to be used at runtime or during testing. In general,
    if a preprocessing step is always going to be needed (for example, tokenization),
    it’s worth considering using it on the training data once and then saving the
    resulting data as a dataset. This will save time if you’re running experiments
    with different configurations of preprocessing steps to find the best configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing among preprocessing techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Table 5.2* is a summary of the preprocessing techniques described in this
    chapter, along with their advantages and disadvantages. It is important for every
    project to consider which techniques will lead to improved results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 5.2 – Advantages and disadvantages of preprocessing techniques
  prefs: []
  type: TYPE_NORMAL
- en: Many techniques, such as spelling correction, have the potential to introduce
    errors because the technology is not perfect. This is particularly true for less
    well-studied languages, for which the relevant algorithms can be less mature than
    those of better-studied languages.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth starting with an initial test with only the most necessary techniques
    (such as tokenization) and introducing additional techniques only if the results
    of the initial test are not good enough. Sometimes, the errors introduced by preprocessing
    can cause the overall results to get worse. It is important to keep evaluating
    results during the investigation to make sure that results aren’t getting worse.
    Evaluation will be discussed in detail in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how to find and use natural language data, including
    finding data for a specific application as well as using generally available corpora.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed a wide variety of techniques for preparing data for NLP, including
    annotation, which provides the foundation for supervised learning. We also discussed
    common preprocessing steps that remove noise and decrease variation in the data
    and allow machine learning algorithms to focus on the most informative differences
    among different categories of texts. Another important set of topics covered in
    this chapter had to do with privacy and ethics – how to ensure the privacy of
    information included in text data and how to ensure that crowdsourcing workers
    who are generating data or who are annotating data are treated fairly.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will discuss exploratory techniques for getting an overall
    picture of a dataset, such as summary statistics (word frequencies, category frequencies,
    and so on). It will also discuss visualization tools (such as matplotlib) that
    can provide the kinds of insights that can be best obtained by looking at graphical
    representations of text data. Finally, it will discuss the kinds of decisions
    that can be made based on visualization and statistical results.
  prefs: []
  type: TYPE_NORMAL
