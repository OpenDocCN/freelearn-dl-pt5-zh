<html><head></head><body>
		<div>
			<div id="_idContainer182" class="Content">
			</div>
		</div>
		<div id="_idContainer183" class="Content">
			<h1 id="_idParaDest-169">6. <a id="_idTextAnchor194"/>LSTMs, GRUs, and Advanced RNNs</h1>
		</div>
		<div id="_idContainer209" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will study and implement advanced models and variations of the plain <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>) that overcome some of RNNs' practical drawbacks and are among the best performing deep learning models at the moment. We will start by understanding the drawbacks of plain RNNs and see how the novel idea of <strong class="bold">Long Short-Term Memory</strong> overcomes them. We will then see and implement a <strong class="bold">Gated Recurrent Unit</strong> based model. We will also work with bidirectional and stacked RNNs and explore attention-based models. By the end of this chapter, you will have built and assessed the performance of these models on a sentiment classification task, observing for yourself the trade-offs in choosing the different models.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor195"/>Introduction</h1>
			<p>Let's say you're working with product reviews for a mobile phone and your task is to classify the sentiment in the reviews as being positive or negative. You encounter a review that says: <em class="italic">"The phone does not have a great camera, or an amazingly vivid display, or an excellent battery life, or great connectivity, or other great features that make it the best."</em> Now, when you read this, you can easily identify that the sentiment in the review is negative, despite the presence of many positive phrases such as <em class="italic">"excellent battery life"</em> and <em class="italic">"makes it the best"</em>. You understand that the presence of the term <em class="italic">"not"</em> right toward the beginning of the text negates everything else that comes after.</p>
			<p>Will the models we've created so far be able to identify the sentiment in such a case? Probably not, because if your models don't realize that the term <em class="italic">"not"</em> toward the beginning of the sentences is important and needs to be connected strongly to the output several terms later, they won't be able to identify the sentiment correctly. This, unfortunately, is a major drawback of plain RNNs.</p>
			<p>In the previous chapter, we looked at a couple of deep learning approaches for dealing with sequences, that is, one-dimensional convolutions and RNNs. We saw that RNNs are extremely powerful models that provide us with a great amount of flexibility to handle different sequence tasks. The plain RNNs that we saw have been subject to plenty of research. Now, we will look at some approaches that have been built on top of RNNs to create new, powerful models that overcome the drawbacks of RNNs. We will look at LSTM, GRUs, stacked and bidirectional LSTMs, and attention-based models. We will apply these models to a sentiment classification task, thereby bringing together the concepts discussed in <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, and <em class="italic">Chapter 5, Deep Learning for Sequences</em>, as well.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor196"/>Long-Range Dependence/Influence</h1>
			<p>The sample mobile phone review we saw in the previous section was an example of a long-range dependence/influence – where a term/value in a sequence has an influence on the assessment of a lot of the subsequent terms/values. Consider the following example, where you need to fill in the blank with a missing country name: <em class="italic">"After a top German university granted her admission for her Masters in Dentistry, Hina was extremely excited to start this new phase of her career with international exposure and couldn't wait till the end of the month to book her flight to ____."</em></p>
			<p>The correct answer, of course, is <strong class="bold">Germany</strong>, arriving at which would require you to understand the importance of the term "German", which appears at the beginning of the sentence, on the outcome at the end of the sentence. This is another example of long-range dependence. The following figure shows the long-range dependence of the answer, "<em class="italic">Germany</em>", on the term "<em class="italic">German</em>" appearing early in the sentence:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B15385_06_01.jpg" alt="Figure 6.1: Long-range dependence&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: Long-range dependence</p>
			<p>To get the best outcome, we need to be able to handle long-range dependencies. In the context of deep learning models and RNNs, this would mean that learning (or the backpropagation of errors) needs to happen smoothly and effectively over many time steps. This is easier said than done, primarily because of the vanishing gradient problem.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor197"/>The Vanishing Gradient Problem</h1>
			<p>One of the biggest challenges while training standard feedforward deep neural networks is the vanishing gradient problem (as discussed in <em class="italic">Chapter 2, Neural Networks</em>). As the model gets more and more layers, backpropagating the errors all the way back to the initial layers becomes increasingly difficult. Layers close to the output will be "learning"/updated at a good pace, but by the time the error propagates to the initial layers, its value will have diminished greatly and have little or no effect on the parameters for the initial layers.</p>
			<p>With RNNs, this problem is further compounded, as the parameters need to be updated not only along the depth but also for the time steps. If we have one hundred time steps in the inputs (which isn't uncommon, especially when working with text), the network needs to propagate the error (calculated at the 100th time step) all the way back to the first time step. For plain RNNs, this task can be a bit too much to handle. This is where RNN variants can come in useful.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Another practical issue with training deep networks is the exploding gradient problem, where the gradient values get very high – too high to be represented by the system. This issue has a rather simple workaround called "<strong class="bold">Gradient Clipping</strong>", which means capping the values of the gradient.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor198"/>Sequence Models for Text Classification</h1>
			<p>In <em class="italic">Chapter 5, Deep Learning for Sequences</em>, we learned that RNNs perform extremely well on sequence-modeling tasks and provide high performance on text-related tasks. In this chapter, we will use plain RNNs and variants of RNNs on a sentiment classification task: processing the input sequence and predicting whether the sentiment is positive or negative.</p>
			<p>We'll use the IMDb reviews dataset for this task. The dataset contains 50,000 movie reviews, along with their sentiment – 25,000 highly polar movie reviews for training and 25,000 for testing. A few reasons for using this dataset are as follows:</p>
			<ul>
				<li>It is very conveniently available to load Keras (tokenized version) with a single command.</li>
				<li>The dataset is commonly used for testing new approaches/models. This should help you compare your results with other approaches easily.</li>
				<li>Longer sequences in the data (IMDb reviews can get very long) help us assess the differences between the variants of RNNs better.</li>
			</ul>
			<p>Let's get started by building our first model using plain RNNs and then benchmark the future model performances against that of the plain RNN. Let's start with data preprocessing and formatting the model.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor199"/>Loading Data</h2>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure that you work on all the exercises and example codes in this chapter in the same Jupyter Notebook. Note that the code in this section will load the dataset. To ensure all exercises and example codes that follow work, please ensure that you do not skip this section. You can access the complete code for the exercises at<span class="P---URL"> </span><a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p>
			<p>To begin, you need to start a new Jupyter Notebook and import the <strong class="source-inline">imdb</strong> module from the Keras datasets. Note that unless mentioned otherwise, the code and exercises for the rest of this chapter should continue in the same Jupyter Notebook:</p>
			<p class="source-code">from tensorflow.keras.datasets import imdb</p>
			<p>With the module imported, importing the dataset (tokenized and separated into train and test sets) is as easy as running <strong class="source-inline">imdb.load_data</strong>. The only parameter we need to provide is the vocabulary size we wish to use. Recall that the vocabulary size is the total number of unique terms we wish to consider for the modeling process. When we specify a vocabulary size, <em class="italic">V</em>, we work with the top <em class="italic">V</em> terms in the data. Here, we will specify a vocabulary size of 8,000 for our models (an arbitrary choice; you can modify this as desired) and load the data using the <strong class="source-inline">load_data</strong> method, as shown here:</p>
			<p class="source-code">vocab_size = 8000</p>
			<p class="source-code">(X_train, y_train), (X_test, y_test) = imdb.load_data\</p>
			<p class="source-code">                                       (num_words=vocab_size)</p>
			<p>Let's inspect the <strong class="source-inline">X_train</strong> variable to see what we are working with. Let's print the type of it and the type of constituting elements, and also have a look at one of the elements:</p>
			<p class="source-code">print(type(X_train))</p>
			<p class="source-code">print(type(X_train[5]))</p>
			<p class="source-code">print(X_train[5])</p>
			<p>We will see the following output:</p>
			<p class="source-code">&lt;class 'numpy.ndarray'&gt;</p>
			<p class="source-code">&lt;class 'list'&gt;</p>
			<p class="source-code">[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, </p>
			<p class="source-code"> 2, 32, 85, 156, 45, 40, </p>
			<p class="source-code"> 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, </p>
			<p class="source-code"> 3804, 8, 4, 226, 65,</p>
			<p class="source-code"> 12, 43, 127, 24, 2, 10, 10]</p>
			<p>The <strong class="source-inline">X_train</strong> variable is a <strong class="source-inline">numpy</strong> array – each element of the array is a list representing the text for a single review. The terms in the text are present as numerical tokens instead of raw tokens. This is a very convenient format.</p>
			<p>The next step is to define an upper limit on the length of the sequences that we'll work with and limit all sequences to the defined maximum length. We'll use <strong class="source-inline">200</strong> – an arbitrary choice, in this case – to quickly get started with the model-building process<strong class="bold">*</strong>. For our purpose, we'll pick <strong class="source-inline">200</strong> steps so that the networks don't get too heavy, and because <strong class="source-inline">200</strong> time steps are sufficient to demonstrate the different RNN approaches. Let's define the <strong class="source-inline">maxlen</strong> variable:</p>
			<p class="source-code">maxlen = 200</p>
			<p>The next step is to get all our sequences to the same length using the <strong class="source-inline">pad_sequences</strong> utility from Keras.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">*</strong>Ideally, we would analyze the lengths of the sequences and identify one that covers most of the reviews. We'll perform these steps in the activity at the end of the chapter, in which we'll use ideas from not only the current chapter, but also from <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, and <em class="italic">Chapter 5, Deep Learning for Sequences</em>, bringing this all together in a single activity.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor200"/>Staging and Preprocessing Our Data</h2>
			<p>The <strong class="source-inline">pad_sequences</strong> utility from the <strong class="source-inline">sequences</strong> module in Keras helps us in getting all the sequences to a specified length. If the input sequence is shorter than the specified length, the utility pads the sequence with a reserved token (indicating a blank/missing). If the input sequence is longer than the specified length, the utility truncates the sequence to limit it. In the following example, we will apply the <strong class="source-inline">pad_sequences</strong> utility to our test and train datasets:</p>
			<p class="source-code">from tensorflow.keras import preprocessing</p>
			<p class="source-code">X_train = preprocessing.sequence.pad_sequences\</p>
			<p class="source-code">          (X_train, maxlen=maxlen)</p>
			<p class="source-code">X_test = preprocessing.sequence.pad_sequences\</p>
			<p class="source-code">         (X_test, maxlen=maxlen)</p>
			<p>To understand the result of the steps, let's see the output for a particular instance in the training data:</p>
			<p class="source-code">print(X_train[5])</p>
			<p>The processed instance is as follows:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B15385_06_02.jpg" alt="Figure 6.2: Result of pad_sequences&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2: Result of pad_sequences</p>
			<p>We can see that there are plenty of <strong class="source-inline">0</strong>s at the beginning of the result. As you may have inferred, this is the padding that's done by the <strong class="source-inline">pad_sequence</strong> utility because the input sequence was shorter than <strong class="source-inline">200</strong>. Padding at the beginning of the sequence is the default behavior of the utility. For a sequence that is less than the specified limit, the truncation, by default, is done from the left – that is, the last <strong class="source-inline">200</strong> terms would be retained. All instances in the output now have <strong class="source-inline">200</strong> terms. The dataset is now ready for modeling.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The default behavior of the utility is to pad the beginning of the sequence and truncate from the left. These can be important hyperparameters. If you believe that the first few terms are the most important for the prediction, you may want to truncate the last terms by specifying the "<strong class="source-inline">truncating</strong>" parameter as "<strong class="source-inline">post</strong>". Similarly, to have padding toward the end of the sequence, you can set "<strong class="source-inline">padding</strong>" to "<strong class="source-inline">post</strong>".</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor201"/>The Embedding Layer</h1>
			<p>In <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, we discussed that we can't feed text directly into a neural network, and therefore need good representations. We discussed that embeddings (low-dimensional, dense vectors) are a great way of representing text. To pass the embeddings into the neural network's layers, we need to employ the embedding layer.</p>
			<p>The functionality of the embedding layer is two-fold:</p>
			<ul>
				<li>For any input term, perform a lookup and return its word embedding/vector</li>
				<li>During training, learn these word embeddings</li>
			</ul>
			<p>The part about looking up is straightforward – the word embeddings are stored as a matrix of the <strong class="source-inline">V × D</strong> dimensionality, where <strong class="source-inline">V</strong> is the vocabulary size (the number of unique terms considered) and <strong class="source-inline">D</strong> is the length/dimensionality of each vector. The following figure illustrates the embedding layer. The input term, "<strong class="source-inline">life</strong>", is passed to the embedding layer, which performs a lookup and returns the corresponding vector of length <strong class="source-inline">D</strong>. This vector, which is the representation for the term <strong class="source-inline">life</strong>, is fed to the hidden layer.</p>
			<p>What do we mean by learning these embeddings while training the predictive model? Aren't word embeddings learned by using an algorithm such as <strong class="source-inline">word2vec</strong>, which tries to predict the center word based on context terms (remember the CBOW architecture we discussed in <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>)? Well, yes and no:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B15385_06_03.jpg" alt="Figure 6.3: Embedding layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Embedding layer</p>
			<p>The <strong class="source-inline">word2vec</strong> approach had the objective of learning a representation that captures the meaning of the term. Therefore, predicting the target word based on context was a perfect formulation for the objective. In our case, the objective is different – we wish to learn representations that help us best predict the sentiment in the text. It makes sense, then, to learn the representation that works explicitly toward our objective.</p>
			<p>The embedding layer is always the first layer in the model. You can follow it up with any architecture of your choice (RNNs, in our case). We randomly initialize the vectors, essentially the weights in the embedding layer. While the model trains, the weights are updated in a way that predicts the outcome in a better way. The weights learned, and therefore the word vectors, are then tuned to the task. This is a very useful step – why use generic representations when you can tune them to your task?</p>
			<p>The embedding layer in Keras has two main parameters:</p>
			<ul>
				<li><strong class="source-inline">input_dim</strong> : The number of unique terms in the vocabulary, that is, the vocabulary size</li>
				<li><strong class="source-inline">output_dim</strong> : The dimension of the embedding/the length of the word vector</li>
			</ul>
			<p>The <strong class="source-inline">input_dim</strong> parameter needs to be set to the vocabulary size being employed. The <strong class="source-inline">output_dim</strong> parameter specifies the length of the embedding vector for each term.</p>
			<p>Note that the embedding layer in Keras also allows you to use your own specified weight matrix in the embedding layer. This means you can use pre-trained embeddings (such as <strong class="source-inline">GloVe</strong>, or even embeddings you trained in a different model) in the embedding layer. The <strong class="source-inline">GloVe</strong> model has been trained on billions of tokens and it could be useful to leverage this powerful general representation.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you use pre-trained embeddings, you also have the option to make them trainable in your model – essentially, use <strong class="source-inline">GloVe</strong> embeddings as a starting point and fine-tune them for your task. This is a great example of transfer learning for text.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor202"/>Building the Plain RNN Model</h1>
			<p>In the next exercise, we will build our first model for the sentiment classification task using plain RNNs. The model architecture we'll use is depicted in the following figure, which demonstrates how the model would process an input sentence "<strong class="source-inline">Life is good</strong>", with the term "<strong class="source-inline">Life</strong>" coming in at time step <strong class="source-inline">T=0</strong> and "<strong class="source-inline">good</strong>" appearing at time step <strong class="source-inline">T=2</strong>. The model will process the inputs one by one, using the embedding layer to look up the word embeddings that will be passed to the hidden layers. The classification will be done when the final term, "<strong class="source-inline">good</strong>", is processed at time step <strong class="source-inline">T=2</strong>. We'll use Keras to build and train our models:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B15385_06_04.jpg" alt="Figure 6.4: Architecture using an embedding layer and RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4: Architecture using an embedding layer and RNN</p>
			<h2 id="_idParaDest-178">Exercise 6.01: Buil<a id="_idTextAnchor203"/>ding and Training an RNN Model for Sentiment Classification</h2>
			<p>In this exercise, we will build and train an RNN model for sentiment classification. Initially, we will define the architecture for the recurrent and prediction layers, and we will assess the model's performance on the test data. We will add the embedding layer and some dropout and complete the model definition by adding the RNN layer, dropout, and a dense layer to finish. Then, we'll check the accuracy of the predictions on the test data to assess how well the model generalizes. Follow these steps to complete this exercise:</p>
			<ol>
				<li>Let's begin by setting the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> random number generation, to get, to the best extent possible, reproducible results. We'll import <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> and set the seed using the following commands:<p class="source-code">impo<a id="_idTextAnchor204"/>rt numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">np.random.seed(42)</p><p class="source-code">tf.random.set_seed(42)</p><p class="callout-heading">Note</p><p class="callout">Even though we have set the seeds for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> to achieve reproducible results, there are a lot more causes for variation, owing to which you may get a result that's different from ours. This applies to all the models we'll use from now on. While the values you see may be different, the output you see should largely agree with ours. If the model's performance is very different, you may want to tweak the number of epochs – the reason for this being that the weights in neural networks are initialized randomly, so the starting points for you and us could be slightly different, and we may reach a similar position when training a different number of epochs.</p></li>
				<li>Now, let's continue by importing all the necessary packages and layers and initializing a sequential model named <strong class="source-inline">model_rnn</strong> using the following commands:<p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers \</p><p class="source-code">import SimpleRNN, Flatten, Dense, Embedding, \</p><p class="source-code">SpatialDropout1D, Dropout</p><p class="source-code">model_rnn = Sequential()</p></li>
				<li>Now, we need to specify the embedding layer. The <strong class="source-inline">input_dim</strong> parameter needs to be set to the <strong class="source-inline">vocab_size</strong> variable. For the <strong class="source-inline">output_dim</strong> parameter, we'll choose <strong class="source-inline">32</strong>. Recall from <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, that this is a hyperparameter and you may want to experiment with this to get better results. Let's specify the embedding layer and use dropout (to minimize overfitting) using the following commands:<p class="source-code">model_rnn.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_rnn.add(SpatialDropout1D(0.4))</p><p>Note that the dropout employed here is <strong class="source-inline">SpatialDropout1D</strong> – this version performs the same function as regular dropout layer, but instead of dropping individual elements, it drops entire one-dimensional feature maps (vectors, in our case). </p></li>
				<li>Add a <strong class="source-inline">SimpleRNN</strong> layer with <strong class="source-inline">32</strong> neurons to the model (chosen arbitrarily; another hyperparameter to tune):<p class="source-code">model_rnn.add(SimpleRNN(32))</p></li>
				<li>Next, add a dropout layer with <strong class="source-inline">40%</strong> dropout (again, an arbitrary choice):<p class="source-code">model_rnn.add(Dropout(0.4))</p></li>
				<li>Add a dense layer with a <strong class="source-inline">sigmoid</strong> activation function to complete the model architecture. This is the output layer that makes the prediction:<p class="source-code">model_rnn.add(Dense(1, activation='sigmoid'))</p></li>
				<li>Compile the model and view the model summary:<p class="source-code">model_rnn.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer='rmsprop', metrics=['accuracy'])</p><p class="source-code">model_rnn.summary()</p><p>The model summary is as follows:</p><div id="_idContainer188" class="IMG---Figure"><img src="image/B15385_06_05.jpg" alt="Figure 6.5: Summary of the plain RNN model&#13;&#10;"/></div><p class="figure-caption">Figure 6.5: Summary of the plain RNN model</p><p>We can see that there are <strong class="source-inline">258,113</strong> parameters, most of which are present in the embedding layer. The reason for this is that the word embeddings are being learned during the training – so we're learning the embedding matrix, which is of dimensionality <strong class="source-inline">vocab_size(8000) × output_dim(32)</strong>.</p><p>Let's proceed and train the model (with the hyperparameters that we've observed to provide the best result with this data and architecture).</p></li>
				<li>Fit the model on the train data with a batch size of <strong class="source-inline">128</strong> for <strong class="source-inline">10</strong> epochs (both of these are hyperparameters that you can tune). Use a validation split of <strong class="source-inline">0.2</strong> – monitoring this will give us a sense of the model performance on unseen data:<p class="source-code">history_rnn = model_rnn.fit(X_train, y_train, \</p><p class="source-code">                            batch_size=128, \</p><p class="source-code">                            validation_split=0.2, \</p><p class="source-code">                            epochs = 10)</p><p>The training output for the last five epochs will be as follows. Depending on your system configuration, this step could take more or less time than it did here for us:</p><div id="_idContainer189" class="IMG---Figure"><img src="image/B15385_06_06.jpg" alt="Figure 6.6: Training the plain RNN model – the final five epochs&#13;&#10;"/></div><p class="figure-caption">Figure 6.6: Training the plain RNN model – the final five epochs</p><p>From the training output, we can see that the validation accuracy goes up to about 86%. Let's make predictions on the test set and check the performance of the model.</p></li>
				<li>Make predictions on the test data using the <strong class="source-inline">predict_classes</strong> method of the model and use the <strong class="source-inline">accuracy_score</strong> method from <strong class="source-inline">sklearn</strong>:<p class="source-code">y_test_pred = model_rnn.predict_classes(X_test)</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">print(accuracy_score(y_test, y_test_pred))</p><p>The accuracy of the test is as follows:</p><p class="source-code">0.85128</p><p>We can see that the model does a decent job. We used a simple architecture with <strong class="source-inline">32</strong> neurons and used a vocabulary size of just <strong class="source-inline">8000</strong>. Tweaking these and other hyperparameters may get you better results and you are encouraged to do so.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Oa2trm">https://packt.live/2Oa2trm</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p> In this exercise, we have seen how to build an RNN-based model for text. We saw how an embedding layer can be used to derive word vectors for the task at hand. These word vectors are the representations for each incoming term, which are passed to the RNN layer. We have seen that even a simple architecture can give us good results. Now, let's discuss how this model can be used to make predictions on new, unseen reviews.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor205"/>Making Predictions on Unseen Data</h1>
			<p>Now that you've trained your model on some data and assessed its performance on the test data, the next thing is to learn how to use this model to predict the sentiment for new data. That is the purpose of the model, after all – being able to predict the sentiment for data previously unseen by the model. Essentially, for any new review in the form of raw text, we should be able to classify its sentiment.</p>
			<p>The key step for this would be to create a process/pipeline that converts the raw text into a format the predictive model understands. This would mean that the new text would need to undergo exactly the same preprocessing steps that were performed on the text data that was used to train the model. The function for preprocessing needs to return formatted text for any input raw text. The complexity of this function depends on the steps performed on the train data. If tokenization was the only preprocessing step performed, then the function only needs to perform tokenization.</p>
			<p>Our model (<strong class="source-inline">model_rnn</strong>) was trained on IMDb reviews that were tokenized, had their case lowered, had punctuation removed, had a defined vocabulary size, and were converted into a sequence of indices. Our function/pipeline for preparing data for the RNN model needs to perform the same steps. Let's work toward creating our own function. To begin, let's create a new variable called "<strong class="source-inline">inp_review</strong>" containing the text "<em class="italic">An excellent movie</em>" using the following code. This is the variable containing the raw review text:</p>
			<p class="source-code">inp_review = "An excellent movie!"</p>
			<p>The sentiment in the text is positive. If the model is working well enough, it should predict the sentiment as positive.</p>
			<p>First, we must tokenize this text into its constituent terms, normalize its case, and remove punctuation. To do so, we need to import the <strong class="source-inline">text_to_word_sequence</strong> utility from Keras using the following code:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.text \</p>
			<p class="source-code">import text_to_word_sequence</p>
			<p>To check if it works as we expect, we can apply this to the <strong class="source-inline">inp_review</strong> variable, as shown in the following code:</p>
			<p class="source-code">text_to_word_sequence(inp_review)</p>
			<p>The tokenized sentence will be as follows:</p>
			<p class="source-code">['an', 'excellent', 'movie']</p>
			<p>We can see that it works just as expected – the case has been normalized, the sentences have been tokenized, and punctuation has been removed from the input text. The next step would be to use a defined vocabulary for the data. This would require using the same vocabulary that was used by TensorFlow when we loaded the data. The vocabulary and the term-to-index mapping can be loaded using the <strong class="source-inline">get_word_index</strong> method from the <strong class="source-inline">imdb</strong> module (that we employed to load the code). The following code can be used to load the vocabulary into a dictionary named <strong class="source-inline">word_map</strong>:</p>
			<p class="source-code">word_map = imdb.get_word_index()</p>
			<p>This dictionary contains the mapping for about 88.6 K terms that were available in the raw reviews data. We loaded the data with a vocabulary size of <strong class="source-inline">8000</strong>, thereby using the first <strong class="source-inline">8000</strong> indices from the mapping. Let's create our mapping with limited vocabulary so that we can use the same terms/indices that the training data used. We'll limit the mapping to <strong class="source-inline">8000</strong> terms by sorting the <strong class="source-inline">word_map</strong> variable on the index and picking the first <strong class="source-inline">8000</strong> terms, as follows:</p>
			<p class="source-code">vocab_map = dict(sorted(word_map.items(), \</p>
			<p class="source-code">                 key=lambda x: x[1])[:vocab_size])</p>
			<p>The vocab map will be a dictionary containing the term for index mapping for the <strong class="source-inline">8000</strong> terms in the vocabulary. Using this mapping, we'll convert the tokenized sentence into a sequence of term indices by performing a lookup for each term and returning the corresponding index. Using the following code, we'll define a function that accepts raw text, applies the <strong class="source-inline">text_to_word_sequence</strong> utility to it, performs a lookup from <strong class="source-inline">vocab_map</strong>, and returns the corresponding sequence of integers:</p>
			<p class="source-code">def preprocess(review):</p>
			<p class="source-code">    inp_tokens = text_to_word_sequence(review)</p>
			<p class="source-code">    seq = []</p>
			<p class="source-code">    for token in inp_tokens:</p>
			<p class="source-code">        seq.append(vocab_map.get(token))</p>
			<p class="source-code">    return seq</p>
			<p>We can apply this function to the <strong class="source-inline">inp_review</strong> variable, like so:</p>
			<p class="source-code">preprocess(inp_review)</p>
			<p>The output is as follows:</p>
			<p class="source-code">[32, 318, 17]</p>
			<p>This is the sequence of term indices corresponding to the raw text. Note that the data is now in the same format as the IMDb data we loaded. This sequence of indices can be fed to the RNN model (using the <strong class="source-inline">predict_classes</strong> method) to classify the sentiment, as shown in the following code. If the model is working well enough, it should predict the sentiment as positive:</p>
			<p class="source-code">model_rnn.predict_classes([preprocess(inp_review)])</p>
			<p>The output prediction is <strong class="source-inline">1</strong> (positive), just as we expected:</p>
			<p class="source-code">array([[1]])</p>
			<p>Let's apply the function to another raw text review and supply it to the model for prediction. Let's update the <strong class="source-inline">inp_review</strong> variable so that it contains the text "<strong class="source-inline">Don't watch this movie – poor acting, poor script, bad direction.</strong>" The sentiment in the review is negative. We expect the model to classify it as such:</p>
			<p class="source-code">inp_review = "Don't watch this movie"\</p>
			<p class="source-code">             " - poor acting, poor script, bad direction."</p>
			<p>Let's apply our preprocessing function to the <strong class="source-inline">inp_review</strong> variable and make a prediction using the following code:</p>
			<p class="source-code">model_rnn.predict_classes([preprocess(inp_review)])</p>
			<p>The prediction is <strong class="source-inline">0</strong>, as shown here:</p>
			<p class="source-code">array([[0]])</p>
			<p>The predicted sentiment is negative, just as we would expect the model to behave.</p>
			<p>We applied this pipeline in the form of a function on a single review, but you can very easily apply this to a whole collection of reviews to make predictions using the model. You are now ready to classify the sentiment of any new review using the RNN model we trained.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The pipeline we built here is specifically for this dataset and model. This is not a generic processing function that you can utilize for predictions from any model. The vocabulary used, the cleanup that was done, the patterns the model learned – these were all specific to this task and dataset. For any other model, you need to create your pipeline accordingly.</p>
			<p>The higher-level approach can be employed to make processing pipelines for other models too. Depending on the data, the preprocessing steps, and setting up where the model will be deployed, the pipeline can vary. All these factors also affect the steps you may want to include in the model building process. Therefore, we encourage to you to start thinking about these aspects right away when you begin the whole modeling process.</p>
			<p>We saw how to make predictions on unseen data using the trained RNN model, thereby giving us an understanding of the end-to-end process. In the next section, we'll begin working with variants of RNNs. The implementation-related ideas we've discussed so far are applicable to all the subsequent models.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor206"/>LSTMs, GRUs, and Other Variants</h1>
			<p>The idea behind plain RNNs is very powerful and the architecture has shown tremendous promise. Due to this, researchers have experimented with the architecture of RNNs to find ways to overcome the one major drawback (the vanishing gradient problem) and exploit the power of RNNs. This led to the development of LSTMs and GRUs, which have now practically replaced RNNs. Indeed, these days, when we refer to RNNs, we usually refer to LSTMs, GRUs, or their variants.</p>
			<p>This is because these variants are designed specifically to handle the vanishing gradient problem and learn long-range dependencies. Both approaches have outperformed plain RNNs significantly in most tasks around sequence modeling, and the difference is especially higher for long sequences. The paper titled <em class="italic">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em> (available at <a href="https://arxiv.org/abs/1406.1078">https://arxiv.org/abs/1406.1078</a>) performs an empirical analysis of the performance of plain RNNs, LSTMs, and GRUs. How have these approaches overcome the drawbacks of plain RNNS? We'll understand this in the next section, where we'll discuss LSTMs in detail.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor207"/>LSTMs</h2>
			<p>Let's think about this for a moment. Knowing the architecture of the plain RNN, how can we tweak it, or what can be done differently to capture long-range influences? We can't add more layers; that would be counterproductive for sure, as every added layer would compound the problem. One idea (available at <a href="https://pubmed.ncbi.nlm.nih.gov/9377276">https://pubmed.ncbi.nlm.nih.gov/9377276</a>), proposed in 1997 by Sepp Hochreiter and Jurgen Schmidhuber, is to use an explicit value (state) that does not pass through activations. If we had a cell (corresponding to a neuron for plain RNNs) value flowing freely and not through activations, this value could potentially help us model long-range dependence. This is the first key difference in an LSTM – an explicit cell state.</p>
			<p>The cell state can be thought of as a way to identify and store information over multiple time steps. Essentially, we are identifying some value as the long-term memory of the network that helps us predict the output better and taking care to retain this value as long as required.</p>
			<p>But how do we regulate the flow of this cell state? How do we decide when to update the value and by how much? For this, Hochreiter and Schmidhuber proposed the use of <em class="italic">gating mechanisms</em> as a way to regulate how and when to update the value of the cell state. This is the other key difference in an LSTM. The freely flowing cell state, together with the regulatory mechanisms, allow the LSTM to perform extremely well on longer sequences and provide it with all its predictive power.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A detailed treatment of the inner workings of the LSTM and the associated math is beyond the scope of this book. For those interested in reading further, <a href="https://packt.live/3gL42Ib">https://packt.live/3gL42Ib</a> is a good reference that provides a good visual understanding of LSTMs.</p>
			<p>Let's understand the intuition behind the working of the LSTM. The following figure shows the internals of the LSTM cell. Apart from the usual outputs, that is, the hidden state, the LSTM cell also outputs a cell "state". The hidden state holds the short-term memory, while the cell state holds the long-term memory:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B15385_06_07.jpg" alt="Figure 6.7: The LSTM cell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7: The LSTM cell</p>
			<p>This view of the internals can be intimidating, which is why we'll look at a more abstracted view, as can be seen in <em class="italic">Figure 6.8</em>. The first thing to notice is that the only operations that take place on the cell state are two linear operations – a multiplication and an addition. The cell state does not pass through any activation function. This is why we said that the cell state flows freely. This free-flow setup is also called a "Constant Error Carousel" – a moniker you <em class="italic">don't </em>need to remember.</p>
			<p>The output of the <strong class="source-inline">FORGET</strong> block is multiplied by the cell state. Because the output of this block is between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> (modeled by the sigmoid activation), a multiplication of this with the cell state will regulate how much of the previous cell state is to be forgotten. If the <strong class="source-inline">FORGET</strong> block outputs <strong class="source-inline">0</strong>, the previous cell state is completely forgotten; while for output <strong class="source-inline">1</strong>, the cell state is completely retained. Note that the inputs to the <strong class="source-inline">FORGET</strong> gate are the output from the hidden layer from the previous time step (<strong class="source-inline">h</strong><span class="subscript">t-1</span>) and the new input at the present time step, <strong class="source-inline">x</strong><span class="subscript">t</span> (for a layer deep in the network, this could be the output from the previous hidden layer):</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B15385_06_08.jpg" alt="Figure 6.8: Abstracted view of the LSTM cell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8: Abstracted view of the LSTM cell</p>
			<p>In the preceding figure, we can see that after the cell state is multiplied by the <strong class="source-inline">FORGET</strong> block's result, the next decision is how much to update the cell state by. This comes from the <strong class="source-inline">UPDATE</strong> block's output, which is added (note the plus sign) to the processed cell state. This way, the processed cell state is updated. That's all the operations that are performed on the previous cell state, <strong class="source-inline">(C</strong><span class="subscript">t-1</span><strong class="source-inline">)</strong>, to give us the new cell state, <strong class="source-inline">(C</strong><span class="subscript">t</span><strong class="source-inline">)</strong>, as an output. This is how the long-term memory of the cell is regulated. The cell also needs to update the hidden state. This operation takes place in the <strong class="source-inline">OUTPUT</strong> block and is pretty much the same as the update in a plain RNN. The only difference is that the explicit cell state is multiplied by the output from the sigmoid to form the final hidden state, <strong class="source-inline">h</strong><span class="subscript">t</span>.</p>
			<p>Now that we understand the individual blocks/gates, let's see them marked on the following detailed figure. This should clarify how these gating mechanisms come together to regulate the flow of information in an LSTM:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B15385_06_09.jpg" alt="Figure 6.9: The LSTM cell explained&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9: The LSTM cell explained</p>
			<p>To make this example more concrete, let's take a look at the following figure and understand how the cell state is updated. We can assume the previous cell state, <strong class="source-inline">(C</strong><span class="subscript">t-1</span><strong class="source-inline">)</strong>, was <strong class="source-inline">5</strong>. How much of this value should be propagated is decided by the output of the <strong class="source-inline">FORGET</strong> gate. The output value of the <strong class="source-inline">FORGET</strong> gate is multiplied by the previous cell state, <strong class="source-inline">C</strong><span class="subscript">t-1</span>. In this case, the output of the forget block is <strong class="source-inline">0.5</strong>, resulting in <strong class="source-inline">2.5</strong> as the processed cell state being passed. This value (<strong class="source-inline">2.5</strong>) then encounters the addition from the <strong class="source-inline">UPDATE</strong> gate. Since the <strong class="source-inline">UPDATE</strong> gate output value of <strong class="source-inline">-0.8</strong>, the result of the addition is <strong class="source-inline">1.7</strong>. This is the final, updated cell state, <strong class="source-inline">C</strong><span class="subscript">t</span>, that is passed to the next time step:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B15385_06_10.jpg" alt="Figure 6.10: LSTM cell state update example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10: LSTM cell state update example</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor208"/>Parameters in an LSTM</h1>
			<p>LSTMs are built on plain RNNs. If you simplified the LSTM and removed all the gates, retaining only the tanh function for the hidden state update, you would have a plain RNN. The number of activations that the information – the new input data at time <strong class="source-inline">t</strong> and the previous hidden state at time <strong class="source-inline">t-1</strong> (<strong class="source-inline">x</strong><span class="subscript">t</span> and <strong class="source-inline">h</strong><span class="subscript">t-1</span>) – passes through in an LSTM is four times the number that it passes through in a plain RNN. The activations are applied once in the forget gate, twice in the update gate, and once in the output gate. The number of weights/parameters in an LSTM is, therefore, four times the number of parameters in a plain RNN.</p>
			<p>In <em class="italic">Chapter 5</em>, <em class="italic">Deep Learning For Sequences,</em> in the section titled <em class="italic">Parameters in an RNN</em>, we calculated the number of parameters in a plain RNN and saw that we already have a quite a few parameters to work with (<strong class="source-inline">n</strong><span class="superscript">2</span><strong class="source-inline"> + nk + nm</strong>, where <strong class="source-inline">n</strong> is the number of neurons in the hidden layer, <strong class="source-inline">m</strong> is the number of inputs, and <strong class="source-inline">k</strong> is the dimension of the output layer). With LSTMs, we saw that the number is four times this. Needless to say, we have a lot of parameters in an LSTM, and that isn't necessarily a good thing, especially when working with smaller datasets.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor209"/>Exercise 6.02: LSTM-Based Sentiment Classification Model</h2>
			<p>In this exercise, we will build a simple LSTM-based model to predict sentiment on our data. We will continue with the same setup we used previously (that is, the number of cells, embedding dimensions, dropout, and so on). Thus, you must continue this exercise in the same Jupyter Notebook. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the LSTM layer from Keras <strong class="source-inline">layers</strong>:<p class="source-code">from tensorflow.keras.layers import LSTM</p></li>
				<li>Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add a 40% spatial dropout:<p class="source-code">model_lstm = Sequential()</p><p class="source-code">model_lstm.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_lstm.add(SpatialDropout1D(0.4))</p></li>
				<li>Add an LSTM layer with <strong class="source-inline">32</strong> cells:<p class="source-code">model_lstm.add(LSTM(32))</p></li>
				<li>Add the dropout (<strong class="source-inline">40%</strong> dropout) and dense layers, compile the model, and print the model summary:<p class="source-code">model_lstm.add(Dropout(0.4))</p><p class="source-code">model_lstm.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_lstm.compile(loss='binary_crossentropy', \</p><p class="source-code">                   optimizer='rmsprop', metrics=['accuracy'])</p><p class="source-code">model_lstm.summary()</p><p>The model summary is as follows:</p><div id="_idContainer194" class="IMG---Figure"><img src="image/B15385_06_11.jpg" alt="Figure 6.11: Summary of the LSTM model&#13;&#10;"/></div><p class="figure-caption">Figure 6.11: Summary of the LSTM model</p><p>We can see from the model summary that the number of parameters in the LSTM layer is <strong class="source-inline">8320</strong>. A quick check can confirm that this is exactly four times the number of parameters in the plain RNN layer we saw in <em class="italic">Exercise 6.01,</em> <em class="italic">Building and Training an RNN Model for Sentiment Classification</em>, which is in line with our expectations. Next, let's fit the model on the training data.</p></li>
				<li>Fit on the training data for <strong class="source-inline">5</strong> epochs (this gives us the best result for the model) with a batch size of <strong class="source-inline">128</strong>:<p class="source-code">history_lstm = model_lstm.fit(X_train, y_train, \</p><p class="source-code">                              batch_size=128, \</p><p class="source-code">                              validation_split=0.2, \</p><p class="source-code">                              epochs=5)</p><p>The output from the training process is as follows:</p><div id="_idContainer195" class="IMG---Figure"><img src="image/B15385_06_12.jpg" alt="Figure 6.12: LSTM training output&#13;&#10;"/></div><p class="figure-caption">Figure 6.12: LSTM training output</p><p>Notice that training the LSTM took much longer than it does with plain RNNs. Again, considering the architecture of the LSTM and the sheer number of parameters, this was expected. Also, note that the validation accuracy is significantly higher than that of the plain RNN. Let's check the performance on the test data in terms of the accuracy score.</p></li>
				<li>Make predictions on the test set and print the accuracy score:<p class="source-code">y_test_pred = model_lstm.predict_classes(X_test)</p><p class="source-code">print(accuracy_score(y_test, y_test_pred))</p><p>The accuracy is printed out as follows:</p><p class="source-code">0.87032</p><p>The accuracy we got (87%) is a significant improvement from the accuracy we got using plain RNNs (85.1%). It looks like the extra parameters and the extra predictive power from the cell state came in handy for our task.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Oa2trm">https://packt.live/2Oa2trm</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how we can employ LSTMs for sentiment classification of text. The training time was significantly higher, and the number of parameters is higher too. But in the end, even this simple architecture (without any hyperparameter tuning) gave better results than the plain RNN. You are encouraged to tune the hyperparameters further to get the most out of the powerful LSTM architecture.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor210"/>LSTM versus Plain RNNs</h1>
			<p>We saw that LSTMs are built on top of plain RNNs, with the primary goal of addressing the vanishing gradient problem to enable modeling long-range dependencies. Looking at the following figure tells us that a plain RNN passes only the hidden state (the short-term memory), whereas an LSTM passes the hidden state as well as the explicit cell state (the long-term memory), giving it more power. So, when the term "<strong class="source-inline">good</strong>" is being processed in the LSTM, the recurrent layer also passes the cell states holding the long-term memory:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B15385_06_13.jpg" alt="Figure 6.13: Plain RNNs (left) and LSTMs (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13: Plain RNNs (left) and LSTMs (right)</p>
			<p>In practice, does this mean that you always need an LSTM? The answer to this question, as with most questions in data science and especially deep learning, is, "it depends". To understand these considerations, we need to understand the benefits and drawbacks of LSTMs compared to plain RNNs.</p>
			<p><strong class="bold">Benefits of LSTMs:</strong></p>
			<ul>
				<li>More powerful, as it uses more parameters and an explicit cell state</li>
				<li>Models long-range dependencies better</li>
			</ul>
			<p><strong class="bold">Drawbacks of LSTMs:</strong></p>
			<ul>
				<li>Many more parameters</li>
				<li>Takes more time to train</li>
				<li>More prone to overfitting</li>
			</ul>
			<p>If you have long sequences to work with, LSTM would be a good choice. If you have a small dataset and the sequences you are dealing with are short (&lt;10), then you're probably okay to use a plain RNN, owing to there being a lower number of parameters (although you could also try LSTMs, making sure to use regularization to avoid overfitting). A larger dataset with long sequences would probably extract the most out of powerful models such as LSTMs. Note that training LSTMs is computationally expensive and time-consuming, so if you have an extremely large dataset, training LSTMs may not be the most practical approach. Of course, all these statements should serve merely as guidance – the best approach would be what works best for your data and your task.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor211"/>Gated Recurrence Units</h1>
			<p>In the previous section, we saw that LSTMs have a lot of parameters and seem much more complex than the regular RNN. You may be wondering, are all these apparent complications really necessary? Can the LSTM be simplified a little without it losing significant predictive power? Researchers wondered the same for a while, and in 2014, Kyunghyun Cho and their team proposed the GRU as an alternative to LSTMs in their paper (<a href="https://arxiv.org/abs/1406.1078">https://arxiv.org/abs/1406.1078</a>) on machine translation.</p>
			<p>GRUs are simplified forms of LSTMs and aim at reducing the number of parameters while retaining the power of the LSTM. In tasks around speech modeling and language modeling, GRUs provide the same performance as LSTMs, but with fewer parameters and faster training times.</p>
			<p>One major simplification done in a GRU is the omission of the explicit cell state. This sounds counterintuitive considering that the freely flowing cell state was what gave the LSTM its power, right? What really gave LSTMs all that power was the freely flowing nature of the cell state and not the cell state itself? Indeed, if the cell state were also subject to activations, LSTMs probably wouldn't have had the success they did:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B15385_06_14.jpg" alt="Figure 6.14: Gated Recurrent Unit&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14: Gated Recurrent Unit</p>
			<p>So, the freely flowing values is the key differentiating idea. GRUs retain this idea, by allowing the hidden state to flow freely. Let's look at the preceding figure to understand what this means. GRUs allow the hidden state to pass through freely. Another way to look at this is that GRUs effectively bring the idea of the cell state (as in LSTMs) to the hidden state.</p>
			<p>We still need to regulate the flow of the hidden state, though, so we still have gates. GRUs combine the forget gate and update gate into a single update gate. To understand the motivation behind this, consider this – if we forget a cell state, and don't update it, what are we really doing? Maybe there is merit in having a single update operation. This is the second major difference in the architecture.</p>
			<p>As a result of these two changes, GRUs have the data pass through three activations instead of four, as in LSTMs, reducing the number of parameters. While GRUs still have three times the number of parameters of a plain RNN, these have 75% of the parameters of LSTMs, and that is a welcome change. We still have information flowing freely through the network and this should allow us to model long-range dependencies.</p>
			<p>Let's see how a GRU-based model performs on our task of sentiment classification.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor212"/>Exercise 6.03: GRU-Based Sentiment Classification Model</h2>
			<p>In this exercise, we will build a simple GRU-based model to predict sentiments in our data. We will continue with the same setup that we used previously (that is, the number of cells, embedding dimensions, dropout, and so on). Using GRUs instead of LSTMs in the model is as simple as replacing "<strong class="source-inline">LSTM</strong>" with "<strong class="source-inline">GRU</strong>" when adding the layer. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">GRU</strong> layer from Keras <strong class="source-inline">layers</strong>:<p class="source-code">from tensorflow.keras.layers import GRU</p></li>
				<li>Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add 40% spatial dropout:<p class="source-code">model_gru = Sequential()</p><p class="source-code">model_gru.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_gru.add(SpatialDropout1D(0.4))</p></li>
				<li>Add a GRU layer with 32 cells. Set the <strong class="source-inline">reset_after</strong> parameter to <strong class="source-inline">False</strong> (this is a minor TensorFlow 2 implementation detail in order to maintain consistency with the implementation of plain RNNs and LSTMs):<p class="source-code">model_gru.add(GRU(32, reset_after=False))</p></li>
				<li>Add the dropout (40%) and dense layers, compile the model, and print the model summary:<p class="source-code">model_gru.add(Dropout(0.4))</p><p class="source-code">model_gru.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_gru.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer='rmsprop', metrics=['accuracy'])</p><p class="source-code">model_gru.summary()</p><p>The model summary is as follows:</p><div id="_idContainer198" class="IMG---Figure"><img src="image/B15385_06_15.jpg" alt="Figure 6.15: Summary of the GRU model&#13;&#10;"/></div><p class="figure-caption">Figure 6.15: Summary of the GRU model</p><p>From the summary of the GRU model, we can see that the number of parameters in the GRU layer is <strong class="source-inline">6240</strong>. You can check that this is exactly three times the number of parameters in the plain RNN layer we saw in <em class="italic">Exercise 6.01,</em> <em class="italic">Building and Training an RNN Model for Sentiment Classification</em>, and <strong class="source-inline">0.75</strong> times the parameters of the LSTM layer we saw in <em class="italic">Exercise 6.02,</em> <em class="italic">LSTM-Based Sentiment Classification Model</em> – again, this is in line with our expectations. Next, let's fit the model on the training data.</p></li>
				<li>Fit on the training data for four epochs (which gives us the best result):<p class="source-code">history_gru = model_gru.fit(X_train, y_train, \</p><p class="source-code">                            batch_size=128, \</p><p class="source-code">                            validation_split=0.2, \</p><p class="source-code">                            epochs = 4)</p><p>The output from the training process is as follows:</p><div id="_idContainer199" class="IMG---Figure"><img src="image/B15385_06_16.jpg" alt="Figure 6.16: GRU training output&#13;&#10;"/></div><p class="figure-caption">Figure 6.16: GRU training output</p><p>Notice that training the GRUs also took much longer than plain RNNs but was faster than LSTMs. The validation accuracy is better than the plain RNN and seems close to that of the LSTM. Let's see how the model fares on the test data.</p></li>
				<li>Make predictions on the test set and print the accuracy score:<p class="source-code">y_test_pred = model_gru.predict_classes(X_test)</p><p class="source-code">accuracy_score(y_test, y_test_pred)</p><p>The accuracy is printed out as follows:</p><p class="source-code">0.87156</p><p>We can see that the accuracy of the GRU model (87.15%) is very similar to that of the LSTM (87%) and is higher than the plain RNN. This is an important point – GRUs are simplifications of LSTMs that aim to provide similar accuracy with fewer parameters. Our exercise here shows this is true.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Oa2trm">https://packt.live/2Oa2trm</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how we can employ GRUs for the sentiment classification of text. The training time was slightly lower than the LSTM model and the number of parameters is lower. Even this simple architecture (without any hyperparameter tuning) gave better results than the plain RNN model and gave results similar to the LSTM model.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor213"/>LSTM versus GRU</h2>
			<p>So, which one should you choose? The LSTM has more parameters and an explicit cell state designed to store long-term memory. The GRU has fewer parameters, which means faster training, and also has a free-flowing cell state to allow it to model long-range dependencies.</p>
			<p>An empirical evaluation (available at <a href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a>) by Junyoung Chung, Yoshua Bengio, and their team in 2014 on music-modeling and speech-modeling tasks showed that both LSTMs and GRUs are markedly superior to plain RNNs. They also found that GRUs are on par with LSTMs in terms of performance. They remarked that tuning hyperparameters such as layer size is probably more important than choosing between LSTM and GRU.</p>
			<p>In 2018, Gail Weiss, Yoav Goldberg, and their team demonstrated and concluded that LSTMs outperform GRUs in tasks that require unbounded counting, that is, those that need to handle sequences of an arbitrary length. The Google Brain team, in 2018, also showed that the performance of LSTMs is superior to GRUs when it comes to machine translation. This leads us to think that the extra power that LSTMs bring may be very useful in certain applications.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor214"/>Bidirectional RNNs</h1>
			<p>The RNN models we've just looked at – LSTMs, GRUs – are powerful indeed and provide extremely good results when it comes to sequence-processing tasks. Now, let's discuss how to make them even more powerful, and the methods that yield the amazing successes in deep learning that you have been hearing about.</p>
			<p>Let's begin with the idea of bidirectional RNNs. The idea applies to all variants of RNNs, including, but not limited to, LSTMs and GRUs. Bidirectional RNNs process the sequence in both directions, allowing the network to have both backward and forward information about the sequence, providing it with a much richer context:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B15385_06_17.jpg" alt="Figure 6.17: Bidirectional LSTM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17: Bidirectional LSTM</p>
			<p>The bidirectional model essentially employs two RNNs in parallel – one as the "<strong class="bold">forward layer</strong>" and the other as the "<strong class="bold">backward layer</strong>". As shown in the preceding figure, the forward layer processes the sequence in the order of its elements. For the sentence, "<em class="italic">Life is good</em>", the forward layer will process the term "<em class="italic">Life</em>" first, followed by "<em class="italic">is</em>", followed by "<em class="italic">good</em>" – no different from the usual RNN layer. The backward layer reverses this order – it processes "good" first, followed by "is", followed by "Life". At each step, the states of the forward and the backward layers are concatenated to form the output.</p>
			<p>What kind of tasks benefit the most from this architecture? Looking at both sides of the context helps resolve any ambiguity about the term at hand. When we read a statement such as "<em class="italic">The stars</em>", we're not sure as to what "<em class="italic">stars</em>" we're reading about – is it the stars in the sky or movie stars? But when we also see the terms coming later in the sequence and read "<em class="italic">The stars at the movie premiere</em>", we're confident that this sentence is about movie stars. The tasks that can benefit the most from such a setup are machine translation, parts-of-speech tagging, named entity recognition, and word prediction tasks, to list a few. Bidirectional RNNs show performance gains for general text classification tasks as well. Let's apply a bidirectional LSTM-based model to our sentiment classification task.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor215"/>Exercise 6.04: Bidirectional LSTM-Based Sentiment Classification Model</h2>
			<p>In this exercise, we will use bidirectional LSTMs to predict sentiment on our data. We'll be using the bidirectional wrapper from Keras to create bidirectional layers on LSTMs (you could create a bidirectional GRU model by simply replacing <strong class="source-inline">LSTM</strong> with <strong class="source-inline">GRU</strong> in the wrapper). Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">Bidirectional</strong> layer from Keras <strong class="source-inline">layers</strong>. This layer is essentially a wrapper you can use around other RNNs:<p class="source-code">from tensorflow.keras.layers import Bidirectional</p></li>
				<li>Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add a 40% spatial dropout:<p class="source-code">model_bilstm = Sequential()</p><p class="source-code">model_bilstm.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_bilstm.add(SpatialDropout1D(0.4))</p></li>
				<li>Add a <strong class="source-inline">Bidirectional</strong> wrapper to an LSTM layer with <strong class="source-inline">32</strong> cells:<p class="source-code">model_bilstm.add(Bidirectional(LSTM(32)))</p></li>
				<li>Add the dropout (40%) and dense layers, compile the model, and print the model summary:<p class="source-code">model_bilstm.add(Dropout(0.4))</p><p class="source-code">model_bilstm.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_bilstm.compile(loss='binary_crossentropy', \</p><p class="source-code">                     optimizer='rmsprop', metrics=['accuracy'])</p><p class="source-code">model_bilstm.summary()</p><p>The summary is as follows:</p><div id="_idContainer201" class="IMG---Figure"><img src="image/B15385_06_18.jpg" alt="Figure 6.18: Summary of the bidirectional LSTM model&#13;&#10;"/></div><p class="figure-caption">Figure 6.18: Summary of the bidirectional LSTM model</p><p>Note the parameters of the model shown in the preceding screenshot. Not surprisingly, the bidirectional LSTM layer has <strong class="source-inline">16640</strong> parameters – twice the number of parameters that the LSTM layer (<strong class="source-inline">8320</strong> parameters) had in <em class="italic">Exercise 6.02, LSTM-Based Sentiment Classification Model</em>. This is eight times the parameters of the plain RNN. Next, let's fit the model on the training data.</p></li>
				<li>Fit the training data for four epochs with a batch size of <strong class="source-inline">128</strong>:<p class="source-code">history_bilstm = model_bilstm.fit(X_train, y_train, \</p><p class="source-code">                                  batch_size=128, \</p><p class="source-code">                                  validation_split=0.2, \</p><p class="source-code">                                  epochs = 4)</p><p>The output from training is as follows:</p><div id="_idContainer202" class="IMG---Figure"><img src="image/B15385_06_19.jpg" alt="Figure 6.19: Bidirectional LSTM training output&#13;&#10;"/></div><p class="figure-caption">Figure 6.19: Bidirectional LSTM training output</p><p>Notice that, as we expect, training bidirectional LSTMs takes much longer than regular LSTMs, and several times longer than plain RNNs. The validation accuracy seems to be closer to the LSTM's accuracy.</p></li>
				<li>Make predictions on the test set and print the accuracy score:<p class="source-code">y_test_pred = model_bilstm.predict_classes(X_test)</p><p class="source-code">accuracy_score(y_test, y_test_pred)</p><p>The accuracy is as follows:</p><p class="source-code">0.877</p></li>
			</ol>
			<p>The accuracy we received here (87.7%) is a slight improvement over the LSTM model's accuracy, which was 87%. Again, you can tune the hyperparameters even further to extract the most out of this powerful architecture. Note that we had twice the number of parameters compared to the LSTM model, and eight times the parameters of the plain RNN. Working with a large dataset may make the performance differences bigger.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Oa2trm">https://packt.live/2Oa2trm</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor216"/>Stacked RNNs</h1>
			<p>Now, let's look at another approach we can follow to extract more power from RNNs. In all the models we've looked at in this chapter, we've used a single layer for the RNN layer (plain RNN, LSTM, or GRU). Going deeper, that is, adding more layers, has typically helped us for feedforward networks so that we can learn more complex patterns/features in the deeper layers. There is merit in trying this idea for recurrent networks. Indeed, stacked RNNs do seem to give us more predictive power.</p>
			<p>The following figure illustrates a simple two-layer stacked LSTM model. Stacking RNNs simply means feeding the output of one RNN layer to another RNN layer. The RNN layers can output sequences (that is, output at each time step) and these can be fed, like any input sequence, into the subsequent RNN layer. In terms of implementation through code, stacking RNNs is as simple as returning sequences from one layer, and providing this as input to the next RNN layer, that is, the immediate next layer:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B15385_06_20.jpg" alt="Figure 6.20: Two-layer stacked RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20: Two-layer stacked RNN</p>
			<p>Let's see the stacked RNN (LSTM) in action by using it on our sentiment classification task.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor217"/>Exercise 6.05: Stacked LSTM-Based Sentiment Classification Model</h2>
			<p>In this exercise, we will "go deeper" into the RNN architecture by stacking two LSTM layers to predict sentiment in our data. We will continue with the same setup that we used in the previous exercises (the number of cells, embedding dimensions, dropout, and so on) for the other layers. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add 40% spatial dropout:<p class="source-code">model_stack = Sequential()</p><p class="source-code">model_stack.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_stack.add(SpatialDropout1D(0.4))</p></li>
				<li>Add an LSTM layer with <strong class="source-inline">32</strong> cells. Make sure to specify <strong class="source-inline">return_sequences</strong> as <strong class="source-inline">True</strong> in the LSTM layer. This will return the output of the LSTM at each time step, which can then be passed to the next LSTM layer:<p class="source-code">model_stack.add(LSTM(32, return_sequences=True))</p></li>
				<li>Add another LSTM layer with <strong class="source-inline">32</strong> cells. This time, you don't need to return sequences. You can either specify the <strong class="source-inline">return_sequences</strong> option as <strong class="source-inline">False</strong> or skip it altogether (the default value is <strong class="source-inline">False</strong>):<p class="source-code">model_stack.add(LSTM(32, return_sequences=False))</p></li>
				<li>Add the dropout (50% dropout; this is higher since we're building a more complex model) and dense layers, compile the model, and print the model summary:<p class="source-code">model_stack.add(Dropout(0.5))</p><p class="source-code">model_stack.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_stack.compile(loss='binary_crossentropy', \</p><p class="source-code">                    optimizer='rmsprop', \</p><p class="source-code">                    metrics=['accuracy'])</p><p class="source-code">model_stack.summary()</p><p>The summary is as follows:</p><div id="_idContainer204" class="IMG---Figure"><img src="image/B15385_06_21.jpg" alt="Figure 6.21: Summary of the stacked LSTM model&#13;&#10;"/></div><p class="figure-caption">Figure 6.21: Summary of the stacked LSTM model</p><p>Note that the stacked LSTM model has the same number of parameters as the bidirectional model. Let's fit the model on the training data.</p></li>
				<li>Fit the model on the training data for four epochs:<p class="source-code">history_stack = model_stack.fit(X_train, y_train, \</p><p class="source-code">                                batch_size=128, \</p><p class="source-code">                                validation_split=0.2, \</p><p class="source-code">                                epochs = 4)</p><p>The output from training is as follows:</p><div id="_idContainer205" class="IMG---Figure"><img src="image/B15385_06_22.jpg" alt="Figure 6.22: Stacked LSTM training output&#13;&#10;"/></div><p class="figure-caption">Figure 6.22: Stacked LSTM training output</p><p>Training stacked LSTMs took less time than training bidirectional LSTMs. The validation accuracy seems to be close to that of the bidirectional LSTM model.</p></li>
				<li>Make predictions on the test set and print the accuracy score:<p class="source-code">y_test_pred = model_stack.predict_classes(X_test)</p><p class="source-code">accuracy_score(y_test, y_test_pred)</p><p>The accuracy is printed out as follows:</p><p class="source-code">0.87572</p></li>
			</ol>
			<p>The accuracy of <strong class="source-inline">87.6%</strong> is an improvement over the LSTM model (<strong class="source-inline">87%</strong>) and is practically the same as that of the bidirectional model (<strong class="source-inline">87.7%</strong>). This is a somewhat significant improvement over the performance of the regular LSTM model, considering that we're working with a rather small dataset. The larger your dataset is, the more you can benefit from these sophisticated architectures. Try tuning the hyperparameters in order to get the most out of this powerful architecture.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZPO2g">https://packt.live/31ZPO2g</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Oa2trm">https://packt.live/2Oa2trm</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor218"/>Summarizing All the Models</h1>
			<p>In this chapter, we've looked at different variants of RNNs – from plain RNNs to LSTMs to GRUs. We also looked at the bidirectional approach and the stacking approach to using RNNs. Now is a good time to take a holistic look at things and make a comparison between the models. Let's look at the following table, which compares the five models in terms of parameters, training time, and performance (that is, the level of accuracy on our dataset):</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B15385_06_23.jpg" alt="Figure 6.23: Comparing the five models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.23: Comparing the five models</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As mentioned earlier in the chapter, while working through the practical elements, you may have obtained values different from the ones shown above; however, the test accuracies you obtain should largely agree with ours. If the model's performance is very different, you may want to tweak the number of epochs.</p>
			<p>Plain RNNs are the lowest on parameters and have the lowest training times but have the lowest accuracy of all the models. This is in line with our expectations – we are dealing with sequences that are 200 characters in length, and we know not to expect much from plain RNNs, and that gated RNNs (LSTMs, GRUs) are more suitable. Indeed, LSTMs and GRUs do perform significantly better than plain RNNs. But the accuracy comes at the cost of significantly higher training times, and several times the parameters, making these models more prone to overfitting.</p>
			<p>The approaches of stacking and using bidirectional processing seem to provide an incremental benefit in terms of predictive power, but this is at the cost of significantly higher training times and several times the parameters. The stacked and bidirectional approaches gave us the highest accuracy, even on this small dataset.</p>
			<p>While the performance results are specific to our dataset, the gradation in performance we see here is fairly common. The stacked and bidirectional models are present in many of the solutions today that provide state-of-the-art results in various tasks. With a larger dataset and when working with much longer sequences, we would expect the differences in model performances to be larger.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor219"/>Attention Models</h1>
			<p>Attention models were first introduced in late 2015 by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio in their influential and seminal paper (<a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>) that demonstrated the state-of-the-art results of English-to-French translation. Since then, this idea has been used for many sequence-processing tasks with great success, and attention models are becoming increasingly popular. While a detailed explanation and mathematical treatment is beyond the scope of this book, let's understand the intuition behind the idea that is considered by many big names in the field of deep learning as a significant development in our approach to sequence modeling.</p>
			<p>The intuition behind attention can be best understood using an example from the task it was developed for – translation. When a novice human translates a long sentence between languages, they don't translate the entire sentence in one go. They break the original sentence down into smaller, manageable chunks, thereby generating a translation for each chunk sequentially. For each chunk, there would be a part that is the most important for the translation task, that is, where you need to pay the most attention:</p>
			<p> </p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/B15385_06_24.jpg" alt="Figure 6.24: Idea of attention simplified&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.24: Idea of attention simplified</p>
			<p>The preceding figure shows a simple example where we're translating the sentence, "<strong class="source-inline">Azra is moving to Berlin</strong>", into French. The French translation is, "<strong class="source-inline">Azra déménage à Berlin</strong>". To get the first term in the French translation, "<strong class="source-inline">Azra</strong>", we need to pay attention primarily to the first term in the original sentence (underscored by a light gray line) and maybe a bit to the second (underscored by a dark gray line) – these terms get higher importance (weight). The remaining parts of the sentence aren't relevant. Similarly, to generate the term "<strong class="source-inline">déménage</strong>" in the output, we need to pay attention to the terms "<strong class="source-inline">is</strong>" and "<strong class="source-inline">moving</strong>". The importance of each term toward the output term is expressed as weights. This is known as "<strong class="bold">alignment</strong>".</p>
			<p>These alignments can be seen in the following figure, which was sourced from the original paper (<a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>). It beautifully demonstrates what the model identified as most important for each term in the output. A lighter color in a cell in the grid means a higher weight for the corresponding input term in the column. We can see that for the output term "<strong class="source-inline">marin</strong>", the model correctly identifies "<strong class="source-inline">marine</strong>" as the most important input term to pay attention to. Similarly, it has identified "<strong class="source-inline">environment</strong>" as the most important term for "<strong class="source-inline">environnement</strong>", "<strong class="source-inline">known</strong>" for "<strong class="source-inline">connu</strong>", and so on. Pretty neat, isn't it?</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B15385_06_25.jpg" alt="Figure 6.25: The alignment learned by the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.25: The alignment learned by the model</p>
			<p>While attention models were originally designed for translation tasks, the models have been employed on a variety of other tasks with good success. That being said, note that the attention models have a very high number of parameters. The models are typically employed on bidirectional LSTM layers and add additional weights for the importance values. A high number of parameters makes the model more prone to overfitting, which means they will need much larger datasets to utilize their power.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor220"/>More Variants of RNNs</h1>
			<p>We've seen quite a few variations of RNNs in this chapter – covering all the prominent ones and the major upcoming (in terms of popularity) variations. Sequence modeling and its associated architectures are a hot area of research, and we see plenty of developments coming in every year. Many variants aim to make lighter models with fewer parameters that aren't as hardware hungry as current RNNs. <strong class="bold">Clockwork RNNs</strong> (<strong class="bold">CWRNNs</strong>) are a recent development and show great success. There are also <strong class="bold">Hierarchal Attention Networks</strong>, built on the idea of attention, but ultimately also propose that you shouldn't use RNNs as building blocks. There's a lot going on in this exciting area, so keep your eyes and ears open for the next big idea.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor221"/>Activity 6.01: Sentiment Analysis of Amazon Product Reviews</h2>
			<p>So far, we've looked at the variants of RNNs and used them to predict sentiment on movie reviews from the IMDb dataset. In this activity, we will build a sentiment classification model on Amazon product reviews. The data contains reviews for several categories of products. The original dataset, available at <a href="https://snap.stanford.edu/data/web-Amazon.html">https://snap.stanford.edu/data/web-Amazon.html</a>, is huge; therefore, we have sampled 50,000 reviews for this activity.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The sampled dataset, which has been split into train and test sets, can be found at <a href="https://packt.live/3iNTUjN">https://packt.live/3iNTUjN</a>.</p>
			<p>This activity will bring together the concepts and methods we discussed in this chapter and those discussed in <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, and <em class="italic">Chapter 5, Deep Learning for Sequences</em>. You will begin by performing a detailed text cleanup and conduct preprocessing to get it ready for the deep learning model. You will also use embeddings to represent text. For the prediction part, you will employ stacked LSTMs (two layers) and two dense layers.</p>
			<p>For convenience (and awareness), you will also utilize the <strong class="source-inline">Tokenizer</strong> API from TensorFlow (Keras) to convert the cleaned-up text into the corresponding sequences. The <strong class="source-inline">Tokenizer</strong> combines the function of the tokenizer from <strong class="source-inline">NLTK</strong> with the <strong class="source-inline">vectorizer</strong> (<strong class="source-inline">CountVectorizer</strong>/ <strong class="source-inline">TfIdfVectorizer</strong>) by tokenizing the text first and then learning a vocabulary from a dataset. Let's see it in action by creating some toy data using the following command:</p>
			<p class="source-code">sents = ["life is good", "good life", "good"]</p>
			<p>The <strong class="source-inline">Tokenizer</strong> can be imported, instantiated, and fit on the toy data using the following commands:</p>
			<p class="source-code">tok = Tokenizer()</p>
			<p class="source-code">tok.fit_on_texts(sents)</p>
			<p>Once the vocabulary has been trained on the toy data (index learned for each term), we can convert the input text into a corresponding sequence of indices for the terms. Let's convert the toy data into the corresponding sequences of indices using the <strong class="source-inline">texts_to_sequences</strong> method of the tokenizer:</p>
			<p class="source-code">tok.texts_to_sequences(sents)</p>
			<p>We'll get the following output:</p>
			<p class="source-code">[[2, 3, 1], [1, 2], [1]]</p>
			<p>Now, the data format is the same as that of the IMDb dataset we've used throughout this chapter, and it can be processed in a similar fashion.</p>
			<p>With this, you are now ready to get started. The following are the high-level steps you will need to follow to complete this activity:</p>
			<ol>
				<li value="1">Read in the data files for the train and test sets (<strong class="source-inline">Amazon_reviews_train.csv</strong> and <strong class="source-inline">Amazon_reviews_test.csv</strong>). Examine the shapes of the datasets and print out the top five records from the train data.</li>
				<li>For convenience when it comes to processing, separate the raw text and the labels for the train and test set. Print the first two reviews from the train text. You should have the following four variables: <strong class="source-inline">train_raw</strong> comprising the raw text for the train data, <strong class="source-inline">train_labels</strong> with labels for the train data, <strong class="source-inline">test_raw</strong> containing raw text for the test data, and <strong class="source-inline">test_labels</strong> with labels for the test data.</li>
				<li>Normalize the case and tokenize the test and train texts using NLTK's <strong class="source-inline">word_tokenize</strong> (after importing it, of course – hint: use list comprehension for cleaner code). Print the first review from the train data to check if the tokenization worked. Download <strong class="source-inline">punkt</strong> from NLTK if you haven't used the tokenizer before.</li>
				<li>Import <strong class="source-inline">stopwords</strong> (built in to NLTK) and punctuation from the string module. Define a function (<strong class="source-inline">drop_stop</strong>) to remove these tokens from any input tokenized sentence. Download <strong class="source-inline">stopwords</strong> from NLTK if you haven't used it before.</li>
				<li>Using the defined function (<strong class="source-inline">drop_stop</strong>), remove the redundant stop words from the train and the test texts. Print the first review of the processed train texts to check if the function worked.</li>
				<li>Using <strong class="source-inline">Porter Stemmer</strong> from NLTK, stem the tokens for the train and test data.</li>
				<li>Create the strings for each of the train and text reviews. This will help us work with the utilities in Keras to create and pad the sequences. Create the <strong class="source-inline">train_texts</strong> and <strong class="source-inline">test_texts</strong> variables. Print the first review from the processed train data to confirm it.</li>
				<li>From the Keras preprocessing utilities for text (<strong class="source-inline">keras.preprocessing.text</strong>), import the <strong class="source-inline">Tokenizer</strong> module. Define a vocabulary size of <strong class="source-inline">10000</strong> and instantiate the tokenizer with this vocabulary.</li>
				<li>Fit the tokenizer on the train texts. This works just like <strong class="source-inline">CountVectorizer</strong> did in <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, and trains the vocabulary. After fitting, use the <strong class="source-inline">texts_to_sequences</strong> method of the tokenizer on the train and test sets to create the sequences for them. Print the sequence for the first review in the train data.</li>
				<li>We need to find the optimal length of the sequences to process in the model. Get the length of the reviews from the train set into a list and plot the histogram of the lengths.</li>
				<li>The data is now in the same format as the IMDb data we used in the chapter. Using a sequence length of <strong class="source-inline">100</strong> (define the <strong class="source-inline">maxlen = 100</strong> variable), use the <strong class="source-inline">pad_sequences</strong> method from the <strong class="source-inline">sequence</strong> module in Keras' preprocessing utilities (<strong class="source-inline">keras.preprocessing.sequence</strong>) to limit the sequences to <strong class="source-inline">100</strong> for both the train and test data. Check the shape of the result for the train data.</li>
				<li>To build the model, import all the necessary layers from Keras (<strong class="source-inline">embedding</strong>, <strong class="source-inline">spatialdropout</strong>, <strong class="source-inline">LSTM</strong>, <strong class="source-inline">dropout</strong>, and <strong class="source-inline">dense</strong>) and import the <strong class="source-inline">Sequential</strong> model. Initialize the <strong class="source-inline">Sequential</strong> model.</li>
				<li>Add an embedding layer with <strong class="source-inline">32</strong> as the vector size (<strong class="source-inline">output_dim</strong>). Add a spatial dropout of <strong class="source-inline">40%</strong>.</li>
				<li>Build a stacked LSTM model with <strong class="source-inline">2</strong> layers with <strong class="source-inline">64</strong> cells each. Add a dropout layer with <strong class="source-inline">40%</strong> dropout.</li>
				<li>Add a dense layer with <strong class="source-inline">32</strong> neurons with <strong class="source-inline">relu</strong> activation, then a <strong class="source-inline">50%</strong> dropout layer, followed by another dense layer of <strong class="source-inline">32</strong> neurons with <strong class="source-inline">relu</strong> activation, and follow this up with another dropout layer with <strong class="source-inline">50%</strong> dropout.</li>
				<li>Add a final dense layer with a single neuron with <strong class="source-inline">sigmoid</strong> activation and compile the model. Print the model summary.</li>
				<li>Fit the model on the training data with a <strong class="source-inline">20%</strong> validation split and a batch size of <strong class="source-inline">128</strong>. Train for <strong class="source-inline">5</strong> epochs.</li>
				<li>Make a prediction on the test set using the <strong class="source-inline">predict_classes</strong> method of the model. Using the <strong class="source-inline">accuracy_score</strong> method from <strong class="source-inline">scikit-learn</strong>, calculate the accuracy on the test set. Also, print out the confusion matrix.</li>
			</ol>
			<p>With the preceding parameters, you should get about <strong class="source-inline">86%</strong> accuracy. With some hyperparameter tuning, you should be able to get a significantly higher accuracy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 416.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor222"/>Summary</h1>
			<p>In this chapter, we started by understanding the reasons for plain RNNs not being practical for very large sequences – the main culprit being the vanishing gradient problem, which makes modeling long-range dependencies impractical. We saw the LSTM as an update that performs extremely well for long sequences, but it is rather complicated and has a large number of parameters. GRU is an excellent alternative that is a simplification over LSTM and works well on smaller datasets.</p>
			<p>Then, we started looking at ways to extract more power from these RNNs by using bidirectional RNNs and stacked layers of RNNs. We also discussed attention mechanisms, a significant new approach that provides state-of-the-art results in translation but can also be employed on other sequence-processing tasks. All of these are extremely powerful models that have changed the way several tasks are performed and form the basis for models that produce state-of-the-art results. With active research in the area, we expect things to only get better as more novel variants and architectures are released.</p>
			<p>Now that we've discussed a variety of powerful modeling approaches, in the next chapter, we will be ready to discuss a very interesting topic in the deep learning domain that enables AI to be creative – <strong class="bold">Generative Adversarial Networks</strong>.</p>
		</div>
	</body></html>