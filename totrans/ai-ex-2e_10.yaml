- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conceptual Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding cutting-edge machine learning and deep learning theory only marks the
    beginning of your adventure. The knowledge you have acquired should help you become
    an AI visionary. Take everything you see as opportunities and see how AI can fit
    into your projects. Reach the limits and skydive beyond them.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on decision-making through visual representations and explains
    the motivation that led to **conceptual representation learning** (**CRL**) and
    **metamodels** (**MM**), which form **CRLMMs**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concept learning is our human ability to partition the world from chaos to
    categories, classes, sets, and subsets. As a child and young adult, we acquire
    many classes of things and concepts. For example, once we understand what a "hole"
    is, we can apply it to anything we see that is somewhat empty: a black hole, a
    hole in the wall, a hole in a bank account if money is missing or overspent, and
    hundreds of other cases.'
  prefs: []
  type: TYPE_NORMAL
- en: By performing concept learning, we humans do not have to learn the same concept
    over and over again for each case. For example, a hole is a hole. So when we see
    a new situation such as a crater, we know it's just a "big" hole. I first registered
    a word2vector patent early in my career. Then I rapidly applied it to concept2vector
    algorithms. I then designed and developed the CRLMM method successfully for **automatic
    planning and scheduling** (**APS**) software, cognitive chatbots, and more, as
    we will see in the following chapters. The metamodel term means that I applied
    one single model to many different domains, just as we humans do.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual representations also provide visual images of concepts. To plan,
    humans need to visualize necessary information (events, locations, and so on)
    and more critical *visual dimensions* such as *image concepts*. A human being
    thinks in *mental images*. When we think, mental images flow through our minds
    with numbers, sounds, odors, and sensations, transforming our environment into
    fantastic multidimensional representations similar to video clips.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An approach to CRLMM in three steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning to avoid developing a new program for each variation of a
    similar case
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain learning to avoid developing a new program each time the domain changes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The motivation for using CRLMM
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the years, I've successfully implemented CRL in C++, Java, and logic programming
    (Prolog) in various forms on corporate sites. In this chapter, I'll use Python
    to illustrate the approach with TensorFlow 2.x with the **convolutional neural
    network (CNN)** built in *Chapter 9*, *Abstract Image Classification with Convolutional
    Neural Networks (CNNs)*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transfer learning using a CNN model trained to generalize image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain learning to extend image recognition trained in one field to another field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll begin this chapter by looking at the benefits of transfer learning and
    how concept learning can boost this process.
  prefs: []
  type: TYPE_NORMAL
- en: Generating profit with transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning means that we can use a model we designed and trained in another
    similar case. This will make the model very profitable since we do not have to
    design a new model and write a new program for every new case. You will thus generate
    profit for your company or customer by lowering the cost of new implementations
    of your trained model. Think of a good AI model as a reusable tool when applied
    to similar cases. This is why concept learning, being more general and abstract,
    is profitable. That is how we humans adapt.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to reasoning and thinking in general, we use mental images with
    some words. Our thoughts contain concepts, on which we build solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The trained model from *Chapter 9*, *Abstract Image Classification with Convolutional
    Neural Networks (CNNs)*, can now classify images of a certain type. In this section,
    the trained model will be loaded and then generalized through transfer learning
    to classify similar images.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that I did not use many images for the example. My goal was
    to explain the process, not to go into building large datasets, which is a task
    in itself. The primary goal is to *understand* CNNs and conceptual learning representations.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning provides a cost-effective way of using trained models for
    other purposes within the same company, such as the food processing company described
    in *Chapter 9*, *Abstract Image Classification with Convolutional Neural Networks
    (CNNs)*.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter describes how the food processing company used the model for other
    similar purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The company that succeeds in doing this will progressively generalize the use
    of the solution. By doing so, inductive abstraction will take place and lead to
    other AI projects, which will prove gratifying to the management of a corporation
    and the teams providing the solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive thinking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Induction uses inferences to reach a conclusion. For example, a food processing
    conveyor belt with missing products will lead to packaging productivity problems.
    If an insufficient amount of products reaches the packaging section, this will
    slow down the whole production process.
  prefs: []
  type: TYPE_NORMAL
- en: By observing similar problems in other areas of the company, inferences from
    managers will come up, such as *if insufficient amounts of products flow through
    the process, production will slow down*.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive abstraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The project team in charge of improving efficiency in any company needs to find
    an *abstract representation* of a problem to implement a solution through organization
    or software. This book deals with the AI side of solving problems. Organizational
    processes need to define how AI will fit in, with several on-site meetings.
  prefs: []
  type: TYPE_NORMAL
- en: The problem AI needs to solve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this particular example, each section of the factory has an **optimal production
    rate** (**OPR**) defined per hour or per day, for example. The equation of an
    OPR per hour can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OPR : min(*p*(*s*)) <= OPR <= max(*p*(*s*))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p* is the production rate of a given section (the different production departments
    of a factory) *s*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*s*) is the production rate of the section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: min(*p*(*s*)) is the historical minimum (trial and error over months of analysis).
    Under that level, the whole production process will slow down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: max(*p*(*s*)) is the historical maximum. Over that level, the whole production
    process will slow down as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OPR is the optimal production rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first time somebody sees this equation, it seems difficult to understand.
    The difficulty arises because you have to visualize the process, which is the
    goal of this chapter. Every warehouse, industry, and service uses production rates
    as a constraint to reach profitable levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualization requires representation at two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that if a packaging department is not receiving enough products, it will
    have to slow down or even stop sometimes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that if a packaging department receives too many products, it will
    not be able to package them. If the input is a conveyor belt with no intermediate
    storage (present-day trends), then it will have to be slowed down, slowing down
    or stopping the processes before that point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, slowing down production leads to bad financial results and critical
    sales problems through late deliveries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, an OPR gap is a problem. To solve this problem, another level
    of abstraction is required. First, let''s break down the OPR equation into two
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: OPR >= min(*p*(*s*))
  prefs: []
  type: TYPE_NORMAL
- en: OPR <= max(*p*(*s*))
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s find a higher control level through variance variable *v*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*v*[min] = |OPR – min(*p*(*s*))|'
  prefs: []
  type: TYPE_NORMAL
- en: '*v*[max] = |OPR – max(*p*(*s*))|'
  prefs: []
  type: TYPE_NORMAL
- en: '*v*[min] and *v*[max] are the absolute values of the variance in both situations
    (not enough products to produce and too many to produce respectively).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final representation is through a single control, detection, and learning
    rate (the Greek letter gamma):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance between the optimal production rate of a given section of a company
    and its minimum speed (products per hour) will slow the following section down.
    If too few cakes (*v*[min]), for example, are produced, then the cake packaging
    department will be waiting and will have to stop. If too many cakes are produced
    (*v*[max]), then the section will have to slow down or stop. Both variances would
    create problems in a company that cannot manage intermediate storage easily, which
    is the case with the food processing industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this single ![](img/B15438_10_002.png) concept, introduced in *Chapter
    9*, *Abstract Image Classification with Convolutional Neural Networks (CNNs)*,
    the TensorFlow 2.x CNN can start learning a fundamental production concept: what
    a physical gap is. Let''s go back to humans. Once we understand that a gap is
    some kind of hole or empty space, we can identify and represent thousands of situations
    with that one gap concept that is here converted into a parameter named gamma
    (![](img/B15438_10_003.png)). Let''s explore the concept and then implement it.'
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/B15438_10_004.png) gap concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Teaching the CNN the gap concept will help it extend its thinking power to
    many fields:'
  prefs: []
  type: TYPE_NORMAL
- en: A gap in production, as explained before
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A gap in a traffic lane for a self-driving vehicle to move into
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any incomplete, deficient area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any opening or window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's teach a CNN the ![](img/B15438_10_005.png) gap concept, or simply, ![](img/B15438_10_006.png).
    The symbol ![](img/B15438_10_007.png) of a gap is the Greek letter "gamma," so
    it is simply pronounced "gamma." We thus lead to teaching a CNN how to recognize
    a gap we will call gamma (![](img/B15438_10_005.png)). The goal is for a CNN to
    understand the abstract concept of an empty space, a hole represented by the word
    *gap* and the Greek letter gamma (![](img/B15438_10_005.png)).
  prefs: []
  type: TYPE_NORMAL
- en: To achieve that goal, the CNN model that was trained and saved in *Chapter 9*,
    *Abstract Image Classification with Convolutional Neural Networks (CNNs)*, now
    needs to be loaded and used. To grasp the implications of the ![](img/B15438_10_010.png)
    concept, imagine the cost of not producing enough customer orders or having piles
    of unfinished products everywhere. The financial transposition of the physical
    gap is a profit **variance** on set goals. We all know the pain those variances
    lead to.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the trained TensorFlow 2.x model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The technical goal is to load and use the trained CNN model and then use the
    same model for other similar areas. The practical goal is to teach the CNN how
    to use the ![](img/B15438_10_011.png) **concept** to enhance the thinking abilities
    of the scheduling, chatbot, and other applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the model has two main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model to compile and classify new images without training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the parameters used layer by layer and displaying the weights reached
    during the learning and training phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will load and display the model without training
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and displaying the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A limited number of headers suffice to read a saved model with `READ_MODEL.py`,
    as implemented in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model3.h5` model saved is now loaded from its file, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The loaded model needs to be compiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Reading and displaying the model is not a formality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Printing the structure provides useful information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The trained model might or might not work on all datasets. In that case, the
    following output would point to problems that can be fixed through its structure,
    for example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the global structure has been displayed, it is possible to look into the
    structure of each layer. For example, we can peek into the `conv2d` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Each parameter contains very useful information. For example, `''padding'':''valid''`
    means that padding has not been applied. In this model, the number and size of
    the kernels provide satisfactory results without padding, and the shape decreases
    to the final status layer (classification), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: However, suppose you want to control the output shape of a layer so that the
    spatial dimensions do not decrease faster than necessary. One reason could be
    that the next layer will explore the edges of the image and that we need to explore
    them with kernels that fit the shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, padding of size 1 can be added with `0` values, as shown in the
    following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | **1** | **3** | **24** | **4** | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | **3** | **7** | **8** | **5** | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | **6** | **4** | **5** | **4** | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | **5** | **4** | **3** | **1** | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: A padding of size 2 would add two rows and columns around the initial shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, fine-tuning your training model by adding as many options
    as necessary will improve the quality of the results. The weights can be viewed
    by extracting them from the saved model file layer by layer, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the weights used by the program will provide useful information about
    the way the optimization process was carried out by the program. Sometimes, a
    program will get stuck, and the weights might seem off track. After all, a CNN
    can contain imperfections like any other program.
  prefs: []
  type: TYPE_NORMAL
- en: 'A look at the following output, for example, can help understand where the
    system went wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can now use the loaded and checked model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model to use it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading the model with `CNN_CONCEPT_STRATEGY.py` requires a limited number
    of headers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading the model is done by using the same code as in `READ_MODEL.py`, described
    previously. Once you load it, compile the model with the `model.compile` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The model used for this example and the image identification function has been
    implemented in two parts. First, we''re loading and resizing the image with the
    following function, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The model expects another dimension in the input array to predict, so one is
    added to fit the model. In this example, one image at a time needs to be identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'I added the following two prediction methods and returned one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are two prediction methods because, basically, every component needs to
    be checked in a CNN during a project's implementation phase, to choose the best
    and fastest ones. To test `prediction2`, just change the `return` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Once a CNN is running, it can prove difficult to find out what went wrong. Checking
    the output of each layer and component while building the network saves fine-tuning
    time once the full-blown model produces thousands of results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example detects product ![](img/B15438_10_012.png) **gaps** on
    a conveyor belt in a food processing factory. The program loads the first image
    stored in the `classify` directory to predict its value. The program describes
    the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The program displays (optional) the shaped image, as follows, which shows that
    the conveyor belt has a sufficient number of products at that point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Output (shaped image)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program then makes and displays its prediction `0`, meaning no real gap
    has been found on the conveyor belt on this production line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Seeking...` means it is going to analyze the second image in the classify
    direction. It loads, displays, and predicts its value as shown in the following
    frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Output (shaped image)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction (*value* = 1) correctly detected gaps on the conveyor belt,
    as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that the predictions of the CNN have been verified, the implementation strategy
    needs approval. A CNN contains marvels of applied mathematics. CNNs epitomize
    deep learning themselves. A researcher could easily spend hundreds of hours studying
    them.
  prefs: []
  type: TYPE_NORMAL
- en: However, applied mathematics in the business world requires profitability. As
    such, the components of CNNs appear to be ever-evolving concepts. Added kernels,
    activation functions, pooling, flattening, dense layers, and compiling and training
    methods act as a starting point for architectures, not as a finality.
  prefs: []
  type: TYPE_NORMAL
- en: Using transfer learning to be profitable or see a project stopped
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At some point, a company will demand results and may shelve a project if those
    results are not delivered. If a spreadsheet represents a faster sufficient solution,
    a deep learning project will face potential competition and rejection. Many engineers
    learning artificial intelligence have to assume the role of standard SQL reporting
    experts before accessing real AI projects. Transfer learning is a profitable solution
    that can boost the credibility of an IT department.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning appears to be a solution to the present cost of building and
    training a CNN program. Your model might just pay off that way. The idea is to
    get a basic AI model rolling profits in fast for your customer and management.
    Then, you will have everybody's attention. To do that, you must define a strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a deep learning CNN expert comes to a top manager saying that this CNN model
    can classify CIFAR-10 images of dogs, cats, cars, plants, and more, the answer
    will be, *so what? My 3-year-old child can too. In fact, so can my dog!*
  prefs: []
  type: TYPE_NORMAL
- en: The IT manager in that meeting might even blurt out something like, "We have
    all the decision tools we need right now, and our profits are increasing. Why
    would we invest in a CNN?"
  prefs: []
  type: TYPE_NORMAL
- en: The core problem of marketing AI to real-world companies is that it relies upon
    a belief in the necessity of a CNN in the first place. Spreadsheets, SQL queries,
    standard automation, and software do 99% of the job. Most of the time, it does
    not take a CNN to replace many jobs; just an automated spreadsheet, a query, or
    standard, straightforward software is enough. Jobs have been sliced into simple-enough
    parts to replace humans with basic software for decades.
  prefs: []
  type: TYPE_NORMAL
- en: Before presenting a CNN, a data scientist has to find out how much the company
    can earn using it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding, designing, building, and running a CNN does not mean much regarding
    business. All the hard work we put into understanding and running these complex
    programs will add up to nothing if we cannot prove that a solution will generate
    profit. Without profit, the implementation costs cannot be recovered, and nobody
    will listen to a presentation about even a fantastic program.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a model efficiently means implementing it in one area of a company
    and then other domains for a good return on investment.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a food processing company, for example, one of the packaging lines has a
    performance problem. Sometimes, randomly, some of the cakes are missing on the
    conveyor belt, as shown in the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Food processing company example'
  prefs: []
  type: TYPE_NORMAL
- en: To start a cost-effective project, a cheap webcam could be installed over the
    conveyor belt. It'll take a random sample picture every 10 seconds and process
    it to find the holes shown in the interval in the center of the image. We can
    clearly see an empty space, a gap, a hole. If a hole is detected, it means some
    cakes have not made it to the conveyor belt (production errors).
  prefs: []
  type: TYPE_NORMAL
- en: A 2% to 5% productivity rate increase could be obtained by automatically sending
    a signal to the production robot when some cakes are missing. The production robot
    will then send a signal to the production line to increase production to compensate
    for the missing units in real-time. This type of automatic control already exists
    in various forms of automated food production lines. However, this provides a
    low-cost way to start implementing this on a production line.
  prefs: []
  type: TYPE_NORMAL
- en: Making the model profitable by using it for another problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's say that the food processing experiment on the conveyor belt turns out
    to work well enough with dataset type *d*[1] and the CNN model *M* to encourage
    generalization to another dataset, *d*[2], in the same company.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning consists of going from *M*(*d*[1]) to *M*(*d*[2]) using the
    same CNN model *M*, with some limited, cost-effective additional training. Variations
    will appear, but they can be solved by shifting a few parameters and working on
    the input data following some basic dataset preparation rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting**: When the model fits the training data quickly with 100% accuracy,
    this may or may not be a problem. In the case of classifying holes on the conveyor
    belt, overfitting might not prove critical. The shapes are always the same, and
    the environment remains stable. However, in an unstable situation with all sorts
    of different images or products, then overfitting will limit the effectiveness
    of a system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfitting**: If the accuracy drops down to low levels, such as 20%, then
    the CNN will not work. The datasets and parameters need optimizing. Maybe the
    number of samples needs to be increased for *M*(*d*[2]), or reduced, or split
    into different groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: Regularization, in general, involves the process of finding
    how to fix the generalization problem of *M*(*d*[2]), not the training errors
    of *M*(*d*[2]). Maybe an activation function needs some improvements, or the way
    the weights have been implemented requires attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no limit to the number of methods you can apply to find a solution,
    just like standard software program improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Where transfer learning ends and domain learning begins
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transfer learning can be used for similar types of objects or images in this
    example, as explained. The more similar images you can train within a company
    with the same model, the more return on investment (ROI) it will produce, and
    the more this company will ask you for more AI innovations.
  prefs: []
  type: TYPE_NORMAL
- en: Domain learning takes a model such as the one described in *Chapter 9*, *Abstract
    Image Classification with Convolutional Neural Networks (CNNs)*, and can generalize
    it. The generalization process will lead us to domain learning.
  prefs: []
  type: TYPE_NORMAL
- en: Domain learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section on domain learning builds a bridge between classic transfer learning,
    as described previously, and another use of domain learning I have found profitable
    on corporate projects: teaching a machine a concept (CRLMM). In this chapter,
    we are focusing on teaching a machine to learn how to recognize a gap in situations
    other than at the food processing company.'
  prefs: []
  type: TYPE_NORMAL
- en: How to use the programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can read this whole chapter first to grasp the concepts or play with the
    programs first. Do as you feel is best for you. In any case, `CNN_TDC_STRATEGY.py`
    loads trained models (you do not have to train them again for this chapter) and
    `CNN_CONCEPT_STRATEGY.py` trains the models.
  prefs: []
  type: TYPE_NORMAL
- en: The trained models used in this section
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section uses `CNN_TDC_STRATEGY.py` to apply the trained models to the
    target concept images. `READ_MODEL.py` (as shown previously) was converted into
    `CNN_TDC_STRATEGY.py` by adding variable directory paths (for the `model3.h5`
    files and images) and classification messages, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The loaded model now targets the images to classify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Each subdirectory of the model contains four subdirectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`classify`: Contains the images to classify'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: The trained `model3.h5` used to classify the images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_set`: The test set of conceptual images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_set`: The training set of conceptual images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have explored the directory structure of our model, let's see how
    to use it in different situations.
  prefs: []
  type: TYPE_NORMAL
- en: The trained model program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this chapter, you do not need to train a model. It was already trained
    in *Chapter 9*, *Abstract Image Classification with Convolutional Neural Networks
    (CNNs)*. The directory paths have become variables to access the subdirectories
    described previously. The paths can be called, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You do not need to run training for this chapter. The models were trained and
    automatically stored in their respective subdirectories on the virtual machine
    delivered with the book. This means that when you need to detect gaps for various
    types of images, you can simply change the scenario to fit the type of images
    you will be receiving from the frames of a webcam: cakes, cars, fabric, or abstract
    symbols.'
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, focus on understanding the concepts. You can read the chapter
    without running the programs, open them without running them, or run them—whatever
    makes you comfortable. The main goal is to grasp the concepts to prepare for the
    subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We have loaded the model and a scenario. Now, we are going to use our trained
    model to detect if a production line is loaded or underloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Gap – loaded or underloaded
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The gap concept has just become a polysemy image-concept (polysemy means different
    meanings, as explained in *Chapter 6*, *Innovating AI with Google Translate*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cake situation, the ![](img/B15438_10_013.png) gap was negative in its
    *g*[1] subset of meaning and concepts applied to a CNN, relating it to negative
    images *n* + *g*[1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ng*[1] = {missing, not enough, slowing production down … bad}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full-of-products image was positive, *p* + *g*[2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*pg*[2] = {good production flow, no gap}'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the CNN is learning how to distinguish an abstract representation,
    not simply an image, like for the cakes. Another subset of ![](img/B15438_10_010.png)
    (the conceptual gap dataset) is loaded/underloaded. A "gap" is not a specific
    object but a general concept that can be applied to hundreds of different cases.
    This is why I use the term "conceptual gap dataset."
  prefs: []
  type: TYPE_NORMAL
- en: The following abstract image is loaded. The squares represent production machines,
    and the arrows represent the load-in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the *x* axis represents time and the *y* axis represents machine
    production resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Abstract image 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNN model runs and produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The CNN recognizes this as a correctly loaded model. The task goes beyond classifying.
    The system needs to recognize this to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another image produces a different result. In this case, an underloaded gap
    appears in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Abstract image 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the CNN has a different output, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Read "unloaded" as "underloaded." Unloaded or underloaded represents empty
    spaces in any case. The gap concept ![](img/B15438_10_015.png) has added two other
    subsets, *g*[3] and *g*[4], to its dataset. We now have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The four *g*[1] to *g*[4] subsets of ![](img/B15438_10_005.png) are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ng*[1] = {missing, not enough, slowing production down … bad}'
  prefs: []
  type: TYPE_NORMAL
- en: '*pg*[2] = *pg*[2] = {good production flow, no gap}'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[3] = {loaded}'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[4] = {unloaded}'
  prefs: []
  type: TYPE_NORMAL
- en: The remaining problem will take some time to solve. *g*[4] (gap) can sometimes
    represent an opportunity for a machine that does not have a good workload to be open
    to more production. In some cases, *g*[4] becomes *pg*[4] (*p* = positive). In
    other cases, it will become *ng*[4] (*n* = negative) if production rates go down.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to identify a "gap" in production lines. As explained,
    a "gap" is a generic concept for spaces everywhere. We will now explore jammed
    or "open" traffic lanes.
  prefs: []
  type: TYPE_NORMAL
- en: Gap – jammed or open lanes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model in this chapter can be extended to other domains. A self-driving car
    needs to recognize whether it is in a traffic jam or not. Also, a self-driving
    car has to know how to change lanes when it detects enough space (a gap) to do
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces two new subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[5] = {traffic jam, heavy traffic … too much traffic}'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[6] = {open lane, light traffic … normal traffic}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model now detects *g*[5] (a traffic jam), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Traffic jam example'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output appears correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*g*[6] comes out right as well, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Traffic jam example'
  prefs: []
  type: TYPE_NORMAL
- en: 'A potential lane change has become possible, as detected by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We have applied our CNN "gap" detection model to several types of images. We
    can now go deeper into the theory of conceptual datasets using "gaps" as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Gap datasets and subsets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, the ![](img/B15438_10_005.png) (the gap conceptual dataset)
    has begun to learn several subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In which:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ng*[1] = {missing, not enough, slowing production down … bad}'
  prefs: []
  type: TYPE_NORMAL
- en: '*pg*[2] = *pg*[2] = {good production flow, no gap}'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[2] = {loaded}'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[3] = {unloaded}'
  prefs: []
  type: TYPE_NORMAL
- en: '*pg*[4] = {traffic jam, heavy traffic … too much traffic}'
  prefs: []
  type: TYPE_NORMAL
- en: '*ng*[5] = {open lane, light traffic … normal traffic}'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that *g*[2] and *g*[3] do not have labels yet. The food processing context
    provided the labels. Concept detection requires a context, which CRLMMs will provide.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing the ![](img/B15438_10_020.png) (the gap conceptual dataset)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generalization of ![](img/B15438_10_015.png) (the gap conceptual dataset)
    will provide a conceptual tool for metamodels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_022.png) (the gap conceptual dataset) refers to negative,
    positive, or undetermined space between two elements (objects, locations, or products
    on a production line).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_023.png) (gamma) also refers to a gap in time: too long,
    not long enough, too short, or not short enough.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_023.png) represents the distance between two locations: too
    far or too close.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_025.png) can represent a misunderstanding or an understanding
    between two parties: a divergence of opinions or a convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: All of these examples refer to gaps in space and time viewed as space.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of conceptual representation learning metamodels applied to dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A CRLMM converts images into concepts. These abstract concepts will then be
    embedded in vectors that become logits for a softmax function, and in turn, will
    be converted into parameters for complex artificial intelligence programs' automatic
    scheduling, cognitive chatbots, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of a concept is that it can apply to many different areas. With
    just one concept, "gap" (a hole, empty space, and so on), you can describe hundreds
    if not thousands of cases.
  prefs: []
  type: TYPE_NORMAL
- en: In some artificial intelligence projects, dimensionality reduction does not
    produce good results at all. When scheduling maintenance of airplanes, rockets,
    and satellite launchers, for example, thousands of features enter the system without
    leaving any out. A single missing screw in a rocket in the wrong place could cause
    a disaster. A single mistake in the engine of an airplane can cause an accident,
    a single component of a satellite can impair its precision.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality must be taken into account. Some use the expression "curse of
    dimensionality" and I often prefer the "blessing" of dimensionality. Let's have
    a look at both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The number of features for a given project can reach large numbers. The following
    example contains 1,024 dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: The curse of dimensionality'
  prefs: []
  type: TYPE_NORMAL
- en: Each dot in the preceding representation represents a dimension that can be
    the features in an image for example.
  prefs: []
  type: TYPE_NORMAL
- en: Asking a CNN to analyze thousands of features in an image, for example, might
    make it impossible to reach an accurate result. Since each layer is supposed to
    reduce the size of the data analyzed to extract important features, too many dimensions
    might make the training of the model impossible. Remember, each dimension can
    contain a feature that requires weights to be trained. If there are too many,
    the training becomes either too long or too difficult to calculate.
  prefs: []
  type: TYPE_NORMAL
- en: Following standard CNN designs provides a good starting point. The limit of
    this approach occurs when the result does not meet expectations, such as in some
    of the cases we will look at in the upcoming chapters. In those cases, CRLMMs
    will increase the productivity of the solution, providing a useful abstract model.
  prefs: []
  type: TYPE_NORMAL
- en: When a solution requires a large number of unreduced dimensions, kernels, pooling,
    and other dimension reduction methods cannot be applied, CRLMMs will provide a
    *pair of glasses* for the system. That's when the "blessing" of dimensionality
    comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: The blessing of dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some projects, when the model reaches limits that comprise a project, dimensionality
    is a blessing.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example of rocket manufacturing using our CNN model. We want to
    identify gaps on a surface. This surface contains tiles to protect the rocket
    from overheating when it goes through the atmosphere. A gap in those tiles could
    cause a fatal accident.
  prefs: []
  type: TYPE_NORMAL
- en: If we take a few tiles out, take a picture and run it through our CNN model,
    it will *probably* detect a gap. The difference between that probability and an
    error could mean a critical failure of the rocket.
  prefs: []
  type: TYPE_NORMAL
- en: This means that we might not want to reduce the number of features, which in
    turn need weights and add up to high numbers of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We could decide not to use pooling, which groups several dimensions into one
    as we saw. That could create calculations problems as we saw in the previous paragraph.
    Either there would be too many weights to calculate or the calculation could take
    too long.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, we could reduce the size of the frame to the smallest portion
    of the rocket component we are examining. We could decide that our camera will
    only scan the smallest surface possible at a time sending minimal-sized frames
    to our CNN.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, even with no pooling, the layers would contain more data, but
    the calculation would remain reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: The "blessing" of dimensionality, in this case, resides in the fact that by
    avoiding pooling (grouping), we are examining more details that can make our model
    much more reliable to detect small cracks since we would train it to see very
    small gaps.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality usually leads to dimensionality reduction. But as
    we have just seen, it doesn't have to be so.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the CNN architecture built in *Chapter 9*, *Abstract Image
    Classification with Convolutional Neural Networks (CNNs)*, was loaded to classify
    physical gaps in a food processing company. The model uses image concepts, taking
    CNNs to another level. Neural networks can tap into their huge cognitive potential,
    opening the doors to the future of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the trained models were applied to transfer learning by identifying similar
    types of images. Some of those images represented concepts that led the trained
    CNN to identify ![](img/B15438_10_026.png) concept gaps. Image concepts represent
    an avenue of innovative potential adding cognition to neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_006.png) concept gaps were applied to different fields using
    the CNN as a training and classification tool in domain learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_10_028.png) concept gaps have two main properties: negative
    n-gaps and positive p-gaps. To distinguish one from the other, a CRLMM provides
    a useful add-on. In the food processing company, installing a webcam on the right
    food processing conveyor belt provided a context for the system to decide whether
    a gap was positive or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: With these concepts in mind, let's build a solution for **advanced planning
    and scheduling** (**APS**) in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The curse of dimensionality leads to reducing dimensions and features in machine
    learning algorithms. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer learning determines the profitability of a project. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reading `model.h5` does not provide much information. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numbers without meaning are enough to replace humans. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chatbots prove that body language doesn't mean that much. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Present-day ANNs provide enough theory to solve all AI requests. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chatbots can now replace humans in all situations. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Self-driving cars have been approved and do not need conceptual training. (Yes
    | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Industries can implement AI algorithms for all of their needs. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More on Keras layers: [https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on concept learning: [https://www.sciencedirect.com/topics/psychology/concept-learning](https://www.sciencedirect.com/topics/psychology/concept-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on cognitive sciences, the brain, and the mind for conceptual models:
    [http://catalog.mit.edu/schools/science/brain-cognitive-sciences/](http://catalog.mit.edu/schools/science/brain-cognitive-sciences/),
    [https://symsys.stanford.edu/undergraduatesconcentrations/cognitive-science-cogsci-concentration](https://symsys.stanford.edu/undergraduatesconcentrations/cognitive-science-cogsci-concentration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
