- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Frameworks and Containers on SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker supports many popular ML and DL frameworks. Framework support
    in SageMaker is achieved using prebuilt Docker containers for inference and training
    tasks. Prebuilt SageMaker containers provide a great deal of functionality, and
    they allow you to implement a wide range of use cases with minimal coding. There
    are also real-life scenarios where you need to have a custom, runtime environment
    for training and/or inference tasks. To address these cases, SageMaker provides
    a flexible **Bring-Your-Own** (**BYO**) container feature.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review key supported DL frameworks and corresponding
    container images. Then, we will focus our attention on the two most popular DL
    frameworks, TensorFlow and PyTorch, and learn how to use them in Amazon SageMaker.
    Additionally, we will review a higher-level, state-of-the-art framework, Hugging
    Face, for NLP tasks, and its implementation for Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will understand how to use and extend prebuilt SageMaker containers
    based on your use case requirements, as well as learning about the SageMaker SDK
    and toolkits, which simplify writing training and inference scripts that are compatible
    with Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In later sections, we will dive deeper into how to decide whether to use prebuilt
    SageMaker containers or BYO containers. Then, we will develop a SageMaker-compatible
    BYO container.
  prefs: []
  type: TYPE_NORMAL
- en: 'These topics will be covered in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring DL frameworks on SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SageMaker DL containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing BYO containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to decide which container strategy
    to choose based on your specific problem requirements and chosen DL framework.
    Additionally, you will understand the key aspects of training and inference script
    development, which are compatible with Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Using SageMaker DL containers* and *Developing BYO containers* sections,
    we will provide walk-through code samples, so you can develop practical skills.
    Full code examples are available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along with this code, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account and IAM user with the permissions to manage Amazon SageMaker
    resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 and the SageMaker SDK ([https://pypi.org/project/sagemaker/](https://pypi.org/project/sagemaker/))
    installed on your development machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker installed on your development machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use SageMaker P2 instances for training purposes, you will likely need to
    request a service limit increase on your AWS account. For more details, please
    view [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring DL frameworks on SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing this book, Amazon SageMaker supports the following frameworks,
    where DL frameworks are marked with an asterisk:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkML Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chainer*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache MXNet*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning containers – including TensorFlow- and PyTorch-enabled
    containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list of supported frameworks could change in the future. Be sure
    to check the official SageMaker documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will primarily focus on the two most popular choices: **TensorFlow**
    and **PyTorch**. Both are open source frameworks with a large and vibrant communities.
    Depending on the specific use case or model architecture, one or the other framework
    might have a slight advantage. However, it’s safe to assume that both frameworks
    are comparable in terms of features and performance. In many practical scenarios,
    the choice between TensorFlow or PyTorch is made based on historical precedents
    or individual preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: Another framework that we will discuss in this book is **Hugging Face**. This
    is a high-level framework that provides access to SOTA models, training, and inference
    facilities for NLP tasks (such as text classification, translation, and more).
    Hugging Face is a set of several libraries (transformers, datasets, tokenizers,
    and accelerate) designed to simplify building SOTA NLP models. Under the hood,
    Hugging Face libraries use TensorFlow and PyTorch primitives (collectively known
    as “backends”) to perform computations. Users can choose which backend to use
    based on specific runtime requirements. Given its popularity, Amazon SageMaker
    has recently added support for the Hugging Face libraries in separate prebuilt
    containers for training and inference tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Container sources
  prefs: []
  type: TYPE_NORMAL
- en: Sources of SageMaker DL containers are available on the public GitHub repository
    at [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers).
    In certain cases, it can be helpful to review relevant Dockerfiles to understand
    the runtime configuration of prebuilt containers. Container images are available
    in AWS public registries at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the supported frameworks, SageMaker provides separate training
    and inference containers. We have separate containers for these two tasks because
    of the following considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Training and inference tasks might have different runtime requirements. For
    example, you might choose to run your training and inference tasks on different
    compute platforms. This will result in different sets of accelerators and performance
    optimization tweaks in your container, depending on your specific task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and inference tasks require different sets of auxiliary scripts; for
    instance, standing up a model server in the case of inference tasks. Not separating
    training and inference containers could result in bloated container sizes and
    intricate APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this reason, we will always explicitly identify the container we are using
    depending on the specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Specific to DL containers, AWS also defines separate GPU-based and CPU-based
    containers. GPU-based containers require the installation of additional accelerators
    to be able to run computations on GPU devices (such as the CUDA toolkit).
  prefs: []
  type: TYPE_NORMAL
- en: Model requirements
  prefs: []
  type: TYPE_NORMAL
- en: When choosing a SageMaker DL container, always consider the model requirements
    for compute resources. For the majority of SOTA models, it’s recommended that
    you use GPU-based compute instances to achieve acceptable performance. Choose
    your DL container accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A TensorFlow container has two major versions: 1.x (maintenance mode) and 2.x
    (the latest version). Amazon SageMaker supports both versions and provides inference
    and training containers. In this book, all of the code examples and general commentary
    are done assuming TensorFlow v2.x.'
  prefs: []
  type: TYPE_NORMAL
- en: AWS updates with frequently supported minor TensorFlow versions. The latest
    supported major version is 2.10.0.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker provides inference and training containers for PyTorch. The
    latest version is 1.12.1.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS provides Hugging Face containers in two flavors: PyTorch and TensorFlow
    backends. Each backend has separate training and inference containers.'
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker Python SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS provides a convenient Python SDK that simplifies interactions with supported
    DL frameworks via the Estimator, Model, and Predictor classes. Each supported
    framework has a separate module with the implementation of respective classes.
    For example, here is how you import Predict, Estimator, and Model classes for
    the PyTorch framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the SageMaker Python SDK workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – How SageMaker Python SDK works with image URIs  ](img/B17519_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – How SageMaker Python SDK works with image URIs
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a better intuition, let’s do a quick example of how to run a training
    job using a PyTorch container with a specific version using SageMaker Python SDK.
    For a visual overview, please refer to *Figure 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we decide which framework to use and import the respective `Pytorch`
    `estimator` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When instantiating the PyTorch `estimator` object, we need to provide several
    more parameters including the framework version and the Python version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When executing this code, SageMaker Python SDK automatically validates user
    input, including the framework version and the Python version. If the requested
    container exists, then SageMaker Python SDK retrieves the appropriate container
    image URI. If there is no container with the requested parameters, SageMaker Python
    SDK will throw an exception.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During the `fit()` call, a correct container image URI will be provided to
    the SageMaker API, so the training job will be running inside the SageMaker container
    with PyTorch v1.8 and Python v3.7 installed. Since we are requesting a GPU-based
    instance, a training container with the CUDA toolkit installed will be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using custom images
  prefs: []
  type: TYPE_NORMAL
- en: Please note that if, for some reason, you would prefer to provide a direct URI
    to your container image, you can do it using the `image_uri` parameter that is
    supported by the `model` and `estimator` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a deep dive into SageMaker DL containers, starting with the
    available prebuilt containers for the TensorFlow, PyTorch, and Hugging Face frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker DL containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker supports several container usage patterns. Also, it provides
    you with Training and Inference Toolkits that simplify using prebuilt containers
    and developing BYO containers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to choose the most efficient container usage
    pattern for your use case and how to use the available SageMaker toolkits to implement
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Container usage patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker provides you with the flexibility to choose whether to use
    prebuilt containers “as is” (known as **Script Mode**), **BYO containers**, or
    modify prebuilt containers.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the choice of approach is driven by specific model runtime requirements,
    available resources, and engineering expertise. In the next few subsections, we
    will discuss when to choose one approach over another.
  prefs: []
  type: TYPE_NORMAL
- en: Script Mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In script mode, you define which prebuilt container you’d like to use and then
    provide one or more scripts with the implementation of your training or inference
    logic. Additionally, you can provide any other dependencies (proprietary or public)
    that will be exported to the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Both training and inference containers in script mode come with preinstalled
    toolkits that provide common functionality such as downloading data to containers
    and model artifacts, starting jobs, and others. We will look at further details
    of the SageMaker **Inference Toolkit** and **Training Toolkit** later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Script Mode is suitable for the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Prebuilt containers satisfy your runtime requirements, or you can install any
    dependencies without needing to rebuild the container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to minimize the time spent on developing and testing your containers
    or you don’t have the required expertise to do so
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will review how to prepare your first training
    and inference scripts and run them on SageMaker in script mode.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying prebuilt containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to use SageMaker’s prebuilt containers is to modify them. In this
    case, you will use one of the prebuilt containers as a base image for your custom
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying prebuilt containers can be beneficial in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to add additional dependencies (for instance, ones that need to be
    compiled from sources) or reconfigure the runtime environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to minimize the development and testing efforts of your container and
    rely for the most part on the functionality of the base container tested by AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please note that when you extend a prebuilt container, you will be responsible
    for the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Dockerfile with the implementation of your runtime environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and storing your container in a Container registry such as **Amazon
    Elastic Container Registry** (**ECR**) or private Docker registries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later in this chapter, we see an example of how to extend a prebuilt PyTorch
    container for a training task.
  prefs: []
  type: TYPE_NORMAL
- en: BYO containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many scenarios in which you might need to create a custom container,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You have unique runtime requirements that cannot be addressed by extending the
    prebuilt container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to compile frameworks and libraries from sources for specific hardware
    platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are using DL frameworks that are not supported natively by SageMaker (for
    instance, JAX)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom container compatible with SageMaker inference and training
    resources requires development efforts, an understanding of Docker containers,
    and specific SageMaker requirements. Therefore, it’s usually recommended that
    you consider script mode or extending a prebuilt container first and choose to
    use a BYO container only if the first options do not work for your particular
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker toolkits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simplify the development of custom scripts and containers that are compatible
    with Amazon SageMaker, AWS created Python toolkits for training and inference
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Toolkits provide the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish consistent runtime environments and locations for storing code assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENTRYPOINT` scripts to run tasks when the container is started'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these toolkits helps to simplify and speed up the development
    of SageMaker-compatible containers, so let’s review them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The Training Toolkit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The SageMaker Training Toolkit has several key functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It establishes a consistent runtime environment, setting environment variables
    and a directory structure to store the input and output artifacts of model training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – The directory structure in SageMaker-compatible containers ](img/B17519_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – The directory structure in SageMaker-compatible containers
  prefs: []
  type: TYPE_NORMAL
- en: 'The Training Toolkit sets up the following directories in the training container:'
  prefs: []
  type: TYPE_NORMAL
- en: The `/opt/ml/input/config` directory with the model hyperparameters and the
    network layout used for distributed training as JSON files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `/opt/ml/input/data` directory with input data when S3 is used as data storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `/opt/ml/code/` directory, containing code assets to run training job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `/opt/ml/model/` directory, containing the resulting model; SageMaker automatically
    copies it to Amazon S3 after training completion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It executes the entrypoint script and handles success and failure statuses.
    In the case of a training job failure, the output will be stored in `/opt/ml/output/failure`.
    For successful executions, the toolkit will write output to the `/opt/ml/success`
    directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default, all prebuilt training containers already have a training toolkit
    installed. If you wish to use it, you will need to install it on your container
    by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you will need to copy all of the code dependencies into your container
    and define a special environmental variable in your main training script, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The training toolkit package is available in the PyPI ([pypi.org](http://pypi.org))
    package and the SageMaker GitHub repository ([https://github.com/aws/sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)).
  prefs: []
  type: TYPE_NORMAL
- en: Inference Toolkit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Inference Toolkit** implements a model serving stack that is compatible
    with SageMaker inference services. It comes together with an open source **Multi-Model
    Server** (**MMS**) to serve models. It has the following key functions:'
  prefs: []
  type: TYPE_NORMAL
- en: To establish runtime environments, such as directories to store input and output
    artifacts of inference and environmental variables. The directory structure follows
    the layout of the training container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement a handler service that is called from the model server to load
    the model into memory, and handle model inputs and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement default serializers and deserializers to handle inference requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Inference Toolkit package is available in the PyPi ([pypi.org](http://pypi.org))
    package and the GitHub repository ([https://github.com/aws/sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit)).
  prefs: []
  type: TYPE_NORMAL
- en: Developing for script mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have an understanding of SageMaker’s container ecosystem, let’s
    implement several learning projects to build practical skills. In this first example,
    we will use SageMaker script mode to train our custom NLP model and deploy it
    for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Problem overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we will learn how to develop training and inference scripts
    using the Hugging Face framework. We will leverage prebuilt SageMaker containers
    for Hugging Face (with the PyTorch backend).
  prefs: []
  type: TYPE_NORMAL
- en: 'We chose to solve a typical NLP task: text classification. We will use the
    `20 Newsgroups` dataset, which assembles ~20,000 newsgroup documents across 20
    different newsgroups (categories). There are a number of model architectures that
    can address this task. Usually, current SOTA models are based on Transformer architecture.
    Autoregressive models such as **BERT** and its various derivatives are suitable
    for this task. We will use a concept known as **transfer learning**, where a model
    that is pretrained for one task is used for a new task with minimal modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a baseline model, we will use model architecture known as **DistilBERT**,
    which provides high accuracy on a wide variety of tasks and is considerably smaller
    than other models (for instance, the original BERT model). To adapt the model
    for a classification task, we would need to add a classification layer, which
    will be trained during our training to recognize articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The model architecture for the text classification task ](img/B17519_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The model architecture for the text classification task
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hugging Face Transformers library simplifies model selection and modification
    for fine-tuning in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides a rich model zoo with a number of pretrained models and tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a simple model API to modify the baseline model for fine-tuning a specific
    task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It implements inference pipelines, combining data preprocessing and actual inference
    together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full source code of this learning project is available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Developing a training script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When running SageMaker training jobs, we need to provide a training script.
    Additionally, we might provide any other dependencies. We can also install or
    modify Python packages that are installed on prebuilt containers via the `requirements.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use a new feature of the Hugging Face framework to
    fine-tune a multicategory classifier using the Hugging Face Trainer API. Let’s
    make sure that the training container has the newer Hugging Face Transformer library
    installed. For this, we create the `requirements.txt` file and specify a minimal
    compatible version. Later, we will provide this file to our SageMaker training
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to develop the training script. Let’s review some key components
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'At training time, SageMaker starts training by calling `user_training_script
    --arg1 value1 --arg2 value2 ...`. Here, `arg1..N` are training hyperparameters
    and other miscellaneous parameters provided by users as part of training job configuration.
    To correctly kick off the training process in our script, we need to include `main
    guard` within our script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To correctly capture the parameters, the training script needs to be able to
    parse command-line arguments. We use the Python `argparse` library to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train()` method is responsible for running end-to-end training jobs. It
    includes the following components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calling `_get_tokenized_dataset` to load and tokenize datasets using a pretrained
    DistilBERT tokenizer from the Hugging Face library.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and configuring the DistilBERT model from the Hugging Face model zoo.
    Please note that we update the default configuration for classification tasks
    to adjust for our chosen number of categories.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Hugging Face Trainer and starting the training process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the training is done, we save the trained model:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'So far in our script, we have covered key aspects: handling configuration settings
    and model hyperparameters, loading pretrained models, and starting training using
    the Hugging Face Trainer API.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting the training job
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have our training script and dependencies ready, we can proceed with
    the training and schedule a training job via SageMaker Python SDK. We start with
    the import of the Hugging Face Estimator object and get the IAM execution role
    for our training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define the hyperparameters of our model and training processes.
    These variables will be passed to our script at training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After that, the training job will be scheduled and executed. It will take 10–15
    minutes for it to complete, then the trained model and other output artifacts
    will be added to Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an inference script for script mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a trained model, let’s deploy it as a SageMaker real-time endpoint.
    We will use the prebuilt SageMaker Hugging Face container and will only provide
    our inference script. The inference requests will be handled by the **AWS MMS**,
    which exposes the HTTP endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using prebuilt inference containers, SageMaker automatically recognizes
    our inference script. According to SageMaker convention, the inference script
    has to contain the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_fn(model_dir)` is executed at the container start time to load the model
    into memory. This method takes the model directory as an input argument. You can
    use `model_fn()` to initialize other components of your inference pipeline, such
    as the tokenizer in our case. Note, Hugging Face Transformers have a convenient
    Pipeline API that allows us to combine data preprocessing (in our case, text tokenization)
    and actual inference in a single object. Hence, instead of a loaded model, we
    return an inference pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`transform_fn(inference_pipeline, data, content_type, accept_type)` is responsible
    for running the actual inference. Since we are communicating with an end client
    via HTTP, we also need to do payload deserialization and response serialization.
    In our sample example, we expect a JSON payload and return a JSON payload; however,
    this can be extended to any other formats based on the requirements (for example,
    CSV and Protobuf):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes, combining deserialization, inference, and serialization in a single
    method can be inconvenient. Alternatively, SageMaker supports a more granular
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_fn(request_body, request_content_type)` runs deserialization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predict_fn(deser_input, model)` performs predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_fn(prediction, response_content_type)` runs the serialization of predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the `transform_fn()` method is mutually exclusive with the `input_fn()`,
    `predict_fn()`, and `output_fn()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Text Classification endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we are ready to deploy and test our Newsgroup Classification endpoint.
    We can use the `estimator.create_model()` method to configure our model deployment
    parameters, specifically the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the inference script and other dependencies that will be uploaded by
    SageMaker to an endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Identify the inference container. If you provide the `transformers_version`,
    `pytorch_version`, and `py_version` parameters, SageMaker will automatically find
    an appropriate prebuilt inference container (if it exists). Alternatively, you
    can provide `image_uri` to directly specify the container image you wish to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the parameters of our endpoint such as the number and type
    of instances behind it. The `model.deploy()` method starts the inference deployment
    (which, usually, takes several minutes) and returns a `Predictor` object to run
    inference requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, let’s explore how to extend pre-built DL containers.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the prebuilt containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will reuse code assets from the script mode example. However, unlike the
    previous container, we will modify our runtime environment and install the latest
    stable Hugging Face Transformer from the GitHub master branch. This modification
    will be implemented in our custom container image.
  prefs: []
  type: TYPE_NORMAL
- en: First off, we need to identify which base image we will use. AWS has published
    all of the available DL containers at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
  prefs: []
  type: TYPE_NORMAL
- en: Since we plan to use reinstall from scratch HugggingFace Transformer library
    anyway, we might choose the PyTorch base image. At the time of writing, the latest
    PyTorch SageMaker container was `763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04`.
    Note that this container URI is for the AWS East-1 region and will be different
    for other AWS regions. Please consult the preceding referenced AWS article on
    the correct URI for your region.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a new container, we will need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Dockerfile with runtime instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the container image locally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Push the new container image to the **container registry**. In this example,
    we will use ECR as a container registry: a managed service from AWS, which is
    well integrated into the SageMaker ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let’s create a Dockerfile for our extended container.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a Dockerfile for our extended container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To extend the prebuilt SageMaker container, we need to have at least the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: A SageMaker PyTorch image to use as a base.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required dependencies installed, such as the latest PyTorch and Hugging
    Face Transformers from the latest Git master branch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy our training script from the previous example into the container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define the `SAGEMAKER_SUBMIT_DIRECTORY` and `SAGEMAKER_PROGRAM` environmental
    variables, so SageMaker knows which training script to execute when the container
    starts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we are ready to build and push this container image to ECR. You can find
    the `bash` script to do this in the chapter repository.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling a training job
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have our extended PyTorch container in ECR, we are ready to execute
    a SageMaker training job. The training job configuration will be similar to the
    script mode example with one notable difference: instead of the `HuggingFaceEstimator`
    object, we will use a generic SageMaker `Estimator` object that allows us to work
    with custom images. Note that you need to update the `image_uri` parameter with
    reference to the image URI in your ECR instance. You can find it by navigating
    to the ECR service on your AWS Console and finding the extended container there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After completing the training job, we should expect similar training outcomes
    as those shown in the script mode example.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a BYO container for inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to build a SageMaker-compatible inference
    container using an official TensorFlow image, prepare an inference script and
    model server, and deploy it for inference on SageMaker Hosting.
  prefs: []
  type: TYPE_NORMAL
- en: Problem overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will develop a SageMaker-compatible container for inference. We will use
    the latest official TensorFlow container as a base image and use AWS MMS as a
    model server. Please note that MMS is one of many ML model serving options that
    can be used. SageMaker doesn’t have any restrictions on a model server other than
    that it should serve models on port `8080`.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the serving container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When deploying a serving container to the endpoint, SageMaker runs the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To comply with this requirement, it’s recommended that you use the exec format
    of the `ENTRYPOINT` instruction in your Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review our BYO Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the latest TensorFlow container as a base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We install general and SageMaker-specific dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We copy our model serving scripts to the container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specify `ENTRYPOINT` and the CMD instructions to comply with the SageMaker
    requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s put it into action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the latest official TensorFlow container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install Java, as required by MMS and any other common dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the entrypoint script to the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Copy the default custom service file to handle incoming data and inference
    requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an entrypoint script and its default parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this example, we don’t intend to cover MMS and the development of inference
    scripts in detail. However, it’s worth highlighting some key script aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dockerd_entrypoint.py` is an executable that starts the MMS server when the
    `serve` argument is passed to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_handler.py` implements model-loading and model-serving logics. Note
    that the `handle()` method checks whether the model is already loaded into memory.
    If it’s not, it will load a model into memory once and then proceed to the handling
    serving request, which includes the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deserializing the request payload
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Serializing predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the SageMaker endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To schedule the deployment of the inference endpoint, we use the generic `Model`
    class from SageMaker Python SDK. Note that since we downloaded the model from
    a public model zoo, we don’t need to provide a `model_data` parameter (hence,
    its value is `None`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It might take several minutes to fully deploy the endpoint and start the model
    server. Once it’s ready, we can call the endpoint using the `boto3.sagemaker-runtime`
    client, which allows you to construct the HTTP request and send the inference
    payload (or image, in our case) to a specific SageMaker endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code will, most likely, return an object in the image based on model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed how SageMaker provides support for the ML and DL
    frameworks using Docker containers. After reading this chapter, you should now
    know how to select the most appropriate DL container usage pattern according to
    your specific use case requirements. We learned about SageMaker toolkits, which
    simplifies developing SageMaker-compatible containers. In later sections, you
    gained practical knowledge of how to develop custom containers and scripts for
    training and inference tasks on Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the SageMaker development environment
    and how to efficiently develop and troubleshoot your DL code. Additionally, we
    will learn about DL-specific tools and interfaces that the SageMaker development
    environment provides to simplify the building, deploying, and monitoring of your
    DL models.
  prefs: []
  type: TYPE_NORMAL
