- en: '*Chapter 4:* Building and Training a Feedforward Neural Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073), *Getting
    Started with Neural Networks*, you learned the basic theory behind neural networks
    and deep learning. This chapter sets that knowledge into practice. We will implement
    two very simple classification examples: a multiclass classification using the
    **iris flower** dataset, and a binary classification using the adult dataset,
    also known as the **census income** dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: These two datasets are quite small and the corresponding classification solutions
    are also quite simple. A fully connected feedforward network will be sufficient
    in both examples. However, we decided to show them here as toy examples to describe
    all of the required steps to build, train, and apply a fully connected feedforward
    classification network with **KNIME Analytics Platform** and **KNIME Keras Integration**.
  prefs: []
  type: TYPE_NORMAL
- en: These steps include commonly used preprocessing techniques, the design of the
    neural architecture, the setting of the activation functions, the training and
    application of the network, and lastly, the evaluation of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, this chapter covers the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Feedforward Neural Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and Applying the Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073), *Getting Started
    with Neural Networks*, we introduced the backpropagation algorithm, which is used
    by gradient descent algorithms to train a neural network. These algorithms work
    on numbers and can't handle nominal/categorical input features or class values.
    Therefore, nominal input features or nominal output values must be encoded into
    numerical values if we want the network to make use of them. In this section,
    we will show several numerical encoding techniques and the corresponding nodes
    in KNIME Analytics Platform to carry them out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides that, we will also go through many other classic data preprocessing
    steps to feed machine learning algorithms: creating training, validation, and
    test sets from the original dataset; normalization; and missing value imputation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, we will also show you how to import data, how to perform a few
    additional data operations, and some commonly used tricks within KNIME Analytics
    Platform. The workflows described in this chapter are available on the KNIME Hub:
    [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%204/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%204/).'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and Classification Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into the different preprocessing steps, let''s have a quick
    look at the two selected datasets and the associated classification examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification of three iris flowers based on the Iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of income (binary class) based on the data from the adult dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first dataset gives us an example of a multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Iris dataset consists of examples of flowers from three species of iris
    plants: Iris-setosa, Iris-virginica, and Iris-versicolor. Each flower is described
    through four measures: sepal length (cm), sepal width (cm), petal length (cm),
    and petal width (cm). This is a small dataset with 50 examples for each species,
    with 150 samples in total. *Figure 4.1* shows an overview of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to train a neural network with one hidden layer (eight units and
    the ReLU activation function) to distinguish the three species from each other
    based on the four input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of the Iris dataset is displayed in the following tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Overview of the Iris dataset, used here to implement a multiclass
    classification](img/B16391_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Overview of the Iris dataset, used here to implement a multiclass
    classification
  prefs: []
  type: TYPE_NORMAL
- en: The second example dataset provides us with a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adult dataset consists of 32,561 samples of people living in the US. Each
    record describes a person through 14 demographics features, including their current
    annual income (> 50K/<= 50K). *Figure 4.2* shows an overview of the features in
    the dataset: numerical features, such as age and hours worked per week, and nominal
    features, such as work class and marital status.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to train a neural network to predict whether a person earns more
    or less than 50K per year, using all the other attributes as input features. The
    network we want to use should have two hidden layers, each one with eight units
    and the ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the census income dataset displayed in tables looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Overview of the adult dataset, used here to implement a binary
    class classification](img/B16391_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Overview of the adult dataset, used here to implement a binary
    class classification
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To get an overview of the dataset, you can use the **Data Explorer** node. This
    node displays some statistical measures of the input data within an interactive
    view. In *Figure 4.1* and *Figure 4.2*, you can see the view of the node for the
    two example datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize the Iris dataset, it consists of four numerical features, plus
    the iris nominal class; the adult dataset consists of 14 mixed features, numerical
    and nominal. The first step in the data preparation would, therefore, be to transform
    all nominal features into numerical ones. Let's move on, then, to the encoding
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding of Nominal Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nominal features, sometimes also called categorical features, can assume only
    string-type values. For example, the only possible values for a feature describing
    hair color can be a string type, such as `black`, `brown`, `blond`, and `red`;
    a feature describing gender traditionally assumes only two string-type values,
    `female` and `male`; and the possible values for an education feature can be strings,
    such as `Doctorate`, `Masters`, `Bachelors`, or `Some-college`. This last example
    is taken from the column named `education` in the adult dataset. These values
    should be transformed into numbers before being fed to a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to encode nominal features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integer encoding**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-hot encoding**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integer encoding assigns an integer value to each possible nominal value of
    a feature. For example, `"black"` can be 1, `"brown"` can be 2, `"blond"` can
    be 3, and `"red"` can be 4\. We have chosen the numbers 1, 2, 3, and 4 but it
    could have been any other set of numbers. This approach introduces an artificial
    relationship between the different values – for example, that `"black"` is closer
    to `"brown"` than to `"red"`. This can reflect a true relationship across values
    in ordinal or hierarchical features, such as education, where `"Doctorate"` is
    closer to `"Masters"` than to `"Some-college"`. However, in other cases, such
    as the previously mentioned hair color one, it introduces a new additional relationship
    that does not reflect reality and can bias the model during learning. Generally
    speaking, using the integer encoding approach on nominal unordered features can
    lead to worse-performing models.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot vector encoding overcomes this problem by representing each feature
    with a vector, where the distance across all the vectors is always the same. The
    vector consists of the same quantity of binary components as possible values in
    the original feature. Each component is then associated with one of the values
    and is set to `1` for that value; the other components remain set to `0`. In the
    hair color example, `"black"` becomes ![](img/Formula_B16391_04_001.png), `"brown"`
    becomes ![](img/Formula_B16391_04_002.png), `"blond"` becomes ![](img/Formula_B16391_04_003.png),
    and `"red"` becomes ![](img/Formula_B16391_04_004.png).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A one-hot vector is a vector with a single `1` and all other values being `0`.
    It can be used to encode different classes without adding any artificial distance
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see now how to implement these encodings with KNIME nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Integer Encoding in KNIME Analytics Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform integer encoding, you can use the **Category to Number** node. This
    node has one data input port (represented by a black triangle in the following
    diagram) and two output ports:'
  prefs: []
  type: TYPE_NORMAL
- en: A data output port (black triangle) with the integer-encoded data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PMML model output port (blue square) with the mapping rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 4.3* shows you the node, as well as its configuration window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The Category to Number node performs an integer encoding on
    the selected columns](img/B16391_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – The Category to Number node performs an integer encoding on the
    selected columns
  prefs: []
  type: TYPE_NORMAL
- en: In the upper part of the configuration window, you can select the string-type
    input columns to apply the integer encoding to. The columns in the **Include**
    framework will be transformed, while the columns in the **Exclude** framework
    will be left unchanged. You can move columns from one framework to the other using
    the buttons between them.
  prefs: []
  type: TYPE_NORMAL
- en: By default, values in the original columns are replaced with the integer-encoded
    values. However, the **Append columns** checkbox creates additional columns for
    the integer-encoded values so as not to overwrite the original columns. If you
    activate this checkbox, you can also define a custom suffix for the new columns'
    headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the lower part of the configuration window, you can define the encoding
    rule: the start value, the increment, the maximum allowed number of categories,
    and an integer value for all missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: The default integer value is transferred to the output PMML transformation model.
    **PMML** stands for **Predictive Model Markup Language** and is a way to describe
    and exchange predictive models between different applications. The PMML model
    at the blue square output port contains the mapping function built in this node
    and to be applied to other datasets. When applying this integer encoding PMML
    model, the default value is assigned to all input values not represented by the
    current mapping (if any). If no default value is present, a missing value will
    be used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To apply the same integer encoding mapping stored in the PMML output port to
    another dataset, you can use the **Category to Number (Apply)** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Category to Number** node defines the mapping automatically. This means
    you cannot manually define which nominal value should be represented by which
    integer value. If you wish to do so, you have other options in KNIME Analytics
    Platform, and we will introduce two of them: the **Cell Replacer** node and the
    **Rule Engine** node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Cell Replacer** node replaces cell values in a column according to a
    dictionary table. It has two inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The top input for the table with the target column whose values are to be replaced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lower input for the dictionary table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 4.4* shows the configuration window of the Cell Replacer node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The Cell Replacer node implements an encoding mapping based
    on a dictionary](img/B16391_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – The Cell Replacer node implements an encoding mapping based on
    a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: In the upper part of the configuration window, you can select the target column
    from the input table at the top input port; this means the column whose values
    you want to replace based on the dictionary values.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Dictionary table** part of the configuration window, you can select,
    from the data table at the lower input port, the column with the lookup values
    – that is, the **Input (Lookup)** column – and the column containing the replacement
    values – that is, the **Output (Replacement)** column.
  prefs: []
  type: TYPE_NORMAL
- en: Any occurrence in the target column (first input) that matches the lookup value
    is replaced with the corresponding replacement value. The result is stored in
    the output column, which is either added to the table or replaces the original
    target column.
  prefs: []
  type: TYPE_NORMAL
- en: Missing values are treated as ordinary values; that is, they are valid values
    both as lookup and replacement values. If there are duplicates in the lookup column
    in the dictionary table, the last occurrence (lowest row) defines the replacement
    pair.
  prefs: []
  type: TYPE_NORMAL
- en: For the integer encoding example, you need a dictionary table to map the nominal
    values and the integer values. For example, each education level should be mapped
    to a corresponding integer value. You can then feed the original dataset into
    the top input port and this map/dictionary table into the lowest input port.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The **Table Creator** node can be helpful to manually create the lookup table.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have a dictionary table and you don't want to create one, you can
    use the **Rule Engine** node.
  prefs: []
  type: TYPE_NORMAL
- en: The Rule Engine node transforms the values in the input columns according to
    a set of manually defined rules, which are defined in its configuration window.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.5* shows you the configuration window of the Rule Engine node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The Rule Engine node implements an integer encoding from user-defined
    rules](img/B16391_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The Rule Engine node implements an integer encoding from user-defined
    rules
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `=>`, in the form of `"antecedent => consequence"`. The results are
    either inserted into a new column or replace the values in a selected column.
    For each data row in the input table, the rule-matching process moves from the
    top rule to the lowest: the first matching rule determines the outcome, and then
    the rule process stops. The last default condition, collecting all the remaining
    data rows, is expressed as `"TRUE => consequence"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of a rule may be a string (in between `"` or `/` symbols), a number,
    a Boolean constant, or a reference to another column. If no rule matches, the
    outcome is a missing value. References to other columns are represented by the
    column name in between `$`. You can insert a column reference by hand or by double-clicking
    on a column in **Column List** on the left side of the configuration window.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the **Expression** panel, you find the **Function**, **Column List**,
    and **Flow Variable List** panels. The **Function** panel lists all functions,
    the **Column List** panel lists all input columns, and **Flow Variable List**
    contains all the available flow variables. Double-clicking on any of them adds
    them to the **Expression** window with the right syntax. Also, selecting any of
    the functions shows a description as well as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, there are many ways to implement integer encoding in KNIME Analytics
    Platform. We introduced three options:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Category to Number** node offers an automatic, easy approach if you do
    not want to define the mapping by hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Cell Replacer** node is really useful if you have a lookup table at hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Rule Engine** node is useful if you want to manually define the mapping
    between the nominal values and the integer values via a set of rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at one-hot encoding in KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot-Encoding in KNIME Analytics Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform one-hot encoding on nominal features, there is the **One to Many**
    node. This node takes the list of nominal values available in a column, builds
    a vector with as many components, and produces the one-hot encoding of each value:
    one value to become many binary cells, hence the name.'
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window, you can select the string-type columns on which
    to perform the one-hot encoding. For each column, as many new columns will be
    created as there are different values. The header of each new column will be the
    original value in the nominal column and its cells take a value of either `0`
    or `1`, depending on the presence or absence of the header value in the original
    column.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.6* shows the configuration window of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The One to Many node implements the one-hot encoding for nominal
    features](img/B16391_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – The One to Many node implements the one-hot encoding for nominal
    features
  prefs: []
  type: TYPE_NORMAL
- en: Creating one-hot encoded vectors leads to very large and very sparse data tables
    with many zeros. This can weigh on the workflow performance during execution.
    The Keras Learner node does accept large and sparse one-hot-encoded data tables.
    However, it also offers a very nice optional feature that avoids this whole step
    of explicitly creating the data table with the one-hot-encoded vectors. It can
    create the one-hot-encoded vectors internally from an integer-encoded version
    of the original column. In this way, the one-hot encoding representation of the
    data remains hidden within the **Keras Network Learner** node and is never passed
    from node to node. In this case, the value of each integer-encoded cell must be
    presented to the Keras Network Learner node as a collection type cell. To create
    a collection type cell, you can use the **Create Collection Column** node. In
    the *Training the Network* section of this chapter, you will see how to configure
    the Keras Network Learner node properly to make use of this feature.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.7* shows the configuration window of the **Create Collection Column**
    node. In the **Exclude-Include** frame, you select one or more columns to aggregate
    in a collection-type column. In the lower part of the configuration window, you
    can decide whether to remove the original columns and define the new collection
    type column''s name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The Create Collection Column node aggregates the values from
    multiple columns as a collection into one single column](img/B16391_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The Create Collection Column node aggregates the values from multiple
    columns as a collection into one single column
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that for this two-step one-hot encoding – first integer encoding, then
    one-hot encoding – you need to create the integer encoding column with one of
    the nodes listed in the previous section, and then apply the **Create Collection
    Column** node to just one column: the integer-encoded column that we have just
    created.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding of Categorical Target Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last chapter, we talked about different activation functions and loss
    functions that can be used to build network architectures and train networks to
    solve classification problems. Of course, the activation function in the output
    layer and the loss function must match. Not only that, but the class encoding
    must also match the chosen activation and loss function. This means that not only
    must nominal input features be encoded, but class values too. The same encoding
    techniques and nodes, as described in this section, can be used for class encoding
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: A common approach to binary classification is to encode the two classes with
    ![](img/Formula_B16391_04_005.png) and ![](img/Formula_B16391_04_006.png) and
    then to train the network to predict the probability for the ![](img/Formula_B16391_04_007.png)
    class. In this case, either the Category to Number node or the Rule Engine node
    can work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a multiclass problem, there are also two options to encode the
    class column: the **One to Many** node on its own or the Category to Number followed
    by the Create Collection Column node.'
  prefs: []
  type: TYPE_NORMAL
- en: Another recommended preprocessing step for neural networks is normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most neural networks are trained using some variant of stochastic gradient descent
    with the backpropagation algorithm to calculate the gradient. Input features with
    non-comparable ranges can create problems during learning, as the input features
    with the largest range can overpower the calculation of the weight update, possibly
    even overshooting a local minimum. This can create oscillations and slow down
    the convergence of the learning process. To speed up the learning phase, it is
    recommended to normalize the data in advance; for example, by using the z-score
    normalization so that the values in each column are Gaussian-distributed with
    a mean of 0.0 and a standard deviation of 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 4.8*, you can see the **Normalizer** node and its configuration
    window, as well as the **Normalizer (Apply)** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The Normalizer node creates a normalization function for the
    selected input columns. The Normalizer (Apply) node applies the same normalization
    function to another dataset](img/B16391_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The Normalizer node creates a normalization function for the selected
    input columns. The Normalizer (Apply) node applies the same normalization function
    to another dataset
  prefs: []
  type: TYPE_NORMAL
- en: The Normalizer node creates a normalization function on the selected input columns
    and normalizes them. The Normalizer (Apply) node takes an external predefined
    normalization function and applies it to the input data. A classic case for the
    application of this pair of nodes is on training and test sets. The Normalizer
    node normalizes the training data and the Normalizer (Apply) node applies the
    same normalization transformation to the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Normalizer node has one data input port and two output ports:'
  prefs: []
  type: TYPE_NORMAL
- en: One data output port with the normalized input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One model output port containing the normalization parameters, which can be
    used on another dataset in a Normalizer (Apply) node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the configuration window of the Normalizer node, you can select the numerical
    columns to normalize and the normalization method.
  prefs: []
  type: TYPE_NORMAL
- en: The configuration window of the Normalizer (Apply) node is minimal since all
    of the necessary parameters are contained in the input normalization model.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: With a Partitioning node, you can create the training and test sets *before*
    normalizing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Other Helpful Preprocessing Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Missing values can be a problem when training a neural network, as the backpropagation
    algorithm can't handle them. The placeholder value to represent missing values
    in a KNIME data table is a red question mark.
  prefs: []
  type: TYPE_NORMAL
- en: A powerful node to impute missing values is the **Missing Value** node. This
    node allows you to select between many imputation methods, such as mean value,
    fixed value, and most frequent value, to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.9* shows the two tabs of the configuration window of the node. In
    the first tab, the **Default** tab, you can select an imputation method to apply
    to all columns of the same type in the dataset; all columns besides those set
    in the second tab of the configuration, the **Column Settings** tab. In this second
    tab, you can define the imputation method for each individual column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The Missing Value node selects among many imputation methods
    for missing values](img/B16391_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The Missing Value node selects among many imputation methods for
    missing values
  prefs: []
  type: TYPE_NORMAL
- en: Most neural networks are trained in a supervised way. Therefore, another necessary
    step is the creation of a training set and a test set, and optionally a validation
    set. To create different disjoint subsets, you can use the **Partitioning** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the configuration window of the Partitioning node in *Figure 4.10*, you
    can set the size for the first partition, by either an absolute or a relative
    percentage number. Below that, you can set the sampling technique to create this
    first subset, by random extraction following the data distribution according to
    the categories in a selected column (stratified sampling), linearly every *n*
    data rows, or just sequentially starting from the top. The top output port produces
    the resulting partition; the lower output port produces all other remaining data
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The Partitioning node creates two disjoint subsets](img/B16391_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The Partitioning node creates two disjoint subsets
  prefs: []
  type: TYPE_NORMAL
- en: For classification problems, the **Stratified sampling** option is recommended.
    It ensures that the distribution of the categories in the selected column is (approximately)
    retained in the two partitions. For time-series analysis, the **Take from top**
    option is preferable, if your data is sorted ascending by date. Samples further
    back in time will be in one partition and more recent samples in the other.
  prefs: []
  type: TYPE_NORMAL
- en: To create an additional validation set, a sequence of two Partitioning nodes
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: We have talked about encoding for categorical features, normalization for numerical
    features, missing value imputation, and partitioning of the dataset. It is likely
    that those are not the only nodes you might need to prepare your data for the
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the data preparation works in practice, by implementing the data
    preparation part on the two example datasets we have described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation on the Iris Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Figure 4.11*, you can see the part of the workflow dedicated to accessing
    and preparing the data for the upcoming neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The workflow starts with reading the Iris dataset using the **Table Reader**
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can find the dataset in the data folder for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the dataset has only numerical input features (petal and sepal measures),
    there is no need for encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – This workflow snippet shows the preprocessing for the data
    in the iris dataset example](img/B16391_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – This workflow snippet shows the preprocessing for the data in
    the iris dataset example
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the target variable contains three different categories: the names
    of each flower species. The categories in this nominal column need to be converted
    into numbers via some encoding technique. To avoid the introduction of non-existent
    relationships, we opted for one-hot encoding. To implement the one-hot encoding,
    we chose the combination of integer encoding via nodes and one-hot encoding within
    the Keras Learner node. We will talk about the one-hot encoding internal to the
    Keras Learner node in the *Training the Network* section. Here, we will focus
    on the creation of an integer encoding of the flower classes inside a collection
    type column:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to transform the species names into an index, we use the `class` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, we pass the results from the Rule Engine node through a `class` column,
    and we exclude all other columns in the configuration window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the training and test sets are created with a **Partitioning** node, using
    75% of the data for training and the remaining 25% for testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, the data is normalized using the z-score normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Iris dataset is quite small and quite well defined. Only a few nodes, the
    minimum required, were sufficient to implement the data preparation part.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see now what happens on a more complex (but still small) dataset, such
    as the adult dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation on the Adult Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The workflow in *Figure 4.12* is part of the example on income prediction that
    reads and preprocesses the adult dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – This workflow snippet shows the preprocessing for the data
    in the adult dataset example](img/B16391_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – This workflow snippet shows the preprocessing for the data in
    the adult dataset example
  prefs: []
  type: TYPE_NORMAL
- en: 'Like for the Iris dataset, you can find the two datasets used in the workflow
    in the data folder for this chapter: the adult dataset and a dictionary Excel
    sheet. In the adult dataset, education levels are spelled out as text. The dictionary
    Excel file provides a map between the education levels and the corresponding standard
    integer codes. We could use these integer codes as the numerical encoding of the
    education input feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, the **Cell Replacer** node replaces all educational levels with the corresponding
    codes. We get one encoding practically without effort.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the nominal columns have missing values. Inside the `"Missing"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we proceed with the encoding of all other nominal features, besides education.
    For the following features, an integer encoding is used, implemented by the **Category
    to Number** node: marital status, race, and sex. We can afford to use the integer
    encoding here, because the features are either binary or with just a few categories.'
  prefs: []
  type: TYPE_NORMAL
- en: For the remaining nominal features – work class, occupation, relationship, and
    native-country – one-hot encoding is used, implemented by the **One to Many**
    node. Remember that this node creates one new column for each value in each of
    the selected columns. So, after this transformation, the dataset has 82, instead
    of the original 14, features.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the training, validation, and test sets are created with a sequence of
    two `Income` class column.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the `Income` column gets integer encoded on all subsets and all their
    data is normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To hide complexity and to tidy up your workflows, you can create **metanodes**.
    Metanodes are depicted as gray nodes and contain sub-workflows of nodes. To create
    a metanode, select the nodes you want to hide, right-click, and select **Create
    Metanode**.
  prefs: []
  type: TYPE_NORMAL
- en: Our data is ready. Let's now build the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Feedforward Neural Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a neural network architecture using the KNIME Keras integration, you
    can use a chain of Keras layer nodes. The available nodes to construct layers
    are grouped by categories in the **Keras->Layers** folder in the **Node Repository**,
    such as **Advanced Activations**, **Convolution**, **Core**, **Embedding**, and
    **Recurrent**, to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer displayed in the **Keras->Layers** folder has a specialty. For example,
    layers in **Advanced Activations** create layers with units with specific activation
    functions; layers in **Convolution** create layers for convolutional neural networks;
    **Core** contains all classic layers, such as the **Input** layer to collect the
    input values and the **Dense** layer for a fully connected feedforward neural
    network; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore many of these layers along the way in this book. However, in
    this current chapter, we will limit ourselves to the basic layers needed in a
    fully connected feedforward neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The first layer in any network is the layer that receives the input values.
    Let's start from the Keras Input Layer node.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Input Layer Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a neural network always starts with defining the input layer of the
    network. The **Keras Input Layer** node can help you with this task. Indeed, this
    node builds the required inputs for the network to accept the input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left of *Figure 4.13*, you can see the Keras Input Layer node and on
    the right its configuration window. As you can see, the node does not have an
    input port, just one output port of a different shape and color (red square) from
    the nodes encountered so far: this is the **Keras Network Port**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The Keras Input Layer node defines the input layer of your
    neural network](img/B16391_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The Keras Input Layer node defines the input layer of your neural
    network
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The color and shape of a port indicate which ports can be connected with each
    other. Most of the time, only ports of the same color and shape can be connected,
    but there are exceptions. For example, you can connect a gray square, which is
    a Python DL port, with a Keras port, a red square.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer node has a configuration window, with the setting options required
    for this specific layer. Compared to other layer nodes, this node has a simple
    configuration window with only a few setting options.
  prefs: []
  type: TYPE_NORMAL
- en: The most important setting is **Shape**. **Shape** allows you to define the
    input shape of your network, meaning how many neurons your input layer has. Remember,
    the number of neurons in the input layer has to match the number of your preprocessed
    input columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Iris dataset has four features that we will use as inputs: sepal length,
    sepal width, petal length, and petal width. Therefore, the input shape here is
    4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, in the configuration window of the Keras Input Layer node, you
    can set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Name prefix** for the layer, so it is easier to identify it later on (optional).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Batch size** (optional). Remember, the batch size is one of the setting
    options for the training. The recommended way is to define the batch size in the
    learner and executor node. In addition, you have the option to define it here.
    If a batch size is defined, then the batch size option in the learner and executor
    nodes are not available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Data type** and **Data format** of the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your network now has its first layer, the input layer. Now, you can continue
    to build your network by creating and connecting the next layer node to the output
    of the Keras Input Layer node – for example, a **Keras Dense Layer** node.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To create a node and immediately connect it to an existing node, select the
    existing node in the workflow editor and double-click the new node in the Node
    Repository. This will create the new node and connect it automatically to the
    selected existing node.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Dense Layer Node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Keras Dense Layer** node implements a classic layer in a feedforward fully
    connected network. The parameters to set here are the number of neural units and
    the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.14* shows the configuration window of this node. The setting options
    are split into two tabs: **Options** and **Advanced**.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Options** tab contains the most important settings, such as the number
    of neurons, also known as units, and the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the **Input tensor** setting defines the part of the input tensor
    coming from the previous node. In a feedforward network, the input tensor is the
    output tensor from the previous layer. However, some layer nodes – such as, for
    example, the **Keras LSTM Layer** node – create not just one hidden output tensor,
    but multiple. In such cases, you must select one among the different input tensors,
    or hidden states, produced by the previous layer node. Keras Input Layer, like
    Keras Dense Layer, produces only one output vector and this is what we select
    as input tensor to our Keras Dense Layer node.
  prefs: []
  type: TYPE_NORMAL
- en: In the upper part of the **Advanced** tab, you can select how to randomly initialize
    the weights and biases of the network; this means the starting values for all
    weights and biases before the first iteration of the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: The lower part of the **Advanced** tab allows you to add norm regularization
    for the weights in this layer. Norm regularization is a technique to avoid overfitting,
    which we introduced in [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073),
    *Getting Started with Neural Networks*. In the configuration window, you can select
    whether to apply it to the kernel weight matrix, the bias vector, and/or the layer
    activation. After activating the corresponding checkbox, you can select between
    using the L1 norm as a penalty term, the L2 norm as a penalty term, or both. Lastly,
    you can set the value for the regularization parameter, ![](img/Formula_B16391_04_008.png),
    for the penalty terms and constraints on the weight and bias values.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the **Keras Input Layer** node and multiple Keras Dense Layer nodes,
    you can build a feedforward network for many different tasks, such as, for example,
    to classify iris flowers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The Keras Dense Layer node allows you to add a fully connected
    layer to your neural network, including a selection of commonly used activation
    functions](img/B16391_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The Keras Dense Layer node allows you to add a fully connected
    layer to your neural network, including a selection of commonly used activation
    functions
  prefs: []
  type: TYPE_NORMAL
- en: Configuration of the other layer nodes is similar to what was described here
    for the dense and input layers, and you will learn more about them in the next
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Since both basic examples used in this chapter refer to feedforward networks,
    we now have all of the necessary pieces to build both feedforward neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Neural Network for Iris Flower Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the multiclass classification problem using the Iris dataset, the goal
    was to build a fully connected feedforward neural network with three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: One input layer with four units, one for each input feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One hidden layer with eight units and the ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One output layer with three units, one for each output class, meaning one for
    each iris species, with the softmax activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We opted for the ReLU activation function in the hidden layer for its better
    performance when used in hidden layers, and for the softmax activation function
    in the output layer for its probabilistic interpretability. The output unit with
    the highest output from the softmax function is the unit with the highest class
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.15* shows the neural network architecture used for the iris classification
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – A diagram of the feedforward network used for the iris flower
    example](img/B16391_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – A diagram of the feedforward network used for the iris flower
    example
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.16* shows the workflow snippet with the three layer nodes building
    the network and their configuration windows, including the number of units and
    activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – This workflow snippet builds the neural network in Figure 4.15
    for the Iris dataset example. The configuration windows below them show you the
    nodes'' configurations](img/B16391_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – This workflow snippet builds the neural network in Figure 4.15
    for the Iris dataset example. The configuration windows below them show you the
    nodes' configurations
  prefs: []
  type: TYPE_NORMAL
- en: The input layer has four input units, `Shape = 4`, for the four numerical input
    features. The first Keras Dense Layer node, which is the hidden layer, has eight
    units and uses the ReLU activation function. In the output layer, the softmax
    activation function is used with three units, one unit for each class.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In the last layer, the name prefix **Output** has been used. This makes it easier
    to identify the layer in the **Executor** node and has the advantage that the
    layer name doesn't change if more Keras Dense Layer nodes are added as hidden
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Neural Network for Income Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second proposed example was a binary classification problem: predicting
    income (greater or lower than 50K per year) in the adult dataset. Here, we adopted
    a neural network with two hidden layers, with four layers in total:'
  prefs: []
  type: TYPE_NORMAL
- en: One input layer with 81 units, as many as input features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One hidden layer with six units and the ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One more hidden layer with six units and the ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One output layer with one unit and the sigmoid activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output layer uses a classic implementation for the binary classification
    problem: one single unit with the sigmoid activation function. The sigmoid function,
    spanning a range of 0 and 1, can easily implement class attribution using `0`
    for one class and `1` for the other. Thus, for a binary classification problem,
    where the two classes are encoded as `0` and `1`, one sigmoid function alone can
    produce the probability for the class encoded as `1`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.17* shows you the workflow snippet that builds this fully connected
    feedforward neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – This workflow snippet builds the fully connected feedforward
    neural network used as a solution for the adult dataset example](img/B16391_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – This workflow snippet builds the fully connected feedforward neural
    network used as a solution for the adult dataset example
  prefs: []
  type: TYPE_NORMAL
- en: After preprocessing, the adult dataset ends up having 82 columns, 81 input features,
    and the target column. Therefore, the input layer has `Shape = 81`. Next, the
    two hidden layers are built using two Keras Dense Layer nodes with `Units = 6`
    and the ReLU activation function. The output layer consists of a Keras Dense Layer
    node, again with `Units = 1` and the sigmoid activation function.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you've learned how to build a feedforward neural network using
    the KNIME Keras integration nodes. The next step is to set the other required
    parameters for the network training, such as, for example, the loss function,
    and then to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have the data ready and we have the network. The goal of this section is
    to show you how to train the network with the data in the training set. This requires
    the selection of the loss function, the setting of the training parameters, the
    specification of the training set and the validation set, and the tracking of
    the training progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key node for network training and for all these training settings is the
    **Keras Network Learner** node. This is a really powerful, really flexible node,
    with many possible settings, distributed over four tabs: **Input Data**, **Target
    Data**, **Options**, and **Advanced Options**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Keras Network Learner node has three input ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Top port**: The neural network you want to train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Middle port**: The training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lowest port**: The optional validation set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has one output port, exporting the trained network.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the node has the **Learning Monitor** view, which you can use to
    monitor the network training progress.
  prefs: []
  type: TYPE_NORMAL
- en: Let's find out first how to select the loss function before we continue with
    the training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073), *Getting
    Started with Neural Networks*, we introduced many loss functions, each one suitable
    for a specific task, as the last design choice for your network. For example,
    mean squared error is commonly used in regression problems or categorical cross-entropy
    in multiclass classification problems. In the lower part of the **Target Data**
    tab, you can either select between different standard prepackaged loss functions
    or define your own custom loss function using Python (see *Figure 4.18*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – In the Target Data tab of the Keras Network Learner node, you
    can select the target columns and the loss function](img/B16391_04_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – In the Target Data tab of the Keras Network Learner node, you
    can select the target columns and the loss function
  prefs: []
  type: TYPE_NORMAL
- en: Now that the network structure is defined and you have selected the correct
    loss function, the next step is to define which columns of the input dataset are
    the inputs for your network and which column contains the target values.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Input and Output Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Defining the input and output columns is something you can do in the **Input
    Data** and **Target Data** tabs. Let's focus first on the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data is the data that your network expects as input, which means
    the columns that fit the input size of the network. In the **Input Data** tab,
    the number of input neurons for the selected network and the consequent shape
    are reported at the very top:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – In the Input Data tab of the Keras Network Learner node, you
    can'
  prefs: []
  type: TYPE_NORMAL
- en: select the input column(s) and the correct conversion](img/B16391_04_019.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – In the Input Data tab of the Keras Network Learner node, you can
    select the input column(s) and the correct conversion
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you must select the conversion type; this means the transformation for
    the selected input columns into a format that is accepted by the network input
    specification. The possible conversion types are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**From Collection of Number (integer) to One-Hot Tensor**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Number (double)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Number (integer)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Collection of Number (double)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Collection of Number (integer)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Image**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion type 1, **From Collection of Numbers (integer) to One-Hot Tensor**,
    is a really helpful transformation when the network requires one-hot vectors.
    Instead of creating a matrix with all the one-hot vectors, which takes up space
    and resources, you can input a sequence of integer-encoded values and then transform
    them, one by one, into one-hot vectors. During execution, the node creates the
    one-hot vectors and inputs them into the network. The whole process is hidden
    from the end user and no additional large, sparse data tables are created.
  prefs: []
  type: TYPE_NORMAL
- en: The other conversion types just take the input columns in the specified format
    (double, integer, or image) and present them to the network.
  prefs: []
  type: TYPE_NORMAL
- en: After selecting the conversion type, you can select the input columns to the
    network through an include-exclude frame. Notice that the frame has been pre-loaded
    with all the input columns matching the selected conversion type.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now select the target column. The target data must match the specifications
    from the output layer. This means that, if your output layer has 20 units, your
    target data must be 20-dimensional vectors; or, if your output layer has only
    one unit, your target data must also consist of one single value for each training
    sample or data row.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Target Data** tab, at the very top, the number of neurons in the output
    layer of the network and the resulting shape is reported. Like in the **Input
    Data** tab, here you can select from many conversion options to translate from
    the input dataset into the network specifications. The menu, with all the available
    conversion types to select from, has been preloaded with the conversion types
    that fit the specifications of the output layer of the network.
  prefs: []
  type: TYPE_NORMAL
- en: For multiclass classification problems, the conversion type from a collection
    of numbers (integer) to one-hot tensor is really helpful. Instead of creating
    the one-hot vectors in advance, you need only to encode the position of the class
    (1) in the input collection cell.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the Training Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the network and the loss function have been defined, the next step
    is to set the training parameters. For example, which optimizer do you want to
    use? How many epochs do you want to train for? There are many parameters to be
    defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the training parameters can be found in the **Options** and **Advanced
    Options** tabs. In *Figure 4.20*, you can see the **Options** tab of the **Keras
    Network Learner** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – In the Options tab of the Keras Network Learner node, you can
    set all the training parameters](img/B16391_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – In the Options tab of the Keras Network Learner node, you can
    set all the training parameters
  prefs: []
  type: TYPE_NORMAL
- en: In the upper part of the **Options** tab, in the configuration window, you can
    define the number of epochs and the batch size. This determines the number of
    data rows from the training and validation sets to feed the network in batches
    within each training iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you defined a batch size in the Keras Input Layer node, the batch size settings
    are deactivated.
  prefs: []
  type: TYPE_NORMAL
- en: Under that, there are two checkboxes. One shuffles the training data randomly
    before each epoch, and one sets a random seed. Shuffling the training data often
    improves the learning process. Indeed, updating the network with the same batches
    in the same order in each epoch can have a detrimental effect on the convergence
    speed of the training. If the shuffling checkbox is selected, the random seed
    checkbox becomes active and the displayed number is used to generate the random
    sequence for the shuffling operation. The usage of a random seed produces a repeatable
    random shuffling procedure and therefore allows us to repeat the results of a
    specific training run. Clicking the **New seed** button generates a new random
    seed and a new random shuffling procedure. Disabling the checkbox for the random
    seed creates a new seed for each node execution.
  prefs: []
  type: TYPE_NORMAL
- en: In the lower part of the **Options** tab, you can select the **Optimizer algorithm**,
    and its parameters to use during training. The optimizer algorithm is the training
    algorithm. For example, you can select the **RMSProp** optimizer and then the
    corresponding **Learning rate** and **Learning rate decay** values. When the node
    is selected, the **Description** panel on the right is populated with details
    about the node. A list of optimizers is provided, as well as links to the original
    Keras library explaining all the parameters required in this frame.
  prefs: []
  type: TYPE_NORMAL
- en: At the very bottom of the **Options** tab, you can constrain the size of the
    gradient values. If **Clip norm** is checked, the gradients whose L2 norm exceeds
    the given norm will be clipped to that norm. If **Clip value** is checked, the
    gradients whose absolute value exceeds the given value will be clipped to that
    value (or the negated value, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: The **Advanced Options** tab contains a few additional settings for special
    termination and learning rate reduction cases. The last option allows you to specify
    which GPU to use on systems with multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking the Training Progress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After setting all the training parameters, you can start training your network
    by executing the node. While executing the node, you can check the learning progress
    in the **Learning Monitor** view. You can open the **Learning Monitor** view by
    right-clicking on the Keras Network Learner node and selecting **View: Learning
    Monitor**; see *Figure 4.21*.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, the **Learning Monitor** view shows the evolution of the accuracy
    curve, in red, on the training set after each weight update, which means after
    a data batch has passed through the network. The accuracy values are reported
    on the *y* axis and the progressive number of the batch on the *x* axis.
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on **Loss** above the line plot shows the loss curve on the training
    set instead of the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about the training progress is available in the **Keras Log
    Output** view. This can be selected in the top part of the Keras Learning node''s
    view, in the last tab after **Accuracy** and **Loss**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – The Learning Monitor view shows the progress of the learning
    process](img/B16391_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – The Learning Monitor view shows the progress of the learning process
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Learning Monitor** view of the Keras Network Learner node allows you
    to track the learning of your model. You can open it by right-clicking on the
    executing node and selecting **View: Learning Monitor**.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a validation set, a blue line appears in the accuracy/loss
    plot. The blue line shows the corresponding progress of the training procedure
    on the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Under the plot, you have the option to zoom in on the *x* axis – the batch axis
    – to see the progress after each batch in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The **Smoothing** checkbox introduces the moving average curve of the original
    accuracy or loss curve. The **Log Scale** checkbox changes the curve representation
    to a logarithmic scale for a more detailed evaluation of the training run.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, at the bottom of the view, you can see the **Stop learning** button.
    This is an option for on-demand early stopping of the training process. If training
    is stopped before it is finished, the network is saved in the current status.
  prefs: []
  type: TYPE_NORMAL
- en: Training Settings for Iris Flower Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the iris flower classification example based on the Iris dataset, we used
    the following settings in the Network Learner node.
  prefs: []
  type: TYPE_NORMAL
- en: In the first tab, the **Input Data** tab, the four numerical inputs are selected
    as the input features. During the data preparation part, we applied no nominal
    feature encoding on the input features. So, we just feed them as they are into
    the input layer of the network, by using the **From Number (double)** conversion
    type.
  prefs: []
  type: TYPE_NORMAL
- en: In the second tab, the `class_collection` input column, containing the integer-encoded
    class as a collection, and we applied the **From Collection of Number (integer)
    to One-Hot Tensor** conversion. Therefore, during the execution, the Keras Network
    Learner node creates the one-hot encoding version of the three classes in a three-dimensional
    vector, as it is also required to match the network output. In the lower part
    of this second tab, select the **Categorical cross entropy** loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In the third tab, named **Options**, the training parameters are defined. The
    network is trained using 50 epochs, a training batch size of 5, and the **RMSProp**
    optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: The settings in the **Advanced Options** tab are left inactive, by default.
  prefs: []
  type: TYPE_NORMAL
- en: Training Settings for Income Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a multiclass classification problem, such as the income prediction example
    based on the adult dataset, the settings are a bit different. We used the following
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: In the first tab, `Income`, is in the `Exclude` part. Here, in the data preparation
    phase, some of the input features were already numerical and have not been encoded,
    some have been integer encoded, and some have been one-hot encoded via KNIME native
    nodes. So, all input features are ready to be fed as they are into the network.
    Notice that, since we decided to mix integer encoding, one-hot encoding, and original
    features, the only possible encoding applicable to all those different features
    is a simple **From Number** type of transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in the second tab, `0` or `1`. This also fits the one output from the
    sigmoid function in the output layer of the network. In the include-exclude frame,
    only the target column, `Income`, is included. Next, the **Binary cross entropy**
    loss function is selected, to fit a binary classification problem such as this
    one.
  prefs: []
  type: TYPE_NORMAL
- en: In the third tab, **Options**, we set the network to be trained for 80 epochs
    with a training batch size of 80 data rows. In this example, we also use a validation
    set, to be able to already see, during training, the network progress on data
    not included in the training set. For the processing of the validation set, a
    batch size of 40 data rows is set. Lastly, we select **Adam** as the optimizer
    for this training process.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the settings in the last tab, **Advanced Options**, are disabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Applying the Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the neural network has been trained, the last step is to apply the
    network to the test set and evaluate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To execute a trained network, you can use the **Keras Network Executor** node,
    as in *Figure 4.22*. The node has two input ports: a Keras network port for the
    trained network and a data input port for the test set or new data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first tab of the configuration window, named **Options**, you can select,
    in the upper part, the backend engine, the batch size for the input data, and
    whether to also keep the original input columns in the output data table.
  prefs: []
  type: TYPE_NORMAL
- en: Under that, you can specify the input columns and the required conversion. Like
    in the Keras Network Learner node, the input specifications from the neural network
    are printed at the top. Remember that, since you are using the same network and
    the same format for the data, the settings for the input features must be the
    same as the ones in the Keras Network Learner node.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last part of this tab, you can add the settings for the output(s). First,
    you need to specify where to take the output from; this should be the output layer
    from the input network. To add one output layer, click on the **add output** button.
    In the new window, you see a menu containing all layers from the input network.
    If you configured prefixes in the layer nodes, you could see them in the drop-down
    menu, making it easier for you to recognize the layer of interest. Select the
    output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – The Keras Network Executor node runs the network on new data.
    In the configuration window, you can select the outputs by clicking on the add
    output button](img/B16391_04_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – The Keras Network Executor node runs the network on new data.
    In the configuration window, you can select the outputs by clicking on the add
    output button
  prefs: []
  type: TYPE_NORMAL
- en: In all use cases included in this book, the last layer of the network is used
    as the output layer. This layer is easily recognizable, as it is the only one
    without the **(hidden)** suffix in the drop-down list.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can also output the output of a hidden layer, for example, for debugging
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, select the appropriate conversion type, to get the output values in
    the shape you prefer – for example, in one cell as a list (**To List of Number
    (double)**) or with a new column for each output unit (**To Number (double)**).
    In this last case, you can define a prefix to append to the names of the output
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: The **Advanced Options** part contains settings to let the network run on GPU-enabled
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the Predictions and Evaluating the Network Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the use case, the network outputs might need some postprocessing
    to extract the predictions. For example, in a binary classification problem, with
    one output unit and a sigmoid activation function, the output value is the probability
    for the class encoded as `1`. In this case, to produce the actual class assignment,
    you could apply a threshold to the probability inside a Rule Engine node.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is the evaluation of the model. To evaluate a classification model,
    you can use either the **Scorer** node or the **ROC Curve** node. The output of
    the Scorer node gives you common performance metrics, such as the accuracy, Cohen's
    kappa, or the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'Another really nice node to evaluate the performance for the binary classification
    problem is the **Binary Classification Inspector** node. The node is part of the
    KNIME Machine Learning Interpretability Extension: [https://hub.knime.com/knime/extensions/org.knime.features.mli/latest](https://hub.knime.com/knime/extensions/org.knime.features.mli/latest).'
  prefs: []
  type: TYPE_NORMAL
- en: For the evaluation of regression solutions, the **Numeric Scorer** node calculates
    some error measures, such as mean squared error, root mean squared error, mean
    absolute error, mean absolute percentage error, mean signed difference, and R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Network Trained to Classify Iris Flowers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Figure 4.23*, you can see the part of the workflow that applies the trained
    network, extracts the predictions, and evaluates the network trained to classify
    iris flowers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – This workflow snippet applies the trained network and extracts'
  prefs: []
  type: TYPE_NORMAL
- en: and evaluates the predictions for the iris flower example](img/B16391_04_023.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.23 – This workflow snippet applies the trained network and extracts
    and evaluates the predictions for the iris flower example
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window of the `"dense_2/Softmax:0_"`, as a **Conversion**
    type of **To Number (double)** is selected. As the Iris dataset has three different
    possible class values, the node adds three new columns with the three probabilities
    for the three classes. Another conversion option is **To List of Number (double)**.
    This conversion option would lead to only one new column, with all the class probabilities
    in one cell packaged as a list.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the predictions are extracted with the Rule Engine node. The probabilities
    for the different classes are in the `$Output_1/Softmax:0_0for class 0`, and `Output_1/Softmax:0_1`
    columns for class 1, and `Output_1/Softmax:0_2` for class 2\. Here, the class
    with the highest probability is selected as the predicted outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The first rule checks whether the class encoded as `0` has the highest probability
    by comparing it to the probability for the other two classes. The second rule
    does the same for the class encoded as `1`, and the third rule for the class encoded
    as `2`. The last rule defines a default value.
  prefs: []
  type: TYPE_NORMAL
- en: 'These rules are applied with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, the Scorer node is used to evaluate network performance.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Network Trained for Income Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The same node combination with different settings can be used to apply the trained
    network, extract the predictions, and evaluate the model for the income prediction
    example on the adult dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window of the Keras Network Executor node, in the `dense_3`
    output layer is added as the output. In this case, the output of the network is
    the probability for the class encoded as `1`, `">50K"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the Rule Engine node checks whether the output probability is higher
    or lower than the `0.5` threshold using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, the network performance is evaluated with the Scorer node.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have gone through the whole process, from data access and data
    preparation to defining, training, applying, and evaluating a neural network using
    KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the end of this chapter, where you have learned how to perform
    the different steps involved in training a neural network in KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: We started with common preprocessing steps, including different encodings, normalization,
    and missing value handling. Next, you learned how to define a neural network architecture
    by using different Keras layer nodes without writing code. We then moved on to
    the training of the neural network and you learned how to define the loss function,
    as well as how you can monitor the learning progress, apply the network to new
    data, and extract the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Each section closed with small example sessions, preparing you to perform all
    these steps on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you will see how these steps can be applied to the first
    use case of the book: fraud detection using an autoencoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check your level of understanding of the concepts presented in this chapter
    by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How can you set the loss function to train your neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using the Keras Loss Function node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using the Keras Output Layer node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) In the configuration window of the Keras Network Learner node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) In the configuration window of the Keras Network Executor node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can you one-hot encode your features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using the One Hot Encoding node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using the One to Many node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) By creating an integer encoding using the Category to Number node and afterward,
    the Integer to One Hot Encoding node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By creating an integer encoding, transforming it into a collection cell,
    and selecting the right conversion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can you define the number of neurons for the input of your network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using a Keras Input Layer node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using a Keras Dense Layer node without any input network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The input dimension is set automatically based on the selected features in
    the Keras Network Learner node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By using a Keras Start Layer node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can you monitor the training of your neural network on a validation set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Feed a validation set into the optional input port of the Keras Network Learner
    node and open the training monitor view. The performance of the validation set
    is shown in red.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Click on the **apply on validation set** button in the training monitor view.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Feed a validation set into the optional input port of the Keras Network Learner
    node and open the training monitor view. The performance of the validation set
    is shown in blue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Feed a validation set into the optional input port of the Keras Network Learner
    node and open the validation set tab of the training monitor view. Build a workflow
    to read the Iris dataset and to train a neural network with one hidden layer (eight
    units and the ReLU activation function) to distinguish the three species from
    each other based on the four input features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
