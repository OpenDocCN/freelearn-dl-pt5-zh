- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Advanced Regularization in Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的高级正则化
- en: A full book could be written about regularization in **natural language processing**
    (**NLP**). NLP is a wide field that consists of many topics, ranging from simple
    classification such as review ranking to complex models and solutions such as
    ChatGPT. This chapter will merely scratch the surface of what can reasonably be
    done with simple NLP solutions such as classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 **自然语言处理**（**NLP**）的正则化可以写成一本完整的书。NLP 是一个广泛的领域，涵盖了许多主题，从简单的分类任务（如评论排序）到复杂的模型和解决方案（如
    ChatGPT）。本章仅会略微触及使用简单 NLP 解决方案（如分类）能够合理完成的内容。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Regularization using a word2vec embedding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 word2vec 嵌入的正则化
- en: Data augmentation using word2vec
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 word2vec 的数据增强
- en: Zero-shot inference with pre-trained models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型进行零-shot 推理
- en: Regularization with BERT embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BERT 嵌入的正则化
- en: Data augmentation using GPT-3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT-3 的数据增强
- en: By the end of this chapter, you will be able to take advantage of advanced methods
    for NLP tasks such as word embeddings and transformers, as well as be able to
    use data augmentation to generate synthetic training data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够利用高级方法处理 NLP 任务，如词嵌入和 transformers，并能使用数据增强生成合成训练数据。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use various NLP solutions and tools, so we will require
    the following libraries:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用各种 NLP 解决方案和工具，因此我们需要以下库：
- en: NumPy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: Gensim
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim
- en: NLTK
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK
- en: PyTorch
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformers
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers
- en: OpenAI
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI
- en: Regularization using a word2vec embedding
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 word2vec 嵌入的正则化
- en: In this recipe, we will use a pre-trained word2vec embedding to improve the
    results of a task thanks to transfer learning. We will compare the results to
    the initial *Training a GRU* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206),
    on the IMDb dataset for review classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用预训练的 word2vec 嵌入，借助迁移学习来提高任务的结果。我们将结果与 [*第 8 章*](B19629_08.xhtml#_idTextAnchor206)中的
    *训练 GRU* 任务进行比较，数据集为 IMDb 的评论分类。
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: A word2vec is a rather old type of word embedding in the NLP landscape and has
    been widely used in many NLP tasks. While recent techniques are sometimes more
    powerful, the word2vec approach remains efficient and cost-effective.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 是自然语言处理（NLP）领域中一种相对较旧的词嵌入方法，已广泛应用于许多 NLP 任务。尽管近期的技术有时更强大，但 word2vec
    方法仍然高效且具有成本效益。
- en: Without getting into the details of word2vec, a commonly used model is a 300-dimensional
    embedding; each word in the vocabulary is embedded into a vector of 300 values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入讨论 word2vec 的细节，一个常用的模型是 300 维的嵌入；词汇表中的每个单词都会被嵌入到一个包含 300 个值的向量中。
- en: 'word2vec is usually trained on a large corpus of texts. There are two main
    approaches for training a word2vec that can be roughly described as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 通常在大规模的文本语料库上进行训练。训练 word2vec 的主要方法有两种，基本可以描述如下：
- en: '**Continuous bag of words** (**CBOW**): Uses the context of surrounding words
    in a sentence to predict a missing word'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋模型**（**CBOW**）：使用句子中周围词的上下文来预测缺失的词'
- en: '**skip-gram**: Uses a word to predict its surrounding context'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**skip-gram**：使用一个词来预测其周围的上下文'
- en: 'An example of these two approaches is proposed in *Figure 9**.1*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的示例见 *图 9.1*：
- en: '![Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods](img/B19629_09_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – CBOW（左）和 skip-gram（右）方法的训练数据示例](img/B19629_09_01.jpg)'
- en: Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – CBOW（左）和 skip-gram（右）方法的训练数据示例
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, CBOW is usually easier to train, while skip-gram may have better
    performance for rarer words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中，CBOW 通常更容易训练，而 skip-gram 对稀有词的表现可能更好。
- en: The goal is not to train our own word2vec, but simply to reuse a trained one
    and take advantage of transfer learning to get a performance boost in our predictions.
    In this recipe, instead of training our own embedding before feeding the **gated
    recurrent unit** (**GRU**), we will simply reuse a pre-trained word2vec embedding,
    and then only train our GRU on top of these embeddings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 目标不是训练我们自己的 word2vec，而是简单地重用一个已经训练好的模型，并利用迁移学习来提升我们预测的性能。在这个步骤中，我们将不再训练自己的嵌入，而是直接重用一个预训练的
    word2vec 嵌入，然后只在这些嵌入的基础上训练我们的 GRU。
- en: 'For that, we will again work on the IMDb dataset classification task: a dataset
    containing texts of movie reviews as inputs and associated binary labels, positive
    or negative. The dataset for this can be downloaded with the Kaggle API:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将再次进行IMDb数据集分类任务：这是一个包含电影评论文本作为输入和相关二进制标签（正面或负面）的数据集。可以通过Kaggle API下载此数据集：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command will install the required libraries:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将安装所需的库：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it…
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In this recipe, we will train a GRU for binary classification on the IMDb review
    dataset. Compared to the original recipe, the main difference is in *step 5*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将训练一个GRU模型，用于在IMDb评论数据集上进行二分类。与原始食谱相比，主要的区别在于*第5步*：
- en: 'Import the following necessary libraries:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下必要的库：
- en: '`torch` and some related modules and classes for the neural network'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`及其相关模块和类，用于神经网络'
- en: '`train_test_split` and `LabelEncoder` from `scikit-learn` for preprocessing'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自`scikit-learn`的`train_test_split`和`LabelEncoder`进行预处理
- en: '`AutoTokenizer` from `transformers` to tokenize the reviews'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自`transformers`的`AutoTokenizer`来标记化评论
- en: '`pandas` to load the dataset'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`用于加载数据集'
- en: '`numpy` for data manipulation'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`用于数据处理'
- en: '`matplotlib` for visualization'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`用于可视化'
- en: '`gensim` for the word2vec embedding and `nltk` for work tokenization'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`gensim`进行word2vec嵌入，使用`nltk`进行文本标记化处理
- en: 'If you haven’t done so yet, you will need to add the `nltk.download(''punkt'')`
    line to download some required utility instances, as shown here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，你需要添加`nltk.download('punkt')`这一行，以下载一些必要的工具实例，如下所示：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the pre-trained word2vec model, which contains a 300-dimension embedding.
    The model is about 1.6 GB and may take some time to download, depending on your
    available bandwidth:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的word2vec模型，该模型包含300维的嵌入。该模型大约有1.6GB，下载可能需要一些时间，具体取决于你的带宽：
- en: '[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the data from the CSV file with `pandas`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`从CSV文件加载数据：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Split the data into train and test sets using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据拆分为训练集和测试集，测试集大小为20%，并指定随机状态以确保可复现性：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Implement the dataset’s `TextClassificationDataset` class, which handles the
    data. This is where the word2vec embedding is computed:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现数据集的`TextClassificationDataset`类，它处理数据。此处计算word2vec嵌入：
- en: '[PRE10]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At instantiation, each input movie is converted into an embedding in two ways
    with the `embed` method:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化时，每个输入电影通过`embed`方法以两种方式转换为嵌入：
- en: Each movie review is tokenized with a word tokenizer (basically splitting sentences
    into words).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个电影评论都通过一个单词标记器进行标记化（基本上是将句子分割为单词）。
- en: Then, a vector of `max_words` length is computed, containing the word2vec embeddings
    of `max_words` first words in the review. If the review is shorter than `max_words`
    words, the vector is filled with zero padding.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，计算一个`max_words`长度的向量，包含评论中前`max_words`个单词的word2vec嵌入。如果评论少于`max_words`个单词，则使用零填充该向量。
- en: 'Then, we must instantiate the `TextClassificationDataset` objects for the train
    and test sets, as well as the related data loaders. The maximum number of words
    is set to `64`, as is the batch size:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须为训练集和测试集实例化`TextClassificationDataset`对象，以及相关的数据加载器。最大单词数设置为`64`，批处理大小也设置为：
- en: '[PRE42]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we must implement the GRU classifier model. Since the embedding was computed
    at the data loading step, this model is directly computing a three-layer GRU,
    followed by a fully connected layer with a sigmoid activation function:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须实现GRU分类模型。由于嵌入是在数据加载步骤中计算的，因此该模型直接计算一个三层GRU，并随后应用一个带有sigmoid激活函数的全连接层：
- en: '[PRE52]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, we must instantiate the GRU model. The embedding dimension, defined by
    the word2vec model, is `300`. We have chosen a hidden dimension of `32` so that
    each GRU layer is made up of 32 units:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须实例化GRU模型。由word2vec模型定义的嵌入维度为`300`。我们选择了`32`作为隐藏维度，因此每个GRU层由32个单元组成：
- en: '[PRE72]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Then, we must instantiate the optimizer as an `Adam` optimizer with a learning
    rate of `0.001`; the loss is defined as the binary cross-entropy loss since this
    is a binary classification task:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须实例化优化器为`Adam`优化器，学习率为`0.001`；损失定义为二元交叉熵损失，因为这是一个二分类任务：
- en: '[PRE81]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Train the model for `20` epochs using the `train_model` function and store
    the loss and accuracy for both the train and test sets at each epoch. The implementation
    of the `train_model` function can be found in this book’s GitHub repository at
    [https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Here is the typical output after 20 epochs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Plot the BCE loss for the train and test sets:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Here is the plot for it:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Binary cross-entropy loss as a function of the epoch](img/B19629_09_02.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Binary cross-entropy loss as a function of the epoch
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, while the train loss keeps decreasing over the 20 epochs, the
    test loss soon reaches a minimum at around 5 epochs, to then increase, indicating
    overfitting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the accuracy for the train and test sets:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Here is the plot for this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Accuracy as a function of the epoch](img/B19629_09_03.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Accuracy as a function of the epoch
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: As expected with the loss, the accuracy keeps increasing for the train set.
    For the test set, it reaches a maximum value of about 81% (against 77% without
    word2vec embedding, as shown in the previous chapter). Word2vec embedding allowed
    us to improve the results slightly, though the results may improve much more if
    we adjust the other hyperparameters accordingly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we used the embeddings as an array in this recipe, they can be used differently;
    for example, we can use an average of all the embeddings in a sentence or other
    statistical information.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Also, while word2vec works well enough in many cases here, a few embeddings
    can be derived with a more specialized approach, such as doc2vec, which is sometimes
    more powerful for documents and longer texts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Wikipedia article about word2vec is a valuable resource as it specifies
    many relevant publications: [https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3](https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'This documentation from Google is also useful: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation using word2vec
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to regularize a model and get better performance is to have more data.
    Collecting data is not always easy or possible, but synthetic data can be an affordable
    way to improve performance. We’ll do this in this recipe.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using word2vec embeddings, you can generate new, synthetic data that has a close
    semantic meaning. By doing this, it is fairly easy for a given word to get the
    most similar words in a given vocabulary.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, using word2vec and a few parameters, we’ll see how we can generate
    new sentences with a close semantic meaning. We will only apply it to a given
    sentence as an example and propose how to integrate it into a full training pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The only required libraries are `numpy` and `gensim`, both of which can be installed
    with `pip install` `numpy gensim`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to complete this recipe:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to import the necessary libraries – `numpy` for random calls
    and `gensim` for word2vec model loading:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Load a pre-trained word2vec model. This may take some time if the model hasn’t
    been downloaded and stored in a local cache already. Also, this is a rather large
    model, so it may take some time to load in memory:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Implement the `replace_words_with_similar` function so that you can randomly
    replace `word` with another semantically close word:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Hopefully, the comments are self-explanatory, but here is what this function
    does:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: It splits the input text into words by using a simple split (a word tokenizer
    can be used as well)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each word, it checks for the following:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the word is in the word2vec vocabulary
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the word is not in a list of stop words (to be defined)
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a random probability is above the threshold probability (to draw random words)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a word fulfills the previous checks, the following is computed:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_similar` most similar words'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One of those words is picked randomly
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the similarity score of this word is above a given threshold, add it to the
    output sentence
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If no updated word has been added, just add the original word so that the overall
    sentence remains logical
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '`sim_threshold`: The similarity threshold'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probability`: The probability for a word to be replaced with a similar word'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_similar`: The number of similar words to compute for a given word'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words`: A list of words not to be replaced in case some words are specifically
    important or have several meanings'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the `replace_words_with_similar` function we just implemented to a given
    sentence:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The code output is as follows. It allows us to change a few words while keeping
    the overall meaning:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Thanks to this data augmentation technique, it is possible to generate more
    diverse data so that we can make the models more robust and regularized.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to add such a data generation function to a classification task would
    be to add it at the data-loading step. This would generate synthetic data on-the-fly
    and could allow us to regularize the model. It could be added to the dataset class,
    as shown here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: See also
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Documentation about the `most_similar` function of the word2vec model can be
    found at [https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot inference with pre-trained models
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NLP field has faced many major advances in the last few years, which means
    that many pre-trained, efficient models can be reused. These pre-trained, freely
    available models allow us to approach some NLP tasks with zero-shot inference
    since we can reuse those models. We’ll try this approach in this recipe.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: We sometimes use zero-shot inference (or zero-shot learning) and few-shot learning.
    Zero-shot learning means being able to perform a task without any training for
    this specific task; few-shot learning means performing a task while training only
    on a few samples.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot inference is the act of reusing pre-trained models without doing any
    fine-tuning. There are many very powerful, free-to-use models available that can
    do just as well as a trained model of our own. Since the available models are
    trained on huge datasets with massive computational power, it is sometimes hard
    to compete with an in-house model that’s been trained on much less data and with
    less computational power.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: That being said, sometimes, training on small, well-curated, task-specific data
    can do wonders and provide much better performance. It’s all a matter of context.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Also, we sometimes have data without labels, so supervised learning is not a
    possibility. In such cases, labeling a small subsample of the data ourselves and
    evaluating a zero-shot approach against this data can be useful.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will reuse a pre-trained model on the Tweets dataset and
    classify tweets as negative, neutral, or positive. Since no training is needed,
    we will directly evaluate the model on the test set so that it can be compared
    with the results we obtained with a simple RNN in the *Training an RNN* recipe
    from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we need to download the dataset locally. It can be downloaded with
    the Kaggle API and then unzipped with the following command
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'The libraries that are required to run this recipe can be installed with the
    following command:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: How to do it…
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following necessary functions and models:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`numpy` for data manipulation'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` for loading the data'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` from `scikit-learn` to split the dataset'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy_score` from `scikit-learn` to compute the accuracy score'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline` from `transformers` for instantiating the zero-shot classifier'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'Load the dataset. In our case, the only columns of interest are `text` for
    the features and `airline_sentiment` for the labels:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Here is the output of this code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – First five rows of the dataset for the considered columns](img/B19629_09_04.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – First five rows of the dataset for the considered columns
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into train and test sets, with the same parameters as in the
    *Regularization using a word2vec embedding* recipe so that the results can be
    compared: `test_size` set to `0.2` and `random_state` set to `0`. Since there
    is no training, we will only use the test set:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Instantiate the classifier using a `transformers` pipeline with the following
    parameters:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`task="zero-shot-classification"`: This will instantiate a zero-shot classification
    pipeline'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model="facebook/bart-large-mnli"`: This will specify the model to be used
    for this pipeline'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Note
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: When this is first called, it may download several files and the model itself,
    and it may take some time.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the candidate labels in an array. These candidate labels are needed for
    zero-shot classification:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Compute the predictions on the test set and store them in an array:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: Refer to the *There’s more…* section for a few details about what the classifier
    does, as well as its outputs.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the accuracy score of the predictions:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'The computed accuracy score is as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: We got an accuracy score of 74.5%, which is equivalent to the results we had
    after training a simple RNN in the *Training an RNN* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
    Without any training costs and large, labeled datasets, we can get the same performance
    thanks to this zero-shot classification.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning comes with a cost since pre-trained language models are usually
    rather large and may require large computational power to run at scale.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at an example of the input and output of `classifier` to get a better
    understanding of what it does:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'The output of this code is as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Here’s what we can see:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'An input sentence: `I love to learn` `about regularization`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Candidate labels: `positive`, `negative`, and `neutral`'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is a dictionary with the following key values:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '`''sequence''`: The input sequence'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''labels''`: The input candidate labels'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''scores''`: A list of scores, one for each label, sorted in decreasing order'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Since the scores are always sorted in decreasing order, the labels may have
    a different order.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the predicted class can be computed with the following code, which
    will retrieve the `argmax` values of the scores and the associated label:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: In our case, that would output `positive`.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model card of the bart-large-mnli model: [https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A tutorial from Hugging Face about zero-shot classification: [https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification](https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The documentation about the `transformers` pipelines, which allows us to do
    much more than zero-shot classification: [https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with BERT embeddings
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to how we used a pre-trained word2vec model to compute the embeddings,
    it is possible to use the embeddings of a pre-trained BERT model, a transformer-based
    model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, after quickly explaining the BERT model, we will train a model
    using BERT embeddings.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '**BERT** stands for **Bidirectional Encoder Representation from Transformers**
    and is a model that was proposed by Google in 2018\. It was first deployed in
    late 2019 in Google Search for English queries, as well as for many other languages.
    The BERT model has been proven effective in several NLP tasks, including text
    classification and question-answering.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Before quickly explaining what BERT is, let’s take a step back and look at what
    **attention mechanisms** and **transformers** are.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms are widely used in NLP, and more and more in other fields
    such as computer vision, since their introduction in 2017\. The high-level idea
    of an attention mechanism is to compute a weight for each input token concerning
    other tokens in the given sequence. Compared to RNNs, which process inputs as
    sequences, the attention mechanism considers the whole sequence at once. This
    allows attention-based models to handle long-range dependencies in sequences more
    efficiently since the attention mechanism can be considered agnostic to the sequence
    length.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are a type of neural network based on self-attention. They usually
    start with an embedding, as well as an absolute positional encoding that attention
    layers are trained on. Those layers usually use multi-head attention to capture
    various aspects of the input sequence. For more details, you can read the original
    paper, *Attention Is All You Need* (refer to the *See also* section for the paper).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Since BERT uses absolute positional encodings, you are advised to use padding
    on the right, if any padding is used.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'The BERT model was built on top of `transformers` and is made of 12 transformer-based
    encoding layers for the base model (24 layers for the large model), for about
    110 million parameters. More interestingly, it was pre-trained in an unsupervised
    fashion, using two methods:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked language**: 15% of the tokens in a sequence are randomly masked, and
    the model is trained to predict the masked tokens'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next sentence prediction**: Given two sentences, the model is trained to
    predict whether they are consecutive in a given text'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This pre-training approach is summarized in the following diagram, which was
    extracted from the BERT paper called *BERT: Pre-training of Deep Bidirectional
    Transformers for* *Language Understanding*:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – BERT pre-training diagram proposed in the original article,
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](img/B19629_09_05.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 – BERT pre-training diagram proposed in the original article, BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: While word2vec embedding is context-free (no matter the context, a word embedding
    remains the same), BERT gives a different embedding for a given word, depending
    on its surroundings. This makes sense since a given word may have a different
    meaning in two sentences (for example, apple or Apple can be either a fruit or
    a company, depending on the context).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然word2vec嵌入是无上下文的（无论上下文如何，词嵌入保持不变），但BERT根据周围环境为给定词语提供不同的嵌入。这是有道理的，因为一个给定的词在两个句子中的意思可能不同（例如，apple或Apple可以是水果也可以是公司，取决于上下文）。
- en: Getting ready
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will reuse the Tweets dataset, which can be downloaded
    and then unzipped locally with the following command line:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，我们将重用Tweets数据集，可以通过以下命令行下载并解压到本地：
- en: '[PRE167]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: The necessary libraries can be installed with `pip install torch scikit-learn`
    `transformers pandas`.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 必要的库可以通过`pip install torch scikit-learn` `transformers pandas`安装。
- en: How to do it…
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In this recipe, we will train a simple logistic regression on top of pre-trained
    BERT embeddings:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将在预训练BERT嵌入上训练一个简单的逻辑回归模型：
- en: 'Make the required imports:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '`torch` for device management if you have a GPU'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有GPU，可以使用`torch`进行设备管理。
- en: The `train_test_split` method and the `LogisticRegression` class from `scikit-learn`
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`方法和`scikit-learn`中的`LogisticRegression`类。
- en: The related `BERT` classes from `transformers`
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`中的相关`BERT`类。'
- en: '`pandas` for data loading'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据。
- en: 'Here is the code for this:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE168]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Load the dataset with `pandas`:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据集：
- en: '[PRE169]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'Split the dataset into train and test sets, keeping the same parameters as
    in the *Zero-shot inference* recipe with pre-trained models, so that we can compare
    their performance later:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集，保持与*零-shot推理*食谱中预训练模型相同的参数，以便稍后比较它们的表现：
- en: '[PRE170]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'Instantiate the tokenizer and the BERT model. Instantiating the model is a
    multi-step process:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化tokenizer和BERT模型。实例化模型是一个多步骤的过程：
- en: First, instantiate the model’s configuration with the `BertConfig` class.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`BertConfig`类实例化模型的配置。
- en: Then, instantiate `BertModel` with random weights.
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用随机权重实例化`BertModel`。
- en: Load the weights of the pre-trained model (this will display a warning since
    not all the weights will have been loaded).
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型的权重（这将显示一个警告，因为并非所有权重都已加载）。
- en: Load the model on the GPU, if any, and set the model to `eval` mode.
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有GPU，将模型加载到GPU上，并将模型设置为`eval`模式。
- en: 'Here is the code for this:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE173]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: You may get some warning messages since some layers have no pre-trained weights.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会收到一些警告消息，因为某些层没有预训练权重。
- en: 'Compute the embeddings for the train and test sets. This is a two-step process:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集和测试集的嵌入。这是一个两步过程：
- en: Compute the tokens with the tokenizer (and optionally load the tokens on the
    GPU, if any).
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用tokenizer计算令牌（并可以选择将令牌加载到GPU上，如果有的话）。
- en: Then, compute the embeddings.
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，计算嵌入。
- en: For more details about the inputs and outputs of the BERT model, check out the
    *There’s more…* subsection of this recipe.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 有关BERT模型输入输出的更多细节，请查看本食谱中的*更多…*小节。
- en: 'Here is the code for this:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE174]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'Then, instantiate and train a logistic regression model. It may require more
    iterations than the default model allows. Here, it has been set to `10,000`:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，实例化并训练一个逻辑回归模型。它可能需要比默认模型更多的迭代次数。在这里，已将其设置为`10,000`：
- en: '[PRE175]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Finally, print the accuracy on the train and test sets:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，打印训练集和测试集的准确性：
- en: '[PRE178]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'You should have output results similar to the following:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似于以下的输出结果：
- en: '[PRE184]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: We get a final result of about 79% accuracy on the test set and 80% on the train
    set. As a comparison, using zero-shot inference and a simple RNN on this same
    dataset both provided 74% accuracy.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上得到了大约79%的最终准确率，在训练集上得到了80%。作为对比，使用零-shot推理和简单的RNN在同一数据集上提供的准确率为74%。
- en: There’s more…
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: To get a better understanding of what the tokenizer computes and what the BERT
    model outputs, let’s have a look at an example.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解tokenizer计算的内容以及BERT模型输出的内容，让我们看一个例子。
- en: 'First, let’s apply the tokenizer to a sentence:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将tokenizer应用于一个句子：
- en: '[PRE185]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'This outputs the following:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE186]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'As we can see, the tokenizer returns three outputs:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，tokenizer返回三个输出：
- en: '`input_ids`: This is the index of tokens in the vocabulary.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`：这是词汇表中令牌的索引。'
- en: '`token_type_ids`: The sentence number. This is only useful for paired sentences,
    as in the original training of BERT.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`：句子的编号。这对于配对句子才有用，就像BERT最初训练时一样。'
- en: '`attention_mask`: This is where the model will focus. As we can see, it’s only
    been set to `1` for actual tokens, and then to `0` for padding.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three lists are fed to the BERT model so that it can compute its output.
    The output is made up of the following two tensors:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state`: The values of the last hidden state, whose shape is `[batch_size,`
    `max_length, 768]`'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output`: The pooled values of the outputs over the sequence steps,
    whose shape is `[``batch_size, 768]`'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many other types of embeddings exist and can be more or less powerful, depending
    on the task at hand. For example, OpenAI also proposes embeddings that can be
    made available using an API. For example, the following code allows us to have
    embeddings for a given sentence:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: This would return a 1,536-dimensional embedding for the given sentence that
    could then be used for classification or other tasks.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, to use these embeddings, you would need to do the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Install the `openai` library with `pip` `install openai`
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an API key on the OpenAI website
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a working payment method
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper introducing transformers, *Attention is all you* *need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The BERT model card: [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The BERT paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information about the OpenAI embeddings, here is the official documentation:
    [https://platform.openai.com/docs/guides/embeddings/use-cases](https://platform.openai.com/docs/guides/embeddings/use-cases)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation using GPT-3
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models are becoming more and more powerful, especially in NLP. Using
    these to generate new, synthetic data sometimes allows us to significantly improve
    the results we get and regularize models. We’ll learn how to do this in this recipe.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While models such as BERT are effective at tasks such as text classification,
    they usually do not perform very well when it comes to text generation.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Other types of models, such as **generative pre-trained transformer** (**GPT**)
    models, can be quite impressive at generating new data. In this recipe, we will
    use the OpenAI API and GPT-3.5 to generate synthetic yet realistic data. Having
    more data is key to having more regularization in our models, and data generation
    is one way to collect more data.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, you will need to install the OpenAI library with `pip` `install
    openai`.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Also, since the API we will use is not free, it is necessary to create an OpenAI
    account with a generated API key and a working payment method.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API key
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: You can easily create an API key in your profile by accessing the **API** **keys**
    section.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In the *There’s more…* section, we will provide a free alternative – that is,
    using GPT-2 to generate new data – but it will have less realistic results. For
    that to work, you must have Hugging Face’s `transformers` library, which you can
    install with `pip` `install transformers`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will simply query GPT-3.5 to generate a few positive and
    negative movie reviews so that we have more data to train a movie review classification
    model. Of course, this can be derived from any classification task, as well as
    many other NLP tasks:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `openai` library, as follows:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE188]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'Provide your OpenAI API key:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: This is just a code example – never share your API key on public repositories.
    Use alternatives such as environment variables instead.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate three positive examples using the `ChatCompletion` API:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'There are several parameters here:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '`model`: `gpt-3.5-turbo`, which performs well and is cost-efficient. It is
    based on GPT-3.5.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages`: There can be three types of messages:'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system`: A formatting message, followed by alternating user and assistant
    messages'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`: A user message'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant`: An assistant message; we won’t use this'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens`: The maximum number of tokens in the output.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature`: This is usually between 0 and 2\. A larger value means more
    randomness.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: The number of desired outputs.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can display the output-generated sentences:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Similarly, let’s generate and display three negative examples of movie reviews:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'The following code shows the three reviews that were generated:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: The generated examples are well written and probably good enough to be written
    by a human. Also, it would be possible to generate more neutral, more random,
    longer, or shorter examples if needed, which is quite convenient.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternatively, it is possible to use the GPT-2 model for free, even though the
    results are less realistic. Let’s learn how to do this.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s instantiate a text generation pipeline based on GPT-2:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: 'This generates some text. The behavior is not the same, and it only handles
    text completion, so you must propose the beginning of a piece of text for the
    model to complete it automatically:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'This outputs the following three reviews:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: As we can see, the results are less interesting and realistic than with GPT-3,
    but they can still be useful if we apply some manual filtering.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chat completion documentation from OpenAI: [https://platform.openai.com/docs/guides/chat](https://platform.openai.com/docs/guides/chat)'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text completion documentation from OpenAI: [https://platform.openai.com/docs/guides/completion](https://platform.openai.com/docs/guides/completion)'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text generation documentation from HuggingFace: [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2 model card: [https://huggingface.co/gpt2](https://huggingface.co/gpt2)'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
