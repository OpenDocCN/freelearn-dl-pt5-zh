- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Regularization in Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A full book could be written about regularization in **natural language processing**
    (**NLP**). NLP is a wide field that consists of many topics, ranging from simple
    classification such as review ranking to complex models and solutions such as
    ChatGPT. This chapter will merely scratch the surface of what can reasonably be
    done with simple NLP solutions such as classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization using a word2vec embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation using word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-shot inference with pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with BERT embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation using GPT-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to take advantage of advanced methods
    for NLP tasks such as word embeddings and transformers, as well as be able to
    use data augmentation to generate synthetic training data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use various NLP solutions and tools, so we will require
    the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization using a word2vec embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a pre-trained word2vec embedding to improve the
    results of a task thanks to transfer learning. We will compare the results to
    the initial *Training a GRU* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206),
    on the IMDb dataset for review classification.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A word2vec is a rather old type of word embedding in the NLP landscape and has
    been widely used in many NLP tasks. While recent techniques are sometimes more
    powerful, the word2vec approach remains efficient and cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Without getting into the details of word2vec, a commonly used model is a 300-dimensional
    embedding; each word in the vocabulary is embedded into a vector of 300 values.
  prefs: []
  type: TYPE_NORMAL
- en: 'word2vec is usually trained on a large corpus of texts. There are two main
    approaches for training a word2vec that can be roughly described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous bag of words** (**CBOW**): Uses the context of surrounding words
    in a sentence to predict a missing word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**skip-gram**: Uses a word to predict its surrounding context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of these two approaches is proposed in *Figure 9**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods](img/B19629_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In practice, CBOW is usually easier to train, while skip-gram may have better
    performance for rarer words.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is not to train our own word2vec, but simply to reuse a trained one
    and take advantage of transfer learning to get a performance boost in our predictions.
    In this recipe, instead of training our own embedding before feeding the **gated
    recurrent unit** (**GRU**), we will simply reuse a pre-trained word2vec embedding,
    and then only train our GRU on top of these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that, we will again work on the IMDb dataset classification task: a dataset
    containing texts of movie reviews as inputs and associated binary labels, positive
    or negative. The dataset for this can be downloaded with the Kaggle API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command will install the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will train a GRU for binary classification on the IMDb review
    dataset. Compared to the original recipe, the main difference is in *step 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` and some related modules and classes for the neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` and `LabelEncoder` from `scikit-learn` for preprocessing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AutoTokenizer` from `transformers` to tokenize the reviews'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` for data manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for visualization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gensim` for the word2vec embedding and `nltk` for work tokenization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you haven’t done so yet, you will need to add the `nltk.download(''punkt'')`
    line to download some required utility instances, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the pre-trained word2vec model, which contains a 300-dimension embedding.
    The model is about 1.6 GB and may take some time to download, depending on your
    available bandwidth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from the CSV file with `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into train and test sets using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the dataset’s `TextClassificationDataset` class, which handles the
    data. This is where the word2vec embedding is computed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At instantiation, each input movie is converted into an embedding in two ways
    with the `embed` method:'
  prefs: []
  type: TYPE_NORMAL
- en: Each movie review is tokenized with a word tokenizer (basically splitting sentences
    into words).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, a vector of `max_words` length is computed, containing the word2vec embeddings
    of `max_words` first words in the review. If the review is shorter than `max_words`
    words, the vector is filled with zero padding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we must instantiate the `TextClassificationDataset` objects for the train
    and test sets, as well as the related data loaders. The maximum number of words
    is set to `64`, as is the batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must implement the GRU classifier model. Since the embedding was computed
    at the data loading step, this model is directly computing a three-layer GRU,
    followed by a fully connected layer with a sigmoid activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must instantiate the GRU model. The embedding dimension, defined by
    the word2vec model, is `300`. We have chosen a hidden dimension of `32` so that
    each GRU layer is made up of 32 units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must instantiate the optimizer as an `Adam` optimizer with a learning
    rate of `0.001`; the loss is defined as the binary cross-entropy loss since this
    is a binary classification task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model for `20` epochs using the `train_model` function and store
    the loss and accuracy for both the train and test sets at each epoch. The implementation
    of the `train_model` function can be found in this book’s GitHub repository at
    [https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the typical output after 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the BCE loss for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Binary cross-entropy loss as a function of the epoch](img/B19629_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Binary cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, while the train loss keeps decreasing over the 20 epochs, the
    test loss soon reaches a minimum at around 5 epochs, to then increase, indicating
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the accuracy for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Accuracy as a function of the epoch](img/B19629_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Accuracy as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: As expected with the loss, the accuracy keeps increasing for the train set.
    For the test set, it reaches a maximum value of about 81% (against 77% without
    word2vec embedding, as shown in the previous chapter). Word2vec embedding allowed
    us to improve the results slightly, though the results may improve much more if
    we adjust the other hyperparameters accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we used the embeddings as an array in this recipe, they can be used differently;
    for example, we can use an average of all the embeddings in a sentence or other
    statistical information.
  prefs: []
  type: TYPE_NORMAL
- en: Also, while word2vec works well enough in many cases here, a few embeddings
    can be derived with a more specialized approach, such as doc2vec, which is sometimes
    more powerful for documents and longer texts.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Wikipedia article about word2vec is a valuable resource as it specifies
    many relevant publications: [https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3](https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This documentation from Google is also useful: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation using word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to regularize a model and get better performance is to have more data.
    Collecting data is not always easy or possible, but synthetic data can be an affordable
    way to improve performance. We’ll do this in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using word2vec embeddings, you can generate new, synthetic data that has a close
    semantic meaning. By doing this, it is fairly easy for a given word to get the
    most similar words in a given vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, using word2vec and a few parameters, we’ll see how we can generate
    new sentences with a close semantic meaning. We will only apply it to a given
    sentence as an example and propose how to integrate it into a full training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The only required libraries are `numpy` and `gensim`, both of which can be installed
    with `pip install` `numpy gensim`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to import the necessary libraries – `numpy` for random calls
    and `gensim` for word2vec model loading:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load a pre-trained word2vec model. This may take some time if the model hasn’t
    been downloaded and stored in a local cache already. Also, this is a rather large
    model, so it may take some time to load in memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `replace_words_with_similar` function so that you can randomly
    replace `word` with another semantically close word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Hopefully, the comments are self-explanatory, but here is what this function
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: It splits the input text into words by using a simple split (a word tokenizer
    can be used as well)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each word, it checks for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the word is in the word2vec vocabulary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the word is not in a list of stop words (to be defined)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a random probability is above the threshold probability (to draw random words)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a word fulfills the previous checks, the following is computed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_similar` most similar words'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One of those words is picked randomly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the similarity score of this word is above a given threshold, add it to the
    output sentence
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If no updated word has been added, just add the original word so that the overall
    sentence remains logical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sim_threshold`: The similarity threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probability`: The probability for a word to be replaced with a similar word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_similar`: The number of similar words to compute for a given word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words`: A list of words not to be replaced in case some words are specifically
    important or have several meanings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the `replace_words_with_similar` function we just implemented to a given
    sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output is as follows. It allows us to change a few words while keeping
    the overall meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to this data augmentation technique, it is possible to generate more
    diverse data so that we can make the models more robust and regularized.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to add such a data generation function to a classification task would
    be to add it at the data-loading step. This would generate synthetic data on-the-fly
    and could allow us to regularize the model. It could be added to the dataset class,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Documentation about the `most_similar` function of the word2vec model can be
    found at [https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot inference with pre-trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NLP field has faced many major advances in the last few years, which means
    that many pre-trained, efficient models can be reused. These pre-trained, freely
    available models allow us to approach some NLP tasks with zero-shot inference
    since we can reuse those models. We’ll try this approach in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We sometimes use zero-shot inference (or zero-shot learning) and few-shot learning.
    Zero-shot learning means being able to perform a task without any training for
    this specific task; few-shot learning means performing a task while training only
    on a few samples.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot inference is the act of reusing pre-trained models without doing any
    fine-tuning. There are many very powerful, free-to-use models available that can
    do just as well as a trained model of our own. Since the available models are
    trained on huge datasets with massive computational power, it is sometimes hard
    to compete with an in-house model that’s been trained on much less data and with
    less computational power.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: That being said, sometimes, training on small, well-curated, task-specific data
    can do wonders and provide much better performance. It’s all a matter of context.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we sometimes have data without labels, so supervised learning is not a
    possibility. In such cases, labeling a small subsample of the data ourselves and
    evaluating a zero-shot approach against this data can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will reuse a pre-trained model on the Tweets dataset and
    classify tweets as negative, neutral, or positive. Since no training is needed,
    we will directly evaluate the model on the test set so that it can be compared
    with the results we obtained with a simple RNN in the *Training an RNN* recipe
    from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we need to download the dataset locally. It can be downloaded with
    the Kaggle API and then unzipped with the following command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'The libraries that are required to run this recipe can be installed with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following necessary functions and models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`numpy` for data manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` for loading the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` from `scikit-learn` to split the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy_score` from `scikit-learn` to compute the accuracy score'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline` from `transformers` for instantiating the zero-shot classifier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the dataset. In our case, the only columns of interest are `text` for
    the features and `airline_sentiment` for the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – First five rows of the dataset for the considered columns](img/B19629_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – First five rows of the dataset for the considered columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into train and test sets, with the same parameters as in the
    *Regularization using a word2vec embedding* recipe so that the results can be
    compared: `test_size` set to `0.2` and `random_state` set to `0`. Since there
    is no training, we will only use the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the classifier using a `transformers` pipeline with the following
    parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`task="zero-shot-classification"`: This will instantiate a zero-shot classification
    pipeline'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model="facebook/bart-large-mnli"`: This will specify the model to be used
    for this pipeline'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When this is first called, it may download several files and the model itself,
    and it may take some time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the candidate labels in an array. These candidate labels are needed for
    zero-shot classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the predictions on the test set and store them in an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Refer to the *There’s more…* section for a few details about what the classifier
    does, as well as its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the accuracy score of the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The computed accuracy score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: We got an accuracy score of 74.5%, which is equivalent to the results we had
    after training a simple RNN in the *Training an RNN* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
    Without any training costs and large, labeled datasets, we can get the same performance
    thanks to this zero-shot classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning comes with a cost since pre-trained language models are usually
    rather large and may require large computational power to run at scale.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at an example of the input and output of `classifier` to get a better
    understanding of what it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what we can see:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An input sentence: `I love to learn` `about regularization`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Candidate labels: `positive`, `negative`, and `neutral`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is a dictionary with the following key values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''sequence''`: The input sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''labels''`: The input candidate labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''scores''`: A list of scores, one for each label, sorted in decreasing order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since the scores are always sorted in decreasing order, the labels may have
    a different order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the predicted class can be computed with the following code, which
    will retrieve the `argmax` values of the scores and the associated label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: In our case, that would output `positive`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model card of the bart-large-mnli model: [https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A tutorial from Hugging Face about zero-shot classification: [https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification](https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The documentation about the `transformers` pipelines, which allows us to do
    much more than zero-shot classification: [https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with BERT embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to how we used a pre-trained word2vec model to compute the embeddings,
    it is possible to use the embeddings of a pre-trained BERT model, a transformer-based
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, after quickly explaining the BERT model, we will train a model
    using BERT embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '**BERT** stands for **Bidirectional Encoder Representation from Transformers**
    and is a model that was proposed by Google in 2018\. It was first deployed in
    late 2019 in Google Search for English queries, as well as for many other languages.
    The BERT model has been proven effective in several NLP tasks, including text
    classification and question-answering.'
  prefs: []
  type: TYPE_NORMAL
- en: Before quickly explaining what BERT is, let’s take a step back and look at what
    **attention mechanisms** and **transformers** are.
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms are widely used in NLP, and more and more in other fields
    such as computer vision, since their introduction in 2017\. The high-level idea
    of an attention mechanism is to compute a weight for each input token concerning
    other tokens in the given sequence. Compared to RNNs, which process inputs as
    sequences, the attention mechanism considers the whole sequence at once. This
    allows attention-based models to handle long-range dependencies in sequences more
    efficiently since the attention mechanism can be considered agnostic to the sequence
    length.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are a type of neural network based on self-attention. They usually
    start with an embedding, as well as an absolute positional encoding that attention
    layers are trained on. Those layers usually use multi-head attention to capture
    various aspects of the input sequence. For more details, you can read the original
    paper, *Attention Is All You Need* (refer to the *See also* section for the paper).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since BERT uses absolute positional encodings, you are advised to use padding
    on the right, if any padding is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The BERT model was built on top of `transformers` and is made of 12 transformer-based
    encoding layers for the base model (24 layers for the large model), for about
    110 million parameters. More interestingly, it was pre-trained in an unsupervised
    fashion, using two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked language**: 15% of the tokens in a sequence are randomly masked, and
    the model is trained to predict the masked tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next sentence prediction**: Given two sentences, the model is trained to
    predict whether they are consecutive in a given text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This pre-training approach is summarized in the following diagram, which was
    extracted from the BERT paper called *BERT: Pre-training of Deep Bidirectional
    Transformers for* *Language Understanding*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – BERT pre-training diagram proposed in the original article,
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](img/B19629_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 – BERT pre-training diagram proposed in the original article, BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While word2vec embedding is context-free (no matter the context, a word embedding
    remains the same), BERT gives a different embedding for a given word, depending
    on its surroundings. This makes sense since a given word may have a different
    meaning in two sentences (for example, apple or Apple can be either a fruit or
    a company, depending on the context).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will reuse the Tweets dataset, which can be downloaded
    and then unzipped locally with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: The necessary libraries can be installed with `pip install torch scikit-learn`
    `transformers pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will train a simple logistic regression on top of pre-trained
    BERT embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` for device management if you have a GPU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `train_test_split` method and the `LogisticRegression` class from `scikit-learn`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The related `BERT` classes from `transformers`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` for data loading'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the dataset with `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset into train and test sets, keeping the same parameters as
    in the *Zero-shot inference* recipe with pre-trained models, so that we can compare
    their performance later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the tokenizer and the BERT model. Instantiating the model is a
    multi-step process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, instantiate the model’s configuration with the `BertConfig` class.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, instantiate `BertModel` with random weights.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the weights of the pre-trained model (this will display a warning since
    not all the weights will have been loaded).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the model on the GPU, if any, and set the model to `eval` mode.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: You may get some warning messages since some layers have no pre-trained weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the embeddings for the train and test sets. This is a two-step process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the tokens with the tokenizer (and optionally load the tokens on the
    GPU, if any).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, compute the embeddings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For more details about the inputs and outputs of the BERT model, check out the
    *There’s more…* subsection of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, instantiate and train a logistic regression model. It may require more
    iterations than the default model allows. Here, it has been set to `10,000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, print the accuracy on the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should have output results similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: We get a final result of about 79% accuracy on the test set and 80% on the train
    set. As a comparison, using zero-shot inference and a simple RNN on this same
    dataset both provided 74% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get a better understanding of what the tokenizer computes and what the BERT
    model outputs, let’s have a look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s apply the tokenizer to a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the tokenizer returns three outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids`: This is the index of tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids`: The sentence number. This is only useful for paired sentences,
    as in the original training of BERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask`: This is where the model will focus. As we can see, it’s only
    been set to `1` for actual tokens, and then to `0` for padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three lists are fed to the BERT model so that it can compute its output.
    The output is made up of the following two tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state`: The values of the last hidden state, whose shape is `[batch_size,`
    `max_length, 768]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output`: The pooled values of the outputs over the sequence steps,
    whose shape is `[``batch_size, 768]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many other types of embeddings exist and can be more or less powerful, depending
    on the task at hand. For example, OpenAI also proposes embeddings that can be
    made available using an API. For example, the following code allows us to have
    embeddings for a given sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: This would return a 1,536-dimensional embedding for the given sentence that
    could then be used for classification or other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, to use these embeddings, you would need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the `openai` library with `pip` `install openai`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an API key on the OpenAI website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a working payment method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper introducing transformers, *Attention is all you* *need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The BERT model card: [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The BERT paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information about the OpenAI embeddings, here is the official documentation:
    [https://platform.openai.com/docs/guides/embeddings/use-cases](https://platform.openai.com/docs/guides/embeddings/use-cases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation using GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models are becoming more and more powerful, especially in NLP. Using
    these to generate new, synthetic data sometimes allows us to significantly improve
    the results we get and regularize models. We’ll learn how to do this in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While models such as BERT are effective at tasks such as text classification,
    they usually do not perform very well when it comes to text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of models, such as **generative pre-trained transformer** (**GPT**)
    models, can be quite impressive at generating new data. In this recipe, we will
    use the OpenAI API and GPT-3.5 to generate synthetic yet realistic data. Having
    more data is key to having more regularization in our models, and data generation
    is one way to collect more data.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, you will need to install the OpenAI library with `pip` `install
    openai`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, since the API we will use is not free, it is necessary to create an OpenAI
    account with a generated API key and a working payment method.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API key
  prefs: []
  type: TYPE_NORMAL
- en: You can easily create an API key in your profile by accessing the **API** **keys**
    section.
  prefs: []
  type: TYPE_NORMAL
- en: In the *There’s more…* section, we will provide a free alternative – that is,
    using GPT-2 to generate new data – but it will have less realistic results. For
    that to work, you must have Hugging Face’s `transformers` library, which you can
    install with `pip` `install transformers`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will simply query GPT-3.5 to generate a few positive and
    negative movie reviews so that we have more data to train a movie review classification
    model. Of course, this can be derived from any classification task, as well as
    many other NLP tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `openai` library, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Provide your OpenAI API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This is just a code example – never share your API key on public repositories.
    Use alternatives such as environment variables instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate three positive examples using the `ChatCompletion` API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are several parameters here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model`: `gpt-3.5-turbo`, which performs well and is cost-efficient. It is
    based on GPT-3.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages`: There can be three types of messages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system`: A formatting message, followed by alternating user and assistant
    messages'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`: A user message'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant`: An assistant message; we won’t use this'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens`: The maximum number of tokens in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature`: This is usually between 0 and 2\. A larger value means more
    randomness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: The number of desired outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can display the output-generated sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, let’s generate and display three negative examples of movie reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code shows the three reviews that were generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: The generated examples are well written and probably good enough to be written
    by a human. Also, it would be possible to generate more neutral, more random,
    longer, or shorter examples if needed, which is quite convenient.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternatively, it is possible to use the GPT-2 model for free, even though the
    results are less realistic. Let’s learn how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s instantiate a text generation pipeline based on GPT-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates some text. The behavior is not the same, and it only handles
    text completion, so you must propose the beginning of a piece of text for the
    model to complete it automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following three reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the results are less interesting and realistic than with GPT-3,
    but they can still be useful if we apply some manual filtering.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chat completion documentation from OpenAI: [https://platform.openai.com/docs/guides/chat](https://platform.openai.com/docs/guides/chat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text completion documentation from OpenAI: [https://platform.openai.com/docs/guides/completion](https://platform.openai.com/docs/guides/completion)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text generation documentation from HuggingFace: [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2 model card: [https://huggingface.co/gpt2](https://huggingface.co/gpt2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
