- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Introduction to Machine Learning with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 1.01: Adding Regularization to the Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will utilize the same logistic regression model from the
    scikit-learn package. This time, however, we will add regularization to the model
    and search for the optimum regularization parameter - a process often called `hyperparameter
    tuning`. After training the models, we will test the predictions and compare the
    model evaluation metrics to the ones that were produced by the baseline model
    and the model without regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the feature data from *Exercise 1.03*, *Appropriate Representation of
    the Data*, and the target data from *Exercise 1.02*, *Cleaning the Data*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a `test` and `train` dataset. Train the data using the training dataset.
    This time, however, use part of the `training` dataset for validation in order
    to choose the most appropriate hyperparameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once again, we will use `test_size = 0.2`, which means that `20%` of the data
    will be reserved for testing. The size of our validation set will be determined
    by how many validation folds we have. If we do `10-fold cross-validation`, this
    equates to reserving `10%` of the `training` dataset to validate our model on.
    Each fold will use a different `10%` of the `training` dataset, and the average
    error across all folds is used to compare models with different hyperparameters.
    Assign a random value to the `random_state` variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the dimensions of the DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, instantiate the models. Try two types of regularization parameters, `l1`
    and `l2`, with 10-fold cross-validation. Iterate our regularization parameter
    from 1x10-2 to 1x106 equally in the logarithmic space to observe how the parameters
    affect the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a logistic regression model with the `l1` regularization parameter, only
    the `liblinear` solver can be used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, fit the models to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.37: Output of the fit command indicating all of the model training
    parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_01_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.37: Output of the fit command indicating all of the model training
    parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, we can see what the value of the regularization parameter was for the
    two different models. The regularization parameter is chosen according to which
    produced a model with the lowest error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `C_` attribute is only available once the model has been trained because
    it is set once the best parameter from the cross-validation process has been determined.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To evaluate the performance of the models, make predictions on the `test` set,
    which we''ll compare against the `true` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To compare these models, calculate the evaluation metrics. First, look at the
    accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, look at the other evaluation metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Observe the values of the coefficients once the model has been trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `coef_` attribute is only available once the model has been trained because
    it is set once the best parameter from the cross-validation process has been determined.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.38: The feature column names and the value of their respective coefficients'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model with l1 regularization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_01_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.38: The feature column names and the value of their respective coefficients
    for the model with l1 regularization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Do the same for the model with an `l2` regularization parameter type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.39: The feature column names and the value of their respective coefficients'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model with l2 regularization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_01_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.39: The feature column names and the value of their respective coefficients
    for the model with l2 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VIoe5M](https://packt.live/2VIoe5M).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Machine Learning versus Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 2.01: Creating a Logistic Regression Model Using Keras'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we are going to create a basic model using the Keras library.
    The model that we will build will classify users of a website into those that
    will purchase a product from a website and those that will not. To do this, we
    will utilize the same online shopping purchasing intention dataset that we did
    previously and attempt to predict the same variables that we did in *Chapter 1*,
    *Introduction to Machine Learning with Keras*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter notebook from the start menu to implement this activity. Load
    in the online shopping purchasing intention datasets, which you can download from
    the GitHub repository. We will use the pandas library for data loading, so import
    the `pandas` library. Ensure you have saved the csv files to an appropriate data
    folder for this chapter first. Alternatively, you can change the path to the files
    that you use in your code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the purposes of this activity, we will not perform any further preprocessing.
    As we did in the previous chapter, we will split the dataset into training and
    testing and leave the testing until the very end when we evaluate our models.
    We will reserve `20%` of our data for testing by setting the `test_size=0.2` parameter,
    and we will create a `random_state` parameter so that we can recreate the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a seed in `numpy` and `tensorflow` for reproducibility. Begin creating
    the model by initializing a model of the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To add a fully connected layer to the model, add a layer of the `Dense` class.
    Here, we include the number of nodes in the layer. In our case, this will be one
    since we are performing binary classification and our desired output is `zero`
    or `one`. Also, specify the input dimensions, which is only done on the first
    layer of the model. It is there to indicate the format of the input data. Pass
    the number of features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a sigmoid activation function to the output of the previous layer to replicate
    the `logistic regression` algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have all the model components in the correct order, we must compile
    the model so that all the learning processes are configured. Use the `adam` optimizer,
    a `binary_crossentropy` for the loss, and track the accuracy of the model by passing
    the parameter into the `metrics` argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the model summary to verify the model is as we expect it to be:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19: A summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_02_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: A summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, fit the model using the `fit` method of the `model` class. Provide the
    training data, as well as the number of epochs and how much data to use for validation
    after each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.20: Using the fit method on the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Using the fit method on the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The values for the loss and accuracy have been stored within the `history`
    variable. Plot the values for each using the loss and accuracy we tracked after
    each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plots show the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.21: The loss and accuracy while fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: The loss and accuracy while fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, evaluate the model on the test data we held out from the beginning,
    which will give an objective evaluation of the performance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code can be found below. Here, the model predicts
    the purchasing intention of users in the test dataset and evaluates the performance
    by comparing it to the real values in `y_test`. Evaluating the model on the test
    dataset produces loss and accuracy values that we can print out:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://packt.live/3dVTQLe](https://packt.live/3dVTQLe).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZxEhV4](https://packt.live/2ZxEhV4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. Deep Learning with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 3.01: Building a Single-Layer Neural Network for Performing Binary
    Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will compare the results of a logistic regression model
    and single-layer neural networks of different node sizes and different activation
    functions. The dataset we will use represents the normalized test results of aircraft
    propeller inspections, while the class represents whether they passed or failed
    a manual visual inspection. We will create models to predict the results of the
    manual inspection when given the automated test results. Follow these steps to
    complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load all the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up a `seed`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the simulated dataset and print the size of `X` and `Y` and the number
    of examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the dataset. The x and y coordinates of each point will be the two input
    features. The color of each record represents the `pass`/`fail` result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: Simulated training data points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.19: Simulated training data points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the `logistic regression` model, which will be a one-node sequential
    model with no hidden layers and a `sigmoid activation` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`100` epochs = `0.3537`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20: The loss details of the last 5 epochs out of 100'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.20: The loss details of the last 5 epochs out of 100'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision boundary on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21: The decision boundary of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.21: The decision boundary of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The linear decision boundary of the logistic regression model is obviously unable
    to capture the circular decision boundary between the two classes and predicts
    all the results as a passed result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a neural network with one hidden layer with three nodes and a `relu
    activation function` and an output layer with one node and a `sigmoid activation
    function`. Finally, compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`200` epochs = `0.0260`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22: The loss details of the last 5 epochs out of 200'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.22: The loss details of the last 5 epochs out of 200'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision boundary that was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.23: The decision boundary for the neural network with a hidden layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: size of 3 and a ReLU activation function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.23: The decision boundary for the neural network with a hidden layer
    size of 3 and a ReLU activation function'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Having three processing units instead of one dramatically improved the capability
    of the model in capturing the non-linear boundary between the two classes. Notice
    that the loss value decreased drastically in comparison to the previous step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a neural network with one hidden layer with six nodes and a `relu activation
    function` and an output layer with one node and a `sigmoid activation function`.
    Finally, compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`400` epochs = `0.0231`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: The loss details of the last 5 epochs out of 400'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.24: The loss details of the last 5 epochs out of 400'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision boundary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: The decision boundary for the neural network with a hidden layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: size of 6 and the ReLU activation function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.25: The decision boundary for the neural network with a hidden layer
    size of 6 and the ReLU activation function'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By doubling the number of units in the hidden layer, the decision boundary of
    the model gets closer to a true circular shape, and the loss value is decreased
    even more in comparison to the previous step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a neural network with one hidden layer with three nodes and a `tanh
    activation function` and an output layer with one node and a `sigmoid activation
    function`. Finally, compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`200` epochs = `0.0426`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.26: The loss details of the last 5 epochs out of 200'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.26: The loss details of the last 5 epochs out of 200'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision boundary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27: The decision boundary for the neural network with a hidden layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: size of 3 and the tanh activation function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.27: The decision boundary for the neural network with a hidden layer
    size of 3 and the tanh activation function'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the `tanh` activation function has eliminated the sharp edges in the decision
    boundary. In other words, it has made the decision boundary smoother. However,
    the model is not performing better since we can see an increase in the loss value.
    We achieved similar loss and accuracy scores when we evaluated on the test dataset,
    despite mentioning previously that the learning parameters for `tanh` are slower
    than they are for `relu`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a neural network with one hidden layer with six nodes and a `tanh activation
    function` and an output layer with one node and a `sigmoid activation function`.
    Finally, compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`400` epochs = `0.0215`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.28: The loss details of the last 5 epochs out of 400'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.28: The loss details of the last 5 epochs out of 400'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision boundary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.29: The decision boundary for the neural network with a hidden layer
    size'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: of 6 and the tanh activation function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.29: The decision boundary for the neural network with a hidden layer
    size of 6 and the tanh activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Again, using the `tanh` activation function instead of `relu` and adding more
    nodes to our hidden layer has smoothed the curves on the decision boundary more,
    fitting the training data better according to the accuracy of the training data.
    We should be careful not to add too many nodes to the hidden layer as we may begin
    to overfit the data. This can be observed by evaluating the test set, where there
    is a slight decrease in the accuracy of the neural network with six nodes compared
    to a neural network with three.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iv0wn1](https://packt.live/3iv0wn1).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2BqumZt](https://packt.live/2BqumZt).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Advanced Fibrosis Diagnosis with Neural Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you are going to use a real dataset to predict whether a
    patient has advanced fibrosis based on measurements such as age, gender, and BMI.
    The dataset consists of information for 1,385 patients who underwent treatment
    dosages for hepatitis C. For each patient, `28` different attributes are available,
    as well as a class label, which can only take two values: `1`, indicating advanced
    fibrosis, and `0`, indicating no indication of advanced fibrosis. This is a binary/two-class
    classification problem with an input dimension equal to 28\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this activity, you will implement different deep neural network architectures
    to perform this classification, plot the trends in training error rates and test
    error rates, and determine how many epochs the final classifier needs to be trained
    for. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary libraries and load the dataset using the pandas `read_csv`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of `records` and `features` in the `feature` dataset and the
    number of unique classes in the `target` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the data and scale it. Following this, split the dataset into the
    `training` and `test` sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a deep neural network with one hidden layer of size `3` and a `tanh
    activation function`, an output layer with one node, and a `sigmoid activation
    function`. Finally, compile the model and print out a summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: The architecture of the neural network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.30: The architecture of the neural network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the `training error rate` and `test error rate` for every epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: A plot of the training error rate and test error rate while
    training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.31: A plot of the training error rate and test error rate while training
    the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Print the values of the best accuracy that was reached on the training set and
    on the test set, as well as the `loss` and `accuracy` that was evaluated on the
    `test` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a deep neural network with two hidden layers of sizes `4` and `2`
    with a `tanh activation function`, an output layer with one node, and a `sigmoid
    activation function`. Finally, compile the model and print out a summary of the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.32: The architecture of the neural network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.32: The architecture of the neural network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot training and test error plots with two hidden layers of size 4 and 2\.
    Print the best accuracy that was reached on the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.33: A plot of the training error and test error rates while training
    the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_03_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.33: A plot of the training error and test error rates while training
    the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Print the values of the best accuracy that was achieved on the `training` set
    and on the `test` set, as well as the `loss` and `accuracy` that was evaluated
    on the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2BrIRMF](https://packt.live/2BrIRMF).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NUl22A](https://packt.live/2NUl22A).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4\. Evaluating Your Model with Cross-Validation Using Keras Wrappers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 4.01: Model Evaluation Using Cross-Validation for an Advanced Fibrosis
    Diagnosis Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we are going to use what we learned in this topic to train
    and evaluate a deep learning model using `k-fold cross-validation`. We will use
    the model that resulted in the best test error rate from the previous activity
    and the goal will be to compare the cross-validation error rate with the training
    set/test set approach error rate. The dataset we will use is the hepatitis C dataset,
    in which we will build a classification model to predict which patients get advanced
    fibrosis. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset and print the number of records and features in the dataset,
    as well as the number of possible classes in the target dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function that returns the Keras model. First, import the necessary
    libraries for Keras. Inside the function, instantiate the sequential model and
    add two dense layers, with the first of `size 4` and the second of `size 2`, both
    with `tanh activation` functions. Add the output layer with a `sigmoid activation`
    function. Compile the model and return the model from the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale the training data using the `StandardScaler` function. Set the seed so
    that the model is reproducible. Define the `n_folds`, `epochs`, and `batch_size`
    hyperparameters. Then, build the Keras wrapper with scikit-learn, define the `cross-validation`
    iterator, perform `k-fold cross-validation`, and store the scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each of the folds, print the accuracy stored in the `results` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3eWgR2b](https://packt.live/3eWgR2b).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iBYtOi](https://packt.live/3iBYtOi).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 4.02: Model Selection Using Cross-Validation for the Advanced Fibrosis
    Diagnosis Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we are going to improve our classifier for the hepatitis
    C dataset by using cross-validation for model selection and hyperparameter selection.
    Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required packages and load the dataset. Scale the dataset using
    the `StandardScaler` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define three functions, each returning a different Keras model. The first model
    should have three hidden layers of `size 4`, the second model should have two
    hidden layers, the first of `size 4` and the second of `size 2`, and the third
    model should have two hidden layers of `size 8`. Use function parameters for the
    activation functions and optimizers so that they can be passed through to the
    model. The goal is to find out which of these three models leads to the lowest
    cross-validation error rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the code that will loop over the three models and perform `5-fold cross-validation`.
    Set the seed so that the models are reproducible and define the `n_folds`, `batch_size`,
    and `epochs` hyperparameters. Store the results from applying the `cross_val_score`
    function when training the models:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s an example output. In this instance, **Model 2** has the best cross-validation
    test accuracy, as you can see below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Choose the model with the highest accuracy score and repeat *step 2* by iterating
    over the `epochs = [100, 200]` and `batches = [10, 20]` values and performing
    `5-fold cross-validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s an example output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, the `batch_size= 10`, `epochs=200` pair has the best cross-validation
    test accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose the batch size and epochs with the highest accuracy score and repeat
    *step 3* by iterating over the `optimizers = [''rmsprop'', ''adam'',''sgd'']`
    and `activations = [''relu'', ''tanh'']` values and performing `5-fold cross-validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `activation='relu'` and `optimizer='rmsprop'` pair has the best cross-validation
    test accuracy. Also, the `activation='tanh'` and `optimizer='sgd'` pair results
    in the second-best performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2D3AIhD](https://packt.live/2D3AIhD).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NUpiiC](https://packt.live/2NUpiiC).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 4.03: Model Selection Using Cross-validation on a Traffic Volume Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you are going to practice model selection using cross-validation
    one more time. Here, we are going to use a simulated dataset that represents a
    target variable representing the volume of traffic in cars/hour across a city
    bridge and various normalized features related to traffic data such as time of
    day and the traffic volume on the previous day. Our goal is to build a model that
    predicts the traffic volume across the city bridge given the various features.
    Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required packages and load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset, print the input and output size for the feature dataset,
    and print the possible classes in the target dataset. Also, print the range of
    the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define three functions, each returning a different Keras model. The first model
    should have one hidden layer of `size 10`, the second model should have two hidden
    layers of `size 10`, and the third model should have three hidden layers of `size
    10`. Use function parameters for the optimizers so that they can be passed through
    to the model. The goal is to find out which of these three models leads to the
    lowest cross-validation error rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the code that will loop over the three models and perform `5-fold cross-validation`.
    Set the seed so that the models are reproducible and define the `n_folds` hyperparameters.
    Store the results from applying the `cross_val_score` function when training the
    models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Model 2` (a two-layer neural network) has the lowest test error rate.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose the model with the lowest test error rate and repeat *step 4* while
    iterating over `epochs = [80, 100]` and `batches = [50, 25]` and performing `5-fold
    cross-validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `batch_size=5` and `epochs=100` pair has the lowest test error rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose the model with the highest accuracy score and repeat *step 2* by iterating
    over `optimizers = [''rmsprop'', ''sgd'', ''adam'']` and performing `5-fold cross-validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`optimizer=''sgd''` has the lowest test error rate, so we should proceed with
    this particular model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31TcYaD](https://packt.live/31TcYaD).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iq6iqb](https://packt.live/3iq6iqb).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5\. Improving Model Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 5.01: Weight Regularization on an Avila Pattern Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will build a Keras model to perform classification on
    the Avila pattern dataset according to given network architecture and hyperparameter
    values. The goal is to apply different types of weight regularization on the model,
    that is, `L1` and `L2`, and observe how each type changes the result. Follow these
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset and split the dataset into a `training set` and a `test set`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a Keras sequential model with three hidden layers, the first of `size
    10`, the second of `size 6`, and the third of `size 4`. Finally, compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data to perform the classification, saving the
    results of the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the trends in training error and test error by importing the necessary
    libraries for plotting the loss and validation loss and saving them in the variable
    that was created when the model was fit to the training process. Print out the
    maximum validation accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.13: A plot of the training error and validation error during training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model without regularization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.13: A plot of the training error and validation error during training
    for the model without regularization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The validation loss keeps decreasing along with the training loss. Despite having
    no regularization, this is a fairly good example of the training process since
    the bias and variance are fairly low.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Redefine the model, adding `L2 regularizers` with `lambda=0.01` to each hidden
    layer of the model. Repeat *steps 3* and *4* to train the model and plot the `training
    error` and `validation error`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.14: A plot of the training error and validation error during training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model with L2 weight regularization (lambda=0.01)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.14: A plot of the training error and validation error during training
    for the model with L2 weight regularization (lambda=0.01)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As shown from the preceding plots, the test error almost plateaus after being
    decreased to a certain amount. The gap between the training error and the validation
    error at the end of the training process (the bias) is slightly smaller, which
    is indicative of reduced overfitting of the model for the training examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the previous step with `lambda=0.1` for the `L2 parameter`—redefine
    the model with the new lambda parameter, fit the model to the training data, and
    repeat *step 4* to plot the training error and validation error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.15: A plot of the training error and validation error during training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model with L2 weight regularization (lambda=0.1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.15: A plot of the training error and validation error during training
    for the model with L2 weight regularization (lambda=0.1)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The training and validation error quickly plateau and are much higher than they
    were for the models we created with a lower `L2 parameter`, indicating that we
    have penalized the model so much that it has not had the flexibility to learn
    the underlying function of the training data. Following this, we will reduce the
    value of the regularization parameter to prevent it from penalizing the model
    as much.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the previous step, this time with `lambda=0.005`. Repeat *step 4* to
    plot the training error and validation error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.16: A plot of the training error and validation error during training
    for the model with L2 weight regularization (lambda=0.005)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.16: A plot of the training error and validation error during training
    for the model with L2 weight regularization (lambda=0.005)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The value for the `L2 weight` regularization achieves the highest accuracy that
    was evaluated on the validation data of all the models with `L2 regularization`,
    but it is slightly lower than without regularization. Again, the test error does
    not increase a significant amount after being decreased to a certain value, which
    is indicative of the model not overfitting the training examples. It seems that
    `L2 weight regularization` with `lambda=0.005` achieves the lowest validation
    error while preventing the model from overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add `L1 regularizers` with `lambda=0.01` to the hidden layers of your model.
    Redefine the model with the new lambda parameter, fit the model to the training
    data, and repeat *step 4* to plot the training error and validation error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.17: A plot of the training error and validation error during training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model with L1 weight regularization (lambda=0.01)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.17: A plot of the training error and validation error during training
    for the model with L1 weight regularization (lambda=0.01)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the previous step with `lambda=0.005` for the `L1 parameter`—redefine
    the model with the new lambda parameter, fit the model to the training data, and
    repeat *step 4* to plot the `training error` and `validation error`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.18: The plot of the training error and validation error during training
    for the model with L1 weight regularization (lambda=0.005)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.18: The plot of the training error and validation error during training
    for the model with L1 weight regularization (lambda=0.005)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It seems that `L1 weight regularization` with `lambda=0.005` achieves a better
    test error while preventing the model from overfitting since the value of `lambda=0.01`
    is too restrictive and prevents the model from learning the underlying function
    of the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add `L1` and `L2 regularizers` with an `L1` of `lambda=0.005` and an `L2` of
    `lambda = 0.005` to the hidden layers of your model. Then, repeat *step 4* to
    plot the training error and validation error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.19: A plot of the training error and validation error during training
    for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.19: A plot of the training error and validation error during training
    for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005'
  prefs: []
  type: TYPE_NORMAL
- en: While `L1` and `L2 regularization` are successful in preventing the model from
    overfitting, the variance in the model is very low. However, the accuracy that's
    obtained on the validation data is not as high as the model that was trained with
    no regularization or the model that was trained with the `L2 regularization` `lambda=0.005`
    or `L1 regularization` `lambda=0.005` parameters individually.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31BUf34](https://packt.live/31BUf34).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38n291s](https://packt.live/38n291s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5.02: Dropout Regularization on the Traffic Volume Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will start with the model from *Activity 4.03*, *Model
    Selection Using Cross-Validation on a Traffic Volume Dataset*, of *Chapter 4*,
    *Evaluating Your Model with Cross-Validation Using Keras Wrappers*. You will use
    the training set/test set approach to train and evaluate the model, plot the trends
    in training error and the generalization error, and observe the model overfitting
    the data examples. Then, you will attempt to improve model performance by addressing
    the overfitting issue through the use of dropout regularization. In particular,
    you will try to find out which layers you should add dropout regularization to
    and what `rate` value will improve this specific model the most. Follow these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset using the pandas `read_csv` function, split the dataset into
    a training set and test set into an `80-20` ratio using `train_test_split`, and
    scale the input data using `StandardScaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a seed so that the model can be reproduced. Next, define a Keras sequential
    model with two hidden layers of `size 10`, both with `ReLU activation` functions.
    Add an output layer with no activation function and compile the model with the
    given hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training data with the given hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the trends for the `training error` and `test error`. Print the best accuracy
    that was reached for the training and validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.20: A plot of the training error and validation error during training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for the model without regularization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.20: A plot of the training error and validation error during training
    for the model without regularization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the training error and validation error values, there is a very small gap
    between the training error and validation error, which is indicative of a low
    variance model, which is good.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Redefine the model by creating the same model architecture. However, this time,
    add a dropout regularization with `rate=0.1` to the first hidden layer of your
    model. Repeat *step 3* to train the model on the training data and repeat *step
    4* to plot the trends for the training and validation errors. Then, print the
    best accuracy that was reached on the validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.21: A plot of the training error and validation error during training
    for the model with dropout regularization (rate=0.1) in the first layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.21: A plot of the training error and validation error during training
    for the model with dropout regularization (rate=0.1) in the first layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a small gap between the training error and the validation error; however,
    the validation error is lower than the training error, indicating that the model
    is not overfitting the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the previous step, this time adding dropout regularization with `rate=0.1`
    to both hidden layers of your model. Repeat *step 3* to train the model on the
    training data and repeat *step 4* to plot the trends for the training and validation
    errors. Then, print the best accuracy that was reached on the validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.22: A plot of the training error and validation error during training
    for'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: the model with dropout regularization (rate=0.1) in both layers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.22: A plot of the training error and validation error during training
    for the model with dropout regularization (rate=0.1) in both layers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The gap between the training error and validation error is slightly higher here,
    mostly due to the increase in the training error as a result of the additional
    regularization on the second hidden layer of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the previous step, this time adding dropout regularization with `rate=0.2`
    in the first layer and `rate=0.1` in the second layer of your model. Repeat *step
    3* to train the model on the training data and repeat *step 4* to plot the trends
    for the training and validation errors. Then, print the best accuracy that was
    reached on the validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.23: A plot of training errors and validation errors while training
    the model with dropout regularization, with rate=0.2 in the first layer and rate
    0.1 in the second layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_05_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.23: A plot of training errors and validation errors while training
    the model with dropout regularization, with rate=0.2 in the first layer and rate
    0.1 in the second layer'
  prefs: []
  type: TYPE_NORMAL
- en: The gap between the training error and validation error is slightly larger due
    to the increase in regularization. In this case, there was no overfitting in the
    original model. As a result, regularization increased the error rate on the training
    and validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38mtDo7](https://packt.live/38mtDo7).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31Isdmu](https://packt.live/31Isdmu).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5.03: Hyperparameter Tuning on the Avila Pattern Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will build a Keras model similar to those in the previous
    activities, but this time, you will add regularization methods to your model as
    well. Then, you will use scikit-learn optimizers to perform tuning on the model
    hyperparameters, including the hyperparameters of the regularizers. Follow these
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset and import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that returns a Keras model with three hidden layers, the
    first of `size 10`, the second of `size 6`, and the third of `size 4`, and apply
    `L2 weight regularization` and a `ReLU activation` function on each hidden layer.
    Compile the model with the given parameters and return it from the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a seed, use a scikit-learn wrapper to wrap the model that we created in
    the previous step, and define the hyperparameters to scan. Finally, perform `GridSearchCV()`
    on the model using the hyperparameter''s grid and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the results for the best cross-validation score that''s stored within
    the variable we created in the fit process. Iterate through all the parameters
    and print the mean of the accuracy across all the folds, the standard deviation
    of the accuracy, and the parameters themselves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 3* using `GridSearchCV()`, `lambda_parameter = [0.001, 0.01, 0.05,
    0.1]`, `batch_size = [20]`, and `epochs = [100]`. Fit the model to the training
    data using `5-fold cross-validation` and print the results for the entire grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Redefine a function that returns a Keras model with three hidden layers, the
    first of `size 10`, the second of `size 6`, and the third of `size 4`, and apply
    `dropout regularization` and a `ReLU activation` function on each hidden layer.
    Compile the model with the given parameters and return it from the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `rate = [0, 0.1, 0.2]` and `epochs = [50, 100]` and perform `GridSearchCV()`
    on the model. Fit the model to the training data using `5-fold cross-validation`
    and print the results for the entire grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 5* using `rate = [0.0, 0.05, 0.1]` and `epochs = [100]`. Fit the
    model to the training data using `5-fold cross-validation` and print the results
    for the entire grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2D7HN0L](https://packt.live/2D7HN0L).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 6.01: Computing the Accuracy and Null Accuracy of a Neural Network
    When We Change the Train/Test Split'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will see that our `null accuracy` and `accuracy` will
    be affected by changing the `train`/`test` split. To implement this, the part
    of the code where the train/test split was defined has to be changed. We will
    use the same dataset that we used in *Exercise 6.02*, *Computing Accuracy and
    Null Accuracy with APS Failure for Scania Trucks Data*. Follow these steps to
    complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries. Load the dataset using the pandas `read_csv`
    function and look at the first `five` rows of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following table shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13: Initial five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.13: Initial five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the `test_size` and `random_state` from `0.20` to `0.3` and `42` to
    `13`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you use a different `random_state`, you may get a different `train`/`test`
    split, which may yield slightly different final results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scale the data using the `StandardScaler` function and use the scaler to scale
    the test data. Convert both into pandas DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `sc.fit_transform()` function transforms the data, and the data is also
    converted into a `NumPy` array. We may need the data later for analysis as a DataFrame
    object, so the `pd.DataFrame()` function reconverts data into a DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the libraries that are required to build a neural network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add five `Dense` layers to the network with `Dropout`. Set the first hidden
    layer so that it has a size of `64` with a dropout rate of `0.5`, the second hidden
    layer so that it has a size of `32` with a dropout rate of `0.4`, the third hidden
    layer so that it has a size of `16` with a dropout rate of `0.3`, the fourth hidden
    layer so that it has a size of `8` with a dropout rate of `0.2`, and the final
    hidden layer so that it has a size of `4` with a dropout rate of `0.1`. Set all
    the activation functions to `ReLU`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an output `Dense` layer with a `sigmoid` activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since the output is binary, we are using the `sigmoid` function. If the output
    is multiclass (that is, more than two classes), then the `softmax` function should
    be used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the network and fit the model. The metric that''s being used here is `accuracy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The metric name, which in our case is `accuracy`, is defined in the preceding
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model with `100` epochs, a batch size of `20`, and a validation split
    of `0.2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the test dataset and print out the values for the `loss`
    and `accuracy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model returns an accuracy of `98.9833%`. But is it good enough? We can only
    get the answer to this question by comparing it against the null accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, compute the null accuracy. The `null accuracy` can be calculated using
    the `value_count` function of the `pandas` library, which we used in *Exercise
    6.01*, *Calculating Null Accuracy on a Pacific Hurricanes Dataset*, of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `null accuracy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3eY7y1E](https://packt.live/3eY7y1E).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2BzBO4n](https://packt.live/2BzBO4n).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 6.02: Calculating the ROC Curve and AUC Score'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ROC curve` and `AUC score` is an effective way to easily evaluate the
    performance of a binary classifier. In this activity, we will plot the `ROC curve`
    and calculate the `AUC score` of a model. We will use the same dataset and train
    the same model that we used in *Exercise 6.03*, *Deriving and Computing Metrics
    Based on a Confusion Matrix*. Continue with the same APS failure data, plot the
    `ROC curve`, and compute the `AUC score` of the model. Follow these steps to complete
    this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries and load the data using the pandas `read_csv` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test datasets using the `train_test_split`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale the feature data so that it has a `mean` of `0` and a `standard deviation`
    of `1` using the `StandardScaler` function. Fit the scaler in the `training data`
    and apply it to the `test data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the Keras libraries that are required for creating the model. Instantiate
    a Keras model of the `Sequential` class and add five hidden layers to the model,
    including dropout for each layer. The first hidden layer should have a size of
    `64` and a dropout rate of `0.5`. The second hidden layer should have a size of
    `32` and a dropout rate of `0.4`. The third hidden layer should have a size of
    `16` and a dropout rate of `0.3`. The fourth hidden layer should have a size of
    `8` and a dropout rate of `0.2`. The final hidden layer should have a size of
    `4` and a dropout rate of `0.1`. All the hidden layers should have `ReLU activation`
    functions and set `kernel_initializer = ''uniform''`. Add a final output layer
    to the model with a sigmoid activation function. Compile the model by calculating
    the accuracy metric during the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data by training for `100` epochs with `batch_size=20`
    and with `validation_split=0.2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model has finished fitting to the training data, create a variable
    that is the result of the model''s prediction on the test data using the model''s
    `predict_proba` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `roc_curve` from scikit-learn and run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`fpr` = False positive rate (1 - specificity)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tpr` = True positive rate (sensitivity)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`thresholds` = The threshold value of `y_pred_prob`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following code to plot the `ROC curve` using `matplotlib.pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14: ROC curve of the APS failure dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.14: ROC curve of the APS failure dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the AUC score using the `roc_auc_score` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The AUC score of `94.4479%` suggests that our model is excellent, as per the
    general acceptable `AUC score` shown above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2NUOgyh](https://packt.live/2NUOgyh).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2As33NH](https://packt.live/2As33NH).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7\. Computer Vision with Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 7.01: Amending Our Model with Multiple Layers and the Use of softmax'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try and improve the performance of our image classification algorithm.
    There are many ways to improve its performance, and one of the most straightforward
    ways is by adding multiple ANN layers to the model, which we will learn about
    in this activity. We will also change the activation from sigmoid to softmax.
    Then, we can compare the result with that of the previous exercise. Follow these
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary Keras libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, initiate the model with the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first layer of the CNN, set the input shape to `(64, 64, 3)`, the dimension
    of each image, and the activation function as a ReLU. Then, add `32` feature detectors
    of size `(3, 3)`. Add two additional convolutional layers with `32` feature detectors
    of size `(3, 3)`, also with `ReLU activation` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`32, (3, 3)` means that there are `32` feature detectors of size `3x3`. As
    a good practice, always start with `32`; you can add `64` or `128` later.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, add the pooling layer with an image size of `2x2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Flatten the output of the pooling layer by adding a flattening layer to the
    `CNN model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first dense layer of the ANN. Here, `128` is the output of the number
    of nodes. As a good practice, `128` is good to get started. `activation` is `relu`.
    As a good practice, the power of two is preferred:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add three more layers to the ANN of the same size, `128`, along with `ReLU
    activation` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the ANN. Replace the sigmoid function with `softmax`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `Adam optimizer` and compute the accuracy during
    the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create training and test data generators. Rescale the training and test images
    by `1/255` so that all the values are between `0` and `1`. Set these parameters
    for the training data generators only – `shear_range=0.2`, `zoom_range=0.2`, and
    `horizontal_flip=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training set from the `training set` folder. `''../dataset/training_set''`
    is the folder where our data has been placed. Our CNN model has an image size
    of `64x64`, so the same size should be passed here too. `batch_size` is the number
    of images in a single batch, which is `32`. `class_mode` is set to `binary` since
    we are working on binary classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 6* for the test by setting the folder to the location of the test
    images, that is, `''../dataset/test_set''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, fit the data. Set the `steps_per_epoch` to `10000` and the `validation_steps`
    to `2500`. The following step might take some time to execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the accuracy has decreased to `46.91%` due to the new softmax activation
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3gj0TiA](https://packt.live/3gj0TiA).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2VIDj7e](https://packt.live/2VIDj7e).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 7.02: Classifying a New Image'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will try to classify another new image, just like we
    did in the preceding exercise. The image hasn''t been exposed to the algorithm,
    so we will use this activity to test our algorithm. You can run any of the algorithms
    in this chapter (although the one that gets the highest accuracy is preferred)
    and then use the model to classify your images. Follow these steps to complete
    this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Run one of the algorithms from this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the image and process it. `''test_image_2.jpg''` is the path of the test
    image. Change the path in the code where you have saved the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can view the class labels using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Process the image by converting it into a `numpy` array using the `img_to_array`
    function. Then, add an additional dimension along the 0th axis using numpy''s
    `expand_dims` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the new image by calling the `predict` method of the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `class_indices` method with an `if…else` statement to map the 0 or
    1 output of the prediction to a class label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`test_image_2` is an image of a flower and was predicted to be a flower.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38ny95E](https://packt.live/38ny95E).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2VIM4Ow](https://packt.live/2VIM4Ow).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8\. Transfer Learning and Pre-Trained Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 8.01: Using the VGG16 Network to Train a Deep Learning Network to
    Identify Images'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `VGG16` network to predict the image given (`test_image_1`). Before
    you start, ensure that you have downloaded the image (`test_image_1`) to your
    working directory. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary `Keras` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the model (note that, at this point, you can also view the architecture
    of the network, as shown in the following code):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`classifier.summary()` shows us the architecture of the network. The following
    points should be noted: it has a four-dimensional input shape (`None, 224, 224,
    3`) and it has three convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The last four layers of the output are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.16: The architecture of the network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.16: The architecture of the network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the image. `''../Data/Prediction/test_image_1.jpg''` is the path of the
    image on our system. It will be different on your system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17: The sample motorbike image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.17: The sample motorbike image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The target size should be `224x 224` since `VGG16` only accepts (`224,224`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the image into an array by using the `img_to_array` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The image should be in a four-dimensional form for `VGG16` to allow further
    processing. Expand the dimension of the image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Preprocess the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.18: Image preprocessing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.18: Image preprocessing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the `predictor` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.19: Creating the predictor variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.19: Creating the predictor variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the shape of the image. It should be (`1,1000`). It''s `1000` because,
    as we mentioned previously, the ImageNet database has `1000` categories of images.
    The predictor variable shows the probabilities of our image being one of those images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the top five probabilities of what our image is using the `decode_predictions`
    function and pass the function of the predictor variable, `y_pred`, and the number
    of predictions and corresponding labels to output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first column of the array is an internal code number. The second is the
    label, while the third is the probability of the image being the label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transform the predictions into a human-readable format. We need to extract
    the most probable label from the output, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we have an `84.33%` probability that the picture is of
    a moped, which is close enough to a motorbike and probably represents the fact
    that motorbikes in the ImageNet dataset were labeled as mopeds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2C4nqRo](https://packt.live/2C4nqRo).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31JMPL4](https://packt.live/31JMPL4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 8.02: Image Classification with ResNet'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will use another pre-trained network, known as `ResNet`.
    We have an image of television located at `../Data/Prediction/test_image_4`. We
    will use the `ResNet50` network to predict the image. Follow these steps to complete
    this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary `Keras` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the ResNet50 model and print a summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`classifier.summary()` shows us the architecture of the network. The following
    points should be noted:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.20: The last four layers of the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.20: The last four layers of the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The last layer predictions (`Dense`) have `1000` values. This means that `VGG16`
    has a total of `1000` labels and that our image will be one of those `1000` labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the image. `''../Data/Prediction/test_image_4.jpg''` is the path of the
    image on our system. It will be different on your system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21: A sample image of a television'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_08_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.21: A sample image of a television'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The target size should be `224x224` since `ResNet50` only accepts (`224,224`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the image into an array by using the `img_to_array` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The image has to be in a four-dimensional form for `ResNet50` to allow further
    processing. Expand the dimensions of the image along the 0th axis using the `expand_dims`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Preprocess the image using the `preprocess_input` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the predictor variable by using the classifier to predict the image
    using it''s `predict` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the shape of the image. It should be (`1,1000`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the top five probabilities of what our image is using the `decode_predictions`
    function and by passing the predictor variable, `y_pred`, as the argument and
    the top number of predictions and corresponding labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first column of the array is an internal code number. The second is the
    label, while the third is the probability of the image matching the label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Put the predictions in a human-readable format. Print the most probable label
    from the output from the result of the `decode_predictions` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38rEe0M](https://packt.live/38rEe0M).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YV5xxo](https://packt.live/2YV5xxo).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9\. Sequential Modeling with Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 9.01: Predicting the Trend of Amazon''s Stock Price Using an LSTM
    with 50 Units (Neurons)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will examine the stock price of Amazon for the last 5
    years—from January 1, 2014, to December 31, 2018\. In doing so, we will try to
    predict and forecast the company''s future trend for January 2019 using an `RNN`
    and `LSTM`. We have the actual values for January 2019, so we can compare our
    predictions to the actual values later. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the dataset using the pandas `read_csv` function and look at the first
    five rows of the dataset using the `head` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.24: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_09_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.24: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are going to make our prediction using the `Open` stock price; therefore,
    select the `Open` stock price column from the dataset and print the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, perform feature scaling by normalizing the data using `MinMaxScaler`
    and setting the range of the features so that they have a minimum value of zero
    and a maximum value of one. Use the `fit_transform` method of the scaler on the
    training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the data to get `60` timestamps from the current instance. We chose
    `60` here as it will give us a sufficient number of previous instances in order
    to understand the trend; technically, this can be any number, but `60` is the
    optimal value. Additionally, the upper bound value here is `1258`, which is the
    index or count of rows (or records) in the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the data to add an extra dimension to the end of `X_train` using NumPy''s
    `reshape` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the following libraries to build the RNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed and initiate the sequential model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an `LSTM` layer to the network with `50` units, set the `return_sequences`
    argument to `True`, and set the `input_shape` argument to `(X_train.shape[1],
    1)`. Add three additional `LSTM` layers, each with `50` units, and set the `return_sequences`
    argument to `True` for the first two. Add a final output layer of size 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `adam` optimizer and use `Mean Squared Error` for
    the loss. Fit the model to the training data for `100` epochs with a batch size
    of `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load and process the test data (which is treated as actual data here) and select
    the column representing the value of `Open` stock data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the data since we will need `60` previous instances to get the
    stock price for each day. Therefore, we will need both the training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape and scale the input to prepare the test data. Note that we are predicting
    the January monthly trend, which has `21` financial days, so in order to prepare
    the test set, we take the lower bound value as `60` and the upper bound value
    as `81`. This ensures that the difference of `21` is maintained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the results by plotting the actual stock price and plotting the predicted
    stock price:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that your results may differ slightly from the actual stock price
    of Amazon.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.25: Real versus predicted stock prices'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_09_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.25: Real versus predicted stock prices'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding plot, the trends of the predicted and real prices
    are pretty much the same; the line has the same peaks and troughs. This is possible
    because of LSTM's ability to remember sequenced data. A traditional feedforward
    neural network would not have been able to forecast this result. This is the true
    power of `LSTM` and `RNNs`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3goQO3I](https://packt.live/3goQO3I).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2VIMq7O](https://packt.live/2VIMq7O).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.02: Predicting Amazon''s Stock Price with Added Regularization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will examine the stock price of Amazon over the last 5
    years, from January 1, 2014, to December 31, 2018\. In doing so, we will try to
    predict and forecast the company''s future trend for January 2019 using RNNs and
    an LSTM. We have the actual values for January 2019, so we will be able to compare
    our predictions with the actual values later. Initially, we predicted the trend
    of Amazon''s stock price using an LSTM with 50 units (or neurons). In this activity,
    we will also add dropout regularization and compare the results with *Activity
    9.01*, *Predicting the Trend of Amazon''s Stock Price Using an LSTM with 50 Units
    (Neurons)*. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the dataset using the pandas `read_csv` function and look at the first
    five rows of the dataset using the `head` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to make our prediction using the `Open` stock price; therefore,
    select the `Open` stock price column from the dataset and print the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, perform feature scaling by normalizing the data using `MinMaxScaler`
    and setting the range of the features so that they have a minimum value of `0`
    and a maximum value of one. Use the `fit_transform` method of the scaler on the
    training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the data to get `60` timestamps from the current instance. We chose
    `60` here as it will give us a sufficient number of previous instances in order
    to understand the trend; technically, this can be any number, but `60` is the
    optimal value. Additionally, the upper bound value here is `1258`, which is the
    index or count of rows (or records) in the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the data to add an extra dimension to the end of `X_train` using NumPy''s
    `reshape` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the following Keras libraries to build the RNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed and initiate the sequential model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an LSTM layer to the network with 50 units, set the `return_sequences`
    argument to `True`, and set the `input_shape` argument to `(X_train.shape[1],
    1)`. Add dropout to the model with `rate=0.2`. Add three additional LSTM layers,
    each with `50` units, and set the `return_sequences` argument to `True` for the
    first two. After each `LSTM` layer, add a dropout with `rate=0.2`. Add a final
    output layer of size `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `adam` optimizer and use `Mean Squared Error` for
    the loss. Fit the model to the training data for `100` epochs with a batch size
    of `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load and process the test data (which is treated as actual data here) and select
    the column representing the value of `Open` stock data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the data since we will need `60` previous instances to get the
    stock price for each day. Therefore, we will need both the training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape and scale the input to prepare the test data. Note that we are predicting
    the January monthly trend, which has `21` financial days, so in order to prepare
    the test set, we take the lower bound value as `60` and the upper bound value
    as `81`. This ensures that the difference of `21` is maintained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the results by plotting the actual stock price and plotting the predicted
    stock price:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that your results may differ slightly to the actual stock price.
  prefs: []
  type: TYPE_NORMAL
- en: '**Expected output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26: Real versus predicted stock prices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_09_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.26: Real versus predicted stock prices'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, the first plot displays the predicted output of the
    model with regularization from Activity 9.02, and the second displays the predicted
    output without regularization from Activity 9.01\. As you can see, adding dropout
    regularization does not fit the data as accurately. So, in this case, it is better
    not to use regularization, or to use dropout regularization with a lower dropout
    rate :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27: Comparing the results of Activity 9.01 and Activity 9.02'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_09_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.27: Comparing the results of Activity 9.01 and Activity 9.02'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YTpxR7](https://packt.live/2YTpxR7).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dY5Bku](https://packt.live/3dY5Bku).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.03: Predicting the Trend of Amazon''s Stock Price Using an LSTM
    with an Increasing Number of LSTM Neurons (100 Units)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will examine the stock price of Amazon over the last 5
    years, from January 1, 2014, to December 31, 2018\. We will try to predict and
    forecast the company''s future trend for January 2019 using `RNNs` with four `LSTM`
    layers, each with `100` units. We have the actual values for January 2019, so
    we will be able to compare our predictions with the actual values later. You can
    also compare the output difference with *Activity 9.01*, *Predicting the Trend
    of Amazon''s Stock Price Using an LSTM with 50 Units (Neurons)*. Follow these
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the dataset using the pandas `read_csv` function and look at the first
    five rows of the dataset using the `head` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to make our prediction using the `Open` stock price; therefore,
    select the `Open` stock price column from the dataset and print the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, perform feature scaling by normalizing the data using `MinMaxScaler`
    and setting the range of the features so that they have a minimum value of zero
    and a maximum value of one. Use the `fit_transform` method of the scaler on the
    training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the data to get `60` timestamps from the current instance. We chose
    `60` here as it will give us a sufficient number of previous instances in order
    to understand the trend; technically, this can be any number, but `60` is the
    optimal value. Additionally, the upper bound value here is `1258`, which is the
    index or count of rows (or records) in the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the data to add an extra dimension to the end of `X_train` using NumPy''s
    `reshape` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the following Keras libraries to build the RNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed and initiate the sequential model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an LSTM layer to the network with `100` units, set the `return_sequences`
    argument to `True`, and set the `input_shape` argument to `(X_train.shape[1],
    1)`. Add three additional `LSTM` layers, each with `100` units, and set the `return_sequences`
    argument to `True` for the first two. Add a final output layer of size `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `adam` optimizer and use `Mean Squared Error` for
    the loss. Fit the model to the training data for `100` epochs with a batch size
    of `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load and process the test data (which is treated as actual data here) and select
    the column representing the value of open stock data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the data since we will need `60` previous instances to get the
    stock price for each day. Therefore, we will need both the training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape and scale the input to prepare the test data. Note that we are predicting
    the January monthly trend, which has `21` financial days, so in order to prepare
    the test set, we take the lower bound value as `60` and the upper bound value
    as `81`. This ensures that the difference of `21` is maintained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the results by plotting the actual stock price and plotting the predicted
    stock price:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that your results may differ slightly from the actual stock price.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.28: Real versus predicted stock prices'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_09_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.28: Real versus predicted stock prices'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we compare the results of the `LSTM` with `50` units (from *Activity
    9.01*, *Predicting the Trend of Amazon''s Stock Price Using an LSTM with 50 Units
    (Neurons)*) and the `LSTM` with `100` units in this activity, we get trends with
    `100` units. Also, note that when we run the `LSTM` with `100` units, it takes
    more computational time than the `LSTM` with `50` units. A trade-off needs to
    be considered in such cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29: Comparing the real versus predicted stock price with 50 and
    100 units'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_09_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.29: Comparing the real versus predicted stock price with 50 and 100
    units'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31NQkQy](https://packt.live/31NQkQy).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZCZ4GR](https://packt.live/2ZCZ4GR).
  prefs: []
  type: TYPE_NORMAL
